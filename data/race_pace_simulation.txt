Looking in indexes: https://pypi.org/simple, http://100.95.241.19
Requirement already satisfied: SekitobaLibrary in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (1.2.111)
Requirement already satisfied: requests in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (2.28.1)
Requirement already satisfied: pandas in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (1.5.2)
Requirement already satisfied: lightgbm in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (3.3.3)
Requirement already satisfied: numpy in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (1.24.1)
Requirement already satisfied: matplotlib in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (3.6.2)
Requirement already satisfied: tqdm in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (4.64.1)
Requirement already satisfied: statistics in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (1.0.3.5)
Requirement already satisfied: boto3 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (1.26.42)
Requirement already satisfied: torch in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (1.13.1)
Requirement already satisfied: mpi4py in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (3.1.4)
Requirement already satisfied: trueskill in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (0.4.5)
Requirement already satisfied: bs4 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (0.0.1)
Requirement already satisfied: jpholiday in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (0.1.8)
Requirement already satisfied: botocore<1.30.0,>=1.29.42 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from boto3->SekitobaLibrary) (1.29.42)
Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from boto3->SekitobaLibrary) (1.0.1)
Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from boto3->SekitobaLibrary) (0.6.0)
Requirement already satisfied: beautifulsoup4 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from bs4->SekitobaLibrary) (4.11.1)
Requirement already satisfied: wheel in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from lightgbm->SekitobaLibrary) (0.38.4)
Requirement already satisfied: scipy in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from lightgbm->SekitobaLibrary) (1.9.3)
Requirement already satisfied: scikit-learn!=0.22.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from lightgbm->SekitobaLibrary) (1.2.0)
Requirement already satisfied: contourpy>=1.0.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (1.0.6)
Requirement already satisfied: cycler>=0.10 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (4.38.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (1.4.4)
Requirement already satisfied: packaging>=20.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (22.0)
Requirement already satisfied: pillow>=6.2.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (9.3.0)
Requirement already satisfied: pyparsing>=2.2.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (3.0.9)
Requirement already satisfied: python-dateutil>=2.7 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from pandas->SekitobaLibrary) (2022.7)
Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from requests->SekitobaLibrary) (2.1.1)
Requirement already satisfied: idna<4,>=2.5 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from requests->SekitobaLibrary) (3.4)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from requests->SekitobaLibrary) (1.26.13)
Requirement already satisfied: certifi>=2017.4.17 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from requests->SekitobaLibrary) (2022.12.7)
Requirement already satisfied: docutils>=0.3 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from statistics->SekitobaLibrary) (0.19)
Requirement already satisfied: typing-extensions in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from torch->SekitobaLibrary) (4.4.0)
Requirement already satisfied: six in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from trueskill->SekitobaLibrary) (1.16.0)
Requirement already satisfied: joblib>=1.1.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm->SekitobaLibrary) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm->SekitobaLibrary) (3.1.0)
Requirement already satisfied: soupsieve>1.2 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from beautifulsoup4->bs4->SekitobaLibrary) (2.3.2.post1)
wrap_data.pickle download finish Gilgamesh
race_cource_info.pickle download finish Gilgamesh
race_pace_analyze_data.pickle download finish Gilgamesh
start rank:4
start rank:5
start rank:2
start rank:1
start rank:3
























1-instance.pickle download finish Gilgamesh
2-instance.pickle download finish Gilgamesh
3-instance.pickle download finish Gilgamesh
4-instance.pickle download finish Gilgamesh
5-instance.pickle download finish Gilgamesh
pace_learn_data.pickle download finish Gilgamesh
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.247132 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59029	valid_1's l2: 2.64309
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.54541	valid_1's l2: 2.59829
[3]	training's l2: 2.50248	valid_1's l2: 2.5575
[4]	training's l2: 2.46485	valid_1's l2: 2.52134
[5]	training's l2: 2.42831	valid_1's l2: 2.48753
[6]	training's l2: 2.39633	valid_1's l2: 2.45845
[7]	training's l2: 2.36527	valid_1's l2: 2.42958
[8]	training's l2: 2.33808	valid_1's l2: 2.40442
[9]	training's l2: 2.31047	valid_1's l2: 2.37853
[10]	training's l2: 2.28537	valid_1's l2: 2.35714
[11]	training's l2: 2.26345	valid_1's l2: 2.33788
[12]	training's l2: 2.24149	valid_1's l2: 2.32029
[13]	training's l2: 2.22132	valid_1's l2: 2.30299
[14]	training's l2: 2.20216	valid_1's l2: 2.28863
[15]	training's l2: 2.18433	valid_1's l2: 2.27474
[16]	training's l2: 2.16675	valid_1's l2: 2.26146
[17]	training's l2: 2.15095	valid_1's l2: 2.24938
[18]	training's l2: 2.13569	valid_1's l2: 2.23798
[19]	training's l2: 2.12113	valid_1's l2: 2.22716
[20]	training's l2: 2.10758	valid_1's l2: 2.21707
[21]	training's l2: 2.09485	valid_1's l2: 2.20848
[22]	training's l2: 2.08298	valid_1's l2: 2.19972
[23]	training's l2: 2.07108	valid_1's l2: 2.19234
[24]	training's l2: 2.05963	valid_1's l2: 2.18586
[25]	training's l2: 2.04832	valid_1's l2: 2.17938
[26]	training's l2: 2.0374	valid_1's l2: 2.17303
[27]	training's l2: 2.02729	valid_1's l2: 2.16724
[28]	training's l2: 2.01753	valid_1's l2: 2.16079
[29]	training's l2: 2.00733	valid_1's l2: 2.15525
[30]	training's l2: 1.99871	valid_1's l2: 2.15047
[31]	training's l2: 1.99019	valid_1's l2: 2.14543
[32]	training's l2: 1.98127	valid_1's l2: 2.14124
[33]	training's l2: 1.97213	valid_1's l2: 2.13666
[34]	training's l2: 1.96383	valid_1's l2: 2.13288
[35]	training's l2: 1.95513	valid_1's l2: 2.12783
[36]	training's l2: 1.94748	valid_1's l2: 2.12486
[37]	training's l2: 1.93989	valid_1's l2: 2.12189
[38]	training's l2: 1.9318	valid_1's l2: 2.11957
[39]	training's l2: 1.92452	valid_1's l2: 2.11631
[40]	training's l2: 1.91765	valid_1's l2: 2.11313
[41]	training's l2: 1.91069	valid_1's l2: 2.1109
[42]	training's l2: 1.90311	valid_1's l2: 2.10754
[43]	training's l2: 1.89571	valid_1's l2: 2.10292
[44]	training's l2: 1.88895	valid_1's l2: 2.10068
[45]	training's l2: 1.88274	valid_1's l2: 2.09916
[46]	training's l2: 1.87545	valid_1's l2: 2.09552
[47]	training's l2: 1.86841	valid_1's l2: 2.09338
[48]	training's l2: 1.86211	valid_1's l2: 2.09127
[49]	training's l2: 1.85567	valid_1's l2: 2.08923
[50]	training's l2: 1.84947	valid_1's l2: 2.08638
[51]	training's l2: 1.84406	valid_1's l2: 2.08504
[52]	training's l2: 1.83825	valid_1's l2: 2.08269
[53]	training's l2: 1.83222	valid_1's l2: 2.08046
[54]	training's l2: 1.82408	valid_1's l2: 2.07818
[55]	training's l2: 1.81852	valid_1's l2: 2.07607
[56]	training's l2: 1.8131	valid_1's l2: 2.07457
[57]	training's l2: 1.80611	valid_1's l2: 2.07203
[58]	training's l2: 1.80105	valid_1's l2: 2.07075
[59]	training's l2: 1.79555	valid_1's l2: 2.06951
[60]	training's l2: 1.7891	valid_1's l2: 2.06682
[61]	training's l2: 1.78444	valid_1's l2: 2.06577
[62]	training's l2: 1.77841	valid_1's l2: 2.06521
[63]	training's l2: 1.77237	valid_1's l2: 2.06326
[64]	training's l2: 1.76659	valid_1's l2: 2.06092
[65]	training's l2: 1.76146	valid_1's l2: 2.05966
[66]	training's l2: 1.75625	valid_1's l2: 2.05922
[67]	training's l2: 1.75137	valid_1's l2: 2.05865
[68]	training's l2: 1.74626	valid_1's l2: 2.05833
[69]	training's l2: 1.74022	valid_1's l2: 2.05646
[70]	training's l2: 1.73532	valid_1's l2: 2.05459
[71]	training's l2: 1.73078	valid_1's l2: 2.05505
[72]	training's l2: 1.72644	valid_1's l2: 2.05509
[73]	training's l2: 1.72075	valid_1's l2: 2.05354
[74]	training's l2: 1.71654	valid_1's l2: 2.05324
[75]	training's l2: 1.7123	valid_1's l2: 2.05265
[76]	training's l2: 1.70771	valid_1's l2: 2.05236
[77]	training's l2: 1.70258	valid_1's l2: 2.05105
[78]	training's l2: 1.69824	valid_1's l2: 2.05024
[79]	training's l2: 1.69325	valid_1's l2: 2.05015
[80]	training's l2: 1.68864	valid_1's l2: 2.04896
[81]	training's l2: 1.68374	valid_1's l2: 2.0477
[82]	training's l2: 1.6799	valid_1's l2: 2.04705
[83]	training's l2: 1.6749	valid_1's l2: 2.04633
[84]	training's l2: 1.67094	valid_1's l2: 2.04548
[85]	training's l2: 1.6672	valid_1's l2: 2.04552
[86]	training's l2: 1.66296	valid_1's l2: 2.04526
[87]	training's l2: 1.65803	valid_1's l2: 2.04445
[88]	training's l2: 1.65383	valid_1's l2: 2.04529
[89]	training's l2: 1.64942	valid_1's l2: 2.043
[90]	training's l2: 1.64593	valid_1's l2: 2.04263
[91]	training's l2: 1.64239	valid_1's l2: 2.04281
[92]	training's l2: 1.63876	valid_1's l2: 2.0427
[93]	training's l2: 1.63481	valid_1's l2: 2.04299
[94]	training's l2: 1.63079	valid_1's l2: 2.04236
[95]	training's l2: 1.62688	valid_1's l2: 2.04147
[96]	training's l2: 1.62296	valid_1's l2: 2.04115
[97]	training's l2: 1.61961	valid_1's l2: 2.04133
[98]	training's l2: 1.61621	valid_1's l2: 2.04097
[99]	training's l2: 1.61275	valid_1's l2: 2.04128
[100]	training's l2: 1.60864	valid_1's l2: 2.04159
[101]	training's l2: 1.60463	valid_1's l2: 2.04066
[102]	training's l2: 1.60101	valid_1's l2: 2.04016
[103]	training's l2: 1.59758	valid_1's l2: 2.03985
[104]	training's l2: 1.59438	valid_1's l2: 2.03996
[105]	training's l2: 1.59068	valid_1's l2: 2.03973
[106]	training's l2: 1.58729	valid_1's l2: 2.03903
[107]	training's l2: 1.58357	valid_1's l2: 2.03924
[108]	training's l2: 1.57904	valid_1's l2: 2.03771
[109]	training's l2: 1.57567	valid_1's l2: 2.03793
[110]	training's l2: 1.57205	valid_1's l2: 2.03777
[111]	training's l2: 1.56837	valid_1's l2: 2.03774
[112]	training's l2: 1.56535	valid_1's l2: 2.03815
[113]	training's l2: 1.56182	valid_1's l2: 2.03888
[114]	training's l2: 1.55836	valid_1's l2: 2.03801
[115]	training's l2: 1.55518	valid_1's l2: 2.03772
[116]	training's l2: 1.55191	valid_1's l2: 2.03678
[117]	training's l2: 1.5485	valid_1's l2: 2.03822
[118]	training's l2: 1.54483	valid_1's l2: 2.03852
[119]	training's l2: 1.54129	valid_1's l2: 2.03901
[120]	training's l2: 1.53811	valid_1's l2: 2.03895
[121]	training's l2: 1.53514	valid_1's l2: 2.03937
[122]	training's l2: 1.53239	valid_1's l2: 2.03942
[123]	training's l2: 1.52897	valid_1's l2: 2.0386
[124]	training's l2: 1.52586	valid_1's l2: 2.03878
[125]	training's l2: 1.52272	valid_1's l2: 2.03969
[126]	training's l2: 1.51967	valid_1's l2: 2.03927
[127]	training's l2: 1.51632	valid_1's l2: 2.03917
[128]	training's l2: 1.51348	valid_1's l2: 2.03968
[129]	training's l2: 1.51033	valid_1's l2: 2.03895
[130]	training's l2: 1.50773	valid_1's l2: 2.0397
[131]	training's l2: 1.50478	valid_1's l2: 2.03955
[132]	training's l2: 1.50141	valid_1's l2: 2.03937
[133]	training's l2: 1.49803	valid_1's l2: 2.03966
[134]	training's l2: 1.49543	valid_1's l2: 2.0395
[135]	training's l2: 1.49264	valid_1's l2: 2.03967
[136]	training's l2: 1.48973	valid_1's l2: 2.04023
[137]	training's l2: 1.48637	valid_1's l2: 2.03981
[138]	training's l2: 1.48347	valid_1's l2: 2.03992
[139]	training's l2: 1.4808	valid_1's l2: 2.03934
[140]	training's l2: 1.47769	valid_1's l2: 2.0388
[141]	training's l2: 1.47522	valid_1's l2: 2.03885
[142]	training's l2: 1.47266	valid_1's l2: 2.03847
[143]	training's l2: 1.46991	valid_1's l2: 2.03796
[144]	training's l2: 1.46709	valid_1's l2: 2.03777
[145]	training's l2: 1.46403	valid_1's l2: 2.03743
[146]	training's l2: 1.46003	valid_1's l2: 2.03643
[147]	training's l2: 1.45731	valid_1's l2: 2.03633
[148]	training's l2: 1.45454	valid_1's l2: 2.03674
[149]	training's l2: 1.452	valid_1's l2: 2.03664
[150]	training's l2: 1.44919	valid_1's l2: 2.03655
[151]	training's l2: 1.44647	valid_1's l2: 2.03602
[152]	training's l2: 1.44353	valid_1's l2: 2.03575
[153]	training's l2: 1.44084	valid_1's l2: 2.03548
[154]	training's l2: 1.43784	valid_1's l2: 2.03519
[155]	training's l2: 1.43485	valid_1's l2: 2.03482
[156]	training's l2: 1.4325	valid_1's l2: 2.03492
[157]	training's l2: 1.43001	valid_1's l2: 2.03469
[158]	training's l2: 1.42752	valid_1's l2: 2.03455
[159]	training's l2: 1.4247	valid_1's l2: 2.03451
[160]	training's l2: 1.42207	valid_1's l2: 2.03414
[161]	training's l2: 1.41997	valid_1's l2: 2.03434
[162]	training's l2: 1.4173	valid_1's l2: 2.03467
[163]	training's l2: 1.415	valid_1's l2: 2.03518
[164]	training's l2: 1.41201	valid_1's l2: 2.03556
[165]	training's l2: 1.40893	valid_1's l2: 2.03456
[166]	training's l2: 1.40596	valid_1's l2: 2.03465
[167]	training's l2: 1.40357	valid_1's l2: 2.03459
[168]	training's l2: 1.40106	valid_1's l2: 2.03498
[169]	training's l2: 1.39875	valid_1's l2: 2.03442
[170]	training's l2: 1.3962	valid_1's l2: 2.03451
[171]	training's l2: 1.39389	valid_1's l2: 2.03455
[172]	training's l2: 1.39138	valid_1's l2: 2.03458
[173]	training's l2: 1.38885	valid_1's l2: 2.03513
[174]	training's l2: 1.3864	valid_1's l2: 2.03506
[175]	training's l2: 1.3841	valid_1's l2: 2.03542
[176]	training's l2: 1.38133	valid_1's l2: 2.03506
[177]	training's l2: 1.37907	valid_1's l2: 2.0347
[178]	training's l2: 1.37705	valid_1's l2: 2.03619
[179]	training's l2: 1.37497	valid_1's l2: 2.03549
[180]	training's l2: 1.37159	valid_1's l2: 2.03599
[181]	training's l2: 1.36901	valid_1's l2: 2.0351
[182]	training's l2: 1.36681	valid_1's l2: 2.03506
[183]	training's l2: 1.36478	valid_1's l2: 2.03506
[184]	training's l2: 1.36195	valid_1's l2: 2.03444
[185]	training's l2: 1.35933	valid_1's l2: 2.03415
[186]	training's l2: 1.35681	valid_1's l2: 2.03438
[187]	training's l2: 1.35476	valid_1's l2: 2.0351
[188]	training's l2: 1.35247	valid_1's l2: 2.0352
[189]	training's l2: 1.35018	valid_1's l2: 2.03517
[190]	training's l2: 1.34754	valid_1's l2: 2.03585
Early stopping, best iteration is:
[160]	training's l2: 1.42207	valid_1's l2: 2.03414
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.193025 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59491	valid_1's l2: 2.6489
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.55383	valid_1's l2: 2.60925
[3]	training's l2: 2.51425	valid_1's l2: 2.57128
[4]	training's l2: 2.47875	valid_1's l2: 2.53808
[5]	training's l2: 2.44452	valid_1's l2: 2.50667
[6]	training's l2: 2.41387	valid_1's l2: 2.47813
[7]	training's l2: 2.38329	valid_1's l2: 2.45124
[8]	training's l2: 2.35602	valid_1's l2: 2.42762
[9]	training's l2: 2.32939	valid_1's l2: 2.40486
[10]	training's l2: 2.30605	valid_1's l2: 2.38301
[11]	training's l2: 2.28198	valid_1's l2: 2.36197
[12]	training's l2: 2.26076	valid_1's l2: 2.3451
[13]	training's l2: 2.23959	valid_1's l2: 2.32755
[14]	training's l2: 2.22	valid_1's l2: 2.31149
[15]	training's l2: 2.201	valid_1's l2: 2.29689
[16]	training's l2: 2.18444	valid_1's l2: 2.28449
[17]	training's l2: 2.1679	valid_1's l2: 2.27266
[18]	training's l2: 2.15176	valid_1's l2: 2.26116
[19]	training's l2: 2.13654	valid_1's l2: 2.24982
[20]	training's l2: 2.12156	valid_1's l2: 2.23938
[21]	training's l2: 2.1079	valid_1's l2: 2.22951
[22]	training's l2: 2.09484	valid_1's l2: 2.2205
[23]	training's l2: 2.08198	valid_1's l2: 2.2115
[24]	training's l2: 2.06932	valid_1's l2: 2.2031
[25]	training's l2: 2.05794	valid_1's l2: 2.19564
[26]	training's l2: 2.04662	valid_1's l2: 2.1899
[27]	training's l2: 2.03562	valid_1's l2: 2.18305
[28]	training's l2: 2.02516	valid_1's l2: 2.1767
[29]	training's l2: 2.01527	valid_1's l2: 2.1712
[30]	training's l2: 2.00529	valid_1's l2: 2.16547
[31]	training's l2: 1.99602	valid_1's l2: 2.15982
[32]	training's l2: 1.98708	valid_1's l2: 2.15434
[33]	training's l2: 1.97833	valid_1's l2: 2.14899
[34]	training's l2: 1.96953	valid_1's l2: 2.14477
[35]	training's l2: 1.96156	valid_1's l2: 2.14174
[36]	training's l2: 1.95363	valid_1's l2: 2.13789
[37]	training's l2: 1.94571	valid_1's l2: 2.13412
[38]	training's l2: 1.93735	valid_1's l2: 2.12979
[39]	training's l2: 1.92945	valid_1's l2: 2.12616
[40]	training's l2: 1.92223	valid_1's l2: 2.12372
[41]	training's l2: 1.91487	valid_1's l2: 2.12053
[42]	training's l2: 1.90738	valid_1's l2: 2.11845
[43]	training's l2: 1.90012	valid_1's l2: 2.11595
[44]	training's l2: 1.89279	valid_1's l2: 2.11407
[45]	training's l2: 1.88476	valid_1's l2: 2.11038
[46]	training's l2: 1.87796	valid_1's l2: 2.10782
[47]	training's l2: 1.87075	valid_1's l2: 2.10524
[48]	training's l2: 1.86417	valid_1's l2: 2.10404
[49]	training's l2: 1.85765	valid_1's l2: 2.10163
[50]	training's l2: 1.85094	valid_1's l2: 2.09912
[51]	training's l2: 1.84505	valid_1's l2: 2.09741
[52]	training's l2: 1.83808	valid_1's l2: 2.09509
[53]	training's l2: 1.8323	valid_1's l2: 2.09535
[54]	training's l2: 1.82566	valid_1's l2: 2.09254
[55]	training's l2: 1.82016	valid_1's l2: 2.09002
[56]	training's l2: 1.81415	valid_1's l2: 2.08799
[57]	training's l2: 1.8082	valid_1's l2: 2.08541
[58]	training's l2: 1.80223	valid_1's l2: 2.08346
[59]	training's l2: 1.79634	valid_1's l2: 2.08153
[60]	training's l2: 1.79055	valid_1's l2: 2.08005
[61]	training's l2: 1.78516	valid_1's l2: 2.07836
[62]	training's l2: 1.77814	valid_1's l2: 2.07616
[63]	training's l2: 1.7731	valid_1's l2: 2.07509
[64]	training's l2: 1.76803	valid_1's l2: 2.07431
[65]	training's l2: 1.76326	valid_1's l2: 2.07364
[66]	training's l2: 1.75692	valid_1's l2: 2.07214
[67]	training's l2: 1.75072	valid_1's l2: 2.07267
[68]	training's l2: 1.74622	valid_1's l2: 2.07213
[69]	training's l2: 1.73975	valid_1's l2: 2.0701
[70]	training's l2: 1.73463	valid_1's l2: 2.0681
[71]	training's l2: 1.72975	valid_1's l2: 2.0672
[72]	training's l2: 1.7255	valid_1's l2: 2.06729
[73]	training's l2: 1.719	valid_1's l2: 2.06448
[74]	training's l2: 1.71361	valid_1's l2: 2.06466
[75]	training's l2: 1.70863	valid_1's l2: 2.06462
[76]	training's l2: 1.70411	valid_1's l2: 2.06389
[77]	training's l2: 1.69872	valid_1's l2: 2.06336
[78]	training's l2: 1.69369	valid_1's l2: 2.06187
[79]	training's l2: 1.68959	valid_1's l2: 2.06132
[80]	training's l2: 1.68545	valid_1's l2: 2.0608
[81]	training's l2: 1.68074	valid_1's l2: 2.06033
[82]	training's l2: 1.67622	valid_1's l2: 2.05995
[83]	training's l2: 1.67049	valid_1's l2: 2.05901
[84]	training's l2: 1.66574	valid_1's l2: 2.05863
[85]	training's l2: 1.66146	valid_1's l2: 2.05822
[86]	training's l2: 1.65756	valid_1's l2: 2.05819
[87]	training's l2: 1.65336	valid_1's l2: 2.05775
[88]	training's l2: 1.64923	valid_1's l2: 2.05678
[89]	training's l2: 1.64427	valid_1's l2: 2.05537
[90]	training's l2: 1.63979	valid_1's l2: 2.05389
[91]	training's l2: 1.63475	valid_1's l2: 2.05208
[92]	training's l2: 1.63083	valid_1's l2: 2.05227
[93]	training's l2: 1.62687	valid_1's l2: 2.05199
[94]	training's l2: 1.62249	valid_1's l2: 2.05218
[95]	training's l2: 1.61853	valid_1's l2: 2.05222
[96]	training's l2: 1.61419	valid_1's l2: 2.04992
[97]	training's l2: 1.61021	valid_1's l2: 2.04946
[98]	training's l2: 1.60646	valid_1's l2: 2.04826
[99]	training's l2: 1.60254	valid_1's l2: 2.04848
[100]	training's l2: 1.59857	valid_1's l2: 2.04864
[101]	training's l2: 1.59384	valid_1's l2: 2.04802
[102]	training's l2: 1.59013	valid_1's l2: 2.04792
[103]	training's l2: 1.58635	valid_1's l2: 2.04762
[104]	training's l2: 1.58292	valid_1's l2: 2.04773
[105]	training's l2: 1.57909	valid_1's l2: 2.04854
[106]	training's l2: 1.57543	valid_1's l2: 2.0483
[107]	training's l2: 1.57128	valid_1's l2: 2.04776
[108]	training's l2: 1.56733	valid_1's l2: 2.04819
[109]	training's l2: 1.56349	valid_1's l2: 2.04694
[110]	training's l2: 1.55949	valid_1's l2: 2.04821
[111]	training's l2: 1.55566	valid_1's l2: 2.04748
[112]	training's l2: 1.55207	valid_1's l2: 2.04758
[113]	training's l2: 1.54889	valid_1's l2: 2.04719
[114]	training's l2: 1.54583	valid_1's l2: 2.04699
[115]	training's l2: 1.5424	valid_1's l2: 2.0473
[116]	training's l2: 1.53899	valid_1's l2: 2.04746
[117]	training's l2: 1.53542	valid_1's l2: 2.04705
[118]	training's l2: 1.53198	valid_1's l2: 2.04727
[119]	training's l2: 1.52873	valid_1's l2: 2.04747
[120]	training's l2: 1.5254	valid_1's l2: 2.04747
[121]	training's l2: 1.52178	valid_1's l2: 2.04793
[122]	training's l2: 1.5187	valid_1's l2: 2.04794
[123]	training's l2: 1.51535	valid_1's l2: 2.04768
[124]	training's l2: 1.51197	valid_1's l2: 2.04759
[125]	training's l2: 1.50829	valid_1's l2: 2.04631
[126]	training's l2: 1.50489	valid_1's l2: 2.04546
[127]	training's l2: 1.50172	valid_1's l2: 2.04527
[128]	training's l2: 1.49878	valid_1's l2: 2.04607
[129]	training's l2: 1.49553	valid_1's l2: 2.04613
[130]	training's l2: 1.49245	valid_1's l2: 2.04597
[131]	training's l2: 1.48922	valid_1's l2: 2.04589
[132]	training's l2: 1.48579	valid_1's l2: 2.04553
[133]	training's l2: 1.4821	valid_1's l2: 2.04521
[134]	training's l2: 1.47904	valid_1's l2: 2.04458
[135]	training's l2: 1.47586	valid_1's l2: 2.0453
[136]	training's l2: 1.47278	valid_1's l2: 2.04585
[137]	training's l2: 1.46828	valid_1's l2: 2.04556
[138]	training's l2: 1.46545	valid_1's l2: 2.04542
[139]	training's l2: 1.46244	valid_1's l2: 2.04507
[140]	training's l2: 1.45962	valid_1's l2: 2.04562
[141]	training's l2: 1.45685	valid_1's l2: 2.0458
[142]	training's l2: 1.4537	valid_1's l2: 2.04602
[143]	training's l2: 1.45042	valid_1's l2: 2.04552
[144]	training's l2: 1.44739	valid_1's l2: 2.04545
[145]	training's l2: 1.44439	valid_1's l2: 2.04578
[146]	training's l2: 1.44131	valid_1's l2: 2.04611
[147]	training's l2: 1.43849	valid_1's l2: 2.04676
[148]	training's l2: 1.43582	valid_1's l2: 2.04628
[149]	training's l2: 1.43274	valid_1's l2: 2.04661
[150]	training's l2: 1.42993	valid_1's l2: 2.04727
[151]	training's l2: 1.42685	valid_1's l2: 2.04672
[152]	training's l2: 1.42425	valid_1's l2: 2.04699
[153]	training's l2: 1.42119	valid_1's l2: 2.04646
[154]	training's l2: 1.41857	valid_1's l2: 2.04656
[155]	training's l2: 1.41567	valid_1's l2: 2.04571
[156]	training's l2: 1.41248	valid_1's l2: 2.04623
[157]	training's l2: 1.40987	valid_1's l2: 2.04636
[158]	training's l2: 1.40679	valid_1's l2: 2.04596
[159]	training's l2: 1.40413	valid_1's l2: 2.04549
[160]	training's l2: 1.40077	valid_1's l2: 2.04435
[161]	training's l2: 1.39791	valid_1's l2: 2.04416
[162]	training's l2: 1.39433	valid_1's l2: 2.04397
[163]	training's l2: 1.39168	valid_1's l2: 2.04388
[164]	training's l2: 1.3888	valid_1's l2: 2.04409
[165]	training's l2: 1.38628	valid_1's l2: 2.04391
[166]	training's l2: 1.38362	valid_1's l2: 2.04452
[167]	training's l2: 1.38104	valid_1's l2: 2.04489
[168]	training's l2: 1.37856	valid_1's l2: 2.04507
[169]	training's l2: 1.37589	valid_1's l2: 2.04546
[170]	training's l2: 1.37335	valid_1's l2: 2.04537
[171]	training's l2: 1.37045	valid_1's l2: 2.04502
[172]	training's l2: 1.3679	valid_1's l2: 2.04496
[173]	training's l2: 1.36548	valid_1's l2: 2.04414
[174]	training's l2: 1.36308	valid_1's l2: 2.04425
[175]	training's l2: 1.36068	valid_1's l2: 2.04431
[176]	training's l2: 1.35797	valid_1's l2: 2.04487
[177]	training's l2: 1.35505	valid_1's l2: 2.0452
[178]	training's l2: 1.35256	valid_1's l2: 2.04472
[179]	training's l2: 1.35016	valid_1's l2: 2.04525
[180]	training's l2: 1.34796	valid_1's l2: 2.04537
[181]	training's l2: 1.34551	valid_1's l2: 2.0454
[182]	training's l2: 1.34327	valid_1's l2: 2.0454
[183]	training's l2: 1.34064	valid_1's l2: 2.04479
[184]	training's l2: 1.3379	valid_1's l2: 2.04503
[185]	training's l2: 1.33496	valid_1's l2: 2.04471
[186]	training's l2: 1.33273	valid_1's l2: 2.04523
[187]	training's l2: 1.3304	valid_1's l2: 2.04541
[188]	training's l2: 1.32811	valid_1's l2: 2.04566
[189]	training's l2: 1.32573	valid_1's l2: 2.04581
[190]	training's l2: 1.3236	valid_1's l2: 2.04658
[191]	training's l2: 1.32129	valid_1's l2: 2.04662
[192]	training's l2: 1.31891	valid_1's l2: 2.04665
[193]	training's l2: 1.31643	valid_1's l2: 2.04701
Early stopping, best iteration is:
[163]	training's l2: 1.39168	valid_1's l2: 2.04388
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216080 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.5958	valid_1's l2: 2.64998
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.55534	valid_1's l2: 2.61129
[3]	training's l2: 2.51627	valid_1's l2: 2.57451
[4]	training's l2: 2.48117	valid_1's l2: 2.54188
[5]	training's l2: 2.44747	valid_1's l2: 2.51104
[6]	training's l2: 2.4171	valid_1's l2: 2.48361
[7]	training's l2: 2.38691	valid_1's l2: 2.45636
[8]	training's l2: 2.35973	valid_1's l2: 2.43317
[9]	training's l2: 2.33309	valid_1's l2: 2.4092
[10]	training's l2: 2.30958	valid_1's l2: 2.38972
[11]	training's l2: 2.28557	valid_1's l2: 2.36904
[12]	training's l2: 2.26467	valid_1's l2: 2.35111
[13]	training's l2: 2.24393	valid_1's l2: 2.33378
[14]	training's l2: 2.22407	valid_1's l2: 2.31799
[15]	training's l2: 2.20637	valid_1's l2: 2.30249
[16]	training's l2: 2.18844	valid_1's l2: 2.28841
[17]	training's l2: 2.17176	valid_1's l2: 2.27625
[18]	training's l2: 2.15511	valid_1's l2: 2.26418
[19]	training's l2: 2.13942	valid_1's l2: 2.25295
[20]	training's l2: 2.12544	valid_1's l2: 2.24361
[21]	training's l2: 2.11113	valid_1's l2: 2.23375
[22]	training's l2: 2.09764	valid_1's l2: 2.22458
[23]	training's l2: 2.08495	valid_1's l2: 2.21636
[24]	training's l2: 2.07246	valid_1's l2: 2.20761
[25]	training's l2: 2.06055	valid_1's l2: 2.20027
[26]	training's l2: 2.04941	valid_1's l2: 2.19202
[27]	training's l2: 2.03899	valid_1's l2: 2.18542
[28]	training's l2: 2.0282	valid_1's l2: 2.18003
[29]	training's l2: 2.01789	valid_1's l2: 2.1734
[30]	training's l2: 2.00781	valid_1's l2: 2.16777
[31]	training's l2: 1.99782	valid_1's l2: 2.16223
[32]	training's l2: 1.98839	valid_1's l2: 2.15681
[33]	training's l2: 1.97881	valid_1's l2: 2.15105
[34]	training's l2: 1.97034	valid_1's l2: 2.14749
[35]	training's l2: 1.9615	valid_1's l2: 2.14339
[36]	training's l2: 1.9529	valid_1's l2: 2.13808
[37]	training's l2: 1.94523	valid_1's l2: 2.13463
[38]	training's l2: 1.9363	valid_1's l2: 2.13097
[39]	training's l2: 1.9284	valid_1's l2: 2.12757
[40]	training's l2: 1.92098	valid_1's l2: 2.12496
[41]	training's l2: 1.91355	valid_1's l2: 2.12149
[42]	training's l2: 1.90577	valid_1's l2: 2.11853
[43]	training's l2: 1.89773	valid_1's l2: 2.11556
[44]	training's l2: 1.89064	valid_1's l2: 2.11328
[45]	training's l2: 1.88357	valid_1's l2: 2.11125
[46]	training's l2: 1.87558	valid_1's l2: 2.10764
[47]	training's l2: 1.8688	valid_1's l2: 2.10624
[48]	training's l2: 1.86231	valid_1's l2: 2.10576
[49]	training's l2: 1.85546	valid_1's l2: 2.10313
[50]	training's l2: 1.84884	valid_1's l2: 2.10208
[51]	training's l2: 1.84253	valid_1's l2: 2.10055
[52]	training's l2: 1.8361	valid_1's l2: 2.0987
[53]	training's l2: 1.82965	valid_1's l2: 2.09695
[54]	training's l2: 1.82399	valid_1's l2: 2.09546
[55]	training's l2: 1.81792	valid_1's l2: 2.09311
[56]	training's l2: 1.81126	valid_1's l2: 2.08998
[57]	training's l2: 1.80492	valid_1's l2: 2.0872
[58]	training's l2: 1.79894	valid_1's l2: 2.08463
[59]	training's l2: 1.79309	valid_1's l2: 2.08284
[60]	training's l2: 1.78666	valid_1's l2: 2.08094
[61]	training's l2: 1.7808	valid_1's l2: 2.08003
[62]	training's l2: 1.77537	valid_1's l2: 2.07755
[63]	training's l2: 1.77029	valid_1's l2: 2.07636
[64]	training's l2: 1.76327	valid_1's l2: 2.07429
[65]	training's l2: 1.75752	valid_1's l2: 2.07313
[66]	training's l2: 1.75258	valid_1's l2: 2.07305
[67]	training's l2: 1.74755	valid_1's l2: 2.07066
[68]	training's l2: 1.74123	valid_1's l2: 2.06854
[69]	training's l2: 1.7358	valid_1's l2: 2.06755
[70]	training's l2: 1.73075	valid_1's l2: 2.06657
[71]	training's l2: 1.72571	valid_1's l2: 2.06697
[72]	training's l2: 1.72053	valid_1's l2: 2.06545
[73]	training's l2: 1.71449	valid_1's l2: 2.06362
[74]	training's l2: 1.70954	valid_1's l2: 2.0628
[75]	training's l2: 1.70504	valid_1's l2: 2.06257
[76]	training's l2: 1.69977	valid_1's l2: 2.06113
[77]	training's l2: 1.69549	valid_1's l2: 2.06045
[78]	training's l2: 1.68996	valid_1's l2: 2.0595
[79]	training's l2: 1.68585	valid_1's l2: 2.05917
[80]	training's l2: 1.68051	valid_1's l2: 2.05853
[81]	training's l2: 1.67563	valid_1's l2: 2.05847
[82]	training's l2: 1.67065	valid_1's l2: 2.05684
[83]	training's l2: 1.66627	valid_1's l2: 2.05633
[84]	training's l2: 1.66191	valid_1's l2: 2.05516
[85]	training's l2: 1.65713	valid_1's l2: 2.05489
[86]	training's l2: 1.65202	valid_1's l2: 2.05414
[87]	training's l2: 1.64654	valid_1's l2: 2.05293
[88]	training's l2: 1.64271	valid_1's l2: 2.05177
[89]	training's l2: 1.63816	valid_1's l2: 2.05117
[90]	training's l2: 1.63428	valid_1's l2: 2.05085
[91]	training's l2: 1.63028	valid_1's l2: 2.05051
[92]	training's l2: 1.62564	valid_1's l2: 2.04983
[93]	training's l2: 1.62159	valid_1's l2: 2.04894
[94]	training's l2: 1.61685	valid_1's l2: 2.04757
[95]	training's l2: 1.61315	valid_1's l2: 2.04745
[96]	training's l2: 1.60898	valid_1's l2: 2.04731
[97]	training's l2: 1.60455	valid_1's l2: 2.04637
[98]	training's l2: 1.60066	valid_1's l2: 2.04581
[99]	training's l2: 1.59638	valid_1's l2: 2.04654
[100]	training's l2: 1.59221	valid_1's l2: 2.0457
[101]	training's l2: 1.58876	valid_1's l2: 2.04536
[102]	training's l2: 1.58513	valid_1's l2: 2.04545
[103]	training's l2: 1.58114	valid_1's l2: 2.0448
[104]	training's l2: 1.57679	valid_1's l2: 2.04379
[105]	training's l2: 1.57323	valid_1's l2: 2.04312
[106]	training's l2: 1.56907	valid_1's l2: 2.04283
[107]	training's l2: 1.56537	valid_1's l2: 2.04355
[108]	training's l2: 1.56126	valid_1's l2: 2.04262
[109]	training's l2: 1.55748	valid_1's l2: 2.04221
[110]	training's l2: 1.55315	valid_1's l2: 2.04203
[111]	training's l2: 1.54954	valid_1's l2: 2.04211
[112]	training's l2: 1.54624	valid_1's l2: 2.0419
[113]	training's l2: 1.5431	valid_1's l2: 2.04126
[114]	training's l2: 1.53948	valid_1's l2: 2.0411
[115]	training's l2: 1.53563	valid_1's l2: 2.04137
[116]	training's l2: 1.53195	valid_1's l2: 2.04199
[117]	training's l2: 1.52846	valid_1's l2: 2.04236
[118]	training's l2: 1.52479	valid_1's l2: 2.04217
[119]	training's l2: 1.52135	valid_1's l2: 2.04131
[120]	training's l2: 1.51796	valid_1's l2: 2.04117
[121]	training's l2: 1.51472	valid_1's l2: 2.04057
[122]	training's l2: 1.51124	valid_1's l2: 2.04071
[123]	training's l2: 1.50803	valid_1's l2: 2.04081
[124]	training's l2: 1.5049	valid_1's l2: 2.04049
[125]	training's l2: 1.50182	valid_1's l2: 2.04005
[126]	training's l2: 1.49826	valid_1's l2: 2.04078
[127]	training's l2: 1.49499	valid_1's l2: 2.04083
[128]	training's l2: 1.49188	valid_1's l2: 2.04075
[129]	training's l2: 1.48838	valid_1's l2: 2.04033
[130]	training's l2: 1.48515	valid_1's l2: 2.03998
[131]	training's l2: 1.48193	valid_1's l2: 2.04021
[132]	training's l2: 1.47855	valid_1's l2: 2.03972
[133]	training's l2: 1.47533	valid_1's l2: 2.03928
[134]	training's l2: 1.4721	valid_1's l2: 2.03897
[135]	training's l2: 1.46858	valid_1's l2: 2.03841
[136]	training's l2: 1.46561	valid_1's l2: 2.0388
[137]	training's l2: 1.46263	valid_1's l2: 2.03853
[138]	training's l2: 1.45916	valid_1's l2: 2.03779
[139]	training's l2: 1.45626	valid_1's l2: 2.0379
[140]	training's l2: 1.45335	valid_1's l2: 2.03825
[141]	training's l2: 1.45014	valid_1's l2: 2.03837
[142]	training's l2: 1.44739	valid_1's l2: 2.03784
[143]	training's l2: 1.44438	valid_1's l2: 2.03729
[144]	training's l2: 1.44119	valid_1's l2: 2.03689
[145]	training's l2: 1.43785	valid_1's l2: 2.03671
[146]	training's l2: 1.43456	valid_1's l2: 2.03649
[147]	training's l2: 1.43169	valid_1's l2: 2.03644
[148]	training's l2: 1.42867	valid_1's l2: 2.03627
[149]	training's l2: 1.42595	valid_1's l2: 2.03651
[150]	training's l2: 1.42336	valid_1's l2: 2.03714
[151]	training's l2: 1.42061	valid_1's l2: 2.03706
[152]	training's l2: 1.41762	valid_1's l2: 2.03594
[153]	training's l2: 1.41435	valid_1's l2: 2.03538
[154]	training's l2: 1.41163	valid_1's l2: 2.03606
[155]	training's l2: 1.4078	valid_1's l2: 2.03442
[156]	training's l2: 1.40499	valid_1's l2: 2.03496
[157]	training's l2: 1.40216	valid_1's l2: 2.03483
[158]	training's l2: 1.39957	valid_1's l2: 2.03519
[159]	training's l2: 1.39698	valid_1's l2: 2.03523
[160]	training's l2: 1.39422	valid_1's l2: 2.03518
[161]	training's l2: 1.39046	valid_1's l2: 2.03456
[162]	training's l2: 1.38794	valid_1's l2: 2.03454
[163]	training's l2: 1.38562	valid_1's l2: 2.03414
[164]	training's l2: 1.38308	valid_1's l2: 2.03417
[165]	training's l2: 1.38037	valid_1's l2: 2.03375
[166]	training's l2: 1.37766	valid_1's l2: 2.03439
[167]	training's l2: 1.37504	valid_1's l2: 2.03456
[168]	training's l2: 1.37205	valid_1's l2: 2.03435
[169]	training's l2: 1.36934	valid_1's l2: 2.0333
[170]	training's l2: 1.36654	valid_1's l2: 2.03272
[171]	training's l2: 1.36417	valid_1's l2: 2.03263
[172]	training's l2: 1.36151	valid_1's l2: 2.03308
Did not meet early stopping. Best iteration is:
[172]	training's l2: 1.36151	valid_1's l2: 2.03308
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233780 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.58634	valid_1's l2: 2.63288
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.53809	valid_1's l2: 2.58203
[3]	training's l2: 2.49606	valid_1's l2: 2.53406
[4]	training's l2: 2.45653	valid_1's l2: 2.49469
[5]	training's l2: 2.42279	valid_1's l2: 2.45878
[6]	training's l2: 2.39125	valid_1's l2: 2.42696
[7]	training's l2: 2.36377	valid_1's l2: 2.401
[8]	training's l2: 2.33725	valid_1's l2: 2.37524
[9]	training's l2: 2.31376	valid_1's l2: 2.35275
[10]	training's l2: 2.29203	valid_1's l2: 2.33081
[11]	training's l2: 2.27226	valid_1's l2: 2.31165
[12]	training's l2: 2.25374	valid_1's l2: 2.29463
[13]	training's l2: 2.23592	valid_1's l2: 2.27827
[14]	training's l2: 2.22016	valid_1's l2: 2.26412
[15]	training's l2: 2.20564	valid_1's l2: 2.25291
[16]	training's l2: 2.19189	valid_1's l2: 2.2405
[17]	training's l2: 2.17893	valid_1's l2: 2.22926
[18]	training's l2: 2.16675	valid_1's l2: 2.22023
[19]	training's l2: 2.15587	valid_1's l2: 2.21199
[20]	training's l2: 2.14401	valid_1's l2: 2.20149
[21]	training's l2: 2.13318	valid_1's l2: 2.19421
[22]	training's l2: 2.12286	valid_1's l2: 2.1867
[23]	training's l2: 2.11315	valid_1's l2: 2.17979
[24]	training's l2: 2.10442	valid_1's l2: 2.17392
[25]	training's l2: 2.09623	valid_1's l2: 2.16803
[26]	training's l2: 2.08781	valid_1's l2: 2.16178
[27]	training's l2: 2.07984	valid_1's l2: 2.15606
[28]	training's l2: 2.07191	valid_1's l2: 2.15036
[29]	training's l2: 2.06453	valid_1's l2: 2.14541
[30]	training's l2: 2.05733	valid_1's l2: 2.14122
[31]	training's l2: 2.05001	valid_1's l2: 2.13564
[32]	training's l2: 2.04329	valid_1's l2: 2.13225
[33]	training's l2: 2.03628	valid_1's l2: 2.1279
[34]	training's l2: 2.03045	valid_1's l2: 2.12513
[35]	training's l2: 2.02269	valid_1's l2: 2.12053
[36]	training's l2: 2.01651	valid_1's l2: 2.11614
[37]	training's l2: 2.0107	valid_1's l2: 2.11188
[38]	training's l2: 2.00375	valid_1's l2: 2.10712
[39]	training's l2: 1.99822	valid_1's l2: 2.10356
[40]	training's l2: 1.99348	valid_1's l2: 2.10118
[41]	training's l2: 1.98772	valid_1's l2: 2.09822
[42]	training's l2: 1.9825	valid_1's l2: 2.09641
[43]	training's l2: 1.97773	valid_1's l2: 2.0955
[44]	training's l2: 1.9723	valid_1's l2: 2.09296
[45]	training's l2: 1.96653	valid_1's l2: 2.08799
[46]	training's l2: 1.96187	valid_1's l2: 2.08612
[47]	training's l2: 1.95665	valid_1's l2: 2.0834
[48]	training's l2: 1.95182	valid_1's l2: 2.08077
[49]	training's l2: 1.94497	valid_1's l2: 2.07781
[50]	training's l2: 1.94007	valid_1's l2: 2.07593
[51]	training's l2: 1.93606	valid_1's l2: 2.07424
[52]	training's l2: 1.93098	valid_1's l2: 2.07375
[53]	training's l2: 1.92688	valid_1's l2: 2.07226
[54]	training's l2: 1.92269	valid_1's l2: 2.07044
[55]	training's l2: 1.91845	valid_1's l2: 2.06964
[56]	training's l2: 1.91225	valid_1's l2: 2.06587
[57]	training's l2: 1.90756	valid_1's l2: 2.06493
[58]	training's l2: 1.90401	valid_1's l2: 2.06432
[59]	training's l2: 1.89994	valid_1's l2: 2.06237
[60]	training's l2: 1.89596	valid_1's l2: 2.06082
[61]	training's l2: 1.892	valid_1's l2: 2.05937
[62]	training's l2: 1.88889	valid_1's l2: 2.05844
[63]	training's l2: 1.88425	valid_1's l2: 2.05628
[64]	training's l2: 1.88082	valid_1's l2: 2.05567
[65]	training's l2: 1.87544	valid_1's l2: 2.0541
[66]	training's l2: 1.87136	valid_1's l2: 2.05309
[67]	training's l2: 1.86782	valid_1's l2: 2.05142
[68]	training's l2: 1.86485	valid_1's l2: 2.05104
[69]	training's l2: 1.86182	valid_1's l2: 2.05003
[70]	training's l2: 1.85796	valid_1's l2: 2.05027
[71]	training's l2: 1.85456	valid_1's l2: 2.04992
[72]	training's l2: 1.85061	valid_1's l2: 2.04947
[73]	training's l2: 1.8474	valid_1's l2: 2.04886
[74]	training's l2: 1.8446	valid_1's l2: 2.04868
[75]	training's l2: 1.84141	valid_1's l2: 2.04777
[76]	training's l2: 1.83829	valid_1's l2: 2.04724
[77]	training's l2: 1.83507	valid_1's l2: 2.04766
[78]	training's l2: 1.83255	valid_1's l2: 2.04766
[79]	training's l2: 1.82938	valid_1's l2: 2.04597
[80]	training's l2: 1.8264	valid_1's l2: 2.04527
[81]	training's l2: 1.82358	valid_1's l2: 2.04542
[82]	training's l2: 1.82049	valid_1's l2: 2.04519
[83]	training's l2: 1.81762	valid_1's l2: 2.04471
[84]	training's l2: 1.81434	valid_1's l2: 2.04464
[85]	training's l2: 1.8117	valid_1's l2: 2.04358
[86]	training's l2: 1.80883	valid_1's l2: 2.04302
[87]	training's l2: 1.80539	valid_1's l2: 2.04195
[88]	training's l2: 1.80241	valid_1's l2: 2.0413
[89]	training's l2: 1.79946	valid_1's l2: 2.04033
[90]	training's l2: 1.79687	valid_1's l2: 2.04042
[91]	training's l2: 1.79295	valid_1's l2: 2.03876
[92]	training's l2: 1.78997	valid_1's l2: 2.03837
[93]	training's l2: 1.78694	valid_1's l2: 2.03741
[94]	training's l2: 1.78477	valid_1's l2: 2.03785
[95]	training's l2: 1.78227	valid_1's l2: 2.03762
[96]	training's l2: 1.77893	valid_1's l2: 2.03717
[97]	training's l2: 1.7763	valid_1's l2: 2.03714
[98]	training's l2: 1.77398	valid_1's l2: 2.03676
[99]	training's l2: 1.77118	valid_1's l2: 2.03667
[100]	training's l2: 1.76866	valid_1's l2: 2.03606
[101]	training's l2: 1.76624	valid_1's l2: 2.03652
[102]	training's l2: 1.76357	valid_1's l2: 2.0361
[103]	training's l2: 1.76099	valid_1's l2: 2.03594
[104]	training's l2: 1.75867	valid_1's l2: 2.03555
[105]	training's l2: 1.75622	valid_1's l2: 2.03462
[106]	training's l2: 1.75216	valid_1's l2: 2.03319
[107]	training's l2: 1.74969	valid_1's l2: 2.0336
[108]	training's l2: 1.7475	valid_1's l2: 2.03355
[109]	training's l2: 1.74511	valid_1's l2: 2.03368
[110]	training's l2: 1.74181	valid_1's l2: 2.03272
[111]	training's l2: 1.73847	valid_1's l2: 2.03115
[112]	training's l2: 1.73632	valid_1's l2: 2.03058
[113]	training's l2: 1.73425	valid_1's l2: 2.02993
[114]	training's l2: 1.73208	valid_1's l2: 2.02996
[115]	training's l2: 1.73015	valid_1's l2: 2.03044
[116]	training's l2: 1.72811	valid_1's l2: 2.03039
[117]	training's l2: 1.72588	valid_1's l2: 2.03063
[118]	training's l2: 1.72372	valid_1's l2: 2.03129
[119]	training's l2: 1.72169	valid_1's l2: 2.03106
[120]	training's l2: 1.71929	valid_1's l2: 2.03047
[121]	training's l2: 1.71715	valid_1's l2: 2.03008
[122]	training's l2: 1.71398	valid_1's l2: 2.03024
[123]	training's l2: 1.7118	valid_1's l2: 2.02979
[124]	training's l2: 1.70888	valid_1's l2: 2.03079
[125]	training's l2: 1.70566	valid_1's l2: 2.02909
[126]	training's l2: 1.70228	valid_1's l2: 2.02892
[127]	training's l2: 1.69993	valid_1's l2: 2.02867
[128]	training's l2: 1.69806	valid_1's l2: 2.0278
[129]	training's l2: 1.69491	valid_1's l2: 2.02573
[130]	training's l2: 1.69268	valid_1's l2: 2.02392
[131]	training's l2: 1.69081	valid_1's l2: 2.02444
[132]	training's l2: 1.68891	valid_1's l2: 2.02474
[133]	training's l2: 1.68665	valid_1's l2: 2.025
[134]	training's l2: 1.68465	valid_1's l2: 2.02391
[135]	training's l2: 1.68254	valid_1's l2: 2.02343
[136]	training's l2: 1.67993	valid_1's l2: 2.02247
[137]	training's l2: 1.6781	valid_1's l2: 2.02184
[138]	training's l2: 1.67594	valid_1's l2: 2.02184
[139]	training's l2: 1.67376	valid_1's l2: 2.02191
[140]	training's l2: 1.67193	valid_1's l2: 2.02177
[141]	training's l2: 1.66968	valid_1's l2: 2.02264
[142]	training's l2: 1.66807	valid_1's l2: 2.02249
[143]	training's l2: 1.66603	valid_1's l2: 2.02208
[144]	training's l2: 1.66367	valid_1's l2: 2.02172
[145]	training's l2: 1.66155	valid_1's l2: 2.02169
[146]	training's l2: 1.65964	valid_1's l2: 2.02111
[147]	training's l2: 1.65663	valid_1's l2: 2.01987
[148]	training's l2: 1.65487	valid_1's l2: 2.02051
[149]	training's l2: 1.65258	valid_1's l2: 2.02083
[150]	training's l2: 1.65053	valid_1's l2: 2.02152
[151]	training's l2: 1.64869	valid_1's l2: 2.02156
[152]	training's l2: 1.64688	valid_1's l2: 2.02147
[153]	training's l2: 1.6446	valid_1's l2: 2.02078
[154]	training's l2: 1.64254	valid_1's l2: 2.02027
[155]	training's l2: 1.64057	valid_1's l2: 2.02088
[156]	training's l2: 1.63888	valid_1's l2: 2.02095
[157]	training's l2: 1.63724	valid_1's l2: 2.02126
[158]	training's l2: 1.63512	valid_1's l2: 2.02094
[159]	training's l2: 1.6332	valid_1's l2: 2.02062
[160]	training's l2: 1.63131	valid_1's l2: 2.02072
[161]	training's l2: 1.62958	valid_1's l2: 2.02079
[162]	training's l2: 1.62769	valid_1's l2: 2.02081
[163]	training's l2: 1.62546	valid_1's l2: 2.02007
[164]	training's l2: 1.62385	valid_1's l2: 2.01974
[165]	training's l2: 1.6221	valid_1's l2: 2.01984
[166]	training's l2: 1.62039	valid_1's l2: 2.02143
[167]	training's l2: 1.61844	valid_1's l2: 2.02321
Did not meet early stopping. Best iteration is:
[167]	training's l2: 1.61844	valid_1's l2: 2.02321
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254474 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.56309	valid_1's l2: 2.61678
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.49422	valid_1's l2: 2.55158
[3]	training's l2: 2.43631	valid_1's l2: 2.49528
[4]	training's l2: 2.38327	valid_1's l2: 2.44541
[5]	training's l2: 2.3375	valid_1's l2: 2.40572
[6]	training's l2: 2.29553	valid_1's l2: 2.36735
[7]	training's l2: 2.25795	valid_1's l2: 2.3358
[8]	training's l2: 2.2254	valid_1's l2: 2.30868
[9]	training's l2: 2.19468	valid_1's l2: 2.28292
[10]	training's l2: 2.16735	valid_1's l2: 2.26273
[11]	training's l2: 2.1416	valid_1's l2: 2.24386
[12]	training's l2: 2.11801	valid_1's l2: 2.22463
[13]	training's l2: 2.09629	valid_1's l2: 2.2097
[14]	training's l2: 2.07655	valid_1's l2: 2.19488
[15]	training's l2: 2.05815	valid_1's l2: 2.18346
[16]	training's l2: 2.04048	valid_1's l2: 2.17123
[17]	training's l2: 2.02397	valid_1's l2: 2.16277
[18]	training's l2: 2.00914	valid_1's l2: 2.15468
[19]	training's l2: 1.99449	valid_1's l2: 2.14661
[20]	training's l2: 1.98116	valid_1's l2: 2.14052
[21]	training's l2: 1.96702	valid_1's l2: 2.13334
[22]	training's l2: 1.95375	valid_1's l2: 2.12831
[23]	training's l2: 1.94062	valid_1's l2: 2.123
[24]	training's l2: 1.92681	valid_1's l2: 2.11648
[25]	training's l2: 1.91476	valid_1's l2: 2.11277
[26]	training's l2: 1.90371	valid_1's l2: 2.10964
[27]	training's l2: 1.89246	valid_1's l2: 2.10584
[28]	training's l2: 1.88203	valid_1's l2: 2.10233
[29]	training's l2: 1.87169	valid_1's l2: 2.10112
[30]	training's l2: 1.86052	valid_1's l2: 2.09764
[31]	training's l2: 1.85007	valid_1's l2: 2.09269
[32]	training's l2: 1.84054	valid_1's l2: 2.08966
[33]	training's l2: 1.83096	valid_1's l2: 2.08555
[34]	training's l2: 1.82127	valid_1's l2: 2.08256
[35]	training's l2: 1.81268	valid_1's l2: 2.08042
[36]	training's l2: 1.8047	valid_1's l2: 2.07683
[37]	training's l2: 1.79607	valid_1's l2: 2.07398
[38]	training's l2: 1.78851	valid_1's l2: 2.07344
[39]	training's l2: 1.77716	valid_1's l2: 2.0686
[40]	training's l2: 1.76716	valid_1's l2: 2.06745
[41]	training's l2: 1.75857	valid_1's l2: 2.06591
[42]	training's l2: 1.75066	valid_1's l2: 2.06371
[43]	training's l2: 1.7418	valid_1's l2: 2.06217
[44]	training's l2: 1.73471	valid_1's l2: 2.06114
[45]	training's l2: 1.72716	valid_1's l2: 2.06024
[46]	training's l2: 1.7174	valid_1's l2: 2.05812
[47]	training's l2: 1.70957	valid_1's l2: 2.05896
[48]	training's l2: 1.70128	valid_1's l2: 2.05773
[49]	training's l2: 1.69438	valid_1's l2: 2.05506
[50]	training's l2: 1.68806	valid_1's l2: 2.05555
[51]	training's l2: 1.68175	valid_1's l2: 2.05518
[52]	training's l2: 1.67475	valid_1's l2: 2.05402
[53]	training's l2: 1.66793	valid_1's l2: 2.05414
[54]	training's l2: 1.66149	valid_1's l2: 2.05393
[55]	training's l2: 1.6537	valid_1's l2: 2.05335
[56]	training's l2: 1.64696	valid_1's l2: 2.05156
[57]	training's l2: 1.63999	valid_1's l2: 2.05182
[58]	training's l2: 1.63435	valid_1's l2: 2.05205
[59]	training's l2: 1.62798	valid_1's l2: 2.0511
[60]	training's l2: 1.62204	valid_1's l2: 2.05141
[61]	training's l2: 1.61626	valid_1's l2: 2.05136
[62]	training's l2: 1.61079	valid_1's l2: 2.0515
[63]	training's l2: 1.60497	valid_1's l2: 2.05127
[64]	training's l2: 1.59988	valid_1's l2: 2.05107
[65]	training's l2: 1.59409	valid_1's l2: 2.04963
[66]	training's l2: 1.58852	valid_1's l2: 2.05011
[67]	training's l2: 1.58231	valid_1's l2: 2.04836
[68]	training's l2: 1.57677	valid_1's l2: 2.0488
[69]	training's l2: 1.57137	valid_1's l2: 2.04818
[70]	training's l2: 1.56596	valid_1's l2: 2.04804
[71]	training's l2: 1.56007	valid_1's l2: 2.0481
[72]	training's l2: 1.55526	valid_1's l2: 2.04841
[73]	training's l2: 1.54732	valid_1's l2: 2.047
[74]	training's l2: 1.54215	valid_1's l2: 2.04665
[75]	training's l2: 1.53675	valid_1's l2: 2.0453
[76]	training's l2: 1.53065	valid_1's l2: 2.04415
[77]	training's l2: 1.52577	valid_1's l2: 2.04513
[78]	training's l2: 1.52109	valid_1's l2: 2.04452
[79]	training's l2: 1.51657	valid_1's l2: 2.0431
[80]	training's l2: 1.51095	valid_1's l2: 2.04228
[81]	training's l2: 1.50666	valid_1's l2: 2.04268
[82]	training's l2: 1.50194	valid_1's l2: 2.0433
[83]	training's l2: 1.49655	valid_1's l2: 2.04347
[84]	training's l2: 1.49201	valid_1's l2: 2.04379
[85]	training's l2: 1.48751	valid_1's l2: 2.04425
[86]	training's l2: 1.48223	valid_1's l2: 2.04322
[87]	training's l2: 1.47763	valid_1's l2: 2.04362
[88]	training's l2: 1.4717	valid_1's l2: 2.04259
[89]	training's l2: 1.46748	valid_1's l2: 2.0428
[90]	training's l2: 1.46219	valid_1's l2: 2.04247
[91]	training's l2: 1.45747	valid_1's l2: 2.04261
[92]	training's l2: 1.45341	valid_1's l2: 2.04296
[93]	training's l2: 1.44926	valid_1's l2: 2.04333
[94]	training's l2: 1.44482	valid_1's l2: 2.04401
[95]	training's l2: 1.4403	valid_1's l2: 2.04495
[96]	training's l2: 1.43656	valid_1's l2: 2.04599
[97]	training's l2: 1.43277	valid_1's l2: 2.04638
[98]	training's l2: 1.42896	valid_1's l2: 2.04733
[99]	training's l2: 1.42489	valid_1's l2: 2.04714
[100]	training's l2: 1.42139	valid_1's l2: 2.04724
[101]	training's l2: 1.41715	valid_1's l2: 2.04664
[102]	training's l2: 1.41284	valid_1's l2: 2.04655
[103]	training's l2: 1.40809	valid_1's l2: 2.04512
[104]	training's l2: 1.40425	valid_1's l2: 2.04563
[105]	training's l2: 1.40069	valid_1's l2: 2.04568
[106]	training's l2: 1.39757	valid_1's l2: 2.0458
[107]	training's l2: 1.39368	valid_1's l2: 2.04613
[108]	training's l2: 1.39005	valid_1's l2: 2.04653
[109]	training's l2: 1.38664	valid_1's l2: 2.04581
[110]	training's l2: 1.38277	valid_1's l2: 2.04531
Early stopping, best iteration is:
[80]	training's l2: 1.51095	valid_1's l2: 2.04228
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.230673 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59707	valid_1's l2: 2.64787
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.55827	valid_1's l2: 2.60643
[3]	training's l2: 2.52123	valid_1's l2: 2.56871
[4]	training's l2: 2.48818	valid_1's l2: 2.5349
[5]	training's l2: 2.4559	valid_1's l2: 2.50462
[6]	training's l2: 2.42787	valid_1's l2: 2.47652
[7]	training's l2: 2.4006	valid_1's l2: 2.44949
[8]	training's l2: 2.37635	valid_1's l2: 2.42521
[9]	training's l2: 2.35287	valid_1's l2: 2.40267
[10]	training's l2: 2.33039	valid_1's l2: 2.38204
[11]	training's l2: 2.31032	valid_1's l2: 2.3634
[12]	training's l2: 2.29137	valid_1's l2: 2.34576
[13]	training's l2: 2.27392	valid_1's l2: 2.32987
[14]	training's l2: 2.25715	valid_1's l2: 2.31558
[15]	training's l2: 2.24076	valid_1's l2: 2.30118
[16]	training's l2: 2.22548	valid_1's l2: 2.28794
[17]	training's l2: 2.21039	valid_1's l2: 2.2752
[18]	training's l2: 2.19673	valid_1's l2: 2.26448
[19]	training's l2: 2.18346	valid_1's l2: 2.25307
[20]	training's l2: 2.17144	valid_1's l2: 2.24342
[21]	training's l2: 2.15967	valid_1's l2: 2.23338
[22]	training's l2: 2.14861	valid_1's l2: 2.22481
[23]	training's l2: 2.13807	valid_1's l2: 2.21586
[24]	training's l2: 2.1278	valid_1's l2: 2.20865
[25]	training's l2: 2.11846	valid_1's l2: 2.20117
[26]	training's l2: 2.10856	valid_1's l2: 2.19374
[27]	training's l2: 2.099	valid_1's l2: 2.18735
[28]	training's l2: 2.09027	valid_1's l2: 2.18105
[29]	training's l2: 2.08126	valid_1's l2: 2.17524
[30]	training's l2: 2.07289	valid_1's l2: 2.1707
[31]	training's l2: 2.06526	valid_1's l2: 2.16527
[32]	training's l2: 2.05775	valid_1's l2: 2.16125
[33]	training's l2: 2.05082	valid_1's l2: 2.15658
[34]	training's l2: 2.04356	valid_1's l2: 2.15232
[35]	training's l2: 2.0368	valid_1's l2: 2.14777
[36]	training's l2: 2.02971	valid_1's l2: 2.14381
[37]	training's l2: 2.02261	valid_1's l2: 2.13938
[38]	training's l2: 2.01587	valid_1's l2: 2.13518
[39]	training's l2: 2.00973	valid_1's l2: 2.13183
[40]	training's l2: 2.00405	valid_1's l2: 2.12894
[41]	training's l2: 1.99814	valid_1's l2: 2.12554
[42]	training's l2: 1.99244	valid_1's l2: 2.12327
[43]	training's l2: 1.98625	valid_1's l2: 2.11992
[44]	training's l2: 1.98044	valid_1's l2: 2.11639
[45]	training's l2: 1.9755	valid_1's l2: 2.11487
[46]	training's l2: 1.97018	valid_1's l2: 2.11286
[47]	training's l2: 1.96424	valid_1's l2: 2.10963
[48]	training's l2: 1.95849	valid_1's l2: 2.10657
[49]	training's l2: 1.95332	valid_1's l2: 2.10467
[50]	training's l2: 1.94742	valid_1's l2: 2.10168
[51]	training's l2: 1.94189	valid_1's l2: 2.10025
[52]	training's l2: 1.93723	valid_1's l2: 2.099
[53]	training's l2: 1.93297	valid_1's l2: 2.09778
[54]	training's l2: 1.92731	valid_1's l2: 2.09579
[55]	training's l2: 1.92238	valid_1's l2: 2.09392
[56]	training's l2: 1.91736	valid_1's l2: 2.09093
[57]	training's l2: 1.91135	valid_1's l2: 2.08798
[58]	training's l2: 1.90663	valid_1's l2: 2.08601
[59]	training's l2: 1.90163	valid_1's l2: 2.08433
[60]	training's l2: 1.89737	valid_1's l2: 2.08237
[61]	training's l2: 1.89328	valid_1's l2: 2.08032
[62]	training's l2: 1.88917	valid_1's l2: 2.07939
[63]	training's l2: 1.88383	valid_1's l2: 2.07773
[64]	training's l2: 1.8801	valid_1's l2: 2.07725
[65]	training's l2: 1.87614	valid_1's l2: 2.07691
[66]	training's l2: 1.87246	valid_1's l2: 2.07598
[67]	training's l2: 1.8673	valid_1's l2: 2.07563
[68]	training's l2: 1.86247	valid_1's l2: 2.07337
[69]	training's l2: 1.85755	valid_1's l2: 2.07156
[70]	training's l2: 1.85353	valid_1's l2: 2.06974
[71]	training's l2: 1.84894	valid_1's l2: 2.06874
[72]	training's l2: 1.8452	valid_1's l2: 2.0683
[73]	training's l2: 1.84032	valid_1's l2: 2.06604
[74]	training's l2: 1.83688	valid_1's l2: 2.06524
[75]	training's l2: 1.8332	valid_1's l2: 2.0646
[76]	training's l2: 1.82797	valid_1's l2: 2.06223
[77]	training's l2: 1.82395	valid_1's l2: 2.06268
[78]	training's l2: 1.82007	valid_1's l2: 2.06134
[79]	training's l2: 1.81667	valid_1's l2: 2.06126
[80]	training's l2: 1.81257	valid_1's l2: 2.06083
[81]	training's l2: 1.80923	valid_1's l2: 2.0596
[82]	training's l2: 1.80603	valid_1's l2: 2.05841
[83]	training's l2: 1.80279	valid_1's l2: 2.0574
[84]	training's l2: 1.79961	valid_1's l2: 2.05661
[85]	training's l2: 1.79605	valid_1's l2: 2.05542
[86]	training's l2: 1.79286	valid_1's l2: 2.0546
[87]	training's l2: 1.78815	valid_1's l2: 2.05257
[88]	training's l2: 1.78465	valid_1's l2: 2.05072
[89]	training's l2: 1.78124	valid_1's l2: 2.05036
[90]	training's l2: 1.77752	valid_1's l2: 2.05028
[91]	training's l2: 1.77422	valid_1's l2: 2.04929
[92]	training's l2: 1.77124	valid_1's l2: 2.04917
[93]	training's l2: 1.76775	valid_1's l2: 2.04706
[94]	training's l2: 1.7647	valid_1's l2: 2.04666
[95]	training's l2: 1.76111	valid_1's l2: 2.04608
[96]	training's l2: 1.75795	valid_1's l2: 2.04602
[97]	training's l2: 1.75475	valid_1's l2: 2.04478
[98]	training's l2: 1.75163	valid_1's l2: 2.04449
[99]	training's l2: 1.74899	valid_1's l2: 2.04477
[100]	training's l2: 1.74609	valid_1's l2: 2.04504
[101]	training's l2: 1.74225	valid_1's l2: 2.04394
[102]	training's l2: 1.73926	valid_1's l2: 2.04379
[103]	training's l2: 1.73661	valid_1's l2: 2.04365
[104]	training's l2: 1.73388	valid_1's l2: 2.04268
[105]	training's l2: 1.73102	valid_1's l2: 2.04196
[106]	training's l2: 1.72829	valid_1's l2: 2.04249
[107]	training's l2: 1.72548	valid_1's l2: 2.04233
[108]	training's l2: 1.7225	valid_1's l2: 2.04265
[109]	training's l2: 1.71856	valid_1's l2: 2.04086
[110]	training's l2: 1.71599	valid_1's l2: 2.04078
[111]	training's l2: 1.71318	valid_1's l2: 2.04096
[112]	training's l2: 1.71045	valid_1's l2: 2.04011
[113]	training's l2: 1.70776	valid_1's l2: 2.03929
[114]	training's l2: 1.70512	valid_1's l2: 2.03904
[115]	training's l2: 1.70244	valid_1's l2: 2.03894
[116]	training's l2: 1.69985	valid_1's l2: 2.03877
[117]	training's l2: 1.69738	valid_1's l2: 2.03873
[118]	training's l2: 1.69438	valid_1's l2: 2.03771
[119]	training's l2: 1.69196	valid_1's l2: 2.03857
[120]	training's l2: 1.68954	valid_1's l2: 2.03877
[121]	training's l2: 1.68692	valid_1's l2: 2.03876
[122]	training's l2: 1.68452	valid_1's l2: 2.03925
[123]	training's l2: 1.68204	valid_1's l2: 2.03972
[124]	training's l2: 1.67948	valid_1's l2: 2.03924
[125]	training's l2: 1.67728	valid_1's l2: 2.03932
[126]	training's l2: 1.67481	valid_1's l2: 2.03986
[127]	training's l2: 1.67196	valid_1's l2: 2.04042
[128]	training's l2: 1.66966	valid_1's l2: 2.04106
[129]	training's l2: 1.66728	valid_1's l2: 2.04155
[130]	training's l2: 1.66471	valid_1's l2: 2.04138
[131]	training's l2: 1.66239	valid_1's l2: 2.0415
[132]	training's l2: 1.65973	valid_1's l2: 2.04067
[133]	training's l2: 1.6572	valid_1's l2: 2.04044
[134]	training's l2: 1.65511	valid_1's l2: 2.04092
[135]	training's l2: 1.65283	valid_1's l2: 2.04064
[136]	training's l2: 1.65046	valid_1's l2: 2.04047
[137]	training's l2: 1.64829	valid_1's l2: 2.04011
[138]	training's l2: 1.64602	valid_1's l2: 2.04026
[139]	training's l2: 1.64345	valid_1's l2: 2.04048
[140]	training's l2: 1.64135	valid_1's l2: 2.04133
[141]	training's l2: 1.63885	valid_1's l2: 2.04237
[142]	training's l2: 1.63657	valid_1's l2: 2.04123
[143]	training's l2: 1.63423	valid_1's l2: 2.04184
[144]	training's l2: 1.63193	valid_1's l2: 2.04163
[145]	training's l2: 1.62955	valid_1's l2: 2.0413
[146]	training's l2: 1.62724	valid_1's l2: 2.04158
[147]	training's l2: 1.62525	valid_1's l2: 2.04101
[148]	training's l2: 1.62319	valid_1's l2: 2.0408
Early stopping, best iteration is:
[118]	training's l2: 1.69438	valid_1's l2: 2.03771
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.239248 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.5733	valid_1's l2: 2.62108
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.5144	valid_1's l2: 2.56181
[3]	training's l2: 2.46421	valid_1's l2: 2.50741
[4]	training's l2: 2.41842	valid_1's l2: 2.46212
[5]	training's l2: 2.37983	valid_1's l2: 2.42364
[6]	training's l2: 2.34571	valid_1's l2: 2.39078
[7]	training's l2: 2.31283	valid_1's l2: 2.35982
[8]	training's l2: 2.28521	valid_1's l2: 2.33361
[9]	training's l2: 2.25947	valid_1's l2: 2.31097
[10]	training's l2: 2.23621	valid_1's l2: 2.29177
[11]	training's l2: 2.21494	valid_1's l2: 2.27144
[12]	training's l2: 2.1941	valid_1's l2: 2.25286
[13]	training's l2: 2.17653	valid_1's l2: 2.23742
[14]	training's l2: 2.15997	valid_1's l2: 2.22346
[15]	training's l2: 2.14422	valid_1's l2: 2.21045
[16]	training's l2: 2.12959	valid_1's l2: 2.19948
[17]	training's l2: 2.11646	valid_1's l2: 2.18833
[18]	training's l2: 2.10344	valid_1's l2: 2.17993
[19]	training's l2: 2.09088	valid_1's l2: 2.17188
[20]	training's l2: 2.07922	valid_1's l2: 2.16454
[21]	training's l2: 2.06793	valid_1's l2: 2.1569
[22]	training's l2: 2.05861	valid_1's l2: 2.15188
[23]	training's l2: 2.04922	valid_1's l2: 2.145
[24]	training's l2: 2.03834	valid_1's l2: 2.14024
[25]	training's l2: 2.02896	valid_1's l2: 2.13525
[26]	training's l2: 2.02026	valid_1's l2: 2.1307
[27]	training's l2: 2.01106	valid_1's l2: 2.12441
[28]	training's l2: 2.00135	valid_1's l2: 2.11834
[29]	training's l2: 1.99389	valid_1's l2: 2.11547
[30]	training's l2: 1.98551	valid_1's l2: 2.11332
[31]	training's l2: 1.97752	valid_1's l2: 2.10806
[32]	training's l2: 1.97005	valid_1's l2: 2.10453
[33]	training's l2: 1.96309	valid_1's l2: 2.10127
[34]	training's l2: 1.95531	valid_1's l2: 2.09789
[35]	training's l2: 1.94777	valid_1's l2: 2.09465
[36]	training's l2: 1.94032	valid_1's l2: 2.09067
[37]	training's l2: 1.9344	valid_1's l2: 2.08769
[38]	training's l2: 1.92804	valid_1's l2: 2.08438
[39]	training's l2: 1.92191	valid_1's l2: 2.08185
[40]	training's l2: 1.91503	valid_1's l2: 2.07851
[41]	training's l2: 1.90792	valid_1's l2: 2.07609
[42]	training's l2: 1.90293	valid_1's l2: 2.07457
[43]	training's l2: 1.89733	valid_1's l2: 2.07273
[44]	training's l2: 1.89153	valid_1's l2: 2.07079
[45]	training's l2: 1.88571	valid_1's l2: 2.0674
[46]	training's l2: 1.88002	valid_1's l2: 2.06725
[47]	training's l2: 1.874	valid_1's l2: 2.06744
[48]	training's l2: 1.86781	valid_1's l2: 2.06659
[49]	training's l2: 1.86258	valid_1's l2: 2.06537
[50]	training's l2: 1.85752	valid_1's l2: 2.06435
[51]	training's l2: 1.85188	valid_1's l2: 2.06185
[52]	training's l2: 1.84693	valid_1's l2: 2.06086
[53]	training's l2: 1.84044	valid_1's l2: 2.06036
[54]	training's l2: 1.8351	valid_1's l2: 2.05932
[55]	training's l2: 1.83056	valid_1's l2: 2.0576
[56]	training's l2: 1.82606	valid_1's l2: 2.05638
[57]	training's l2: 1.8218	valid_1's l2: 2.05624
[58]	training's l2: 1.81741	valid_1's l2: 2.05575
[59]	training's l2: 1.81274	valid_1's l2: 2.05428
[60]	training's l2: 1.80857	valid_1's l2: 2.05458
[61]	training's l2: 1.80215	valid_1's l2: 2.05152
[62]	training's l2: 1.79835	valid_1's l2: 2.05239
[63]	training's l2: 1.79397	valid_1's l2: 2.05223
[64]	training's l2: 1.78941	valid_1's l2: 2.05194
[65]	training's l2: 1.78506	valid_1's l2: 2.05235
[66]	training's l2: 1.78152	valid_1's l2: 2.05345
[67]	training's l2: 1.77798	valid_1's l2: 2.05309
[68]	training's l2: 1.77226	valid_1's l2: 2.05132
[69]	training's l2: 1.76812	valid_1's l2: 2.04963
[70]	training's l2: 1.76368	valid_1's l2: 2.0521
[71]	training's l2: 1.75949	valid_1's l2: 2.05146
[72]	training's l2: 1.75571	valid_1's l2: 2.0518
[73]	training's l2: 1.75232	valid_1's l2: 2.05269
[74]	training's l2: 1.74831	valid_1's l2: 2.05082
[75]	training's l2: 1.74417	valid_1's l2: 2.05048
[76]	training's l2: 1.74094	valid_1's l2: 2.0505
[77]	training's l2: 1.73738	valid_1's l2: 2.04987
[78]	training's l2: 1.73381	valid_1's l2: 2.05004
[79]	training's l2: 1.73021	valid_1's l2: 2.0492
[80]	training's l2: 1.72651	valid_1's l2: 2.04825
[81]	training's l2: 1.72317	valid_1's l2: 2.04786
[82]	training's l2: 1.71959	valid_1's l2: 2.04742
[83]	training's l2: 1.71633	valid_1's l2: 2.0475
[84]	training's l2: 1.71325	valid_1's l2: 2.04733
[85]	training's l2: 1.70952	valid_1's l2: 2.04533
[86]	training's l2: 1.70621	valid_1's l2: 2.04542
[87]	training's l2: 1.70304	valid_1's l2: 2.04561
[88]	training's l2: 1.69931	valid_1's l2: 2.04596
[89]	training's l2: 1.69587	valid_1's l2: 2.04522
[90]	training's l2: 1.69258	valid_1's l2: 2.04599
[91]	training's l2: 1.68975	valid_1's l2: 2.04659
[92]	training's l2: 1.68692	valid_1's l2: 2.0472
[93]	training's l2: 1.68404	valid_1's l2: 2.04715
[94]	training's l2: 1.681	valid_1's l2: 2.04638
[95]	training's l2: 1.67723	valid_1's l2: 2.04671
[96]	training's l2: 1.67362	valid_1's l2: 2.04737
[97]	training's l2: 1.67067	valid_1's l2: 2.04651
[98]	training's l2: 1.66727	valid_1's l2: 2.04586
[99]	training's l2: 1.6637	valid_1's l2: 2.04403
[100]	training's l2: 1.66095	valid_1's l2: 2.04449
[101]	training's l2: 1.65822	valid_1's l2: 2.04506
[102]	training's l2: 1.65297	valid_1's l2: 2.04345
[103]	training's l2: 1.65032	valid_1's l2: 2.04416
[104]	training's l2: 1.64737	valid_1's l2: 2.04316
[105]	training's l2: 1.64409	valid_1's l2: 2.0436
[106]	training's l2: 1.63901	valid_1's l2: 2.03982
[107]	training's l2: 1.63588	valid_1's l2: 2.03987
[108]	training's l2: 1.63323	valid_1's l2: 2.04015
[109]	training's l2: 1.6304	valid_1's l2: 2.04114
[110]	training's l2: 1.62753	valid_1's l2: 2.0402
[111]	training's l2: 1.62464	valid_1's l2: 2.03984
[112]	training's l2: 1.62101	valid_1's l2: 2.03925
[113]	training's l2: 1.61826	valid_1's l2: 2.03941
[114]	training's l2: 1.61532	valid_1's l2: 2.03888
[115]	training's l2: 1.61211	valid_1's l2: 2.03808
[116]	training's l2: 1.60979	valid_1's l2: 2.03789
[117]	training's l2: 1.60695	valid_1's l2: 2.03788
[118]	training's l2: 1.60464	valid_1's l2: 2.03773
[119]	training's l2: 1.60174	valid_1's l2: 2.03808
[120]	training's l2: 1.59889	valid_1's l2: 2.0382
[121]	training's l2: 1.59624	valid_1's l2: 2.03738
[122]	training's l2: 1.59407	valid_1's l2: 2.03703
[123]	training's l2: 1.59151	valid_1's l2: 2.03739
[124]	training's l2: 1.58859	valid_1's l2: 2.03721
[125]	training's l2: 1.5858	valid_1's l2: 2.0387
[126]	training's l2: 1.58322	valid_1's l2: 2.03886
[127]	training's l2: 1.58062	valid_1's l2: 2.03821
[128]	training's l2: 1.57839	valid_1's l2: 2.03806
[129]	training's l2: 1.57585	valid_1's l2: 2.03808
[130]	training's l2: 1.57294	valid_1's l2: 2.03736
[131]	training's l2: 1.57061	valid_1's l2: 2.0382
[132]	training's l2: 1.56807	valid_1's l2: 2.03899
[133]	training's l2: 1.56561	valid_1's l2: 2.03893
[134]	training's l2: 1.56293	valid_1's l2: 2.03785
[135]	training's l2: 1.56051	valid_1's l2: 2.03839
[136]	training's l2: 1.55796	valid_1's l2: 2.0385
[137]	training's l2: 1.55548	valid_1's l2: 2.03857
[138]	training's l2: 1.55283	valid_1's l2: 2.03846
[139]	training's l2: 1.55045	valid_1's l2: 2.03833
[140]	training's l2: 1.54781	valid_1's l2: 2.03791
[141]	training's l2: 1.54491	valid_1's l2: 2.03778
[142]	training's l2: 1.54206	valid_1's l2: 2.03727
[143]	training's l2: 1.53905	valid_1's l2: 2.03666
[144]	training's l2: 1.53688	valid_1's l2: 2.0363
[145]	training's l2: 1.53469	valid_1's l2: 2.03622
[146]	training's l2: 1.53228	valid_1's l2: 2.036
[147]	training's l2: 1.52968	valid_1's l2: 2.03643
[148]	training's l2: 1.52718	valid_1's l2: 2.03655
[149]	training's l2: 1.5247	valid_1's l2: 2.0362
[150]	training's l2: 1.52186	valid_1's l2: 2.0359
[151]	training's l2: 1.51941	valid_1's l2: 2.03682
[152]	training's l2: 1.51737	valid_1's l2: 2.03696
[153]	training's l2: 1.51454	valid_1's l2: 2.03769
[154]	training's l2: 1.51255	valid_1's l2: 2.03702
[155]	training's l2: 1.50994	valid_1's l2: 2.03654
[156]	training's l2: 1.50754	valid_1's l2: 2.03687
[157]	training's l2: 1.50526	valid_1's l2: 2.03767
[158]	training's l2: 1.50346	valid_1's l2: 2.03729
[159]	training's l2: 1.50072	valid_1's l2: 2.03716
[160]	training's l2: 1.49831	valid_1's l2: 2.03721
[161]	training's l2: 1.49571	valid_1's l2: 2.03724
[162]	training's l2: 1.49347	valid_1's l2: 2.03665
[163]	training's l2: 1.49048	valid_1's l2: 2.03565
[164]	training's l2: 1.48842	valid_1's l2: 2.03561
[165]	training's l2: 1.48601	valid_1's l2: 2.03474
[166]	training's l2: 1.48315	valid_1's l2: 2.03438
[167]	training's l2: 1.48084	valid_1's l2: 2.03466
[168]	training's l2: 1.47879	valid_1's l2: 2.03529
[169]	training's l2: 1.47679	valid_1's l2: 2.03496
[170]	training's l2: 1.47493	valid_1's l2: 2.03684
[171]	training's l2: 1.47296	valid_1's l2: 2.03635
[172]	training's l2: 1.47088	valid_1's l2: 2.03621
[173]	training's l2: 1.46887	valid_1's l2: 2.03639
[174]	training's l2: 1.46621	valid_1's l2: 2.03616
[175]	training's l2: 1.46405	valid_1's l2: 2.03692
[176]	training's l2: 1.4617	valid_1's l2: 2.03722
Did not meet early stopping. Best iteration is:
[176]	training's l2: 1.4617	valid_1's l2: 2.03722
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242733 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59646	valid_1's l2: 2.65157
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.55629	valid_1's l2: 2.61461
[3]	training's l2: 2.51744	valid_1's l2: 2.58022
[4]	training's l2: 2.48211	valid_1's l2: 2.54908
[5]	training's l2: 2.44924	valid_1's l2: 2.52051
[6]	training's l2: 2.41727	valid_1's l2: 2.49345
[7]	training's l2: 2.38843	valid_1's l2: 2.46925
[8]	training's l2: 2.35934	valid_1's l2: 2.44531
[9]	training's l2: 2.33344	valid_1's l2: 2.42398
[10]	training's l2: 2.30738	valid_1's l2: 2.40282
[11]	training's l2: 2.28404	valid_1's l2: 2.38301
[12]	training's l2: 2.26026	valid_1's l2: 2.36414
[13]	training's l2: 2.23836	valid_1's l2: 2.3468
[14]	training's l2: 2.21851	valid_1's l2: 2.33184
[15]	training's l2: 2.19846	valid_1's l2: 2.31653
[16]	training's l2: 2.17921	valid_1's l2: 2.30336
[17]	training's l2: 2.16167	valid_1's l2: 2.29021
[18]	training's l2: 2.14473	valid_1's l2: 2.27901
[19]	training's l2: 2.12801	valid_1's l2: 2.26727
[20]	training's l2: 2.11234	valid_1's l2: 2.2571
[21]	training's l2: 2.09655	valid_1's l2: 2.24719
[22]	training's l2: 2.08248	valid_1's l2: 2.23839
[23]	training's l2: 2.06797	valid_1's l2: 2.22932
[24]	training's l2: 2.05423	valid_1's l2: 2.22073
[25]	training's l2: 2.04082	valid_1's l2: 2.21232
[26]	training's l2: 2.02845	valid_1's l2: 2.20591
[27]	training's l2: 2.01543	valid_1's l2: 2.1983
[28]	training's l2: 2.00372	valid_1's l2: 2.19216
[29]	training's l2: 1.99274	valid_1's l2: 2.18595
[30]	training's l2: 1.98191	valid_1's l2: 2.18048
[31]	training's l2: 1.97153	valid_1's l2: 2.17507
[32]	training's l2: 1.96081	valid_1's l2: 2.16793
[33]	training's l2: 1.95074	valid_1's l2: 2.1639
[34]	training's l2: 1.94133	valid_1's l2: 2.15959
[35]	training's l2: 1.93176	valid_1's l2: 2.15555
[36]	training's l2: 1.92245	valid_1's l2: 2.15104
[37]	training's l2: 1.91359	valid_1's l2: 2.14836
[38]	training's l2: 1.90396	valid_1's l2: 2.14391
[39]	training's l2: 1.8949	valid_1's l2: 2.13982
[40]	training's l2: 1.88676	valid_1's l2: 2.13706
[41]	training's l2: 1.87788	valid_1's l2: 2.13368
[42]	training's l2: 1.86992	valid_1's l2: 2.13096
[43]	training's l2: 1.86179	valid_1's l2: 2.12755
[44]	training's l2: 1.85328	valid_1's l2: 2.1247
[45]	training's l2: 1.84584	valid_1's l2: 2.12201
[46]	training's l2: 1.83861	valid_1's l2: 2.11987
[47]	training's l2: 1.83081	valid_1's l2: 2.11664
[48]	training's l2: 1.82329	valid_1's l2: 2.11486
[49]	training's l2: 1.81628	valid_1's l2: 2.11271
[50]	training's l2: 1.80896	valid_1's l2: 2.10968
[51]	training's l2: 1.80174	valid_1's l2: 2.10799
[52]	training's l2: 1.79432	valid_1's l2: 2.10518
[53]	training's l2: 1.78757	valid_1's l2: 2.10417
[54]	training's l2: 1.78085	valid_1's l2: 2.10281
[55]	training's l2: 1.77405	valid_1's l2: 2.10076
[56]	training's l2: 1.76675	valid_1's l2: 2.09863
[57]	training's l2: 1.75991	valid_1's l2: 2.09742
[58]	training's l2: 1.75364	valid_1's l2: 2.0964
[59]	training's l2: 1.7468	valid_1's l2: 2.09432
[60]	training's l2: 1.74029	valid_1's l2: 2.09203
[61]	training's l2: 1.7341	valid_1's l2: 2.09017
[62]	training's l2: 1.7284	valid_1's l2: 2.08909
[63]	training's l2: 1.721	valid_1's l2: 2.08625
[64]	training's l2: 1.71467	valid_1's l2: 2.08458
[65]	training's l2: 1.70879	valid_1's l2: 2.08201
[66]	training's l2: 1.70286	valid_1's l2: 2.0801
[67]	training's l2: 1.69585	valid_1's l2: 2.07787
[68]	training's l2: 1.69031	valid_1's l2: 2.07669
[69]	training's l2: 1.68508	valid_1's l2: 2.07531
[70]	training's l2: 1.67834	valid_1's l2: 2.07382
[71]	training's l2: 1.67277	valid_1's l2: 2.07343
[72]	training's l2: 1.66625	valid_1's l2: 2.07168
[73]	training's l2: 1.66046	valid_1's l2: 2.07081
[74]	training's l2: 1.65507	valid_1's l2: 2.06952
[75]	training's l2: 1.65029	valid_1's l2: 2.06933
[76]	training's l2: 1.64471	valid_1's l2: 2.06846
[77]	training's l2: 1.6393	valid_1's l2: 2.06658
[78]	training's l2: 1.63301	valid_1's l2: 2.06553
[79]	training's l2: 1.62789	valid_1's l2: 2.06549
[80]	training's l2: 1.62155	valid_1's l2: 2.06364
[81]	training's l2: 1.61568	valid_1's l2: 2.06203
[82]	training's l2: 1.6101	valid_1's l2: 2.06086
[83]	training's l2: 1.6052	valid_1's l2: 2.05958
[84]	training's l2: 1.59969	valid_1's l2: 2.05929
[85]	training's l2: 1.59399	valid_1's l2: 2.05851
[86]	training's l2: 1.5894	valid_1's l2: 2.05821
[87]	training's l2: 1.585	valid_1's l2: 2.05841
[88]	training's l2: 1.57985	valid_1's l2: 2.05823
[89]	training's l2: 1.57484	valid_1's l2: 2.05834
[90]	training's l2: 1.56978	valid_1's l2: 2.057
[91]	training's l2: 1.56475	valid_1's l2: 2.05627
[92]	training's l2: 1.55946	valid_1's l2: 2.05522
[93]	training's l2: 1.55521	valid_1's l2: 2.05576
[94]	training's l2: 1.55048	valid_1's l2: 2.05568
[95]	training's l2: 1.54534	valid_1's l2: 2.0545
[96]	training's l2: 1.5407	valid_1's l2: 2.05358
[97]	training's l2: 1.53629	valid_1's l2: 2.0531
[98]	training's l2: 1.53233	valid_1's l2: 2.05255
[99]	training's l2: 1.52743	valid_1's l2: 2.0518
[100]	training's l2: 1.52333	valid_1's l2: 2.05119
[101]	training's l2: 1.51873	valid_1's l2: 2.0498
[102]	training's l2: 1.51415	valid_1's l2: 2.04893
[103]	training's l2: 1.51017	valid_1's l2: 2.04838
[104]	training's l2: 1.50613	valid_1's l2: 2.04833
[105]	training's l2: 1.50132	valid_1's l2: 2.04849
[106]	training's l2: 1.49728	valid_1's l2: 2.04822
[107]	training's l2: 1.49305	valid_1's l2: 2.04769
[108]	training's l2: 1.48867	valid_1's l2: 2.04756
[109]	training's l2: 1.48434	valid_1's l2: 2.04661
[110]	training's l2: 1.48014	valid_1's l2: 2.04557
[111]	training's l2: 1.47609	valid_1's l2: 2.04525
[112]	training's l2: 1.47191	valid_1's l2: 2.0447
[113]	training's l2: 1.46795	valid_1's l2: 2.04453
[114]	training's l2: 1.4643	valid_1's l2: 2.04444
[115]	training's l2: 1.46083	valid_1's l2: 2.04401
[116]	training's l2: 1.45716	valid_1's l2: 2.04423
[117]	training's l2: 1.45365	valid_1's l2: 2.04494
[118]	training's l2: 1.44913	valid_1's l2: 2.04362
[119]	training's l2: 1.44546	valid_1's l2: 2.04318
[120]	training's l2: 1.44148	valid_1's l2: 2.04218
[121]	training's l2: 1.43741	valid_1's l2: 2.04196
[122]	training's l2: 1.43381	valid_1's l2: 2.04164
[123]	training's l2: 1.42996	valid_1's l2: 2.04122
[124]	training's l2: 1.42628	valid_1's l2: 2.04108
[125]	training's l2: 1.42306	valid_1's l2: 2.04071
[126]	training's l2: 1.4191	valid_1's l2: 2.04063
[127]	training's l2: 1.41571	valid_1's l2: 2.04058
[128]	training's l2: 1.41238	valid_1's l2: 2.04033
[129]	training's l2: 1.40861	valid_1's l2: 2.04085
[130]	training's l2: 1.40506	valid_1's l2: 2.04066
[131]	training's l2: 1.40191	valid_1's l2: 2.04087
[132]	training's l2: 1.3984	valid_1's l2: 2.04156
[133]	training's l2: 1.39521	valid_1's l2: 2.04127
[134]	training's l2: 1.39162	valid_1's l2: 2.04131
[135]	training's l2: 1.38818	valid_1's l2: 2.04094
[136]	training's l2: 1.38454	valid_1's l2: 2.04096
[137]	training's l2: 1.38088	valid_1's l2: 2.04052
[138]	training's l2: 1.37772	valid_1's l2: 2.04059
[139]	training's l2: 1.37433	valid_1's l2: 2.04065
[140]	training's l2: 1.37064	valid_1's l2: 2.03974
[141]	training's l2: 1.36718	valid_1's l2: 2.03931
[142]	training's l2: 1.3637	valid_1's l2: 2.03892
[143]	training's l2: 1.36041	valid_1's l2: 2.03859
[144]	training's l2: 1.35733	valid_1's l2: 2.03878
[145]	training's l2: 1.35369	valid_1's l2: 2.03816
[146]	training's l2: 1.35043	valid_1's l2: 2.03887
[147]	training's l2: 1.3473	valid_1's l2: 2.03857
[148]	training's l2: 1.34409	valid_1's l2: 2.03919
[149]	training's l2: 1.3407	valid_1's l2: 2.03959
[150]	training's l2: 1.33753	valid_1's l2: 2.03987
[151]	training's l2: 1.33449	valid_1's l2: 2.03974
[152]	training's l2: 1.33033	valid_1's l2: 2.03874
[153]	training's l2: 1.32725	valid_1's l2: 2.03902
[154]	training's l2: 1.32388	valid_1's l2: 2.03982
[155]	training's l2: 1.32085	valid_1's l2: 2.03973
[156]	training's l2: 1.31756	valid_1's l2: 2.03952
[157]	training's l2: 1.31452	valid_1's l2: 2.03957
[158]	training's l2: 1.31161	valid_1's l2: 2.03934
[159]	training's l2: 1.30825	valid_1's l2: 2.0388
[160]	training's l2: 1.30523	valid_1's l2: 2.03849
[161]	training's l2: 1.30199	valid_1's l2: 2.03821
[162]	training's l2: 1.29906	valid_1's l2: 2.03858
[163]	training's l2: 1.29619	valid_1's l2: 2.03927
[164]	training's l2: 1.29296	valid_1's l2: 2.03898
[165]	training's l2: 1.29017	valid_1's l2: 2.03841
[166]	training's l2: 1.28711	valid_1's l2: 2.03815
[167]	training's l2: 1.2841	valid_1's l2: 2.03808
[168]	training's l2: 1.28148	valid_1's l2: 2.03912
[169]	training's l2: 1.27877	valid_1's l2: 2.0394
[170]	training's l2: 1.27573	valid_1's l2: 2.0394
[171]	training's l2: 1.27304	valid_1's l2: 2.03928
[172]	training's l2: 1.27019	valid_1's l2: 2.03985
[173]	training's l2: 1.2677	valid_1's l2: 2.03978
[174]	training's l2: 1.26467	valid_1's l2: 2.03995
[175]	training's l2: 1.26211	valid_1's l2: 2.03997
[176]	training's l2: 1.25889	valid_1's l2: 2.0398
Did not meet early stopping. Best iteration is:
[176]	training's l2: 1.25889	valid_1's l2: 2.0398
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.236238 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59495	valid_1's l2: 2.64911
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.55364	valid_1's l2: 2.61
[3]	training's l2: 2.51398	valid_1's l2: 2.57271
[4]	training's l2: 2.47833	valid_1's l2: 2.53972
[5]	training's l2: 2.44414	valid_1's l2: 2.50838
[6]	training's l2: 2.41341	valid_1's l2: 2.48049
[7]	training's l2: 2.3827	valid_1's l2: 2.45384
[8]	training's l2: 2.35533	valid_1's l2: 2.43026
[9]	training's l2: 2.32858	valid_1's l2: 2.40687
[10]	training's l2: 2.30462	valid_1's l2: 2.3871
[11]	training's l2: 2.28022	valid_1's l2: 2.36638
[12]	training's l2: 2.25905	valid_1's l2: 2.34733
[13]	training's l2: 2.23811	valid_1's l2: 2.33019
[14]	training's l2: 2.21773	valid_1's l2: 2.31393
[15]	training's l2: 2.19987	valid_1's l2: 2.29955
[16]	training's l2: 2.18157	valid_1's l2: 2.28655
[17]	training's l2: 2.16467	valid_1's l2: 2.27421
[18]	training's l2: 2.14792	valid_1's l2: 2.26203
[19]	training's l2: 2.13204	valid_1's l2: 2.25094
[20]	training's l2: 2.11799	valid_1's l2: 2.24176
[21]	training's l2: 2.10352	valid_1's l2: 2.2318
[22]	training's l2: 2.08984	valid_1's l2: 2.22252
[23]	training's l2: 2.07696	valid_1's l2: 2.21444
[24]	training's l2: 2.06437	valid_1's l2: 2.20558
[25]	training's l2: 2.05233	valid_1's l2: 2.19799
[26]	training's l2: 2.04112	valid_1's l2: 2.19051
[27]	training's l2: 2.02991	valid_1's l2: 2.18499
[28]	training's l2: 2.01937	valid_1's l2: 2.17917
[29]	training's l2: 2.009	valid_1's l2: 2.17345
[30]	training's l2: 1.99851	valid_1's l2: 2.16705
[31]	training's l2: 1.9889	valid_1's l2: 2.16235
[32]	training's l2: 1.97935	valid_1's l2: 2.15689
[33]	training's l2: 1.97016	valid_1's l2: 2.15144
[34]	training's l2: 1.96132	valid_1's l2: 2.14848
[35]	training's l2: 1.95139	valid_1's l2: 2.14346
[36]	training's l2: 1.94277	valid_1's l2: 2.13953
[37]	training's l2: 1.93353	valid_1's l2: 2.13568
[38]	training's l2: 1.92494	valid_1's l2: 2.13146
[39]	training's l2: 1.91697	valid_1's l2: 2.12818
[40]	training's l2: 1.90921	valid_1's l2: 2.12511
[41]	training's l2: 1.90128	valid_1's l2: 2.12266
[42]	training's l2: 1.89362	valid_1's l2: 2.11974
[43]	training's l2: 1.8865	valid_1's l2: 2.11696
[44]	training's l2: 1.87909	valid_1's l2: 2.11376
[45]	training's l2: 1.87185	valid_1's l2: 2.11228
[46]	training's l2: 1.86481	valid_1's l2: 2.10972
[47]	training's l2: 1.85765	valid_1's l2: 2.10713
[48]	training's l2: 1.85095	valid_1's l2: 2.10626
[49]	training's l2: 1.8443	valid_1's l2: 2.10462
[50]	training's l2: 1.83756	valid_1's l2: 2.10204
[51]	training's l2: 1.83094	valid_1's l2: 2.10082
[52]	training's l2: 1.82512	valid_1's l2: 2.09962
[53]	training's l2: 1.81799	valid_1's l2: 2.09689
[54]	training's l2: 1.81178	valid_1's l2: 2.09495
[55]	training's l2: 1.80612	valid_1's l2: 2.09437
[56]	training's l2: 1.79969	valid_1's l2: 2.09232
[57]	training's l2: 1.79296	valid_1's l2: 2.08984
[58]	training's l2: 1.78686	valid_1's l2: 2.08778
[59]	training's l2: 1.78134	valid_1's l2: 2.08639
[60]	training's l2: 1.77489	valid_1's l2: 2.08362
[61]	training's l2: 1.76855	valid_1's l2: 2.08179
[62]	training's l2: 1.76312	valid_1's l2: 2.08086
[63]	training's l2: 1.75711	valid_1's l2: 2.08021
[64]	training's l2: 1.75154	valid_1's l2: 2.07905
[65]	training's l2: 1.74623	valid_1's l2: 2.07852
[66]	training's l2: 1.74	valid_1's l2: 2.07655
[67]	training's l2: 1.73477	valid_1's l2: 2.07575
[68]	training's l2: 1.72836	valid_1's l2: 2.07368
[69]	training's l2: 1.72336	valid_1's l2: 2.07335
[70]	training's l2: 1.71751	valid_1's l2: 2.07133
[71]	training's l2: 1.71214	valid_1's l2: 2.0707
[72]	training's l2: 1.70764	valid_1's l2: 2.06978
[73]	training's l2: 1.70264	valid_1's l2: 2.0683
[74]	training's l2: 1.6962	valid_1's l2: 2.06561
[75]	training's l2: 1.69056	valid_1's l2: 2.06362
[76]	training's l2: 1.68526	valid_1's l2: 2.06165
[77]	training's l2: 1.68018	valid_1's l2: 2.0603
[78]	training's l2: 1.67537	valid_1's l2: 2.05963
[79]	training's l2: 1.67078	valid_1's l2: 2.05935
[80]	training's l2: 1.66585	valid_1's l2: 2.05938
[81]	training's l2: 1.66142	valid_1's l2: 2.05917
[82]	training's l2: 1.65699	valid_1's l2: 2.05943
[83]	training's l2: 1.65154	valid_1's l2: 2.05765
[84]	training's l2: 1.64736	valid_1's l2: 2.05789
[85]	training's l2: 1.64317	valid_1's l2: 2.05829
[86]	training's l2: 1.63814	valid_1's l2: 2.05724
[87]	training's l2: 1.63342	valid_1's l2: 2.05671
[88]	training's l2: 1.62931	valid_1's l2: 2.05656
[89]	training's l2: 1.62391	valid_1's l2: 2.05474
[90]	training's l2: 1.6197	valid_1's l2: 2.05389
[91]	training's l2: 1.61556	valid_1's l2: 2.05359
[92]	training's l2: 1.61138	valid_1's l2: 2.05348
[93]	training's l2: 1.60721	valid_1's l2: 2.05279
[94]	training's l2: 1.60326	valid_1's l2: 2.05322
[95]	training's l2: 1.59839	valid_1's l2: 2.05256
[96]	training's l2: 1.59374	valid_1's l2: 2.05225
[97]	training's l2: 1.5898	valid_1's l2: 2.05156
[98]	training's l2: 1.58598	valid_1's l2: 2.05059
[99]	training's l2: 1.58184	valid_1's l2: 2.04999
[100]	training's l2: 1.57724	valid_1's l2: 2.04913
[101]	training's l2: 1.57383	valid_1's l2: 2.04915
[102]	training's l2: 1.56989	valid_1's l2: 2.0485
[103]	training's l2: 1.56557	valid_1's l2: 2.04811
[104]	training's l2: 1.56199	valid_1's l2: 2.0481
[105]	training's l2: 1.55801	valid_1's l2: 2.04817
[106]	training's l2: 1.55418	valid_1's l2: 2.04724
[107]	training's l2: 1.55018	valid_1's l2: 2.04572
[108]	training's l2: 1.54615	valid_1's l2: 2.04463
[109]	training's l2: 1.54241	valid_1's l2: 2.04439
[110]	training's l2: 1.53859	valid_1's l2: 2.04463
[111]	training's l2: 1.53524	valid_1's l2: 2.04496
[112]	training's l2: 1.53124	valid_1's l2: 2.04582
[113]	training's l2: 1.52672	valid_1's l2: 2.04424
[114]	training's l2: 1.52328	valid_1's l2: 2.04389
[115]	training's l2: 1.51982	valid_1's l2: 2.04405
[116]	training's l2: 1.51628	valid_1's l2: 2.04385
[117]	training's l2: 1.51266	valid_1's l2: 2.04412
[118]	training's l2: 1.50922	valid_1's l2: 2.04451
[119]	training's l2: 1.50589	valid_1's l2: 2.04423
[120]	training's l2: 1.50244	valid_1's l2: 2.04437
[121]	training's l2: 1.49905	valid_1's l2: 2.04525
[122]	training's l2: 1.49585	valid_1's l2: 2.04543
[123]	training's l2: 1.49223	valid_1's l2: 2.04605
[124]	training's l2: 1.4888	valid_1's l2: 2.04635
[125]	training's l2: 1.48548	valid_1's l2: 2.04732
[126]	training's l2: 1.48241	valid_1's l2: 2.04763
[127]	training's l2: 1.47929	valid_1's l2: 2.04808
[128]	training's l2: 1.47616	valid_1's l2: 2.04766
[129]	training's l2: 1.47249	valid_1's l2: 2.04756
[130]	training's l2: 1.46914	valid_1's l2: 2.04738
[131]	training's l2: 1.46585	valid_1's l2: 2.04754
[132]	training's l2: 1.46162	valid_1's l2: 2.04696
[133]	training's l2: 1.45828	valid_1's l2: 2.04699
[134]	training's l2: 1.45518	valid_1's l2: 2.04714
[135]	training's l2: 1.45148	valid_1's l2: 2.04641
[136]	training's l2: 1.44811	valid_1's l2: 2.04603
[137]	training's l2: 1.44459	valid_1's l2: 2.04628
[138]	training's l2: 1.44182	valid_1's l2: 2.04641
[139]	training's l2: 1.43881	valid_1's l2: 2.0465
[140]	training's l2: 1.43588	valid_1's l2: 2.04647
Did not meet early stopping. Best iteration is:
[140]	training's l2: 1.43588	valid_1's l2: 2.04647
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219353 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.58724	valid_1's l2: 2.63972
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.53833	valid_1's l2: 2.59287
[3]	training's l2: 2.49506	valid_1's l2: 2.54971
[4]	training's l2: 2.45585	valid_1's l2: 2.51016
[5]	training's l2: 2.41776	valid_1's l2: 2.47522
[6]	training's l2: 2.38455	valid_1's l2: 2.44578
[7]	training's l2: 2.35249	valid_1's l2: 2.41664
[8]	training's l2: 2.32219	valid_1's l2: 2.38809
[9]	training's l2: 2.29626	valid_1's l2: 2.36334
[10]	training's l2: 2.27151	valid_1's l2: 2.34119
[11]	training's l2: 2.24781	valid_1's l2: 2.32175
[12]	training's l2: 2.22733	valid_1's l2: 2.30412
[13]	training's l2: 2.20656	valid_1's l2: 2.28773
[14]	training's l2: 2.18751	valid_1's l2: 2.27358
[15]	training's l2: 2.16889	valid_1's l2: 2.25862
[16]	training's l2: 2.15163	valid_1's l2: 2.24609
[17]	training's l2: 2.13566	valid_1's l2: 2.23375
[18]	training's l2: 2.12014	valid_1's l2: 2.22358
[19]	training's l2: 2.10592	valid_1's l2: 2.21368
[20]	training's l2: 2.09213	valid_1's l2: 2.20494
[21]	training's l2: 2.07914	valid_1's l2: 2.19643
[22]	training's l2: 2.06665	valid_1's l2: 2.18901
[23]	training's l2: 2.05515	valid_1's l2: 2.18159
[24]	training's l2: 2.0438	valid_1's l2: 2.17498
[25]	training's l2: 2.03295	valid_1's l2: 2.16818
[26]	training's l2: 2.02186	valid_1's l2: 2.16206
[27]	training's l2: 2.0115	valid_1's l2: 2.15617
[28]	training's l2: 2.00247	valid_1's l2: 2.15106
[29]	training's l2: 1.9928	valid_1's l2: 2.14665
[30]	training's l2: 1.9834	valid_1's l2: 2.14157
[31]	training's l2: 1.97367	valid_1's l2: 2.13591
[32]	training's l2: 1.96508	valid_1's l2: 2.13276
[33]	training's l2: 1.95635	valid_1's l2: 2.12837
[34]	training's l2: 1.94855	valid_1's l2: 2.1254
[35]	training's l2: 1.93962	valid_1's l2: 2.12105
[36]	training's l2: 1.93186	valid_1's l2: 2.11742
[37]	training's l2: 1.92358	valid_1's l2: 2.11411
[38]	training's l2: 1.91557	valid_1's l2: 2.11087
[39]	training's l2: 1.90744	valid_1's l2: 2.10664
[40]	training's l2: 1.90077	valid_1's l2: 2.10478
[41]	training's l2: 1.89244	valid_1's l2: 2.10133
[42]	training's l2: 1.8854	valid_1's l2: 2.09766
[43]	training's l2: 1.87861	valid_1's l2: 2.0953
[44]	training's l2: 1.8717	valid_1's l2: 2.09341
[45]	training's l2: 1.8645	valid_1's l2: 2.09146
[46]	training's l2: 1.85603	valid_1's l2: 2.08811
[47]	training's l2: 1.84927	valid_1's l2: 2.08472
[48]	training's l2: 1.84244	valid_1's l2: 2.08315
[49]	training's l2: 1.83634	valid_1's l2: 2.08091
[50]	training's l2: 1.83039	valid_1's l2: 2.07976
[51]	training's l2: 1.8243	valid_1's l2: 2.07737
[52]	training's l2: 1.8167	valid_1's l2: 2.07464
[53]	training's l2: 1.81121	valid_1's l2: 2.07316
[54]	training's l2: 1.80542	valid_1's l2: 2.07176
[55]	training's l2: 1.79827	valid_1's l2: 2.06907
[56]	training's l2: 1.79246	valid_1's l2: 2.06746
[57]	training's l2: 1.78768	valid_1's l2: 2.0667
[58]	training's l2: 1.7818	valid_1's l2: 2.06448
[59]	training's l2: 1.77565	valid_1's l2: 2.06349
[60]	training's l2: 1.76863	valid_1's l2: 2.06132
[61]	training's l2: 1.7626	valid_1's l2: 2.05855
[62]	training's l2: 1.75737	valid_1's l2: 2.05714
[63]	training's l2: 1.75048	valid_1's l2: 2.05498
[64]	training's l2: 1.74446	valid_1's l2: 2.05321
[65]	training's l2: 1.73926	valid_1's l2: 2.05216
[66]	training's l2: 1.73274	valid_1's l2: 2.05028
[67]	training's l2: 1.727	valid_1's l2: 2.05101
[68]	training's l2: 1.72231	valid_1's l2: 2.05105
[69]	training's l2: 1.71753	valid_1's l2: 2.05129
[70]	training's l2: 1.71253	valid_1's l2: 2.05035
[71]	training's l2: 1.70686	valid_1's l2: 2.05028
[72]	training's l2: 1.70217	valid_1's l2: 2.04888
[73]	training's l2: 1.69731	valid_1's l2: 2.04813
[74]	training's l2: 1.69304	valid_1's l2: 2.04828
[75]	training's l2: 1.68845	valid_1's l2: 2.04722
[76]	training's l2: 1.683	valid_1's l2: 2.0469
[77]	training's l2: 1.679	valid_1's l2: 2.04653
[78]	training's l2: 1.6746	valid_1's l2: 2.04555
[79]	training's l2: 1.67031	valid_1's l2: 2.04534
[80]	training's l2: 1.66556	valid_1's l2: 2.04316
[81]	training's l2: 1.66098	valid_1's l2: 2.04301
[82]	training's l2: 1.65747	valid_1's l2: 2.04281
[83]	training's l2: 1.65365	valid_1's l2: 2.04322
[84]	training's l2: 1.64951	valid_1's l2: 2.04291
[85]	training's l2: 1.64522	valid_1's l2: 2.04199
[86]	training's l2: 1.6413	valid_1's l2: 2.04173
[87]	training's l2: 1.63711	valid_1's l2: 2.04134
[88]	training's l2: 1.63204	valid_1's l2: 2.04048
[89]	training's l2: 1.62828	valid_1's l2: 2.04005
[90]	training's l2: 1.62442	valid_1's l2: 2.04086
[91]	training's l2: 1.62066	valid_1's l2: 2.0403
[92]	training's l2: 1.61696	valid_1's l2: 2.04033
[93]	training's l2: 1.61292	valid_1's l2: 2.04031
[94]	training's l2: 1.60877	valid_1's l2: 2.04091
[95]	training's l2: 1.60478	valid_1's l2: 2.0399
[96]	training's l2: 1.59999	valid_1's l2: 2.03767
[97]	training's l2: 1.59584	valid_1's l2: 2.0371
[98]	training's l2: 1.59228	valid_1's l2: 2.03706
[99]	training's l2: 1.58753	valid_1's l2: 2.03538
[100]	training's l2: 1.58396	valid_1's l2: 2.03622
[101]	training's l2: 1.57986	valid_1's l2: 2.03636
[102]	training's l2: 1.57618	valid_1's l2: 2.03618
[103]	training's l2: 1.57276	valid_1's l2: 2.03643
[104]	training's l2: 1.56954	valid_1's l2: 2.03616
[105]	training's l2: 1.56615	valid_1's l2: 2.03568
[106]	training's l2: 1.56225	valid_1's l2: 2.03559
[107]	training's l2: 1.55885	valid_1's l2: 2.03608
[108]	training's l2: 1.55568	valid_1's l2: 2.03636
[109]	training's l2: 1.55225	valid_1's l2: 2.03678
[110]	training's l2: 1.54888	valid_1's l2: 2.03622
[111]	training's l2: 1.54563	valid_1's l2: 2.03602
[112]	training's l2: 1.54211	valid_1's l2: 2.03616
[113]	training's l2: 1.53849	valid_1's l2: 2.03632
[114]	training's l2: 1.5354	valid_1's l2: 2.0369
[115]	training's l2: 1.53216	valid_1's l2: 2.03638
[116]	training's l2: 1.52901	valid_1's l2: 2.03642
[117]	training's l2: 1.52589	valid_1's l2: 2.03636
[118]	training's l2: 1.52245	valid_1's l2: 2.03574
[119]	training's l2: 1.51903	valid_1's l2: 2.03647
[120]	training's l2: 1.51608	valid_1's l2: 2.03592
[121]	training's l2: 1.5131	valid_1's l2: 2.03548
[122]	training's l2: 1.50965	valid_1's l2: 2.03596
[123]	training's l2: 1.50646	valid_1's l2: 2.0369
[124]	training's l2: 1.50344	valid_1's l2: 2.0367
[125]	training's l2: 1.50021	valid_1's l2: 2.03622
[126]	training's l2: 1.49712	valid_1's l2: 2.03595
[127]	training's l2: 1.49436	valid_1's l2: 2.0367
[128]	training's l2: 1.4917	valid_1's l2: 2.03675
[129]	training's l2: 1.48896	valid_1's l2: 2.0367
Early stopping, best iteration is:
[99]	training's l2: 1.58753	valid_1's l2: 2.03538
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.214533 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000387836	valid_1's l2: 0.000377526
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000378768	valid_1's l2: 0.000369154
[3]	training's l2: 0.00037058	valid_1's l2: 0.000361704
[4]	training's l2: 0.00036322	valid_1's l2: 0.000354858
[5]	training's l2: 0.000356371	valid_1's l2: 0.00034858
[6]	training's l2: 0.000350264	valid_1's l2: 0.000342968
[7]	training's l2: 0.000344634	valid_1's l2: 0.000337744
[8]	training's l2: 0.000339363	valid_1's l2: 0.000333336
[9]	training's l2: 0.000334643	valid_1's l2: 0.000329105
[10]	training's l2: 0.000329975	valid_1's l2: 0.000324975
[11]	training's l2: 0.000326011	valid_1's l2: 0.000321665
[12]	training's l2: 0.000322217	valid_1's l2: 0.000318663
[13]	training's l2: 0.000318625	valid_1's l2: 0.000315518
[14]	training's l2: 0.000315207	valid_1's l2: 0.000312602
[15]	training's l2: 0.000312206	valid_1's l2: 0.000310011
[16]	training's l2: 0.000309341	valid_1's l2: 0.000307538
[17]	training's l2: 0.000306565	valid_1's l2: 0.000305279
[18]	training's l2: 0.000304079	valid_1's l2: 0.00030343
[19]	training's l2: 0.000301545	valid_1's l2: 0.000301562
[20]	training's l2: 0.000299386	valid_1's l2: 0.000299972
[21]	training's l2: 0.000297246	valid_1's l2: 0.000298348
[22]	training's l2: 0.000295252	valid_1's l2: 0.000296852
[23]	training's l2: 0.000293411	valid_1's l2: 0.000295668
[24]	training's l2: 0.000291664	valid_1's l2: 0.000294268
[25]	training's l2: 0.000290005	valid_1's l2: 0.000293051
[26]	training's l2: 0.000288354	valid_1's l2: 0.00029181
[27]	training's l2: 0.000286808	valid_1's l2: 0.000290588
[28]	training's l2: 0.000285342	valid_1's l2: 0.00028963
[29]	training's l2: 0.000283914	valid_1's l2: 0.000288787
[30]	training's l2: 0.000282538	valid_1's l2: 0.000287758
[31]	training's l2: 0.000281337	valid_1's l2: 0.000286966
[32]	training's l2: 0.000280171	valid_1's l2: 0.000286304
[33]	training's l2: 0.000279057	valid_1's l2: 0.000285528
[34]	training's l2: 0.000277754	valid_1's l2: 0.000284831
[35]	training's l2: 0.000276617	valid_1's l2: 0.000284028
[36]	training's l2: 0.000275511	valid_1's l2: 0.000283441
[37]	training's l2: 0.000274358	valid_1's l2: 0.000282716
[38]	training's l2: 0.000273394	valid_1's l2: 0.000282295
[39]	training's l2: 0.000272519	valid_1's l2: 0.000281835
[40]	training's l2: 0.000271162	valid_1's l2: 0.000280979
[41]	training's l2: 0.000270144	valid_1's l2: 0.000280372
[42]	training's l2: 0.000269118	valid_1's l2: 0.000279802
[43]	training's l2: 0.000268182	valid_1's l2: 0.000279385
[44]	training's l2: 0.000267166	valid_1's l2: 0.000278773
[45]	training's l2: 0.000266338	valid_1's l2: 0.000278449
[46]	training's l2: 0.000265458	valid_1's l2: 0.000278066
[47]	training's l2: 0.000264581	valid_1's l2: 0.000277593
[48]	training's l2: 0.000263901	valid_1's l2: 0.000277368
[49]	training's l2: 0.000262796	valid_1's l2: 0.000276839
[50]	training's l2: 0.000262125	valid_1's l2: 0.000276594
[51]	training's l2: 0.000261376	valid_1's l2: 0.000276344
[52]	training's l2: 0.000260611	valid_1's l2: 0.000276073
[53]	training's l2: 0.000259937	valid_1's l2: 0.000275859
[54]	training's l2: 0.000259288	valid_1's l2: 0.000275576
[55]	training's l2: 0.000258389	valid_1's l2: 0.000275253
[56]	training's l2: 0.000257377	valid_1's l2: 0.000274732
[57]	training's l2: 0.00025678	valid_1's l2: 0.000274562
[58]	training's l2: 0.000256124	valid_1's l2: 0.00027438
[59]	training's l2: 0.000255305	valid_1's l2: 0.000274062
[60]	training's l2: 0.000254636	valid_1's l2: 0.000273845
[61]	training's l2: 0.000253824	valid_1's l2: 0.000273411
[62]	training's l2: 0.000253259	valid_1's l2: 0.000273326
[63]	training's l2: 0.000252523	valid_1's l2: 0.000272985
[64]	training's l2: 0.000251983	valid_1's l2: 0.000272873
[65]	training's l2: 0.000251457	valid_1's l2: 0.000272768
[66]	training's l2: 0.000250595	valid_1's l2: 0.000272372
[67]	training's l2: 0.000249866	valid_1's l2: 0.000272245
[68]	training's l2: 0.000249248	valid_1's l2: 0.000272044
[69]	training's l2: 0.000248677	valid_1's l2: 0.000271859
[70]	training's l2: 0.000248154	valid_1's l2: 0.000271752
[71]	training's l2: 0.000247562	valid_1's l2: 0.000271647
[72]	training's l2: 0.000247038	valid_1's l2: 0.000271519
[73]	training's l2: 0.000246407	valid_1's l2: 0.00027125
[74]	training's l2: 0.000245816	valid_1's l2: 0.000271039
[75]	training's l2: 0.000245276	valid_1's l2: 0.000270942
[76]	training's l2: 0.000244518	valid_1's l2: 0.000270609
[77]	training's l2: 0.000244012	valid_1's l2: 0.000270634
[78]	training's l2: 0.00024358	valid_1's l2: 0.00027057
[79]	training's l2: 0.000243036	valid_1's l2: 0.000270455
[80]	training's l2: 0.00024244	valid_1's l2: 0.000270457
[81]	training's l2: 0.000241851	valid_1's l2: 0.000270351
[82]	training's l2: 0.000241343	valid_1's l2: 0.00027016
[83]	training's l2: 0.000240796	valid_1's l2: 0.000269951
[84]	training's l2: 0.000240201	valid_1's l2: 0.00026979
[85]	training's l2: 0.000239605	valid_1's l2: 0.000269585
[86]	training's l2: 0.000239174	valid_1's l2: 0.000269491
[87]	training's l2: 0.000238701	valid_1's l2: 0.000269467
[88]	training's l2: 0.000238277	valid_1's l2: 0.000269331
[89]	training's l2: 0.000237771	valid_1's l2: 0.000269275
[90]	training's l2: 0.000237293	valid_1's l2: 0.000269192
[91]	training's l2: 0.000236881	valid_1's l2: 0.000269137
[92]	training's l2: 0.000236279	valid_1's l2: 0.000269003
[93]	training's l2: 0.000235815	valid_1's l2: 0.000268883
[94]	training's l2: 0.000235358	valid_1's l2: 0.000268862
[95]	training's l2: 0.000234777	valid_1's l2: 0.000268821
[96]	training's l2: 0.000234371	valid_1's l2: 0.000268777
[97]	training's l2: 0.000233931	valid_1's l2: 0.000268672
[98]	training's l2: 0.000233505	valid_1's l2: 0.000268659
[99]	training's l2: 0.000233009	valid_1's l2: 0.000268427
[100]	training's l2: 0.00023262	valid_1's l2: 0.000268439
[101]	training's l2: 0.000232187	valid_1's l2: 0.000268401
[102]	training's l2: 0.000231776	valid_1's l2: 0.000268355
[103]	training's l2: 0.000231393	valid_1's l2: 0.000268311
[104]	training's l2: 0.000230949	valid_1's l2: 0.000268301
[105]	training's l2: 0.000230495	valid_1's l2: 0.000268309
[106]	training's l2: 0.000230102	valid_1's l2: 0.000268286
[107]	training's l2: 0.000229696	valid_1's l2: 0.00026829
[108]	training's l2: 0.000229279	valid_1's l2: 0.000268257
[109]	training's l2: 0.000228767	valid_1's l2: 0.000268064
[110]	training's l2: 0.000228354	valid_1's l2: 0.00026804
[111]	training's l2: 0.00022798	valid_1's l2: 0.000268056
[112]	training's l2: 0.000227603	valid_1's l2: 0.000268076
[113]	training's l2: 0.000227162	valid_1's l2: 0.000268016
[114]	training's l2: 0.000226787	valid_1's l2: 0.000267973
[115]	training's l2: 0.000226395	valid_1's l2: 0.000268035
[116]	training's l2: 0.000225941	valid_1's l2: 0.000267812
[117]	training's l2: 0.00022543	valid_1's l2: 0.000267743
[118]	training's l2: 0.000225082	valid_1's l2: 0.000267716
[119]	training's l2: 0.000224703	valid_1's l2: 0.000267657
[120]	training's l2: 0.000224311	valid_1's l2: 0.00026767
[121]	training's l2: 0.00022389	valid_1's l2: 0.000267615
[122]	training's l2: 0.000223516	valid_1's l2: 0.000267616
[123]	training's l2: 0.000223098	valid_1's l2: 0.000267481
[124]	training's l2: 0.000222721	valid_1's l2: 0.000267385
[125]	training's l2: 0.000222374	valid_1's l2: 0.000267401
[126]	training's l2: 0.000222011	valid_1's l2: 0.000267416
[127]	training's l2: 0.000221627	valid_1's l2: 0.000267351
[128]	training's l2: 0.000221172	valid_1's l2: 0.000267203
[129]	training's l2: 0.000220782	valid_1's l2: 0.000267177
[130]	training's l2: 0.000220369	valid_1's l2: 0.000267213
[131]	training's l2: 0.000219971	valid_1's l2: 0.000267162
[132]	training's l2: 0.000219589	valid_1's l2: 0.000267126
[133]	training's l2: 0.000219249	valid_1's l2: 0.000267097
[134]	training's l2: 0.000218888	valid_1's l2: 0.000267086
[135]	training's l2: 0.000218537	valid_1's l2: 0.000267117
[136]	training's l2: 0.000218214	valid_1's l2: 0.000267047
[137]	training's l2: 0.000217873	valid_1's l2: 0.000267105
[138]	training's l2: 0.00021751	valid_1's l2: 0.00026707
[139]	training's l2: 0.000217171	valid_1's l2: 0.000267092
[140]	training's l2: 0.000216826	valid_1's l2: 0.000267145
[141]	training's l2: 0.000216516	valid_1's l2: 0.000267125
[142]	training's l2: 0.000216154	valid_1's l2: 0.000267136
[143]	training's l2: 0.000215829	valid_1's l2: 0.000267113
[144]	training's l2: 0.000215416	valid_1's l2: 0.000267016
[145]	training's l2: 0.000215091	valid_1's l2: 0.000267027
[146]	training's l2: 0.000214773	valid_1's l2: 0.000266976
[147]	training's l2: 0.000214438	valid_1's l2: 0.000267006
[148]	training's l2: 0.000214104	valid_1's l2: 0.000267029
[149]	training's l2: 0.000213743	valid_1's l2: 0.000266959
[150]	training's l2: 0.000213349	valid_1's l2: 0.000266962
[151]	training's l2: 0.000213004	valid_1's l2: 0.000266888
[152]	training's l2: 0.000212722	valid_1's l2: 0.000266927
[153]	training's l2: 0.000212361	valid_1's l2: 0.000266874
[154]	training's l2: 0.000212034	valid_1's l2: 0.000266914
[155]	training's l2: 0.000211734	valid_1's l2: 0.00026697
[156]	training's l2: 0.000211407	valid_1's l2: 0.000266947
[157]	training's l2: 0.000211086	valid_1's l2: 0.000266885
[158]	training's l2: 0.000210781	valid_1's l2: 0.000266861
[159]	training's l2: 0.000210467	valid_1's l2: 0.000266843
[160]	training's l2: 0.000210154	valid_1's l2: 0.000266783
[161]	training's l2: 0.000209841	valid_1's l2: 0.000266824
[162]	training's l2: 0.000209499	valid_1's l2: 0.000266795
[163]	training's l2: 0.000209177	valid_1's l2: 0.000266793
[164]	training's l2: 0.000208894	valid_1's l2: 0.00026679
[165]	training's l2: 0.000208577	valid_1's l2: 0.000266771
[166]	training's l2: 0.00020828	valid_1's l2: 0.000266771
[167]	training's l2: 0.000207969	valid_1's l2: 0.000266811
[168]	training's l2: 0.000207712	valid_1's l2: 0.000266798
[169]	training's l2: 0.000207342	valid_1's l2: 0.000266774
[170]	training's l2: 0.000207049	valid_1's l2: 0.00026676
[171]	training's l2: 0.000206734	valid_1's l2: 0.000266727
[172]	training's l2: 0.000206396	valid_1's l2: 0.000266662
[173]	training's l2: 0.000206101	valid_1's l2: 0.000266643
[174]	training's l2: 0.000205846	valid_1's l2: 0.000266587
[175]	training's l2: 0.00020553	valid_1's l2: 0.000266506
[176]	training's l2: 0.000205256	valid_1's l2: 0.000266503
[177]	training's l2: 0.000204977	valid_1's l2: 0.000266513
[178]	training's l2: 0.000204676	valid_1's l2: 0.00026651
Did not meet early stopping. Best iteration is:
[178]	training's l2: 0.000204676	valid_1's l2: 0.00026651
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215043 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000387345	valid_1's l2: 0.000377394
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000377785	valid_1's l2: 0.00036881
[3]	training's l2: 0.000369133	valid_1's l2: 0.000361111
[4]	training's l2: 0.00036127	valid_1's l2: 0.000353951
[5]	training's l2: 0.000353984	valid_1's l2: 0.000347503
[6]	training's l2: 0.000347503	valid_1's l2: 0.00034212
[7]	training's l2: 0.000341521	valid_1's l2: 0.00033678
[8]	training's l2: 0.000335803	valid_1's l2: 0.000332236
[9]	training's l2: 0.000330462	valid_1's l2: 0.000327671
[10]	training's l2: 0.000325626	valid_1's l2: 0.000323653
[11]	training's l2: 0.000321004	valid_1's l2: 0.00032005
[12]	training's l2: 0.000316886	valid_1's l2: 0.00031677
[13]	training's l2: 0.000313085	valid_1's l2: 0.000313845
[14]	training's l2: 0.000309695	valid_1's l2: 0.000311206
[15]	training's l2: 0.000306331	valid_1's l2: 0.000308387
[16]	training's l2: 0.000303217	valid_1's l2: 0.000305864
[17]	training's l2: 0.00030021	valid_1's l2: 0.000303536
[18]	training's l2: 0.000297461	valid_1's l2: 0.000301468
[19]	training's l2: 0.000295001	valid_1's l2: 0.000299538
[20]	training's l2: 0.000292474	valid_1's l2: 0.000297745
[21]	training's l2: 0.000290153	valid_1's l2: 0.000296142
[22]	training's l2: 0.000287983	valid_1's l2: 0.000294626
[23]	training's l2: 0.000285858	valid_1's l2: 0.000293206
[24]	training's l2: 0.000283924	valid_1's l2: 0.000291847
[25]	training's l2: 0.000282027	valid_1's l2: 0.00029079
[26]	training's l2: 0.00028025	valid_1's l2: 0.000289748
[27]	training's l2: 0.000278479	valid_1's l2: 0.000288637
[28]	training's l2: 0.000276824	valid_1's l2: 0.000287714
[29]	training's l2: 0.000275148	valid_1's l2: 0.000286782
[30]	training's l2: 0.000273706	valid_1's l2: 0.000286054
[31]	training's l2: 0.000272201	valid_1's l2: 0.000285219
[32]	training's l2: 0.000270781	valid_1's l2: 0.000284515
[33]	training's l2: 0.000269516	valid_1's l2: 0.000283802
[34]	training's l2: 0.000268255	valid_1's l2: 0.000283087
[35]	training's l2: 0.000267047	valid_1's l2: 0.000282347
[36]	training's l2: 0.00026573	valid_1's l2: 0.000281566
[37]	training's l2: 0.000264506	valid_1's l2: 0.000281027
[38]	training's l2: 0.000263475	valid_1's l2: 0.000280557
[39]	training's l2: 0.000262141	valid_1's l2: 0.000279755
[40]	training's l2: 0.000261115	valid_1's l2: 0.000279267
[41]	training's l2: 0.000260023	valid_1's l2: 0.000278709
[42]	training's l2: 0.000258924	valid_1's l2: 0.000278168
[43]	training's l2: 0.00025794	valid_1's l2: 0.000277829
[44]	training's l2: 0.000256801	valid_1's l2: 0.000277237
[45]	training's l2: 0.000255707	valid_1's l2: 0.000276656
[46]	training's l2: 0.000254709	valid_1's l2: 0.000276354
[47]	training's l2: 0.000253792	valid_1's l2: 0.000276115
[48]	training's l2: 0.000252653	valid_1's l2: 0.000275537
[49]	training's l2: 0.000251734	valid_1's l2: 0.000275163
[50]	training's l2: 0.000250486	valid_1's l2: 0.000274475
[51]	training's l2: 0.000249656	valid_1's l2: 0.000274207
[52]	training's l2: 0.000248706	valid_1's l2: 0.000273856
[53]	training's l2: 0.000247551	valid_1's l2: 0.000273326
[54]	training's l2: 0.000246798	valid_1's l2: 0.000273142
[55]	training's l2: 0.000245994	valid_1's l2: 0.000272986
[56]	training's l2: 0.000245191	valid_1's l2: 0.00027272
[57]	training's l2: 0.000244159	valid_1's l2: 0.000272277
[58]	training's l2: 0.000243428	valid_1's l2: 0.000272149
[59]	training's l2: 0.000242658	valid_1's l2: 0.000272045
[60]	training's l2: 0.0002418	valid_1's l2: 0.000271675
[61]	training's l2: 0.000241109	valid_1's l2: 0.00027163
[62]	training's l2: 0.000240354	valid_1's l2: 0.000271458
[63]	training's l2: 0.000239533	valid_1's l2: 0.000271154
[64]	training's l2: 0.000238744	valid_1's l2: 0.00027079
[65]	training's l2: 0.000238097	valid_1's l2: 0.000270793
[66]	training's l2: 0.000237421	valid_1's l2: 0.000270628
[67]	training's l2: 0.000236691	valid_1's l2: 0.000270455
[68]	training's l2: 0.000235909	valid_1's l2: 0.000270165
[69]	training's l2: 0.000235304	valid_1's l2: 0.000270083
[70]	training's l2: 0.00023467	valid_1's l2: 0.000269934
[71]	training's l2: 0.000233873	valid_1's l2: 0.000269785
[72]	training's l2: 0.000233188	valid_1's l2: 0.000269724
[73]	training's l2: 0.000232357	valid_1's l2: 0.000269429
[74]	training's l2: 0.000231528	valid_1's l2: 0.000269195
[75]	training's l2: 0.000230815	valid_1's l2: 0.000268872
[76]	training's l2: 0.00023025	valid_1's l2: 0.000268812
[77]	training's l2: 0.00022947	valid_1's l2: 0.00026856
[78]	training's l2: 0.000228825	valid_1's l2: 0.000268486
[79]	training's l2: 0.00022813	valid_1's l2: 0.000268264
[80]	training's l2: 0.000227502	valid_1's l2: 0.000268206
[81]	training's l2: 0.000226801	valid_1's l2: 0.000267981
[82]	training's l2: 0.000226206	valid_1's l2: 0.00026783
[83]	training's l2: 0.000225554	valid_1's l2: 0.000267691
[84]	training's l2: 0.00022504	valid_1's l2: 0.000267626
[85]	training's l2: 0.000224424	valid_1's l2: 0.000267497
[86]	training's l2: 0.000223853	valid_1's l2: 0.00026743
[87]	training's l2: 0.000223256	valid_1's l2: 0.000267409
[88]	training's l2: 0.000222655	valid_1's l2: 0.000267447
[89]	training's l2: 0.000222132	valid_1's l2: 0.000267391
[90]	training's l2: 0.000221562	valid_1's l2: 0.000267371
[91]	training's l2: 0.000221031	valid_1's l2: 0.000267364
[92]	training's l2: 0.000220474	valid_1's l2: 0.000267293
[93]	training's l2: 0.000219922	valid_1's l2: 0.000267304
[94]	training's l2: 0.000219302	valid_1's l2: 0.000267211
[95]	training's l2: 0.000218768	valid_1's l2: 0.00026718
[96]	training's l2: 0.000218236	valid_1's l2: 0.000267238
[97]	training's l2: 0.000217611	valid_1's l2: 0.000267109
[98]	training's l2: 0.000217101	valid_1's l2: 0.000267099
[99]	training's l2: 0.000216571	valid_1's l2: 0.000267007
[100]	training's l2: 0.000215969	valid_1's l2: 0.000266885
[101]	training's l2: 0.000215312	valid_1's l2: 0.000266807
[102]	training's l2: 0.000214781	valid_1's l2: 0.000266807
[103]	training's l2: 0.000214289	valid_1's l2: 0.00026678
[104]	training's l2: 0.000213739	valid_1's l2: 0.000266705
[105]	training's l2: 0.000213168	valid_1's l2: 0.000266686
[106]	training's l2: 0.000212669	valid_1's l2: 0.000266655
[107]	training's l2: 0.000212185	valid_1's l2: 0.000266625
[108]	training's l2: 0.000211639	valid_1's l2: 0.000266461
[109]	training's l2: 0.000211153	valid_1's l2: 0.000266472
[110]	training's l2: 0.000210665	valid_1's l2: 0.000266352
[111]	training's l2: 0.000210131	valid_1's l2: 0.000266285
[112]	training's l2: 0.000209603	valid_1's l2: 0.000266358
[113]	training's l2: 0.000209121	valid_1's l2: 0.000266265
[114]	training's l2: 0.00020859	valid_1's l2: 0.000266314
[115]	training's l2: 0.000208099	valid_1's l2: 0.00026626
[116]	training's l2: 0.000207602	valid_1's l2: 0.000266238
[117]	training's l2: 0.000207019	valid_1's l2: 0.000266257
[118]	training's l2: 0.000206529	valid_1's l2: 0.000266231
[119]	training's l2: 0.000206008	valid_1's l2: 0.000266192
[120]	training's l2: 0.000205538	valid_1's l2: 0.000266203
[121]	training's l2: 0.000205054	valid_1's l2: 0.000266204
[122]	training's l2: 0.000204523	valid_1's l2: 0.000266054
[123]	training's l2: 0.000204066	valid_1's l2: 0.000266086
[124]	training's l2: 0.00020359	valid_1's l2: 0.000266074
[125]	training's l2: 0.000203093	valid_1's l2: 0.000266211
[126]	training's l2: 0.00020266	valid_1's l2: 0.000266227
[127]	training's l2: 0.000202212	valid_1's l2: 0.000266201
[128]	training's l2: 0.000201758	valid_1's l2: 0.000266159
[129]	training's l2: 0.000201343	valid_1's l2: 0.000266153
[130]	training's l2: 0.000200878	valid_1's l2: 0.000266225
[131]	training's l2: 0.000200456	valid_1's l2: 0.000266305
[132]	training's l2: 0.000200027	valid_1's l2: 0.000266373
[133]	training's l2: 0.000199542	valid_1's l2: 0.000266372
[134]	training's l2: 0.000199113	valid_1's l2: 0.000266473
[135]	training's l2: 0.000198616	valid_1's l2: 0.000266418
[136]	training's l2: 0.000198181	valid_1's l2: 0.000266493
[137]	training's l2: 0.000197799	valid_1's l2: 0.000266497
[138]	training's l2: 0.000197376	valid_1's l2: 0.000266498
[139]	training's l2: 0.000196889	valid_1's l2: 0.000266357
[140]	training's l2: 0.000196458	valid_1's l2: 0.000266354
[141]	training's l2: 0.000196033	valid_1's l2: 0.000266238
[142]	training's l2: 0.000195624	valid_1's l2: 0.000266252
[143]	training's l2: 0.000195193	valid_1's l2: 0.000266302
[144]	training's l2: 0.000194793	valid_1's l2: 0.000266382
[145]	training's l2: 0.000194414	valid_1's l2: 0.000266311
[146]	training's l2: 0.000194016	valid_1's l2: 0.000266326
[147]	training's l2: 0.000193628	valid_1's l2: 0.000266332
[148]	training's l2: 0.000193185	valid_1's l2: 0.000266346
[149]	training's l2: 0.000192735	valid_1's l2: 0.000266375
[150]	training's l2: 0.000192352	valid_1's l2: 0.000266391
[151]	training's l2: 0.000191953	valid_1's l2: 0.000266364
[152]	training's l2: 0.00019155	valid_1's l2: 0.00026641
Did not meet early stopping. Best iteration is:
[152]	training's l2: 0.00019155	valid_1's l2: 0.00026641
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207900 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000383756	valid_1's l2: 0.000373673
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000371584	valid_1's l2: 0.000362242
[3]	training's l2: 0.000361182	valid_1's l2: 0.000352988
[4]	training's l2: 0.000351893	valid_1's l2: 0.000344858
[5]	training's l2: 0.000343868	valid_1's l2: 0.000337305
[6]	training's l2: 0.000336983	valid_1's l2: 0.000331512
[7]	training's l2: 0.000330299	valid_1's l2: 0.000325624
[8]	training's l2: 0.000324585	valid_1's l2: 0.000320623
[9]	training's l2: 0.000319452	valid_1's l2: 0.000316263
[10]	training's l2: 0.000314784	valid_1's l2: 0.000312233
[11]	training's l2: 0.000310525	valid_1's l2: 0.000308638
[12]	training's l2: 0.000306973	valid_1's l2: 0.000305758
[13]	training's l2: 0.00030357	valid_1's l2: 0.000303368
[14]	training's l2: 0.000300386	valid_1's l2: 0.000300796
[15]	training's l2: 0.000297535	valid_1's l2: 0.000298595
[16]	training's l2: 0.000294883	valid_1's l2: 0.000296683
[17]	training's l2: 0.000292188	valid_1's l2: 0.000294585
[18]	training's l2: 0.000290041	valid_1's l2: 0.000293273
[19]	training's l2: 0.000287719	valid_1's l2: 0.000291753
[20]	training's l2: 0.000285695	valid_1's l2: 0.000290443
[21]	training's l2: 0.000283783	valid_1's l2: 0.000288888
[22]	training's l2: 0.000282251	valid_1's l2: 0.000287952
[23]	training's l2: 0.000280683	valid_1's l2: 0.000286859
[24]	training's l2: 0.000279294	valid_1's l2: 0.000286073
[25]	training's l2: 0.000277449	valid_1's l2: 0.000284976
[26]	training's l2: 0.000276106	valid_1's l2: 0.000284134
[27]	training's l2: 0.000274764	valid_1's l2: 0.000283347
[28]	training's l2: 0.000273159	valid_1's l2: 0.000282502
[29]	training's l2: 0.000271854	valid_1's l2: 0.000281687
[30]	training's l2: 0.000270639	valid_1's l2: 0.000281248
[31]	training's l2: 0.000269497	valid_1's l2: 0.000280671
[32]	training's l2: 0.000268406	valid_1's l2: 0.000280334
[33]	training's l2: 0.000266966	valid_1's l2: 0.000279525
[34]	training's l2: 0.000265882	valid_1's l2: 0.000278947
[35]	training's l2: 0.000264814	valid_1's l2: 0.000278566
[36]	training's l2: 0.000263709	valid_1's l2: 0.000277973
[37]	training's l2: 0.000262823	valid_1's l2: 0.000277561
[38]	training's l2: 0.000261336	valid_1's l2: 0.000276773
[39]	training's l2: 0.000260467	valid_1's l2: 0.000276604
[40]	training's l2: 0.000259606	valid_1's l2: 0.000276373
[41]	training's l2: 0.000258433	valid_1's l2: 0.000275805
[42]	training's l2: 0.000257616	valid_1's l2: 0.000275494
[43]	training's l2: 0.000256867	valid_1's l2: 0.000275272
[44]	training's l2: 0.000256072	valid_1's l2: 0.000275102
[45]	training's l2: 0.000254844	valid_1's l2: 0.00027451
[46]	training's l2: 0.000254201	valid_1's l2: 0.000274351
[47]	training's l2: 0.000253342	valid_1's l2: 0.000273939
[48]	training's l2: 0.000252512	valid_1's l2: 0.000273681
[49]	training's l2: 0.000251558	valid_1's l2: 0.000273278
[50]	training's l2: 0.000250707	valid_1's l2: 0.000272975
[51]	training's l2: 0.00024997	valid_1's l2: 0.000272756
[52]	training's l2: 0.000249245	valid_1's l2: 0.000272765
[53]	training's l2: 0.00024862	valid_1's l2: 0.000272578
[54]	training's l2: 0.000247847	valid_1's l2: 0.000272292
[55]	training's l2: 0.000247043	valid_1's l2: 0.000271906
[56]	training's l2: 0.000246364	valid_1's l2: 0.000271878
[57]	training's l2: 0.000245582	valid_1's l2: 0.000271563
[58]	training's l2: 0.000245021	valid_1's l2: 0.000271478
[59]	training's l2: 0.000244307	valid_1's l2: 0.000271174
[60]	training's l2: 0.000243746	valid_1's l2: 0.000271056
[61]	training's l2: 0.000243109	valid_1's l2: 0.000271025
[62]	training's l2: 0.000242351	valid_1's l2: 0.000270882
[63]	training's l2: 0.000241716	valid_1's l2: 0.00027083
[64]	training's l2: 0.000241091	valid_1's l2: 0.000270773
[65]	training's l2: 0.000240579	valid_1's l2: 0.000270673
[66]	training's l2: 0.000239859	valid_1's l2: 0.000270538
[67]	training's l2: 0.000239256	valid_1's l2: 0.000270455
[68]	training's l2: 0.000238465	valid_1's l2: 0.000270251
[69]	training's l2: 0.000237896	valid_1's l2: 0.000270147
[70]	training's l2: 0.000237443	valid_1's l2: 0.000269974
[71]	training's l2: 0.000236799	valid_1's l2: 0.000269826
[72]	training's l2: 0.000236235	valid_1's l2: 0.000269848
[73]	training's l2: 0.000235401	valid_1's l2: 0.000269507
[74]	training's l2: 0.000234873	valid_1's l2: 0.00026948
[75]	training's l2: 0.000234339	valid_1's l2: 0.000269424
[76]	training's l2: 0.000233727	valid_1's l2: 0.000269168
[77]	training's l2: 0.000233198	valid_1's l2: 0.000269137
[78]	training's l2: 0.000232503	valid_1's l2: 0.000268937
[79]	training's l2: 0.0002319	valid_1's l2: 0.0002688
[80]	training's l2: 0.000231305	valid_1's l2: 0.000268698
[81]	training's l2: 0.000230785	valid_1's l2: 0.000268587
[82]	training's l2: 0.000230223	valid_1's l2: 0.000268444
[83]	training's l2: 0.000229784	valid_1's l2: 0.000268488
[84]	training's l2: 0.000229165	valid_1's l2: 0.000268331
[85]	training's l2: 0.000228693	valid_1's l2: 0.000268215
[86]	training's l2: 0.000228164	valid_1's l2: 0.000268091
[87]	training's l2: 0.000227702	valid_1's l2: 0.000268126
[88]	training's l2: 0.000227127	valid_1's l2: 0.000267979
[89]	training's l2: 0.000226641	valid_1's l2: 0.000267868
[90]	training's l2: 0.000226171	valid_1's l2: 0.000267927
[91]	training's l2: 0.000225594	valid_1's l2: 0.00026796
[92]	training's l2: 0.00022511	valid_1's l2: 0.000267885
[93]	training's l2: 0.000224665	valid_1's l2: 0.000267944
[94]	training's l2: 0.000224222	valid_1's l2: 0.000267948
[95]	training's l2: 0.000223776	valid_1's l2: 0.000267893
[96]	training's l2: 0.000223266	valid_1's l2: 0.000267849
[97]	training's l2: 0.000222875	valid_1's l2: 0.000267882
[98]	training's l2: 0.00022235	valid_1's l2: 0.000267754
[99]	training's l2: 0.000221916	valid_1's l2: 0.00026768
[100]	training's l2: 0.000221485	valid_1's l2: 0.000267656
[101]	training's l2: 0.00022105	valid_1's l2: 0.000267734
[102]	training's l2: 0.000220536	valid_1's l2: 0.000267644
[103]	training's l2: 0.00022014	valid_1's l2: 0.000267642
[104]	training's l2: 0.000219769	valid_1's l2: 0.000267662
[105]	training's l2: 0.000219298	valid_1's l2: 0.000267571
[106]	training's l2: 0.000218906	valid_1's l2: 0.000267579
[107]	training's l2: 0.000218498	valid_1's l2: 0.000267567
[108]	training's l2: 0.000218067	valid_1's l2: 0.000267561
[109]	training's l2: 0.000217656	valid_1's l2: 0.000267576
[110]	training's l2: 0.000217219	valid_1's l2: 0.000267501
[111]	training's l2: 0.000216765	valid_1's l2: 0.000267429
[112]	training's l2: 0.000216379	valid_1's l2: 0.000267489
[113]	training's l2: 0.000215959	valid_1's l2: 0.00026752
[114]	training's l2: 0.000215549	valid_1's l2: 0.000267418
[115]	training's l2: 0.000215113	valid_1's l2: 0.000267393
[116]	training's l2: 0.000214642	valid_1's l2: 0.00026722
[117]	training's l2: 0.000214274	valid_1's l2: 0.000267228
[118]	training's l2: 0.000213907	valid_1's l2: 0.000267188
[119]	training's l2: 0.000213531	valid_1's l2: 0.00026723
[120]	training's l2: 0.000213157	valid_1's l2: 0.000267181
[121]	training's l2: 0.000212731	valid_1's l2: 0.000267137
[122]	training's l2: 0.000212353	valid_1's l2: 0.000267246
[123]	training's l2: 0.000211824	valid_1's l2: 0.000267178
[124]	training's l2: 0.000211452	valid_1's l2: 0.000267201
[125]	training's l2: 0.00021108	valid_1's l2: 0.000267252
[126]	training's l2: 0.00021072	valid_1's l2: 0.000267304
[127]	training's l2: 0.000210347	valid_1's l2: 0.000267325
[128]	training's l2: 0.000209971	valid_1's l2: 0.000267335
[129]	training's l2: 0.00020961	valid_1's l2: 0.000267307
[130]	training's l2: 0.000209147	valid_1's l2: 0.000267264
[131]	training's l2: 0.000208766	valid_1's l2: 0.000267327
[132]	training's l2: 0.000208349	valid_1's l2: 0.00026739
[133]	training's l2: 0.000207994	valid_1's l2: 0.000267363
[134]	training's l2: 0.000207636	valid_1's l2: 0.000267397
[135]	training's l2: 0.000207257	valid_1's l2: 0.000267391
[136]	training's l2: 0.000206904	valid_1's l2: 0.000267379
[137]	training's l2: 0.000206534	valid_1's l2: 0.000267451
[138]	training's l2: 0.000206135	valid_1's l2: 0.000267406
[139]	training's l2: 0.000205699	valid_1's l2: 0.000267377
[140]	training's l2: 0.000205368	valid_1's l2: 0.000267369
[141]	training's l2: 0.000205021	valid_1's l2: 0.000267358
[142]	training's l2: 0.000204642	valid_1's l2: 0.000267325
[143]	training's l2: 0.000204267	valid_1's l2: 0.000267318
[144]	training's l2: 0.000203892	valid_1's l2: 0.000267253
[145]	training's l2: 0.000203561	valid_1's l2: 0.000267288
[146]	training's l2: 0.000203218	valid_1's l2: 0.000267386
[147]	training's l2: 0.000202942	valid_1's l2: 0.000267357
[148]	training's l2: 0.000202562	valid_1's l2: 0.000267284
[149]	training's l2: 0.000202197	valid_1's l2: 0.000267305
[150]	training's l2: 0.00020187	valid_1's l2: 0.000267278
[151]	training's l2: 0.000201543	valid_1's l2: 0.000267241
Early stopping, best iteration is:
[121]	training's l2: 0.000212731	valid_1's l2: 0.000267137
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.199646 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000390049	valid_1's l2: 0.000379847
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000382734	valid_1's l2: 0.000373502
[3]	training's l2: 0.00037595	valid_1's l2: 0.000367558
[4]	training's l2: 0.000369606	valid_1's l2: 0.00036202
[5]	training's l2: 0.000363772	valid_1's l2: 0.000356805
[6]	training's l2: 0.000358388	valid_1's l2: 0.00035238
[7]	training's l2: 0.000353158	valid_1's l2: 0.000347826
[8]	training's l2: 0.000348346	valid_1's l2: 0.000343609
[9]	training's l2: 0.000343693	valid_1's l2: 0.000339839
[10]	training's l2: 0.000339441	valid_1's l2: 0.000336112
[11]	training's l2: 0.000335487	valid_1's l2: 0.000332579
[12]	training's l2: 0.000331655	valid_1's l2: 0.000329442
[13]	training's l2: 0.00032791	valid_1's l2: 0.000326186
[14]	training's l2: 0.00032457	valid_1's l2: 0.000323471
[15]	training's l2: 0.000321235	valid_1's l2: 0.000320654
[16]	training's l2: 0.000318301	valid_1's l2: 0.000318334
[17]	training's l2: 0.00031539	valid_1's l2: 0.000315991
[18]	training's l2: 0.000312699	valid_1's l2: 0.00031397
[19]	training's l2: 0.00031005	valid_1's l2: 0.000311847
[20]	training's l2: 0.00030759	valid_1's l2: 0.000309821
[21]	training's l2: 0.00030523	valid_1's l2: 0.000308105
[22]	training's l2: 0.000303014	valid_1's l2: 0.000306312
[23]	training's l2: 0.000300801	valid_1's l2: 0.00030449
[24]	training's l2: 0.000298874	valid_1's l2: 0.000303038
[25]	training's l2: 0.000296915	valid_1's l2: 0.000301581
[26]	training's l2: 0.000295009	valid_1's l2: 0.000300205
[27]	training's l2: 0.000293229	valid_1's l2: 0.00029899
[28]	training's l2: 0.000291479	valid_1's l2: 0.000297801
[29]	training's l2: 0.000289816	valid_1's l2: 0.000296758
[30]	training's l2: 0.000288225	valid_1's l2: 0.000295728
[31]	training's l2: 0.000286617	valid_1's l2: 0.000294525
[32]	training's l2: 0.000285176	valid_1's l2: 0.000293611
[33]	training's l2: 0.000283736	valid_1's l2: 0.000292612
[34]	training's l2: 0.000282446	valid_1's l2: 0.000291812
[35]	training's l2: 0.00028113	valid_1's l2: 0.000290987
[36]	training's l2: 0.000279849	valid_1's l2: 0.00029026
[37]	training's l2: 0.000278607	valid_1's l2: 0.0002895
[38]	training's l2: 0.000277322	valid_1's l2: 0.000288728
[39]	training's l2: 0.000276206	valid_1's l2: 0.000288258
[40]	training's l2: 0.000275006	valid_1's l2: 0.000287542
[41]	training's l2: 0.000273931	valid_1's l2: 0.00028699
[42]	training's l2: 0.000272786	valid_1's l2: 0.000286386
[43]	training's l2: 0.000271761	valid_1's l2: 0.000285805
[44]	training's l2: 0.000270833	valid_1's l2: 0.000285362
[45]	training's l2: 0.000269899	valid_1's l2: 0.000284818
[46]	training's l2: 0.000268926	valid_1's l2: 0.000284237
[47]	training's l2: 0.000267996	valid_1's l2: 0.000283853
[48]	training's l2: 0.000267033	valid_1's l2: 0.000283386
[49]	training's l2: 0.000266197	valid_1's l2: 0.000283045
[50]	training's l2: 0.000265227	valid_1's l2: 0.000282475
[51]	training's l2: 0.000264196	valid_1's l2: 0.000281845
[52]	training's l2: 0.000263415	valid_1's l2: 0.000281465
[53]	training's l2: 0.000262563	valid_1's l2: 0.000281084
[54]	training's l2: 0.000261675	valid_1's l2: 0.000280572
[55]	training's l2: 0.000260742	valid_1's l2: 0.000280159
[56]	training's l2: 0.000260007	valid_1's l2: 0.00027981
[57]	training's l2: 0.000259152	valid_1's l2: 0.000279472
[58]	training's l2: 0.000258079	valid_1's l2: 0.000278895
[59]	training's l2: 0.000257322	valid_1's l2: 0.000278458
[60]	training's l2: 0.000256625	valid_1's l2: 0.000278214
[61]	training's l2: 0.000255873	valid_1's l2: 0.000277836
[62]	training's l2: 0.000255135	valid_1's l2: 0.000277553
[63]	training's l2: 0.000254482	valid_1's l2: 0.000277365
[64]	training's l2: 0.000253782	valid_1's l2: 0.00027718
[65]	training's l2: 0.000253038	valid_1's l2: 0.000276992
[66]	training's l2: 0.000252184	valid_1's l2: 0.000276566
[67]	training's l2: 0.000251531	valid_1's l2: 0.000276336
[68]	training's l2: 0.000250865	valid_1's l2: 0.000276146
[69]	training's l2: 0.000250279	valid_1's l2: 0.000276005
[70]	training's l2: 0.000249441	valid_1's l2: 0.000275632
[71]	training's l2: 0.000248788	valid_1's l2: 0.000275478
[72]	training's l2: 0.000248161	valid_1's l2: 0.000275321
[73]	training's l2: 0.000247343	valid_1's l2: 0.000275026
[74]	training's l2: 0.00024676	valid_1's l2: 0.000274897
[75]	training's l2: 0.000246128	valid_1's l2: 0.00027462
[76]	training's l2: 0.000245386	valid_1's l2: 0.000274365
[77]	training's l2: 0.000244801	valid_1's l2: 0.000274229
[78]	training's l2: 0.000244245	valid_1's l2: 0.000274044
[79]	training's l2: 0.00024365	valid_1's l2: 0.000273925
[80]	training's l2: 0.000243103	valid_1's l2: 0.000273816
[81]	training's l2: 0.000242603	valid_1's l2: 0.00027365
[82]	training's l2: 0.000241872	valid_1's l2: 0.000273334
[83]	training's l2: 0.000241372	valid_1's l2: 0.00027329
[84]	training's l2: 0.000240855	valid_1's l2: 0.000273174
[85]	training's l2: 0.000240224	valid_1's l2: 0.000272934
[86]	training's l2: 0.00023967	valid_1's l2: 0.000272784
[87]	training's l2: 0.000239102	valid_1's l2: 0.000272664
[88]	training's l2: 0.000238556	valid_1's l2: 0.000272546
[89]	training's l2: 0.000237956	valid_1's l2: 0.000272363
[90]	training's l2: 0.000237464	valid_1's l2: 0.000272312
[91]	training's l2: 0.000236846	valid_1's l2: 0.000272004
[92]	training's l2: 0.000236325	valid_1's l2: 0.000271781
[93]	training's l2: 0.000235799	valid_1's l2: 0.000271578
[94]	training's l2: 0.000235273	valid_1's l2: 0.00027146
[95]	training's l2: 0.000234781	valid_1's l2: 0.000271355
[96]	training's l2: 0.000234204	valid_1's l2: 0.000271117
[97]	training's l2: 0.000233725	valid_1's l2: 0.00027099
[98]	training's l2: 0.000233207	valid_1's l2: 0.000270822
[99]	training's l2: 0.000232734	valid_1's l2: 0.000270746
[100]	training's l2: 0.00023221	valid_1's l2: 0.000270537
[101]	training's l2: 0.000231666	valid_1's l2: 0.000270445
[102]	training's l2: 0.000231138	valid_1's l2: 0.000270442
[103]	training's l2: 0.000230717	valid_1's l2: 0.000270411
[104]	training's l2: 0.000230136	valid_1's l2: 0.000270235
[105]	training's l2: 0.000229676	valid_1's l2: 0.000270149
[106]	training's l2: 0.000229233	valid_1's l2: 0.000269963
[107]	training's l2: 0.000228727	valid_1's l2: 0.00026981
[108]	training's l2: 0.00022815	valid_1's l2: 0.000269693
[109]	training's l2: 0.000227759	valid_1's l2: 0.000269661
[110]	training's l2: 0.00022728	valid_1's l2: 0.000269603
[111]	training's l2: 0.000226829	valid_1's l2: 0.000269564
[112]	training's l2: 0.000226336	valid_1's l2: 0.0002694
[113]	training's l2: 0.000225972	valid_1's l2: 0.000269398
[114]	training's l2: 0.000225529	valid_1's l2: 0.00026934
[115]	training's l2: 0.00022512	valid_1's l2: 0.000269331
[116]	training's l2: 0.000224655	valid_1's l2: 0.000269275
[117]	training's l2: 0.00022423	valid_1's l2: 0.000269315
[118]	training's l2: 0.000223742	valid_1's l2: 0.000269328
[119]	training's l2: 0.000223299	valid_1's l2: 0.000269238
[120]	training's l2: 0.000222923	valid_1's l2: 0.000269215
[121]	training's l2: 0.00022245	valid_1's l2: 0.000269129
[122]	training's l2: 0.000222011	valid_1's l2: 0.000269063
[123]	training's l2: 0.000221563	valid_1's l2: 0.000268991
[124]	training's l2: 0.000221156	valid_1's l2: 0.000269013
[125]	training's l2: 0.000220778	valid_1's l2: 0.000268986
[126]	training's l2: 0.000220373	valid_1's l2: 0.000268924
[127]	training's l2: 0.000219939	valid_1's l2: 0.000268886
[128]	training's l2: 0.000219559	valid_1's l2: 0.000268837
[129]	training's l2: 0.000219153	valid_1's l2: 0.00026878
[130]	training's l2: 0.000218796	valid_1's l2: 0.000268748
[131]	training's l2: 0.00021838	valid_1's l2: 0.000268739
[132]	training's l2: 0.000217938	valid_1's l2: 0.000268692
[133]	training's l2: 0.000217577	valid_1's l2: 0.000268636
[134]	training's l2: 0.000217176	valid_1's l2: 0.00026854
[135]	training's l2: 0.000216838	valid_1's l2: 0.000268511
[136]	training's l2: 0.00021645	valid_1's l2: 0.000268534
[137]	training's l2: 0.000216114	valid_1's l2: 0.000268537
[138]	training's l2: 0.000215711	valid_1's l2: 0.000268431
[139]	training's l2: 0.000215269	valid_1's l2: 0.000268341
[140]	training's l2: 0.000214896	valid_1's l2: 0.000268326
[141]	training's l2: 0.000214515	valid_1's l2: 0.000268378
[142]	training's l2: 0.000214098	valid_1's l2: 0.000268225
[143]	training's l2: 0.000213698	valid_1's l2: 0.000268151
[144]	training's l2: 0.000213277	valid_1's l2: 0.000268103
[145]	training's l2: 0.000212925	valid_1's l2: 0.000268131
[146]	training's l2: 0.000212469	valid_1's l2: 0.000268088
[147]	training's l2: 0.000212096	valid_1's l2: 0.000268081
[148]	training's l2: 0.000211745	valid_1's l2: 0.000268028
[149]	training's l2: 0.000211341	valid_1's l2: 0.000268004
[150]	training's l2: 0.00021097	valid_1's l2: 0.000267977
[151]	training's l2: 0.0002106	valid_1's l2: 0.000267891
[152]	training's l2: 0.000210239	valid_1's l2: 0.000267809
[153]	training's l2: 0.00020986	valid_1's l2: 0.00026773
[154]	training's l2: 0.000209526	valid_1's l2: 0.000267727
[155]	training's l2: 0.000209136	valid_1's l2: 0.000267659
[156]	training's l2: 0.000208815	valid_1's l2: 0.000267694
[157]	training's l2: 0.000208447	valid_1's l2: 0.000267687
[158]	training's l2: 0.00020808	valid_1's l2: 0.000267509
[159]	training's l2: 0.000207707	valid_1's l2: 0.000267478
[160]	training's l2: 0.000207352	valid_1's l2: 0.000267471
[161]	training's l2: 0.000207038	valid_1's l2: 0.000267503
[162]	training's l2: 0.000206709	valid_1's l2: 0.000267531
[163]	training's l2: 0.000206373	valid_1's l2: 0.000267527
[164]	training's l2: 0.000206004	valid_1's l2: 0.000267408
[165]	training's l2: 0.000205675	valid_1's l2: 0.000267315
[166]	training's l2: 0.000205317	valid_1's l2: 0.000267273
[167]	training's l2: 0.000204993	valid_1's l2: 0.000267276
[168]	training's l2: 0.000204661	valid_1's l2: 0.000267254
[169]	training's l2: 0.00020433	valid_1's l2: 0.000267242
[170]	training's l2: 0.000204006	valid_1's l2: 0.000267328
[171]	training's l2: 0.000203674	valid_1's l2: 0.000267326
[172]	training's l2: 0.000203332	valid_1's l2: 0.000267339
[173]	training's l2: 0.000203001	valid_1's l2: 0.000267364
[174]	training's l2: 0.000202668	valid_1's l2: 0.000267318
[175]	training's l2: 0.000202291	valid_1's l2: 0.00026722
[176]	training's l2: 0.000201986	valid_1's l2: 0.000267236
[177]	training's l2: 0.000201667	valid_1's l2: 0.000267226
[178]	training's l2: 0.000201343	valid_1's l2: 0.000267142
[179]	training's l2: 0.000201032	valid_1's l2: 0.000267158
[180]	training's l2: 0.000200734	valid_1's l2: 0.000267122
[181]	training's l2: 0.000200416	valid_1's l2: 0.000267054
[182]	training's l2: 0.000200049	valid_1's l2: 0.000266962
[183]	training's l2: 0.000199744	valid_1's l2: 0.000266986
[184]	training's l2: 0.000199431	valid_1's l2: 0.000266964
Did not meet early stopping. Best iteration is:
[184]	training's l2: 0.000199431	valid_1's l2: 0.000266964
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219841 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000389953	valid_1's l2: 0.00037965
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000382583	valid_1's l2: 0.000373045
[3]	training's l2: 0.00037576	valid_1's l2: 0.000367107
[4]	training's l2: 0.000369417	valid_1's l2: 0.000361251
[5]	training's l2: 0.000363547	valid_1's l2: 0.000355739
[6]	training's l2: 0.000358067	valid_1's l2: 0.000350821
[7]	training's l2: 0.000352813	valid_1's l2: 0.000346036
[8]	training's l2: 0.00034801	valid_1's l2: 0.000341878
[9]	training's l2: 0.000343362	valid_1's l2: 0.000338018
[10]	training's l2: 0.000339144	valid_1's l2: 0.000334319
[11]	training's l2: 0.000335167	valid_1's l2: 0.000330779
[12]	training's l2: 0.000331184	valid_1's l2: 0.000327186
[13]	training's l2: 0.000327668	valid_1's l2: 0.000324242
[14]	training's l2: 0.000324294	valid_1's l2: 0.000321609
[15]	training's l2: 0.000320943	valid_1's l2: 0.000318955
[16]	training's l2: 0.000318009	valid_1's l2: 0.000316627
[17]	training's l2: 0.000315077	valid_1's l2: 0.000314049
[18]	training's l2: 0.000312323	valid_1's l2: 0.000311881
[19]	training's l2: 0.000309843	valid_1's l2: 0.00030992
[20]	training's l2: 0.000307385	valid_1's l2: 0.000307763
[21]	training's l2: 0.000304962	valid_1's l2: 0.000306012
[22]	training's l2: 0.000302792	valid_1's l2: 0.000304279
[23]	training's l2: 0.000300675	valid_1's l2: 0.000302656
[24]	training's l2: 0.000298611	valid_1's l2: 0.000301119
[25]	training's l2: 0.000296569	valid_1's l2: 0.000299724
[26]	training's l2: 0.000294723	valid_1's l2: 0.000298421
[27]	training's l2: 0.000293	valid_1's l2: 0.000297352
[28]	training's l2: 0.000291297	valid_1's l2: 0.000296161
[29]	training's l2: 0.000289693	valid_1's l2: 0.000295222
[30]	training's l2: 0.000288118	valid_1's l2: 0.000294249
[31]	training's l2: 0.00028665	valid_1's l2: 0.0002932
[32]	training's l2: 0.00028507	valid_1's l2: 0.000292047
[33]	training's l2: 0.000283751	valid_1's l2: 0.000291271
[34]	training's l2: 0.000282311	valid_1's l2: 0.000290322
[35]	training's l2: 0.000281029	valid_1's l2: 0.000289609
[36]	training's l2: 0.000279787	valid_1's l2: 0.000288876
[37]	training's l2: 0.000278515	valid_1's l2: 0.000288068
[38]	training's l2: 0.000277309	valid_1's l2: 0.00028731
[39]	training's l2: 0.000276166	valid_1's l2: 0.000286628
[40]	training's l2: 0.000275109	valid_1's l2: 0.000286114
[41]	training's l2: 0.000273968	valid_1's l2: 0.000285327
[42]	training's l2: 0.000272993	valid_1's l2: 0.000284848
[43]	training's l2: 0.000272009	valid_1's l2: 0.000284272
[44]	training's l2: 0.000270939	valid_1's l2: 0.000283632
[45]	training's l2: 0.000270051	valid_1's l2: 0.000283165
[46]	training's l2: 0.000269189	valid_1's l2: 0.000282686
[47]	training's l2: 0.000268313	valid_1's l2: 0.000282289
[48]	training's l2: 0.000267222	valid_1's l2: 0.000281614
[49]	training's l2: 0.000266286	valid_1's l2: 0.00028118
[50]	training's l2: 0.000265422	valid_1's l2: 0.000280781
[51]	training's l2: 0.000264437	valid_1's l2: 0.000280305
[52]	training's l2: 0.00026351	valid_1's l2: 0.000279767
[53]	training's l2: 0.000262681	valid_1's l2: 0.000279316
[54]	training's l2: 0.000261684	valid_1's l2: 0.000278868
[55]	training's l2: 0.000260853	valid_1's l2: 0.000278538
[56]	training's l2: 0.000260023	valid_1's l2: 0.000278124
[57]	training's l2: 0.000259323	valid_1's l2: 0.000277854
[58]	training's l2: 0.000258594	valid_1's l2: 0.000277591
[59]	training's l2: 0.000257782	valid_1's l2: 0.000277234
[60]	training's l2: 0.000256927	valid_1's l2: 0.000276857
[61]	training's l2: 0.000256125	valid_1's l2: 0.000276443
[62]	training's l2: 0.000255353	valid_1's l2: 0.000276125
[63]	training's l2: 0.000254624	valid_1's l2: 0.000275814
[64]	training's l2: 0.000253974	valid_1's l2: 0.000275573
[65]	training's l2: 0.000253217	valid_1's l2: 0.000275377
[66]	training's l2: 0.000252319	valid_1's l2: 0.000275063
[67]	training's l2: 0.000251683	valid_1's l2: 0.000274895
[68]	training's l2: 0.000251039	valid_1's l2: 0.000274662
[69]	training's l2: 0.000250245	valid_1's l2: 0.000274387
[70]	training's l2: 0.000249663	valid_1's l2: 0.00027423
[71]	training's l2: 0.000249082	valid_1's l2: 0.000274119
[72]	training's l2: 0.000248352	valid_1's l2: 0.000273726
[73]	training's l2: 0.000247549	valid_1's l2: 0.000273328
[74]	training's l2: 0.000246948	valid_1's l2: 0.00027325
[75]	training's l2: 0.000246346	valid_1's l2: 0.000273075
[76]	training's l2: 0.000245687	valid_1's l2: 0.000272812
[77]	training's l2: 0.00024509	valid_1's l2: 0.000272619
[78]	training's l2: 0.000244551	valid_1's l2: 0.000272534
[79]	training's l2: 0.000243804	valid_1's l2: 0.000272151
[80]	training's l2: 0.000243264	valid_1's l2: 0.000272076
[81]	training's l2: 0.000242649	valid_1's l2: 0.000271944
[82]	training's l2: 0.000242122	valid_1's l2: 0.000271833
[83]	training's l2: 0.000241453	valid_1's l2: 0.000271494
[84]	training's l2: 0.000240777	valid_1's l2: 0.000271132
[85]	training's l2: 0.000240275	valid_1's l2: 0.000270994
[86]	training's l2: 0.000239754	valid_1's l2: 0.000270957
[87]	training's l2: 0.000239057	valid_1's l2: 0.000270626
[88]	training's l2: 0.00023838	valid_1's l2: 0.000270415
[89]	training's l2: 0.000237876	valid_1's l2: 0.000270355
[90]	training's l2: 0.000237346	valid_1's l2: 0.000270324
[91]	training's l2: 0.000236801	valid_1's l2: 0.000270269
[92]	training's l2: 0.00023631	valid_1's l2: 0.000270149
[93]	training's l2: 0.000235796	valid_1's l2: 0.000269997
[94]	training's l2: 0.000235263	valid_1's l2: 0.000269911
[95]	training's l2: 0.00023476	valid_1's l2: 0.000269825
[96]	training's l2: 0.000234115	valid_1's l2: 0.000269671
[97]	training's l2: 0.000233563	valid_1's l2: 0.00026949
[98]	training's l2: 0.000233141	valid_1's l2: 0.00026946
[99]	training's l2: 0.000232694	valid_1's l2: 0.000269392
[100]	training's l2: 0.000232213	valid_1's l2: 0.000269307
[101]	training's l2: 0.000231681	valid_1's l2: 0.000269119
[102]	training's l2: 0.00023126	valid_1's l2: 0.000269148
[103]	training's l2: 0.000230698	valid_1's l2: 0.000269006
[104]	training's l2: 0.000230162	valid_1's l2: 0.000268831
[105]	training's l2: 0.000229738	valid_1's l2: 0.000268741
[106]	training's l2: 0.000229301	valid_1's l2: 0.000268627
[107]	training's l2: 0.000228825	valid_1's l2: 0.000268604
[108]	training's l2: 0.000228387	valid_1's l2: 0.000268588
[109]	training's l2: 0.000227858	valid_1's l2: 0.000268449
[110]	training's l2: 0.000227446	valid_1's l2: 0.000268432
[111]	training's l2: 0.000226922	valid_1's l2: 0.000268233
[112]	training's l2: 0.000226506	valid_1's l2: 0.000268156
[113]	training's l2: 0.000226053	valid_1's l2: 0.000268058
[114]	training's l2: 0.000225632	valid_1's l2: 0.000267974
[115]	training's l2: 0.000225218	valid_1's l2: 0.000267927
[116]	training's l2: 0.000224796	valid_1's l2: 0.000267869
[117]	training's l2: 0.000224345	valid_1's l2: 0.000267806
[118]	training's l2: 0.000223902	valid_1's l2: 0.00026773
[119]	training's l2: 0.000223477	valid_1's l2: 0.000267623
[120]	training's l2: 0.000223036	valid_1's l2: 0.000267636
[121]	training's l2: 0.000222567	valid_1's l2: 0.000267597
[122]	training's l2: 0.000222126	valid_1's l2: 0.000267571
[123]	training's l2: 0.000221738	valid_1's l2: 0.000267553
[124]	training's l2: 0.000221306	valid_1's l2: 0.000267568
[125]	training's l2: 0.000220857	valid_1's l2: 0.000267404
[126]	training's l2: 0.000220451	valid_1's l2: 0.000267392
[127]	training's l2: 0.000220057	valid_1's l2: 0.000267355
[128]	training's l2: 0.000219657	valid_1's l2: 0.000267364
[129]	training's l2: 0.000219208	valid_1's l2: 0.00026731
[130]	training's l2: 0.000218838	valid_1's l2: 0.000267371
[131]	training's l2: 0.000218395	valid_1's l2: 0.000267339
[132]	training's l2: 0.000217985	valid_1's l2: 0.000267292
[133]	training's l2: 0.0002175	valid_1's l2: 0.000267177
[134]	training's l2: 0.000217102	valid_1's l2: 0.000267122
[135]	training's l2: 0.000216722	valid_1's l2: 0.000267106
[136]	training's l2: 0.000216327	valid_1's l2: 0.000267146
[137]	training's l2: 0.000215935	valid_1's l2: 0.000267159
[138]	training's l2: 0.000215515	valid_1's l2: 0.000267129
[139]	training's l2: 0.000215134	valid_1's l2: 0.000267078
[140]	training's l2: 0.000214741	valid_1's l2: 0.000267121
[141]	training's l2: 0.000214338	valid_1's l2: 0.000267051
[142]	training's l2: 0.000213881	valid_1's l2: 0.000266949
[143]	training's l2: 0.000213508	valid_1's l2: 0.000266964
[144]	training's l2: 0.00021314	valid_1's l2: 0.000266989
[145]	training's l2: 0.000212714	valid_1's l2: 0.000266947
[146]	training's l2: 0.000212278	valid_1's l2: 0.000266883
[147]	training's l2: 0.000211878	valid_1's l2: 0.000266847
[148]	training's l2: 0.000211512	valid_1's l2: 0.000266803
[149]	training's l2: 0.000211116	valid_1's l2: 0.000266733
[150]	training's l2: 0.000210742	valid_1's l2: 0.000266756
[151]	training's l2: 0.000210399	valid_1's l2: 0.000266732
[152]	training's l2: 0.000210042	valid_1's l2: 0.000266684
[153]	training's l2: 0.000209682	valid_1's l2: 0.000266667
[154]	training's l2: 0.000209257	valid_1's l2: 0.000266564
[155]	training's l2: 0.000208889	valid_1's l2: 0.000266481
[156]	training's l2: 0.00020857	valid_1's l2: 0.000266464
[157]	training's l2: 0.000208211	valid_1's l2: 0.000266511
[158]	training's l2: 0.000207847	valid_1's l2: 0.000266461
[159]	training's l2: 0.000207471	valid_1's l2: 0.000266482
[160]	training's l2: 0.000207144	valid_1's l2: 0.000266507
[161]	training's l2: 0.000206836	valid_1's l2: 0.000266498
[162]	training's l2: 0.000206425	valid_1's l2: 0.000266496
[163]	training's l2: 0.000206059	valid_1's l2: 0.000266506
[164]	training's l2: 0.00020573	valid_1's l2: 0.000266472
[165]	training's l2: 0.000205403	valid_1's l2: 0.000266515
[166]	training's l2: 0.000205059	valid_1's l2: 0.000266496
[167]	training's l2: 0.000204693	valid_1's l2: 0.000266464
[168]	training's l2: 0.000204353	valid_1's l2: 0.000266441
[169]	training's l2: 0.00020403	valid_1's l2: 0.000266521
[170]	training's l2: 0.000203691	valid_1's l2: 0.000266491
[171]	training's l2: 0.000203333	valid_1's l2: 0.000266332
[172]	training's l2: 0.000202917	valid_1's l2: 0.000266284
[173]	training's l2: 0.000202603	valid_1's l2: 0.000266269
[174]	training's l2: 0.000202228	valid_1's l2: 0.000266217
[175]	training's l2: 0.000201916	valid_1's l2: 0.000266183
[176]	training's l2: 0.000201596	valid_1's l2: 0.000266151
[177]	training's l2: 0.000201318	valid_1's l2: 0.000266208
Did not meet early stopping. Best iteration is:
[177]	training's l2: 0.000201318	valid_1's l2: 0.000266208
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207401 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000388241	valid_1's l2: 0.00037804
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000379457	valid_1's l2: 0.000370139
[3]	training's l2: 0.00037164	valid_1's l2: 0.000363169
[4]	training's l2: 0.000364387	valid_1's l2: 0.000356654
[5]	training's l2: 0.000357528	valid_1's l2: 0.000350888
[6]	training's l2: 0.000351563	valid_1's l2: 0.000345415
[7]	training's l2: 0.000345995	valid_1's l2: 0.000340281
[8]	training's l2: 0.000340843	valid_1's l2: 0.000335804
[9]	training's l2: 0.000336078	valid_1's l2: 0.000331456
[10]	training's l2: 0.000331454	valid_1's l2: 0.000327423
[11]	training's l2: 0.000327495	valid_1's l2: 0.000323873
[12]	training's l2: 0.000323518	valid_1's l2: 0.000320615
[13]	training's l2: 0.000319887	valid_1's l2: 0.000317608
[14]	training's l2: 0.000316507	valid_1's l2: 0.000314624
[15]	training's l2: 0.000313332	valid_1's l2: 0.000311985
[16]	training's l2: 0.000310585	valid_1's l2: 0.000309754
[17]	training's l2: 0.000307804	valid_1's l2: 0.000307533
[18]	training's l2: 0.000305231	valid_1's l2: 0.000305549
[19]	training's l2: 0.000302789	valid_1's l2: 0.000303667
[20]	training's l2: 0.000300616	valid_1's l2: 0.000302066
[21]	training's l2: 0.000298497	valid_1's l2: 0.000300612
[22]	training's l2: 0.00029632	valid_1's l2: 0.000298985
[23]	training's l2: 0.000294441	valid_1's l2: 0.000297634
[24]	training's l2: 0.000292615	valid_1's l2: 0.000296522
[25]	training's l2: 0.00029085	valid_1's l2: 0.000295263
[26]	training's l2: 0.000289142	valid_1's l2: 0.000294047
[27]	training's l2: 0.000287648	valid_1's l2: 0.000293072
[28]	training's l2: 0.000286103	valid_1's l2: 0.000292059
[29]	training's l2: 0.000284548	valid_1's l2: 0.000291025
[30]	training's l2: 0.000283159	valid_1's l2: 0.000290141
[31]	training's l2: 0.000281903	valid_1's l2: 0.000289243
[32]	training's l2: 0.000280713	valid_1's l2: 0.000288371
[33]	training's l2: 0.000279353	valid_1's l2: 0.000287455
[34]	training's l2: 0.000278063	valid_1's l2: 0.000286583
[35]	training's l2: 0.00027694	valid_1's l2: 0.000285846
[36]	training's l2: 0.000275958	valid_1's l2: 0.000285196
[37]	training's l2: 0.000274678	valid_1's l2: 0.000284406
[38]	training's l2: 0.000273788	valid_1's l2: 0.000283804
[39]	training's l2: 0.000272551	valid_1's l2: 0.000282959
[40]	training's l2: 0.000271442	valid_1's l2: 0.000282361
[41]	training's l2: 0.000270469	valid_1's l2: 0.000281826
[42]	training's l2: 0.000269621	valid_1's l2: 0.00028149
[43]	training's l2: 0.00026849	valid_1's l2: 0.000280804
[44]	training's l2: 0.00026752	valid_1's l2: 0.000280318
[45]	training's l2: 0.000266635	valid_1's l2: 0.000279785
[46]	training's l2: 0.000265815	valid_1's l2: 0.000279336
[47]	training's l2: 0.000265013	valid_1's l2: 0.000278853
[48]	training's l2: 0.000264154	valid_1's l2: 0.000278474
[49]	training's l2: 0.000263355	valid_1's l2: 0.000278175
[50]	training's l2: 0.000262652	valid_1's l2: 0.00027799
[51]	training's l2: 0.000262001	valid_1's l2: 0.000277787
[52]	training's l2: 0.000260967	valid_1's l2: 0.000277303
[53]	training's l2: 0.000260328	valid_1's l2: 0.000277038
[54]	training's l2: 0.000259628	valid_1's l2: 0.000276776
[55]	training's l2: 0.000258634	valid_1's l2: 0.000276218
[56]	training's l2: 0.000257968	valid_1's l2: 0.000275998
[57]	training's l2: 0.000257256	valid_1's l2: 0.000275713
[58]	training's l2: 0.000256328	valid_1's l2: 0.000275258
[59]	training's l2: 0.000255591	valid_1's l2: 0.000274969
[60]	training's l2: 0.000254963	valid_1's l2: 0.000274673
[61]	training's l2: 0.000254365	valid_1's l2: 0.000274566
[62]	training's l2: 0.000253747	valid_1's l2: 0.000274365
[63]	training's l2: 0.000253003	valid_1's l2: 0.000274005
[64]	training's l2: 0.000252462	valid_1's l2: 0.000273786
[65]	training's l2: 0.000251888	valid_1's l2: 0.000273639
[66]	training's l2: 0.000251311	valid_1's l2: 0.000273475
[67]	training's l2: 0.000250497	valid_1's l2: 0.000273172
[68]	training's l2: 0.000249918	valid_1's l2: 0.000273072
[69]	training's l2: 0.000249106	valid_1's l2: 0.000272577
[70]	training's l2: 0.00024846	valid_1's l2: 0.00027227
[71]	training's l2: 0.000247773	valid_1's l2: 0.000271964
[72]	training's l2: 0.000247213	valid_1's l2: 0.0002718
[73]	training's l2: 0.000246562	valid_1's l2: 0.00027159
[74]	training's l2: 0.000246124	valid_1's l2: 0.000271518
[75]	training's l2: 0.000245586	valid_1's l2: 0.000271418
[76]	training's l2: 0.000244927	valid_1's l2: 0.00027109
[77]	training's l2: 0.000244277	valid_1's l2: 0.000270831
[78]	training's l2: 0.000243797	valid_1's l2: 0.000270771
[79]	training's l2: 0.000243272	valid_1's l2: 0.000270627
[80]	training's l2: 0.000242746	valid_1's l2: 0.00027056
[81]	training's l2: 0.000242131	valid_1's l2: 0.000270341
[82]	training's l2: 0.000241642	valid_1's l2: 0.000270207
[83]	training's l2: 0.000241169	valid_1's l2: 0.000270039
[84]	training's l2: 0.000240723	valid_1's l2: 0.000269913
[85]	training's l2: 0.000240267	valid_1's l2: 0.000269922
[86]	training's l2: 0.000239724	valid_1's l2: 0.000269737
[87]	training's l2: 0.000239313	valid_1's l2: 0.000269615
[88]	training's l2: 0.000238897	valid_1's l2: 0.000269569
[89]	training's l2: 0.000238513	valid_1's l2: 0.000269534
[90]	training's l2: 0.000238075	valid_1's l2: 0.000269453
[91]	training's l2: 0.000237547	valid_1's l2: 0.000269262
[92]	training's l2: 0.000237154	valid_1's l2: 0.000269262
[93]	training's l2: 0.000236758	valid_1's l2: 0.00026918
[94]	training's l2: 0.000236225	valid_1's l2: 0.000268989
[95]	training's l2: 0.00023578	valid_1's l2: 0.000268919
[96]	training's l2: 0.000235373	valid_1's l2: 0.000268964
[97]	training's l2: 0.000234944	valid_1's l2: 0.000268928
[98]	training's l2: 0.000234575	valid_1's l2: 0.000268892
[99]	training's l2: 0.000234104	valid_1's l2: 0.000268699
[100]	training's l2: 0.000233723	valid_1's l2: 0.0002687
[101]	training's l2: 0.000233276	valid_1's l2: 0.00026865
[102]	training's l2: 0.000232896	valid_1's l2: 0.000268597
[103]	training's l2: 0.00023245	valid_1's l2: 0.000268489
[104]	training's l2: 0.000232041	valid_1's l2: 0.000268478
[105]	training's l2: 0.000231542	valid_1's l2: 0.000268437
[106]	training's l2: 0.000231146	valid_1's l2: 0.000268382
[107]	training's l2: 0.000230708	valid_1's l2: 0.000268296
[108]	training's l2: 0.000230192	valid_1's l2: 0.00026819
[109]	training's l2: 0.000229779	valid_1's l2: 0.000268253
[110]	training's l2: 0.000229325	valid_1's l2: 0.000268203
[111]	training's l2: 0.000228891	valid_1's l2: 0.000268115
[112]	training's l2: 0.000228514	valid_1's l2: 0.000268117
[113]	training's l2: 0.000228104	valid_1's l2: 0.000268062
[114]	training's l2: 0.000227731	valid_1's l2: 0.000268023
[115]	training's l2: 0.00022734	valid_1's l2: 0.000267948
[116]	training's l2: 0.000226889	valid_1's l2: 0.000267838
[117]	training's l2: 0.000226533	valid_1's l2: 0.000267752
[118]	training's l2: 0.000226204	valid_1's l2: 0.000267785
[119]	training's l2: 0.00022584	valid_1's l2: 0.000267664
[120]	training's l2: 0.000225454	valid_1's l2: 0.000267523
[121]	training's l2: 0.000225108	valid_1's l2: 0.000267461
[122]	training's l2: 0.00022461	valid_1's l2: 0.000267389
[123]	training's l2: 0.000224253	valid_1's l2: 0.000267375
[124]	training's l2: 0.000223923	valid_1's l2: 0.000267365
[125]	training's l2: 0.000223599	valid_1's l2: 0.000267389
[126]	training's l2: 0.000223191	valid_1's l2: 0.000267332
[127]	training's l2: 0.000222861	valid_1's l2: 0.000267344
[128]	training's l2: 0.00022244	valid_1's l2: 0.000267259
[129]	training's l2: 0.000222071	valid_1's l2: 0.00026728
[130]	training's l2: 0.000221717	valid_1's l2: 0.000267166
[131]	training's l2: 0.0002214	valid_1's l2: 0.000267137
[132]	training's l2: 0.000221013	valid_1's l2: 0.000267157
[133]	training's l2: 0.000220668	valid_1's l2: 0.000267126
[134]	training's l2: 0.000220282	valid_1's l2: 0.000267044
[135]	training's l2: 0.000219972	valid_1's l2: 0.00026708
[136]	training's l2: 0.000219662	valid_1's l2: 0.000267125
[137]	training's l2: 0.000219259	valid_1's l2: 0.00026704
[138]	training's l2: 0.000218903	valid_1's l2: 0.000267003
[139]	training's l2: 0.000218604	valid_1's l2: 0.000267051
[140]	training's l2: 0.000218275	valid_1's l2: 0.000267035
[141]	training's l2: 0.000217916	valid_1's l2: 0.000266892
[142]	training's l2: 0.000217585	valid_1's l2: 0.000266775
[143]	training's l2: 0.00021728	valid_1's l2: 0.000266717
[144]	training's l2: 0.000216952	valid_1's l2: 0.000266678
[145]	training's l2: 0.000216624	valid_1's l2: 0.000266686
[146]	training's l2: 0.000216259	valid_1's l2: 0.000266734
[147]	training's l2: 0.000215904	valid_1's l2: 0.00026662
[148]	training's l2: 0.000215597	valid_1's l2: 0.000266618
[149]	training's l2: 0.000215227	valid_1's l2: 0.000266542
[150]	training's l2: 0.000214924	valid_1's l2: 0.000266554
[151]	training's l2: 0.000214622	valid_1's l2: 0.000266597
[152]	training's l2: 0.000214308	valid_1's l2: 0.000266598
[153]	training's l2: 0.000213985	valid_1's l2: 0.000266568
[154]	training's l2: 0.000213608	valid_1's l2: 0.000266425
[155]	training's l2: 0.000213305	valid_1's l2: 0.000266332
[156]	training's l2: 0.000212995	valid_1's l2: 0.000266251
[157]	training's l2: 0.00021264	valid_1's l2: 0.000266232
[158]	training's l2: 0.000212348	valid_1's l2: 0.000266272
[159]	training's l2: 0.000212064	valid_1's l2: 0.000266271
[160]	training's l2: 0.000211728	valid_1's l2: 0.000266177
[161]	training's l2: 0.000211425	valid_1's l2: 0.000266245
[162]	training's l2: 0.000211121	valid_1's l2: 0.000266179
[163]	training's l2: 0.000210841	valid_1's l2: 0.000266157
[164]	training's l2: 0.000210558	valid_1's l2: 0.000266065
[165]	training's l2: 0.000210238	valid_1's l2: 0.000266036
[166]	training's l2: 0.000209959	valid_1's l2: 0.000266021
[167]	training's l2: 0.000209669	valid_1's l2: 0.00026602
[168]	training's l2: 0.0002094	valid_1's l2: 0.000266057
[169]	training's l2: 0.000209111	valid_1's l2: 0.000266095
[170]	training's l2: 0.000208855	valid_1's l2: 0.000266143
[171]	training's l2: 0.000208558	valid_1's l2: 0.000266093
[172]	training's l2: 0.000208257	valid_1's l2: 0.000266145
[173]	training's l2: 0.000207977	valid_1's l2: 0.000266043
[174]	training's l2: 0.000207708	valid_1's l2: 0.000266038
[175]	training's l2: 0.000207434	valid_1's l2: 0.000266
[176]	training's l2: 0.000207137	valid_1's l2: 0.000266002
[177]	training's l2: 0.000206844	valid_1's l2: 0.000266042
[178]	training's l2: 0.000206579	valid_1's l2: 0.000266109
[179]	training's l2: 0.000206311	valid_1's l2: 0.000266168
[180]	training's l2: 0.000206041	valid_1's l2: 0.00026614
[181]	training's l2: 0.000205726	valid_1's l2: 0.00026605
[182]	training's l2: 0.000205461	valid_1's l2: 0.000266057
[183]	training's l2: 0.000205196	valid_1's l2: 0.000265982
[184]	training's l2: 0.000204909	valid_1's l2: 0.00026606
[185]	training's l2: 0.000204654	valid_1's l2: 0.000266056
[186]	training's l2: 0.000204341	valid_1's l2: 0.000265993
Did not meet early stopping. Best iteration is:
[186]	training's l2: 0.000204341	valid_1's l2: 0.000265993
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.234573 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000387009	valid_1's l2: 0.000376631
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000377304	valid_1's l2: 0.000367595
[3]	training's l2: 0.000368687	valid_1's l2: 0.000359781
[4]	training's l2: 0.000360961	valid_1's l2: 0.000352474
[5]	training's l2: 0.00035391	valid_1's l2: 0.000346138
[6]	training's l2: 0.000347581	valid_1's l2: 0.000340242
[7]	training's l2: 0.000341858	valid_1's l2: 0.000335371
[8]	training's l2: 0.000336561	valid_1's l2: 0.000330671
[9]	training's l2: 0.00033151	valid_1's l2: 0.000326337
[10]	training's l2: 0.000327149	valid_1's l2: 0.000322573
[11]	training's l2: 0.000322991	valid_1's l2: 0.000318857
[12]	training's l2: 0.000319216	valid_1's l2: 0.000315399
[13]	training's l2: 0.000315652	valid_1's l2: 0.00031237
[14]	training's l2: 0.000312541	valid_1's l2: 0.000309626
[15]	training's l2: 0.000309455	valid_1's l2: 0.000307104
[16]	training's l2: 0.000306701	valid_1's l2: 0.000304997
[17]	training's l2: 0.000304086	valid_1's l2: 0.000303023
[18]	training's l2: 0.000301666	valid_1's l2: 0.000301203
[19]	training's l2: 0.000299364	valid_1's l2: 0.000299602
[20]	training's l2: 0.000297134	valid_1's l2: 0.000297843
[21]	training's l2: 0.000295199	valid_1's l2: 0.000296437
[22]	training's l2: 0.000293238	valid_1's l2: 0.000294883
[23]	training's l2: 0.000291427	valid_1's l2: 0.000293595
[24]	training's l2: 0.000289622	valid_1's l2: 0.000292437
[25]	training's l2: 0.000287959	valid_1's l2: 0.000291469
[26]	training's l2: 0.000286474	valid_1's l2: 0.000290364
[27]	training's l2: 0.000284837	valid_1's l2: 0.00028924
[28]	training's l2: 0.00028348	valid_1's l2: 0.000288204
[29]	training's l2: 0.000282048	valid_1's l2: 0.000287384
[30]	training's l2: 0.000280869	valid_1's l2: 0.000286583
[31]	training's l2: 0.000279793	valid_1's l2: 0.000285761
[32]	training's l2: 0.000278688	valid_1's l2: 0.000285044
[33]	training's l2: 0.000277532	valid_1's l2: 0.000284097
[34]	training's l2: 0.000275981	valid_1's l2: 0.000283258
[35]	training's l2: 0.000274711	valid_1's l2: 0.000282421
[36]	training's l2: 0.000273687	valid_1's l2: 0.000282037
[37]	training's l2: 0.000272749	valid_1's l2: 0.000281516
[38]	training's l2: 0.000271515	valid_1's l2: 0.000280831
[39]	training's l2: 0.000270661	valid_1's l2: 0.000280441
[40]	training's l2: 0.000269935	valid_1's l2: 0.000280061
[41]	training's l2: 0.000268672	valid_1's l2: 0.000279405
[42]	training's l2: 0.000267938	valid_1's l2: 0.000279099
[43]	training's l2: 0.000267008	valid_1's l2: 0.000278692
[44]	training's l2: 0.000266243	valid_1's l2: 0.000278402
[45]	training's l2: 0.000265105	valid_1's l2: 0.000277878
[46]	training's l2: 0.000264177	valid_1's l2: 0.000277503
[47]	training's l2: 0.000263356	valid_1's l2: 0.000277124
[48]	training's l2: 0.000262609	valid_1's l2: 0.000276893
[49]	training's l2: 0.000261924	valid_1's l2: 0.000276631
[50]	training's l2: 0.000261035	valid_1's l2: 0.000276355
[51]	training's l2: 0.000260272	valid_1's l2: 0.000276048
[52]	training's l2: 0.000259653	valid_1's l2: 0.000275869
[53]	training's l2: 0.00025872	valid_1's l2: 0.000275485
[54]	training's l2: 0.000258087	valid_1's l2: 0.000275322
[55]	training's l2: 0.00025733	valid_1's l2: 0.000275113
[56]	training's l2: 0.000256511	valid_1's l2: 0.000274783
[57]	training's l2: 0.000255892	valid_1's l2: 0.000274481
[58]	training's l2: 0.000255262	valid_1's l2: 0.000274332
[59]	training's l2: 0.000254545	valid_1's l2: 0.000274036
[60]	training's l2: 0.000253841	valid_1's l2: 0.00027376
[61]	training's l2: 0.000253003	valid_1's l2: 0.000273416
[62]	training's l2: 0.000252173	valid_1's l2: 0.000273109
[63]	training's l2: 0.000251483	valid_1's l2: 0.00027297
[64]	training's l2: 0.000250966	valid_1's l2: 0.00027292
[65]	training's l2: 0.000250332	valid_1's l2: 0.000272723
[66]	training's l2: 0.000249809	valid_1's l2: 0.000272503
[67]	training's l2: 0.000249202	valid_1's l2: 0.000272256
[68]	training's l2: 0.000248653	valid_1's l2: 0.000271987
[69]	training's l2: 0.000248137	valid_1's l2: 0.000271815
[70]	training's l2: 0.000247564	valid_1's l2: 0.000271534
[71]	training's l2: 0.000246999	valid_1's l2: 0.000271476
[72]	training's l2: 0.000246519	valid_1's l2: 0.000271362
[73]	training's l2: 0.000245867	valid_1's l2: 0.000271069
[74]	training's l2: 0.000245332	valid_1's l2: 0.000270878
[75]	training's l2: 0.000244847	valid_1's l2: 0.000270818
[76]	training's l2: 0.000244216	valid_1's l2: 0.000270722
[77]	training's l2: 0.000243667	valid_1's l2: 0.000270616
[78]	training's l2: 0.000243216	valid_1's l2: 0.000270484
[79]	training's l2: 0.000242738	valid_1's l2: 0.000270459
[80]	training's l2: 0.000242251	valid_1's l2: 0.000270354
[81]	training's l2: 0.000241856	valid_1's l2: 0.000270292
[82]	training's l2: 0.000241443	valid_1's l2: 0.00027024
[83]	training's l2: 0.000240922	valid_1's l2: 0.000270072
[84]	training's l2: 0.000240442	valid_1's l2: 0.00027002
[85]	training's l2: 0.000239949	valid_1's l2: 0.000269928
[86]	training's l2: 0.000239393	valid_1's l2: 0.000269872
[87]	training's l2: 0.000238855	valid_1's l2: 0.000269777
[88]	training's l2: 0.000238413	valid_1's l2: 0.000269741
[89]	training's l2: 0.000238	valid_1's l2: 0.000269631
[90]	training's l2: 0.000237568	valid_1's l2: 0.000269514
[91]	training's l2: 0.000237079	valid_1's l2: 0.000269351
[92]	training's l2: 0.000236652	valid_1's l2: 0.000269298
[93]	training's l2: 0.000236101	valid_1's l2: 0.000269271
[94]	training's l2: 0.000235713	valid_1's l2: 0.000269238
[95]	training's l2: 0.000235217	valid_1's l2: 0.000269075
[96]	training's l2: 0.000234797	valid_1's l2: 0.000269078
[97]	training's l2: 0.000234385	valid_1's l2: 0.000269062
[98]	training's l2: 0.000233997	valid_1's l2: 0.000269046
[99]	training's l2: 0.000233579	valid_1's l2: 0.000269036
[100]	training's l2: 0.000233191	valid_1's l2: 0.000269052
[101]	training's l2: 0.000232642	valid_1's l2: 0.000268758
[102]	training's l2: 0.000232257	valid_1's l2: 0.000268724
[103]	training's l2: 0.000231808	valid_1's l2: 0.00026863
[104]	training's l2: 0.00023135	valid_1's l2: 0.00026861
[105]	training's l2: 0.000230903	valid_1's l2: 0.00026857
[106]	training's l2: 0.000230483	valid_1's l2: 0.000268495
[107]	training's l2: 0.000230011	valid_1's l2: 0.000268428
[108]	training's l2: 0.000229584	valid_1's l2: 0.000268343
[109]	training's l2: 0.000229145	valid_1's l2: 0.000268208
[110]	training's l2: 0.000228767	valid_1's l2: 0.000268178
[111]	training's l2: 0.000228413	valid_1's l2: 0.00026807
[112]	training's l2: 0.000227984	valid_1's l2: 0.000267997
[113]	training's l2: 0.000227638	valid_1's l2: 0.000267985
[114]	training's l2: 0.000227274	valid_1's l2: 0.000268033
[115]	training's l2: 0.000226902	valid_1's l2: 0.000267946
[116]	training's l2: 0.000226435	valid_1's l2: 0.00026794
[117]	training's l2: 0.00022604	valid_1's l2: 0.000267864
[118]	training's l2: 0.000225672	valid_1's l2: 0.000267825
[119]	training's l2: 0.000225327	valid_1's l2: 0.000267748
[120]	training's l2: 0.000224948	valid_1's l2: 0.000267719
[121]	training's l2: 0.000224613	valid_1's l2: 0.000267716
[122]	training's l2: 0.000224249	valid_1's l2: 0.000267632
[123]	training's l2: 0.000223868	valid_1's l2: 0.000267536
[124]	training's l2: 0.000223538	valid_1's l2: 0.000267511
[125]	training's l2: 0.00022318	valid_1's l2: 0.000267542
[126]	training's l2: 0.000222787	valid_1's l2: 0.000267518
[127]	training's l2: 0.000222457	valid_1's l2: 0.000267539
[128]	training's l2: 0.000222136	valid_1's l2: 0.000267528
[129]	training's l2: 0.000221769	valid_1's l2: 0.000267498
[130]	training's l2: 0.000221408	valid_1's l2: 0.000267486
[131]	training's l2: 0.000221013	valid_1's l2: 0.000267442
[132]	training's l2: 0.000220565	valid_1's l2: 0.000267283
[133]	training's l2: 0.000220234	valid_1's l2: 0.000267359
[134]	training's l2: 0.000219899	valid_1's l2: 0.000267262
[135]	training's l2: 0.000219532	valid_1's l2: 0.000267203
[136]	training's l2: 0.0002192	valid_1's l2: 0.000267184
[137]	training's l2: 0.000218878	valid_1's l2: 0.000267144
[138]	training's l2: 0.000218478	valid_1's l2: 0.000267122
[139]	training's l2: 0.00021815	valid_1's l2: 0.000267146
[140]	training's l2: 0.000217839	valid_1's l2: 0.000267197
[141]	training's l2: 0.000217483	valid_1's l2: 0.000267241
[142]	training's l2: 0.000217163	valid_1's l2: 0.000267238
[143]	training's l2: 0.000216843	valid_1's l2: 0.000267209
[144]	training's l2: 0.000216524	valid_1's l2: 0.000267287
[145]	training's l2: 0.000216247	valid_1's l2: 0.000267309
[146]	training's l2: 0.000215872	valid_1's l2: 0.000267282
[147]	training's l2: 0.000215526	valid_1's l2: 0.000267322
[148]	training's l2: 0.000215163	valid_1's l2: 0.00026725
[149]	training's l2: 0.000214861	valid_1's l2: 0.000267267
[150]	training's l2: 0.000214546	valid_1's l2: 0.000267319
[151]	training's l2: 0.000214187	valid_1's l2: 0.000267204
[152]	training's l2: 0.0002139	valid_1's l2: 0.000267191
[153]	training's l2: 0.00021361	valid_1's l2: 0.000267171
[154]	training's l2: 0.00021328	valid_1's l2: 0.000267184
[155]	training's l2: 0.000212995	valid_1's l2: 0.00026718
[156]	training's l2: 0.000212688	valid_1's l2: 0.000267193
[157]	training's l2: 0.000212379	valid_1's l2: 0.000267323
[158]	training's l2: 0.00021212	valid_1's l2: 0.00026734
[159]	training's l2: 0.000211824	valid_1's l2: 0.00026726
[160]	training's l2: 0.000211539	valid_1's l2: 0.000267222
[161]	training's l2: 0.000211264	valid_1's l2: 0.00026715
[162]	training's l2: 0.000210939	valid_1's l2: 0.000267121
[163]	training's l2: 0.000210664	valid_1's l2: 0.000267156
[164]	training's l2: 0.000210353	valid_1's l2: 0.000267225
[165]	training's l2: 0.000210023	valid_1's l2: 0.000267185
[166]	training's l2: 0.000209756	valid_1's l2: 0.000267188
[167]	training's l2: 0.000209438	valid_1's l2: 0.000267239
[168]	training's l2: 0.000209157	valid_1's l2: 0.000267299
[169]	training's l2: 0.000208849	valid_1's l2: 0.000267267
[170]	training's l2: 0.000208567	valid_1's l2: 0.000267207
[171]	training's l2: 0.000208281	valid_1's l2: 0.000267109
[172]	training's l2: 0.000208001	valid_1's l2: 0.000267099
[173]	training's l2: 0.000207702	valid_1's l2: 0.000267145
[174]	training's l2: 0.000207429	valid_1's l2: 0.00026715
[175]	training's l2: 0.000207159	valid_1's l2: 0.000267159
[176]	training's l2: 0.000206903	valid_1's l2: 0.000267177
[177]	training's l2: 0.000206632	valid_1's l2: 0.000267231
[178]	training's l2: 0.000206347	valid_1's l2: 0.000267257
[179]	training's l2: 0.000206091	valid_1's l2: 0.000267294
[180]	training's l2: 0.000205818	valid_1's l2: 0.000267263
[181]	training's l2: 0.000205533	valid_1's l2: 0.000267222
[182]	training's l2: 0.000205244	valid_1's l2: 0.000267297
[183]	training's l2: 0.000204961	valid_1's l2: 0.000267252
[184]	training's l2: 0.00020473	valid_1's l2: 0.000267262
Did not meet early stopping. Best iteration is:
[184]	training's l2: 0.00020473	valid_1's l2: 0.000267262
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.232466 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000384184	valid_1's l2: 0.000374735
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000372195	valid_1's l2: 0.000364353
[3]	training's l2: 0.000361595	valid_1's l2: 0.00035491
[4]	training's l2: 0.000352142	valid_1's l2: 0.00034666
[5]	training's l2: 0.000343868	valid_1's l2: 0.000339495
[6]	training's l2: 0.000336501	valid_1's l2: 0.000332958
[7]	training's l2: 0.000329529	valid_1's l2: 0.000326894
[8]	training's l2: 0.000323405	valid_1's l2: 0.000321884
[9]	training's l2: 0.000317854	valid_1's l2: 0.000317371
[10]	training's l2: 0.000312569	valid_1's l2: 0.000313283
[11]	training's l2: 0.000308229	valid_1's l2: 0.000309918
[12]	training's l2: 0.000303989	valid_1's l2: 0.000306594
[13]	training's l2: 0.000299998	valid_1's l2: 0.000303579
[14]	training's l2: 0.000296572	valid_1's l2: 0.000300868
[15]	training's l2: 0.000293206	valid_1's l2: 0.000298348
[16]	training's l2: 0.000289993	valid_1's l2: 0.000296211
[17]	training's l2: 0.000287138	valid_1's l2: 0.000294396
[18]	training's l2: 0.000284486	valid_1's l2: 0.000292814
[19]	training's l2: 0.00028202	valid_1's l2: 0.000291419
[20]	training's l2: 0.000279669	valid_1's l2: 0.000289679
[21]	training's l2: 0.000277384	valid_1's l2: 0.000288352
[22]	training's l2: 0.000275358	valid_1's l2: 0.000287066
[23]	training's l2: 0.000273253	valid_1's l2: 0.000285771
[24]	training's l2: 0.000271295	valid_1's l2: 0.000284794
[25]	training's l2: 0.000269519	valid_1's l2: 0.000283768
[26]	training's l2: 0.000267799	valid_1's l2: 0.000283009
[27]	training's l2: 0.00026632	valid_1's l2: 0.000282447
[28]	training's l2: 0.000264536	valid_1's l2: 0.000281463
[29]	training's l2: 0.000263179	valid_1's l2: 0.00028091
[30]	training's l2: 0.000261698	valid_1's l2: 0.000280131
[31]	training's l2: 0.00026016	valid_1's l2: 0.000279347
[32]	training's l2: 0.000258985	valid_1's l2: 0.000279049
[33]	training's l2: 0.000257311	valid_1's l2: 0.000278281
[34]	training's l2: 0.000256037	valid_1's l2: 0.000277882
[35]	training's l2: 0.000254487	valid_1's l2: 0.000277288
[36]	training's l2: 0.000253161	valid_1's l2: 0.000276694
[37]	training's l2: 0.000251786	valid_1's l2: 0.000276069
[38]	training's l2: 0.000250578	valid_1's l2: 0.000275729
[39]	training's l2: 0.000249476	valid_1's l2: 0.000275398
[40]	training's l2: 0.000248398	valid_1's l2: 0.000275115
[41]	training's l2: 0.000246904	valid_1's l2: 0.000274512
[42]	training's l2: 0.000245809	valid_1's l2: 0.000274199
[43]	training's l2: 0.000244901	valid_1's l2: 0.000273907
[44]	training's l2: 0.000243577	valid_1's l2: 0.000273305
[45]	training's l2: 0.000242544	valid_1's l2: 0.000273147
[46]	training's l2: 0.000241372	valid_1's l2: 0.000272757
[47]	training's l2: 0.000240494	valid_1's l2: 0.000272621
[48]	training's l2: 0.00023934	valid_1's l2: 0.000272196
[49]	training's l2: 0.000238325	valid_1's l2: 0.000272029
[50]	training's l2: 0.000237195	valid_1's l2: 0.000271568
[51]	training's l2: 0.00023618	valid_1's l2: 0.000271183
[52]	training's l2: 0.000235303	valid_1's l2: 0.000271002
[53]	training's l2: 0.000234438	valid_1's l2: 0.000270786
[54]	training's l2: 0.000233409	valid_1's l2: 0.00027044
[55]	training's l2: 0.000232552	valid_1's l2: 0.000270308
[56]	training's l2: 0.000231636	valid_1's l2: 0.000269993
[57]	training's l2: 0.00023091	valid_1's l2: 0.000269941
[58]	training's l2: 0.000230067	valid_1's l2: 0.000269799
[59]	training's l2: 0.000229208	valid_1's l2: 0.000269457
[60]	training's l2: 0.000228453	valid_1's l2: 0.000269308
[61]	training's l2: 0.000227625	valid_1's l2: 0.000269212
[62]	training's l2: 0.000226736	valid_1's l2: 0.00026909
[63]	training's l2: 0.000225966	valid_1's l2: 0.00026903
[64]	training's l2: 0.000225185	valid_1's l2: 0.000269026
[65]	training's l2: 0.000224421	valid_1's l2: 0.000268891
[66]	training's l2: 0.000223672	valid_1's l2: 0.000268755
[67]	training's l2: 0.000222991	valid_1's l2: 0.000268677
[68]	training's l2: 0.000222264	valid_1's l2: 0.000268623
[69]	training's l2: 0.000221415	valid_1's l2: 0.00026853
[70]	training's l2: 0.000220664	valid_1's l2: 0.000268484
[71]	training's l2: 0.000219855	valid_1's l2: 0.000268283
[72]	training's l2: 0.000219142	valid_1's l2: 0.000268161
[73]	training's l2: 0.000218538	valid_1's l2: 0.000268142
[74]	training's l2: 0.000217795	valid_1's l2: 0.000267993
[75]	training's l2: 0.000217058	valid_1's l2: 0.000267859
[76]	training's l2: 0.000216326	valid_1's l2: 0.000267941
[77]	training's l2: 0.000215602	valid_1's l2: 0.000267698
[78]	training's l2: 0.000214831	valid_1's l2: 0.000267495
[79]	training's l2: 0.000214196	valid_1's l2: 0.000267547
[80]	training's l2: 0.000213532	valid_1's l2: 0.000267525
[81]	training's l2: 0.000212885	valid_1's l2: 0.0002675
[82]	training's l2: 0.000212254	valid_1's l2: 0.000267454
[83]	training's l2: 0.000211646	valid_1's l2: 0.000267469
[84]	training's l2: 0.000210911	valid_1's l2: 0.000267417
[85]	training's l2: 0.000210259	valid_1's l2: 0.000267279
[86]	training's l2: 0.000209545	valid_1's l2: 0.000267314
[87]	training's l2: 0.000208921	valid_1's l2: 0.000267345
[88]	training's l2: 0.000208302	valid_1's l2: 0.000267292
[89]	training's l2: 0.00020764	valid_1's l2: 0.000267279
[90]	training's l2: 0.000206933	valid_1's l2: 0.000267093
[91]	training's l2: 0.000206338	valid_1's l2: 0.000267053
[92]	training's l2: 0.000205689	valid_1's l2: 0.000266804
[93]	training's l2: 0.000205106	valid_1's l2: 0.000266819
[94]	training's l2: 0.00020453	valid_1's l2: 0.000266792
[95]	training's l2: 0.00020396	valid_1's l2: 0.000266735
[96]	training's l2: 0.000203363	valid_1's l2: 0.00026681
[97]	training's l2: 0.00020264	valid_1's l2: 0.000266572
[98]	training's l2: 0.000202036	valid_1's l2: 0.000266433
[99]	training's l2: 0.000201459	valid_1's l2: 0.000266356
[100]	training's l2: 0.00020089	valid_1's l2: 0.000266339
[101]	training's l2: 0.000200341	valid_1's l2: 0.000266268
[102]	training's l2: 0.000199793	valid_1's l2: 0.000266199
[103]	training's l2: 0.000199319	valid_1's l2: 0.000266236
[104]	training's l2: 0.000198675	valid_1's l2: 0.00026623
[105]	training's l2: 0.00019812	valid_1's l2: 0.000266227
[106]	training's l2: 0.000197534	valid_1's l2: 0.000266277
[107]	training's l2: 0.00019702	valid_1's l2: 0.000266329
[108]	training's l2: 0.0001965	valid_1's l2: 0.00026629
[109]	training's l2: 0.000196002	valid_1's l2: 0.000266325
[110]	training's l2: 0.000195453	valid_1's l2: 0.000266345
[111]	training's l2: 0.000194998	valid_1's l2: 0.000266357
[112]	training's l2: 0.000194513	valid_1's l2: 0.000266335
[113]	training's l2: 0.000194009	valid_1's l2: 0.000266332
[114]	training's l2: 0.000193502	valid_1's l2: 0.000266429
[115]	training's l2: 0.000192986	valid_1's l2: 0.000266462
[116]	training's l2: 0.000192518	valid_1's l2: 0.000266453
[117]	training's l2: 0.000192017	valid_1's l2: 0.00026657
[118]	training's l2: 0.000191447	valid_1's l2: 0.000266653
[119]	training's l2: 0.000191011	valid_1's l2: 0.000266662
[120]	training's l2: 0.000190465	valid_1's l2: 0.000266757
[121]	training's l2: 0.000189962	valid_1's l2: 0.000266795
[122]	training's l2: 0.00018949	valid_1's l2: 0.000266836
[123]	training's l2: 0.00018898	valid_1's l2: 0.000266806
[124]	training's l2: 0.000188498	valid_1's l2: 0.000266846
[125]	training's l2: 0.000188009	valid_1's l2: 0.00026694
[126]	training's l2: 0.000187541	valid_1's l2: 0.000266987
[127]	training's l2: 0.000187068	valid_1's l2: 0.000266972
[128]	training's l2: 0.000186592	valid_1's l2: 0.000266954
[129]	training's l2: 0.000186136	valid_1's l2: 0.000266863
[130]	training's l2: 0.000185608	valid_1's l2: 0.000266965
[131]	training's l2: 0.000185103	valid_1's l2: 0.00026701
[132]	training's l2: 0.00018464	valid_1's l2: 0.000267014
Early stopping, best iteration is:
[102]	training's l2: 0.000199793	valid_1's l2: 0.000266199
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243208 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000382982	valid_1's l2: 0.000372766
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000370344	valid_1's l2: 0.000360836
[3]	training's l2: 0.000359584	valid_1's l2: 0.000350955
[4]	training's l2: 0.000349989	valid_1's l2: 0.000343012
[5]	training's l2: 0.000341909	valid_1's l2: 0.000335422
[6]	training's l2: 0.000334358	valid_1's l2: 0.000328413
[7]	training's l2: 0.000328021	valid_1's l2: 0.000322482
[8]	training's l2: 0.000322232	valid_1's l2: 0.000317189
[9]	training's l2: 0.000317095	valid_1's l2: 0.00031253
[10]	training's l2: 0.000312467	valid_1's l2: 0.000308819
[11]	training's l2: 0.000308399	valid_1's l2: 0.000305588
[12]	training's l2: 0.000304495	valid_1's l2: 0.000302417
[13]	training's l2: 0.000301135	valid_1's l2: 0.000299863
[14]	training's l2: 0.000298139	valid_1's l2: 0.000297711
[15]	training's l2: 0.000295178	valid_1's l2: 0.000295567
[16]	training's l2: 0.00029263	valid_1's l2: 0.000294098
[17]	training's l2: 0.0002905	valid_1's l2: 0.000292528
[18]	training's l2: 0.000288302	valid_1's l2: 0.000290999
[19]	training's l2: 0.000286273	valid_1's l2: 0.000289627
[20]	training's l2: 0.000284192	valid_1's l2: 0.000288346
[21]	training's l2: 0.000282521	valid_1's l2: 0.000287295
[22]	training's l2: 0.000280746	valid_1's l2: 0.000286184
[23]	training's l2: 0.000279059	valid_1's l2: 0.000285337
[24]	training's l2: 0.000277256	valid_1's l2: 0.000284246
[25]	training's l2: 0.000275827	valid_1's l2: 0.000283533
[26]	training's l2: 0.000274061	valid_1's l2: 0.000282539
[27]	training's l2: 0.000272521	valid_1's l2: 0.000281473
[28]	training's l2: 0.000271227	valid_1's l2: 0.000280774
[29]	training's l2: 0.000269912	valid_1's l2: 0.000279999
[30]	training's l2: 0.000268759	valid_1's l2: 0.000279568
[31]	training's l2: 0.000267339	valid_1's l2: 0.0002786
[32]	training's l2: 0.000266272	valid_1's l2: 0.000278123
[33]	training's l2: 0.000264643	valid_1's l2: 0.00027717
[34]	training's l2: 0.000263487	valid_1's l2: 0.000276678
[35]	training's l2: 0.000262512	valid_1's l2: 0.000276297
[36]	training's l2: 0.000261588	valid_1's l2: 0.000276106
[37]	training's l2: 0.000260321	valid_1's l2: 0.000275386
[38]	training's l2: 0.000259469	valid_1's l2: 0.000275085
[39]	training's l2: 0.000258526	valid_1's l2: 0.000274736
[40]	training's l2: 0.000257731	valid_1's l2: 0.000274475
[41]	training's l2: 0.000256565	valid_1's l2: 0.000273883
[42]	training's l2: 0.000255708	valid_1's l2: 0.000273698
[43]	training's l2: 0.000254545	valid_1's l2: 0.00027319
[44]	training's l2: 0.000253503	valid_1's l2: 0.000272674
[45]	training's l2: 0.000252667	valid_1's l2: 0.000272423
[46]	training's l2: 0.000251814	valid_1's l2: 0.000272042
[47]	training's l2: 0.000251047	valid_1's l2: 0.000271827
[48]	training's l2: 0.000250329	valid_1's l2: 0.00027159
[49]	training's l2: 0.000249565	valid_1's l2: 0.000271291
[50]	training's l2: 0.000248762	valid_1's l2: 0.000271052
[51]	training's l2: 0.000247934	valid_1's l2: 0.000270587
[52]	training's l2: 0.000247238	valid_1's l2: 0.000270422
[53]	training's l2: 0.000246543	valid_1's l2: 0.000270349
[54]	training's l2: 0.000245838	valid_1's l2: 0.000270106
[55]	training's l2: 0.000245204	valid_1's l2: 0.000270063
[56]	training's l2: 0.000244152	valid_1's l2: 0.000269673
[57]	training's l2: 0.000243161	valid_1's l2: 0.000269174
[58]	training's l2: 0.000242523	valid_1's l2: 0.000269034
[59]	training's l2: 0.00024183	valid_1's l2: 0.000268976
[60]	training's l2: 0.00024106	valid_1's l2: 0.000268822
[61]	training's l2: 0.000240335	valid_1's l2: 0.000268616
[62]	training's l2: 0.00023967	valid_1's l2: 0.000268415
[63]	training's l2: 0.000239056	valid_1's l2: 0.00026835
[64]	training's l2: 0.000238357	valid_1's l2: 0.000268196
[65]	training's l2: 0.000237771	valid_1's l2: 0.000268128
[66]	training's l2: 0.000237222	valid_1's l2: 0.000268049
[67]	training's l2: 0.000236525	valid_1's l2: 0.00026777
[68]	training's l2: 0.000235853	valid_1's l2: 0.00026765
[69]	training's l2: 0.000235028	valid_1's l2: 0.000267409
[70]	training's l2: 0.000234486	valid_1's l2: 0.000267242
[71]	training's l2: 0.00023377	valid_1's l2: 0.000266951
[72]	training's l2: 0.00023307	valid_1's l2: 0.00026685
[73]	training's l2: 0.000232496	valid_1's l2: 0.000266854
[74]	training's l2: 0.00023189	valid_1's l2: 0.000266744
[75]	training's l2: 0.000231235	valid_1's l2: 0.000266511
[76]	training's l2: 0.000230703	valid_1's l2: 0.000266617
[77]	training's l2: 0.000230122	valid_1's l2: 0.000266672
[78]	training's l2: 0.000229519	valid_1's l2: 0.000266484
[79]	training's l2: 0.000229	valid_1's l2: 0.000266499
[80]	training's l2: 0.000228472	valid_1's l2: 0.000266419
[81]	training's l2: 0.000227892	valid_1's l2: 0.000266291
[82]	training's l2: 0.000227374	valid_1's l2: 0.000266284
[83]	training's l2: 0.000226836	valid_1's l2: 0.000266167
[84]	training's l2: 0.000226231	valid_1's l2: 0.000266067
[85]	training's l2: 0.000225706	valid_1's l2: 0.000266112
[86]	training's l2: 0.000225126	valid_1's l2: 0.000265974
[87]	training's l2: 0.000224614	valid_1's l2: 0.000265886
[88]	training's l2: 0.000224065	valid_1's l2: 0.000265882
[89]	training's l2: 0.000223449	valid_1's l2: 0.000265843
[90]	training's l2: 0.000222908	valid_1's l2: 0.000265771
[91]	training's l2: 0.000222351	valid_1's l2: 0.000265689
[92]	training's l2: 0.000221925	valid_1's l2: 0.000265751
[93]	training's l2: 0.00022144	valid_1's l2: 0.000265778
[94]	training's l2: 0.000220982	valid_1's l2: 0.000265818
[95]	training's l2: 0.000220493	valid_1's l2: 0.000265889
[96]	training's l2: 0.000220046	valid_1's l2: 0.000265897
[97]	training's l2: 0.000219591	valid_1's l2: 0.000265903
[98]	training's l2: 0.000219155	valid_1's l2: 0.000266025
[99]	training's l2: 0.000218699	valid_1's l2: 0.000266082
[100]	training's l2: 0.000218237	valid_1's l2: 0.000266053
[101]	training's l2: 0.000217816	valid_1's l2: 0.000266104
[102]	training's l2: 0.000217379	valid_1's l2: 0.00026619
[103]	training's l2: 0.000216881	valid_1's l2: 0.000266282
[104]	training's l2: 0.000216441	valid_1's l2: 0.000266284
[105]	training's l2: 0.000215936	valid_1's l2: 0.000266223
[106]	training's l2: 0.000215423	valid_1's l2: 0.000266128
[107]	training's l2: 0.00021503	valid_1's l2: 0.000266253
[108]	training's l2: 0.000214589	valid_1's l2: 0.000266315
[109]	training's l2: 0.000214165	valid_1's l2: 0.000266356
[110]	training's l2: 0.000213741	valid_1's l2: 0.000266267
[111]	training's l2: 0.000213313	valid_1's l2: 0.000266194
[112]	training's l2: 0.000212876	valid_1's l2: 0.000266222
[113]	training's l2: 0.000212436	valid_1's l2: 0.000266178
[114]	training's l2: 0.000212016	valid_1's l2: 0.000266092
[115]	training's l2: 0.000211643	valid_1's l2: 0.000266093
[116]	training's l2: 0.000211216	valid_1's l2: 0.000266181
[117]	training's l2: 0.000210793	valid_1's l2: 0.000266185
Did not meet early stopping. Best iteration is:
[117]	training's l2: 0.000210793	valid_1's l2: 0.000266185
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.229132 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000383709	valid_1's l2: 0.00037381
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000371492	valid_1's l2: 0.000362587
[3]	training's l2: 0.000360891	valid_1's l2: 0.000352628
[4]	training's l2: 0.000351536	valid_1's l2: 0.00034476
[5]	training's l2: 0.000343208	valid_1's l2: 0.000337084
[6]	training's l2: 0.000335932	valid_1's l2: 0.00033081
[7]	training's l2: 0.000329691	valid_1's l2: 0.000325197
[8]	training's l2: 0.000323728	valid_1's l2: 0.000319868
[9]	training's l2: 0.000318625	valid_1's l2: 0.000315142
[10]	training's l2: 0.000313896	valid_1's l2: 0.000311192
[11]	training's l2: 0.000309803	valid_1's l2: 0.000307703
[12]	training's l2: 0.00030583	valid_1's l2: 0.000304611
[13]	training's l2: 0.000302342	valid_1's l2: 0.00030199
[14]	training's l2: 0.000299178	valid_1's l2: 0.000299912
[15]	training's l2: 0.000296098	valid_1's l2: 0.000297626
[16]	training's l2: 0.000293424	valid_1's l2: 0.000295888
[17]	training's l2: 0.000290937	valid_1's l2: 0.000293983
[18]	training's l2: 0.000288498	valid_1's l2: 0.000292351
[19]	training's l2: 0.000286236	valid_1's l2: 0.000290988
[20]	training's l2: 0.000284052	valid_1's l2: 0.000289561
[21]	training's l2: 0.000282192	valid_1's l2: 0.000288366
[22]	training's l2: 0.000280315	valid_1's l2: 0.000287099
[23]	training's l2: 0.000278684	valid_1's l2: 0.000286145
[24]	training's l2: 0.000277087	valid_1's l2: 0.000285043
[25]	training's l2: 0.000275477	valid_1's l2: 0.000284208
[26]	training's l2: 0.000273779	valid_1's l2: 0.000283084
[27]	training's l2: 0.000272301	valid_1's l2: 0.000282323
[28]	training's l2: 0.000271013	valid_1's l2: 0.000281669
[29]	training's l2: 0.000269313	valid_1's l2: 0.00028063
[30]	training's l2: 0.000268032	valid_1's l2: 0.000280122
[31]	training's l2: 0.000266806	valid_1's l2: 0.00027954
[32]	training's l2: 0.000265205	valid_1's l2: 0.000278741
[33]	training's l2: 0.000263893	valid_1's l2: 0.000278027
[34]	training's l2: 0.00026291	valid_1's l2: 0.000277569
[35]	training's l2: 0.000261801	valid_1's l2: 0.000277139
[36]	training's l2: 0.0002608	valid_1's l2: 0.000276725
[37]	training's l2: 0.000259371	valid_1's l2: 0.000276174
[38]	training's l2: 0.000258428	valid_1's l2: 0.000275876
[39]	training's l2: 0.000257467	valid_1's l2: 0.000275515
[40]	training's l2: 0.000256617	valid_1's l2: 0.000275259
[41]	training's l2: 0.000255658	valid_1's l2: 0.000275004
[42]	training's l2: 0.000254591	valid_1's l2: 0.000274528
[43]	training's l2: 0.000253761	valid_1's l2: 0.000274383
[44]	training's l2: 0.000252997	valid_1's l2: 0.000274271
[45]	training's l2: 0.000251737	valid_1's l2: 0.000273545
[46]	training's l2: 0.00025089	valid_1's l2: 0.00027339
[47]	training's l2: 0.000249839	valid_1's l2: 0.000272958
[48]	training's l2: 0.000248945	valid_1's l2: 0.000272585
[49]	training's l2: 0.000248088	valid_1's l2: 0.000272549
[50]	training's l2: 0.000247304	valid_1's l2: 0.000272297
[51]	training's l2: 0.000246629	valid_1's l2: 0.000272146
[52]	training's l2: 0.000245688	valid_1's l2: 0.000271876
[53]	training's l2: 0.000244668	valid_1's l2: 0.000271368
[54]	training's l2: 0.000243901	valid_1's l2: 0.0002712
[55]	training's l2: 0.000243268	valid_1's l2: 0.000271206
[56]	training's l2: 0.000242339	valid_1's l2: 0.000270953
[57]	training's l2: 0.000241611	valid_1's l2: 0.000270909
[58]	training's l2: 0.000240837	valid_1's l2: 0.000270678
[59]	training's l2: 0.000240113	valid_1's l2: 0.000270524
[60]	training's l2: 0.000239365	valid_1's l2: 0.000270252
[61]	training's l2: 0.000238656	valid_1's l2: 0.000270031
[62]	training's l2: 0.000237918	valid_1's l2: 0.000269861
[63]	training's l2: 0.000237294	valid_1's l2: 0.000269791
[64]	training's l2: 0.00023663	valid_1's l2: 0.000269793
[65]	training's l2: 0.000236064	valid_1's l2: 0.00026969
[66]	training's l2: 0.000235207	valid_1's l2: 0.000269569
[67]	training's l2: 0.00023446	valid_1's l2: 0.000269355
[68]	training's l2: 0.00023387	valid_1's l2: 0.000269232
[69]	training's l2: 0.000233113	valid_1's l2: 0.000269022
[70]	training's l2: 0.000232514	valid_1's l2: 0.000268975
[71]	training's l2: 0.000231931	valid_1's l2: 0.000269094
[72]	training's l2: 0.000231274	valid_1's l2: 0.000269002
[73]	training's l2: 0.000230649	valid_1's l2: 0.000268897
[74]	training's l2: 0.000230001	valid_1's l2: 0.000268719
[75]	training's l2: 0.000229408	valid_1's l2: 0.000268803
[76]	training's l2: 0.000228852	valid_1's l2: 0.000268823
[77]	training's l2: 0.000228123	valid_1's l2: 0.000268698
[78]	training's l2: 0.000227622	valid_1's l2: 0.000268758
[79]	training's l2: 0.000227068	valid_1's l2: 0.000268806
[80]	training's l2: 0.000226391	valid_1's l2: 0.000268615
[81]	training's l2: 0.00022577	valid_1's l2: 0.000268634
[82]	training's l2: 0.000225227	valid_1's l2: 0.000268536
[83]	training's l2: 0.000224636	valid_1's l2: 0.000268513
[84]	training's l2: 0.000224088	valid_1's l2: 0.000268512
[85]	training's l2: 0.000223442	valid_1's l2: 0.000268349
[86]	training's l2: 0.000222815	valid_1's l2: 0.000268312
[87]	training's l2: 0.000222271	valid_1's l2: 0.000268307
[88]	training's l2: 0.000221694	valid_1's l2: 0.000268253
[89]	training's l2: 0.000221178	valid_1's l2: 0.000268304
[90]	training's l2: 0.000220558	valid_1's l2: 0.000268263
[91]	training's l2: 0.000220034	valid_1's l2: 0.000268241
[92]	training's l2: 0.000219469	valid_1's l2: 0.000268131
[93]	training's l2: 0.000218969	valid_1's l2: 0.000268002
[94]	training's l2: 0.000218462	valid_1's l2: 0.000268046
[95]	training's l2: 0.000217931	valid_1's l2: 0.000268008
[96]	training's l2: 0.000217408	valid_1's l2: 0.000267868
[97]	training's l2: 0.000216895	valid_1's l2: 0.000267938
[98]	training's l2: 0.000216441	valid_1's l2: 0.000268012
[99]	training's l2: 0.000215933	valid_1's l2: 0.000267997
[100]	training's l2: 0.000215436	valid_1's l2: 0.000268027
[101]	training's l2: 0.000214836	valid_1's l2: 0.000268083
[102]	training's l2: 0.000214348	valid_1's l2: 0.000268182
[103]	training's l2: 0.000213876	valid_1's l2: 0.000268192
[104]	training's l2: 0.000213485	valid_1's l2: 0.000268146
[105]	training's l2: 0.000213029	valid_1's l2: 0.000268116
[106]	training's l2: 0.000212493	valid_1's l2: 0.000268099
[107]	training's l2: 0.00021203	valid_1's l2: 0.000268023
[108]	training's l2: 0.000211531	valid_1's l2: 0.000267879
[109]	training's l2: 0.000211112	valid_1's l2: 0.000267889
[110]	training's l2: 0.000210676	valid_1's l2: 0.000267894
[111]	training's l2: 0.000210211	valid_1's l2: 0.000267907
[112]	training's l2: 0.000209765	valid_1's l2: 0.000267907
[113]	training's l2: 0.000209292	valid_1's l2: 0.000268019
[114]	training's l2: 0.000208857	valid_1's l2: 0.000267974
[115]	training's l2: 0.000208409	valid_1's l2: 0.000268042
[116]	training's l2: 0.000207958	valid_1's l2: 0.000268162
[117]	training's l2: 0.000207491	valid_1's l2: 0.000268159
[118]	training's l2: 0.000207068	valid_1's l2: 0.000268074
[119]	training's l2: 0.000206667	valid_1's l2: 0.00026804
[120]	training's l2: 0.000206228	valid_1's l2: 0.00026797
[121]	training's l2: 0.000205774	valid_1's l2: 0.000268017
[122]	training's l2: 0.000205327	valid_1's l2: 0.000268074
[123]	training's l2: 0.000204917	valid_1's l2: 0.000268123
[124]	training's l2: 0.000204547	valid_1's l2: 0.000268091
[125]	training's l2: 0.000204091	valid_1's l2: 0.000268026
[126]	training's l2: 0.000203658	valid_1's l2: 0.000268049
Early stopping, best iteration is:
[96]	training's l2: 0.000217408	valid_1's l2: 0.000267868
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.223472 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000984436	valid_1's l2: 0.000892225
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000967887	valid_1's l2: 0.000880367
[3]	training's l2: 0.000952541	valid_1's l2: 0.00086841
[4]	training's l2: 0.000937975	valid_1's l2: 0.000857271
[5]	training's l2: 0.000925604	valid_1's l2: 0.000848374
[6]	training's l2: 0.000914297	valid_1's l2: 0.000839742
[7]	training's l2: 0.000904621	valid_1's l2: 0.000833601
[8]	training's l2: 0.000894878	valid_1's l2: 0.000826808
[9]	training's l2: 0.000885432	valid_1's l2: 0.000820243
[10]	training's l2: 0.000876642	valid_1's l2: 0.00081368
[11]	training's l2: 0.000868776	valid_1's l2: 0.000809009
[12]	training's l2: 0.00086274	valid_1's l2: 0.000805208
[13]	training's l2: 0.000856164	valid_1's l2: 0.000800364
[14]	training's l2: 0.000850067	valid_1's l2: 0.000796583
[15]	training's l2: 0.000843893	valid_1's l2: 0.000791817
[16]	training's l2: 0.000839372	valid_1's l2: 0.000788834
[17]	training's l2: 0.00083474	valid_1's l2: 0.000786502
[18]	training's l2: 0.000830088	valid_1's l2: 0.000782632
[19]	training's l2: 0.000824821	valid_1's l2: 0.000779687
[20]	training's l2: 0.000821208	valid_1's l2: 0.000778447
[21]	training's l2: 0.000817308	valid_1's l2: 0.000776033
[22]	training's l2: 0.000813625	valid_1's l2: 0.000774046
[23]	training's l2: 0.000810136	valid_1's l2: 0.000771941
[24]	training's l2: 0.000806414	valid_1's l2: 0.000768699
[25]	training's l2: 0.000802991	valid_1's l2: 0.00076599
[26]	training's l2: 0.00079789	valid_1's l2: 0.000761375
[27]	training's l2: 0.000794325	valid_1's l2: 0.000760333
[28]	training's l2: 0.000791136	valid_1's l2: 0.000758545
[29]	training's l2: 0.0007882	valid_1's l2: 0.000756728
[30]	training's l2: 0.000784893	valid_1's l2: 0.000754812
[31]	training's l2: 0.000781717	valid_1's l2: 0.000752703
[32]	training's l2: 0.000778956	valid_1's l2: 0.000751899
[33]	training's l2: 0.00077636	valid_1's l2: 0.000750563
[34]	training's l2: 0.000773736	valid_1's l2: 0.000749606
[35]	training's l2: 0.000770256	valid_1's l2: 0.000747518
[36]	training's l2: 0.000767393	valid_1's l2: 0.000745459
[37]	training's l2: 0.000765373	valid_1's l2: 0.000744666
[38]	training's l2: 0.000761655	valid_1's l2: 0.00074274
[39]	training's l2: 0.000759238	valid_1's l2: 0.000741194
[40]	training's l2: 0.00075612	valid_1's l2: 0.000739548
[41]	training's l2: 0.000753422	valid_1's l2: 0.000738212
[42]	training's l2: 0.000751482	valid_1's l2: 0.000737223
[43]	training's l2: 0.000749365	valid_1's l2: 0.000736598
[44]	training's l2: 0.000747385	valid_1's l2: 0.000736239
[45]	training's l2: 0.000745133	valid_1's l2: 0.000735725
[46]	training's l2: 0.000743322	valid_1's l2: 0.000735009
[47]	training's l2: 0.000741113	valid_1's l2: 0.000734181
[48]	training's l2: 0.000739327	valid_1's l2: 0.000733702
[49]	training's l2: 0.000736385	valid_1's l2: 0.000732757
[50]	training's l2: 0.000734124	valid_1's l2: 0.000731661
[51]	training's l2: 0.000731928	valid_1's l2: 0.000731164
[52]	training's l2: 0.000730415	valid_1's l2: 0.000730792
[53]	training's l2: 0.000728392	valid_1's l2: 0.000730022
[54]	training's l2: 0.000725777	valid_1's l2: 0.000727955
[55]	training's l2: 0.00072413	valid_1's l2: 0.00072752
[56]	training's l2: 0.000722326	valid_1's l2: 0.000726926
[57]	training's l2: 0.000720079	valid_1's l2: 0.000726354
[58]	training's l2: 0.000718263	valid_1's l2: 0.000725167
[59]	training's l2: 0.000716597	valid_1's l2: 0.000724803
[60]	training's l2: 0.000714532	valid_1's l2: 0.000723654
[61]	training's l2: 0.000712542	valid_1's l2: 0.000723337
[62]	training's l2: 0.000710989	valid_1's l2: 0.000723381
[63]	training's l2: 0.000709643	valid_1's l2: 0.000722927
[64]	training's l2: 0.000707976	valid_1's l2: 0.000722616
[65]	training's l2: 0.000706075	valid_1's l2: 0.000722349
[66]	training's l2: 0.000704556	valid_1's l2: 0.000721692
[67]	training's l2: 0.000702308	valid_1's l2: 0.000719962
[68]	training's l2: 0.000700202	valid_1's l2: 0.000718784
[69]	training's l2: 0.000698228	valid_1's l2: 0.000717376
[70]	training's l2: 0.000696607	valid_1's l2: 0.000717327
[71]	training's l2: 0.000695226	valid_1's l2: 0.00071714
[72]	training's l2: 0.000693674	valid_1's l2: 0.000716831
[73]	training's l2: 0.000692148	valid_1's l2: 0.000716725
[74]	training's l2: 0.000690851	valid_1's l2: 0.000716779
[75]	training's l2: 0.000689524	valid_1's l2: 0.000716521
[76]	training's l2: 0.000688157	valid_1's l2: 0.000716756
[77]	training's l2: 0.000686553	valid_1's l2: 0.000716849
[78]	training's l2: 0.000684515	valid_1's l2: 0.000716347
[79]	training's l2: 0.00068307	valid_1's l2: 0.000716487
[80]	training's l2: 0.00068154	valid_1's l2: 0.000716191
[81]	training's l2: 0.000679959	valid_1's l2: 0.000715895
[82]	training's l2: 0.000678318	valid_1's l2: 0.000715943
[83]	training's l2: 0.000676889	valid_1's l2: 0.000715394
[84]	training's l2: 0.000675539	valid_1's l2: 0.000715482
[85]	training's l2: 0.00067393	valid_1's l2: 0.000715363
[86]	training's l2: 0.000672415	valid_1's l2: 0.000715352
[87]	training's l2: 0.000670788	valid_1's l2: 0.000715109
[88]	training's l2: 0.000669265	valid_1's l2: 0.000715009
[89]	training's l2: 0.000667868	valid_1's l2: 0.00071465
[90]	training's l2: 0.000666079	valid_1's l2: 0.000713984
[91]	training's l2: 0.000664495	valid_1's l2: 0.000713413
[92]	training's l2: 0.00066316	valid_1's l2: 0.000713202
[93]	training's l2: 0.000661739	valid_1's l2: 0.00071318
[94]	training's l2: 0.00066044	valid_1's l2: 0.000712949
[95]	training's l2: 0.000659325	valid_1's l2: 0.000712733
[96]	training's l2: 0.000657541	valid_1's l2: 0.000712392
[97]	training's l2: 0.000655832	valid_1's l2: 0.000712112
[98]	training's l2: 0.000654307	valid_1's l2: 0.000712115
[99]	training's l2: 0.000653009	valid_1's l2: 0.000712243
[100]	training's l2: 0.000651761	valid_1's l2: 0.000711972
[101]	training's l2: 0.000650252	valid_1's l2: 0.000711712
[102]	training's l2: 0.000648998	valid_1's l2: 0.000711847
[103]	training's l2: 0.000647833	valid_1's l2: 0.000711812
[104]	training's l2: 0.000646732	valid_1's l2: 0.000711701
[105]	training's l2: 0.00064532	valid_1's l2: 0.000711651
[106]	training's l2: 0.00064411	valid_1's l2: 0.000711721
[107]	training's l2: 0.000643002	valid_1's l2: 0.000711556
[108]	training's l2: 0.000641555	valid_1's l2: 0.000711265
[109]	training's l2: 0.000640396	valid_1's l2: 0.000710764
[110]	training's l2: 0.000639297	valid_1's l2: 0.000710591
[111]	training's l2: 0.000638024	valid_1's l2: 0.000710462
[112]	training's l2: 0.000636973	valid_1's l2: 0.0007105
[113]	training's l2: 0.000635794	valid_1's l2: 0.000710369
[114]	training's l2: 0.000634652	valid_1's l2: 0.000710468
[115]	training's l2: 0.000633263	valid_1's l2: 0.000710647
[116]	training's l2: 0.000632036	valid_1's l2: 0.000710242
[117]	training's l2: 0.00063091	valid_1's l2: 0.000710305
[118]	training's l2: 0.000629921	valid_1's l2: 0.00071015
[119]	training's l2: 0.00062874	valid_1's l2: 0.000710056
[120]	training's l2: 0.000627391	valid_1's l2: 0.000709741
[121]	training's l2: 0.000625893	valid_1's l2: 0.000709447
[122]	training's l2: 0.000624771	valid_1's l2: 0.000709231
[123]	training's l2: 0.000623892	valid_1's l2: 0.000709268
[124]	training's l2: 0.000622648	valid_1's l2: 0.000709347
[125]	training's l2: 0.000621513	valid_1's l2: 0.000709087
[126]	training's l2: 0.000620303	valid_1's l2: 0.000708897
[127]	training's l2: 0.000618831	valid_1's l2: 0.000708392
[128]	training's l2: 0.000617658	valid_1's l2: 0.000708397
[129]	training's l2: 0.000616665	valid_1's l2: 0.000708702
[130]	training's l2: 0.000615497	valid_1's l2: 0.000708703
[131]	training's l2: 0.000614424	valid_1's l2: 0.000708297
[132]	training's l2: 0.000613197	valid_1's l2: 0.0007082
[133]	training's l2: 0.000611983	valid_1's l2: 0.000708131
[134]	training's l2: 0.000610835	valid_1's l2: 0.000708248
[135]	training's l2: 0.000609658	valid_1's l2: 0.00070821
[136]	training's l2: 0.000608589	valid_1's l2: 0.000708169
[137]	training's l2: 0.000607406	valid_1's l2: 0.000708049
[138]	training's l2: 0.000606221	valid_1's l2: 0.000707618
[139]	training's l2: 0.000605177	valid_1's l2: 0.000707537
[140]	training's l2: 0.00060399	valid_1's l2: 0.00070772
[141]	training's l2: 0.000603141	valid_1's l2: 0.000707628
[142]	training's l2: 0.000602223	valid_1's l2: 0.000707719
[143]	training's l2: 0.00060123	valid_1's l2: 0.000707875
[144]	training's l2: 0.000600187	valid_1's l2: 0.000707848
[145]	training's l2: 0.000599342	valid_1's l2: 0.00070794
[146]	training's l2: 0.000598065	valid_1's l2: 0.000707601
[147]	training's l2: 0.000596833	valid_1's l2: 0.000707368
[148]	training's l2: 0.000595962	valid_1's l2: 0.000707377
[149]	training's l2: 0.000594852	valid_1's l2: 0.00070727
[150]	training's l2: 0.000594011	valid_1's l2: 0.00070745
[151]	training's l2: 0.000592863	valid_1's l2: 0.000707634
[152]	training's l2: 0.000592002	valid_1's l2: 0.000707647
[153]	training's l2: 0.00059118	valid_1's l2: 0.000707729
[154]	training's l2: 0.000590086	valid_1's l2: 0.000707702
[155]	training's l2: 0.000589095	valid_1's l2: 0.000707688
[156]	training's l2: 0.000587997	valid_1's l2: 0.000707538
[157]	training's l2: 0.000586792	valid_1's l2: 0.000707832
[158]	training's l2: 0.000585939	valid_1's l2: 0.000707861
[159]	training's l2: 0.000584917	valid_1's l2: 0.000707987
[160]	training's l2: 0.000584064	valid_1's l2: 0.000708315
[161]	training's l2: 0.000583181	valid_1's l2: 0.000708287
[162]	training's l2: 0.00058219	valid_1's l2: 0.000708147
[163]	training's l2: 0.000581294	valid_1's l2: 0.000708095
[164]	training's l2: 0.000580577	valid_1's l2: 0.000708016
[165]	training's l2: 0.00057953	valid_1's l2: 0.000708031
[166]	training's l2: 0.000578549	valid_1's l2: 0.000707835
[167]	training's l2: 0.000577527	valid_1's l2: 0.000707742
[168]	training's l2: 0.00057651	valid_1's l2: 0.000707718
[169]	training's l2: 0.000575482	valid_1's l2: 0.000707818
[170]	training's l2: 0.000574359	valid_1's l2: 0.000707929
[171]	training's l2: 0.000573306	valid_1's l2: 0.000707951
[172]	training's l2: 0.000572363	valid_1's l2: 0.000707905
[173]	training's l2: 0.000571434	valid_1's l2: 0.000707793
[174]	training's l2: 0.000570425	valid_1's l2: 0.000707852
[175]	training's l2: 0.000569636	valid_1's l2: 0.000707841
[176]	training's l2: 0.000568908	valid_1's l2: 0.000707813
[177]	training's l2: 0.000567749	valid_1's l2: 0.000707773
[178]	training's l2: 0.000566861	valid_1's l2: 0.000708003
[179]	training's l2: 0.000565987	valid_1's l2: 0.000708095
Early stopping, best iteration is:
[149]	training's l2: 0.000594852	valid_1's l2: 0.00070727
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216874 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000979652	valid_1's l2: 0.000889581
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000958939	valid_1's l2: 0.000875262
[3]	training's l2: 0.000941031	valid_1's l2: 0.000862253
[4]	training's l2: 0.000924367	valid_1's l2: 0.000850113
[5]	training's l2: 0.000909811	valid_1's l2: 0.000839526
[6]	training's l2: 0.000897315	valid_1's l2: 0.000831755
[7]	training's l2: 0.00088527	valid_1's l2: 0.000824181
[8]	training's l2: 0.000874411	valid_1's l2: 0.000817244
[9]	training's l2: 0.000865286	valid_1's l2: 0.000811386
[10]	training's l2: 0.000855871	valid_1's l2: 0.000807025
[11]	training's l2: 0.000848443	valid_1's l2: 0.00080324
[12]	training's l2: 0.000839448	valid_1's l2: 0.000795762
[13]	training's l2: 0.000832672	valid_1's l2: 0.000792278
[14]	training's l2: 0.000825363	valid_1's l2: 0.00078736
[15]	training's l2: 0.000818094	valid_1's l2: 0.000782988
[16]	training's l2: 0.000812482	valid_1's l2: 0.000780794
[17]	training's l2: 0.000806803	valid_1's l2: 0.000777261
[18]	training's l2: 0.000801903	valid_1's l2: 0.000774005
[19]	training's l2: 0.000796228	valid_1's l2: 0.000770821
[20]	training's l2: 0.00079139	valid_1's l2: 0.000769083
[21]	training's l2: 0.000786678	valid_1's l2: 0.000766723
[22]	training's l2: 0.000782148	valid_1's l2: 0.000764478
[23]	training's l2: 0.00077631	valid_1's l2: 0.000761349
[24]	training's l2: 0.000771622	valid_1's l2: 0.00075916
[25]	training's l2: 0.000767434	valid_1's l2: 0.000757681
[26]	training's l2: 0.000763114	valid_1's l2: 0.000755464
[27]	training's l2: 0.000758591	valid_1's l2: 0.000753815
[28]	training's l2: 0.000755527	valid_1's l2: 0.000752398
[29]	training's l2: 0.000751969	valid_1's l2: 0.000750485
[30]	training's l2: 0.000748142	valid_1's l2: 0.000748899
[31]	training's l2: 0.000744217	valid_1's l2: 0.000746422
[32]	training's l2: 0.000740097	valid_1's l2: 0.000744246
[33]	training's l2: 0.00073667	valid_1's l2: 0.000743013
[34]	training's l2: 0.000733339	valid_1's l2: 0.000741209
[35]	training's l2: 0.000730308	valid_1's l2: 0.000740532
[36]	training's l2: 0.000726854	valid_1's l2: 0.000738556
[37]	training's l2: 0.000723144	valid_1's l2: 0.000736161
[38]	training's l2: 0.000720219	valid_1's l2: 0.000735449
[39]	training's l2: 0.000717355	valid_1's l2: 0.000734671
[40]	training's l2: 0.000713441	valid_1's l2: 0.000733471
[41]	training's l2: 0.000710598	valid_1's l2: 0.00073249
[42]	training's l2: 0.000706536	valid_1's l2: 0.000729712
[43]	training's l2: 0.000703783	valid_1's l2: 0.000728771
[44]	training's l2: 0.000700671	valid_1's l2: 0.000727615
[45]	training's l2: 0.000698404	valid_1's l2: 0.000726803
[46]	training's l2: 0.000696082	valid_1's l2: 0.000726369
[47]	training's l2: 0.000693822	valid_1's l2: 0.000726386
[48]	training's l2: 0.000691318	valid_1's l2: 0.000725947
[49]	training's l2: 0.000688399	valid_1's l2: 0.000724169
[50]	training's l2: 0.000685886	valid_1's l2: 0.000723595
[51]	training's l2: 0.000683111	valid_1's l2: 0.000722645
[52]	training's l2: 0.000680735	valid_1's l2: 0.000722444
[53]	training's l2: 0.000678593	valid_1's l2: 0.000721957
[54]	training's l2: 0.000675908	valid_1's l2: 0.000721273
[55]	training's l2: 0.000672862	valid_1's l2: 0.000720549
[56]	training's l2: 0.00067005	valid_1's l2: 0.000719765
[57]	training's l2: 0.00066784	valid_1's l2: 0.000719507
[58]	training's l2: 0.000665541	valid_1's l2: 0.000718354
[59]	training's l2: 0.000663028	valid_1's l2: 0.000717821
[60]	training's l2: 0.00066084	valid_1's l2: 0.000717288
[61]	training's l2: 0.000658134	valid_1's l2: 0.000715717
[62]	training's l2: 0.000655302	valid_1's l2: 0.000715562
[63]	training's l2: 0.000653261	valid_1's l2: 0.000715079
[64]	training's l2: 0.000651123	valid_1's l2: 0.000715185
[65]	training's l2: 0.000649065	valid_1's l2: 0.000714974
[66]	training's l2: 0.000647141	valid_1's l2: 0.000714893
[67]	training's l2: 0.000644962	valid_1's l2: 0.000715058
[68]	training's l2: 0.000642825	valid_1's l2: 0.000714509
[69]	training's l2: 0.000640791	valid_1's l2: 0.000714283
[70]	training's l2: 0.000638852	valid_1's l2: 0.000714239
[71]	training's l2: 0.000636923	valid_1's l2: 0.000714227
[72]	training's l2: 0.000635063	valid_1's l2: 0.000714005
[73]	training's l2: 0.000632787	valid_1's l2: 0.000714044
[74]	training's l2: 0.000630265	valid_1's l2: 0.000714039
[75]	training's l2: 0.000628324	valid_1's l2: 0.00071428
[76]	training's l2: 0.000626342	valid_1's l2: 0.000714126
[77]	training's l2: 0.000624739	valid_1's l2: 0.000714141
[78]	training's l2: 0.00062264	valid_1's l2: 0.000714036
[79]	training's l2: 0.00062078	valid_1's l2: 0.000713957
[80]	training's l2: 0.00061897	valid_1's l2: 0.000713373
[81]	training's l2: 0.000616943	valid_1's l2: 0.000712737
[82]	training's l2: 0.000615222	valid_1's l2: 0.000712686
[83]	training's l2: 0.000613101	valid_1's l2: 0.000712776
[84]	training's l2: 0.000611231	valid_1's l2: 0.0007125
[85]	training's l2: 0.000609255	valid_1's l2: 0.000712247
[86]	training's l2: 0.000607743	valid_1's l2: 0.00071223
[87]	training's l2: 0.000606018	valid_1's l2: 0.000712079
[88]	training's l2: 0.000604139	valid_1's l2: 0.000711635
[89]	training's l2: 0.000602399	valid_1's l2: 0.000711551
[90]	training's l2: 0.000600619	valid_1's l2: 0.000711361
[91]	training's l2: 0.000598816	valid_1's l2: 0.000711521
[92]	training's l2: 0.000596806	valid_1's l2: 0.000711631
[93]	training's l2: 0.000594963	valid_1's l2: 0.000711834
[94]	training's l2: 0.000593551	valid_1's l2: 0.000711828
[95]	training's l2: 0.000591792	valid_1's l2: 0.000711715
[96]	training's l2: 0.000590425	valid_1's l2: 0.000711323
[97]	training's l2: 0.000588615	valid_1's l2: 0.000711266
[98]	training's l2: 0.000587158	valid_1's l2: 0.000711176
[99]	training's l2: 0.000585748	valid_1's l2: 0.000711191
[100]	training's l2: 0.00058356	valid_1's l2: 0.000711088
[101]	training's l2: 0.000581875	valid_1's l2: 0.000710659
[102]	training's l2: 0.000580418	valid_1's l2: 0.000711006
[103]	training's l2: 0.000579247	valid_1's l2: 0.000710823
[104]	training's l2: 0.000577607	valid_1's l2: 0.000710781
[105]	training's l2: 0.000575791	valid_1's l2: 0.00071034
[106]	training's l2: 0.000574528	valid_1's l2: 0.000710538
[107]	training's l2: 0.000573173	valid_1's l2: 0.000710658
[108]	training's l2: 0.000571268	valid_1's l2: 0.000710619
[109]	training's l2: 0.000569558	valid_1's l2: 0.000710766
[110]	training's l2: 0.000568305	valid_1's l2: 0.000710502
[111]	training's l2: 0.000567105	valid_1's l2: 0.000710565
[112]	training's l2: 0.000565428	valid_1's l2: 0.000710524
[113]	training's l2: 0.000563986	valid_1's l2: 0.000710349
[114]	training's l2: 0.000562282	valid_1's l2: 0.000710033
[115]	training's l2: 0.000560815	valid_1's l2: 0.000710322
[116]	training's l2: 0.000559265	valid_1's l2: 0.000710379
[117]	training's l2: 0.00055795	valid_1's l2: 0.000710546
[118]	training's l2: 0.000556275	valid_1's l2: 0.000710715
[119]	training's l2: 0.000554722	valid_1's l2: 0.000710578
[120]	training's l2: 0.000553195	valid_1's l2: 0.000710334
[121]	training's l2: 0.000551895	valid_1's l2: 0.000710438
[122]	training's l2: 0.000550633	valid_1's l2: 0.000710506
[123]	training's l2: 0.000548782	valid_1's l2: 0.000710553
[124]	training's l2: 0.000547221	valid_1's l2: 0.00071065
[125]	training's l2: 0.000546109	valid_1's l2: 0.000710453
[126]	training's l2: 0.000544614	valid_1's l2: 0.000710174
[127]	training's l2: 0.000543412	valid_1's l2: 0.000710079
[128]	training's l2: 0.000542163	valid_1's l2: 0.000710078
[129]	training's l2: 0.000540796	valid_1's l2: 0.000709966
[130]	training's l2: 0.000539381	valid_1's l2: 0.000710178
[131]	training's l2: 0.000538292	valid_1's l2: 0.000710129
[132]	training's l2: 0.000536667	valid_1's l2: 0.000709625
[133]	training's l2: 0.000535462	valid_1's l2: 0.000709676
[134]	training's l2: 0.000534337	valid_1's l2: 0.000709705
[135]	training's l2: 0.000532972	valid_1's l2: 0.000709411
[136]	training's l2: 0.000531608	valid_1's l2: 0.000709348
[137]	training's l2: 0.000530459	valid_1's l2: 0.0007094
[138]	training's l2: 0.000528759	valid_1's l2: 0.000708599
[139]	training's l2: 0.000527397	valid_1's l2: 0.000708713
[140]	training's l2: 0.000525942	valid_1's l2: 0.000708767
[141]	training's l2: 0.000524798	valid_1's l2: 0.000708912
[142]	training's l2: 0.00052353	valid_1's l2: 0.000708639
[143]	training's l2: 0.000522114	valid_1's l2: 0.00070869
[144]	training's l2: 0.000520893	valid_1's l2: 0.000708801
[145]	training's l2: 0.000519611	valid_1's l2: 0.000708761
[146]	training's l2: 0.000518258	valid_1's l2: 0.00070865
[147]	training's l2: 0.000516757	valid_1's l2: 0.000708739
[148]	training's l2: 0.000515647	valid_1's l2: 0.000708816
[149]	training's l2: 0.000514247	valid_1's l2: 0.000708688
[150]	training's l2: 0.000513133	valid_1's l2: 0.000708712
[151]	training's l2: 0.000512054	valid_1's l2: 0.000708708
[152]	training's l2: 0.000510732	valid_1's l2: 0.000708841
[153]	training's l2: 0.000509481	valid_1's l2: 0.000708282
[154]	training's l2: 0.00050819	valid_1's l2: 0.00070831
[155]	training's l2: 0.000507012	valid_1's l2: 0.000708418
[156]	training's l2: 0.000505997	valid_1's l2: 0.000708305
[157]	training's l2: 0.000504817	valid_1's l2: 0.000708077
[158]	training's l2: 0.0005035	valid_1's l2: 0.000708043
[159]	training's l2: 0.000502301	valid_1's l2: 0.000708231
[160]	training's l2: 0.000501203	valid_1's l2: 0.00070801
[161]	training's l2: 0.000500105	valid_1's l2: 0.000707884
[162]	training's l2: 0.000499103	valid_1's l2: 0.000707572
[163]	training's l2: 0.000497923	valid_1's l2: 0.000707379
[164]	training's l2: 0.000497077	valid_1's l2: 0.000707305
[165]	training's l2: 0.000495993	valid_1's l2: 0.000707334
[166]	training's l2: 0.000494741	valid_1's l2: 0.000707312
[167]	training's l2: 0.000493655	valid_1's l2: 0.000707256
[168]	training's l2: 0.000492626	valid_1's l2: 0.000707207
[169]	training's l2: 0.000491717	valid_1's l2: 0.000707182
[170]	training's l2: 0.000490729	valid_1's l2: 0.000707285
[171]	training's l2: 0.000489673	valid_1's l2: 0.000707343
[172]	training's l2: 0.000488605	valid_1's l2: 0.000707414
[173]	training's l2: 0.000487529	valid_1's l2: 0.000707409
[174]	training's l2: 0.000486321	valid_1's l2: 0.000707488
[175]	training's l2: 0.000485118	valid_1's l2: 0.000707398
[176]	training's l2: 0.00048396	valid_1's l2: 0.000707468
[177]	training's l2: 0.000483059	valid_1's l2: 0.000707518
[178]	training's l2: 0.000481957	valid_1's l2: 0.000707595
[179]	training's l2: 0.000480649	valid_1's l2: 0.000707268
[180]	training's l2: 0.000479473	valid_1's l2: 0.000707493
[181]	training's l2: 0.000478405	valid_1's l2: 0.000707387
[182]	training's l2: 0.000477406	valid_1's l2: 0.000707408
[183]	training's l2: 0.000476217	valid_1's l2: 0.000707734
[184]	training's l2: 0.000475119	valid_1's l2: 0.000707656
[185]	training's l2: 0.00047428	valid_1's l2: 0.000707715
[186]	training's l2: 0.000473279	valid_1's l2: 0.000707822
[187]	training's l2: 0.000472295	valid_1's l2: 0.000707639
[188]	training's l2: 0.000471379	valid_1's l2: 0.000707719
[189]	training's l2: 0.000470448	valid_1's l2: 0.000707841
[190]	training's l2: 0.000469479	valid_1's l2: 0.000707907
[191]	training's l2: 0.00046844	valid_1's l2: 0.000708133
[192]	training's l2: 0.000467237	valid_1's l2: 0.000708194
[193]	training's l2: 0.000466084	valid_1's l2: 0.000708444
Did not meet early stopping. Best iteration is:
[193]	training's l2: 0.000466084	valid_1's l2: 0.000708444
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.203292 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000982627	valid_1's l2: 0.00089081
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000964045	valid_1's l2: 0.000876463
[3]	training's l2: 0.000947506	valid_1's l2: 0.00086428
[4]	training's l2: 0.000933789	valid_1's l2: 0.000854377
[5]	training's l2: 0.000921021	valid_1's l2: 0.00084522
[6]	training's l2: 0.000909411	valid_1's l2: 0.000837689
[7]	training's l2: 0.000899037	valid_1's l2: 0.000831333
[8]	training's l2: 0.000889286	valid_1's l2: 0.000824464
[9]	training's l2: 0.000879717	valid_1's l2: 0.000818648
[10]	training's l2: 0.000870477	valid_1's l2: 0.000811418
[11]	training's l2: 0.000861649	valid_1's l2: 0.000803953
[12]	training's l2: 0.000855109	valid_1's l2: 0.000800754
[13]	training's l2: 0.000848922	valid_1's l2: 0.000797182
[14]	training's l2: 0.000842182	valid_1's l2: 0.000793374
[15]	training's l2: 0.000836591	valid_1's l2: 0.000790317
[16]	training's l2: 0.000831118	valid_1's l2: 0.000787139
[17]	training's l2: 0.000825791	valid_1's l2: 0.000783336
[18]	training's l2: 0.000820927	valid_1's l2: 0.000779477
[19]	training's l2: 0.00081671	valid_1's l2: 0.000776089
[20]	training's l2: 0.00081252	valid_1's l2: 0.000774237
[21]	training's l2: 0.00080854	valid_1's l2: 0.000772003
[22]	training's l2: 0.000803479	valid_1's l2: 0.000767989
[23]	training's l2: 0.000799687	valid_1's l2: 0.000765641
[24]	training's l2: 0.000796208	valid_1's l2: 0.000763862
[25]	training's l2: 0.000792741	valid_1's l2: 0.000762379
[26]	training's l2: 0.000789054	valid_1's l2: 0.000760225
[27]	training's l2: 0.00078593	valid_1's l2: 0.000758772
[28]	training's l2: 0.000782278	valid_1's l2: 0.000757114
[29]	training's l2: 0.000778325	valid_1's l2: 0.000754253
[30]	training's l2: 0.000775236	valid_1's l2: 0.000752931
[31]	training's l2: 0.000771986	valid_1's l2: 0.000750893
[32]	training's l2: 0.000769305	valid_1's l2: 0.000749788
[33]	training's l2: 0.00076643	valid_1's l2: 0.000748436
[34]	training's l2: 0.000763239	valid_1's l2: 0.000746862
[35]	training's l2: 0.000759919	valid_1's l2: 0.000745005
[36]	training's l2: 0.000757381	valid_1's l2: 0.000743133
[37]	training's l2: 0.000754013	valid_1's l2: 0.000740963
[38]	training's l2: 0.000751441	valid_1's l2: 0.000740673
[39]	training's l2: 0.000749508	valid_1's l2: 0.000740137
[40]	training's l2: 0.00074696	valid_1's l2: 0.000738877
[41]	training's l2: 0.000744112	valid_1's l2: 0.000737376
[42]	training's l2: 0.000741093	valid_1's l2: 0.000735937
[43]	training's l2: 0.000738433	valid_1's l2: 0.000734666
[44]	training's l2: 0.000735695	valid_1's l2: 0.000732834
[45]	training's l2: 0.000733302	valid_1's l2: 0.000732171
[46]	training's l2: 0.000731379	valid_1's l2: 0.000731933
[47]	training's l2: 0.000729203	valid_1's l2: 0.000730691
[48]	training's l2: 0.000727327	valid_1's l2: 0.000730043
[49]	training's l2: 0.000724255	valid_1's l2: 0.000729007
[50]	training's l2: 0.000721933	valid_1's l2: 0.000728209
[51]	training's l2: 0.000720105	valid_1's l2: 0.000727952
[52]	training's l2: 0.000718214	valid_1's l2: 0.000727738
[53]	training's l2: 0.000715913	valid_1's l2: 0.000727337
[54]	training's l2: 0.000714236	valid_1's l2: 0.00072594
[55]	training's l2: 0.000712343	valid_1's l2: 0.0007253
[56]	training's l2: 0.000710691	valid_1's l2: 0.000725156
[57]	training's l2: 0.000707902	valid_1's l2: 0.000723774
[58]	training's l2: 0.000706135	valid_1's l2: 0.000723594
[59]	training's l2: 0.000704368	valid_1's l2: 0.000723398
[60]	training's l2: 0.000702136	valid_1's l2: 0.000722736
[61]	training's l2: 0.000700682	valid_1's l2: 0.000722425
[62]	training's l2: 0.000698648	valid_1's l2: 0.000721541
[63]	training's l2: 0.000696801	valid_1's l2: 0.000721297
[64]	training's l2: 0.00069511	valid_1's l2: 0.000720919
[65]	training's l2: 0.000693502	valid_1's l2: 0.000720813
[66]	training's l2: 0.000691911	valid_1's l2: 0.00072069
[67]	training's l2: 0.000690153	valid_1's l2: 0.000720335
[68]	training's l2: 0.000688321	valid_1's l2: 0.000719782
[69]	training's l2: 0.000686611	valid_1's l2: 0.000719494
[70]	training's l2: 0.000684481	valid_1's l2: 0.000718427
[71]	training's l2: 0.000682651	valid_1's l2: 0.000717324
[72]	training's l2: 0.000680987	valid_1's l2: 0.000717417
[73]	training's l2: 0.000679467	valid_1's l2: 0.000716939
[74]	training's l2: 0.000677815	valid_1's l2: 0.000716834
[75]	training's l2: 0.00067624	valid_1's l2: 0.000717
[76]	training's l2: 0.000674203	valid_1's l2: 0.000715511
[77]	training's l2: 0.000672808	valid_1's l2: 0.000715634
[78]	training's l2: 0.00067102	valid_1's l2: 0.000715204
[79]	training's l2: 0.000669337	valid_1's l2: 0.000714561
[80]	training's l2: 0.000667222	valid_1's l2: 0.00071373
[81]	training's l2: 0.000665842	valid_1's l2: 0.000713323
[82]	training's l2: 0.000664338	valid_1's l2: 0.000712873
[83]	training's l2: 0.000662995	valid_1's l2: 0.000713063
[84]	training's l2: 0.000661254	valid_1's l2: 0.000712886
[85]	training's l2: 0.000659847	valid_1's l2: 0.000713065
[86]	training's l2: 0.000657984	valid_1's l2: 0.000713201
[87]	training's l2: 0.000656554	valid_1's l2: 0.000712859
[88]	training's l2: 0.000655067	valid_1's l2: 0.000712817
[89]	training's l2: 0.000653531	valid_1's l2: 0.00071271
[90]	training's l2: 0.000652083	valid_1's l2: 0.000712535
[91]	training's l2: 0.000650609	valid_1's l2: 0.000712408
[92]	training's l2: 0.000649364	valid_1's l2: 0.000712098
[93]	training's l2: 0.000648061	valid_1's l2: 0.000712233
[94]	training's l2: 0.000646389	valid_1's l2: 0.000712616
[95]	training's l2: 0.000644647	valid_1's l2: 0.00071249
[96]	training's l2: 0.000643252	valid_1's l2: 0.000712151
[97]	training's l2: 0.000641787	valid_1's l2: 0.000711876
[98]	training's l2: 0.000640244	valid_1's l2: 0.000711654
[99]	training's l2: 0.000638858	valid_1's l2: 0.000711082
[100]	training's l2: 0.000637808	valid_1's l2: 0.000710918
[101]	training's l2: 0.000636518	valid_1's l2: 0.000710816
[102]	training's l2: 0.000635227	valid_1's l2: 0.000710818
[103]	training's l2: 0.000633805	valid_1's l2: 0.000710696
[104]	training's l2: 0.000632559	valid_1's l2: 0.000710488
[105]	training's l2: 0.00063128	valid_1's l2: 0.000710517
[106]	training's l2: 0.000629915	valid_1's l2: 0.000710593
[107]	training's l2: 0.000628437	valid_1's l2: 0.000710746
[108]	training's l2: 0.000627139	valid_1's l2: 0.000710694
[109]	training's l2: 0.000625891	valid_1's l2: 0.000710715
[110]	training's l2: 0.000624434	valid_1's l2: 0.000710822
[111]	training's l2: 0.000623075	valid_1's l2: 0.000710874
[112]	training's l2: 0.000621691	valid_1's l2: 0.000710824
[113]	training's l2: 0.000620381	valid_1's l2: 0.000710533
[114]	training's l2: 0.000619026	valid_1's l2: 0.000710628
[115]	training's l2: 0.000617797	valid_1's l2: 0.000710629
[116]	training's l2: 0.000616373	valid_1's l2: 0.000710603
[117]	training's l2: 0.000615219	valid_1's l2: 0.00071059
[118]	training's l2: 0.000613987	valid_1's l2: 0.000710579
[119]	training's l2: 0.000612541	valid_1's l2: 0.000710454
[120]	training's l2: 0.000611165	valid_1's l2: 0.000710327
[121]	training's l2: 0.000609967	valid_1's l2: 0.00071044
[122]	training's l2: 0.000608428	valid_1's l2: 0.000710039
[123]	training's l2: 0.000607338	valid_1's l2: 0.000710348
[124]	training's l2: 0.000606009	valid_1's l2: 0.000710325
[125]	training's l2: 0.000604723	valid_1's l2: 0.000710358
[126]	training's l2: 0.000603367	valid_1's l2: 0.000710125
[127]	training's l2: 0.000602128	valid_1's l2: 0.000710019
[128]	training's l2: 0.000600894	valid_1's l2: 0.000710151
[129]	training's l2: 0.000599745	valid_1's l2: 0.000710156
[130]	training's l2: 0.00059824	valid_1's l2: 0.000709929
[131]	training's l2: 0.000596977	valid_1's l2: 0.000709755
[132]	training's l2: 0.000595992	valid_1's l2: 0.000709578
[133]	training's l2: 0.000594751	valid_1's l2: 0.000709457
[134]	training's l2: 0.000593694	valid_1's l2: 0.000709481
[135]	training's l2: 0.000592537	valid_1's l2: 0.000709589
[136]	training's l2: 0.00059136	valid_1's l2: 0.000709652
[137]	training's l2: 0.00059023	valid_1's l2: 0.000709087
[138]	training's l2: 0.000589023	valid_1's l2: 0.000709311
[139]	training's l2: 0.000588036	valid_1's l2: 0.000709083
[140]	training's l2: 0.000586815	valid_1's l2: 0.00070888
[141]	training's l2: 0.000585478	valid_1's l2: 0.000708922
[142]	training's l2: 0.000584348	valid_1's l2: 0.000708948
[143]	training's l2: 0.000583375	valid_1's l2: 0.00070886
[144]	training's l2: 0.000582136	valid_1's l2: 0.000708689
[145]	training's l2: 0.000580817	valid_1's l2: 0.000708633
[146]	training's l2: 0.000579644	valid_1's l2: 0.000708599
[147]	training's l2: 0.000578508	valid_1's l2: 0.000708627
[148]	training's l2: 0.000577543	valid_1's l2: 0.000708767
[149]	training's l2: 0.000576492	valid_1's l2: 0.000708714
[150]	training's l2: 0.0005752	valid_1's l2: 0.00070915
[151]	training's l2: 0.000574243	valid_1's l2: 0.000709132
[152]	training's l2: 0.000573068	valid_1's l2: 0.000709514
[153]	training's l2: 0.00057189	valid_1's l2: 0.0007096
[154]	training's l2: 0.000570975	valid_1's l2: 0.000709893
[155]	training's l2: 0.000569914	valid_1's l2: 0.000709545
[156]	training's l2: 0.00056892	valid_1's l2: 0.000709477
[157]	training's l2: 0.000567998	valid_1's l2: 0.000709401
[158]	training's l2: 0.000566905	valid_1's l2: 0.000709485
[159]	training's l2: 0.000565877	valid_1's l2: 0.000709201
[160]	training's l2: 0.000564915	valid_1's l2: 0.000709087
[161]	training's l2: 0.000563799	valid_1's l2: 0.000709019
[162]	training's l2: 0.00056271	valid_1's l2: 0.00070855
[163]	training's l2: 0.000561672	valid_1's l2: 0.000708633
[164]	training's l2: 0.000560583	valid_1's l2: 0.000708639
[165]	training's l2: 0.00055956	valid_1's l2: 0.000708713
[166]	training's l2: 0.00055877	valid_1's l2: 0.000708864
[167]	training's l2: 0.00055793	valid_1's l2: 0.000708877
[168]	training's l2: 0.000557052	valid_1's l2: 0.000708828
[169]	training's l2: 0.000556179	valid_1's l2: 0.000708548
[170]	training's l2: 0.000555254	valid_1's l2: 0.000708434
[171]	training's l2: 0.000554318	valid_1's l2: 0.000708462
[172]	training's l2: 0.000553427	valid_1's l2: 0.000708584
[173]	training's l2: 0.000552422	valid_1's l2: 0.000708381
[174]	training's l2: 0.000551144	valid_1's l2: 0.000707751
[175]	training's l2: 0.000550009	valid_1's l2: 0.000707705
[176]	training's l2: 0.000548923	valid_1's l2: 0.000707444
Did not meet early stopping. Best iteration is:
[176]	training's l2: 0.000548923	valid_1's l2: 0.000707444
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204159 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000986179	valid_1's l2: 0.000895353
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000969518	valid_1's l2: 0.000883636
[3]	training's l2: 0.00095444	valid_1's l2: 0.000872154
[4]	training's l2: 0.000940952	valid_1's l2: 0.000862522
[5]	training's l2: 0.000928431	valid_1's l2: 0.000853005
[6]	training's l2: 0.000917335	valid_1's l2: 0.000844981
[7]	training's l2: 0.000907146	valid_1's l2: 0.000838415
[8]	training's l2: 0.000897207	valid_1's l2: 0.000832308
[9]	training's l2: 0.000888258	valid_1's l2: 0.000826259
[10]	training's l2: 0.00088039	valid_1's l2: 0.000821007
[11]	training's l2: 0.000873123	valid_1's l2: 0.000816091
[12]	training's l2: 0.00086559	valid_1's l2: 0.000811675
[13]	training's l2: 0.000859434	valid_1's l2: 0.000808214
[14]	training's l2: 0.000852934	valid_1's l2: 0.00080408
[15]	training's l2: 0.000847232	valid_1's l2: 0.000801312
[16]	training's l2: 0.000840618	valid_1's l2: 0.000795699
[17]	training's l2: 0.000834636	valid_1's l2: 0.000791753
[18]	training's l2: 0.000829321	valid_1's l2: 0.000789071
[19]	training's l2: 0.000824397	valid_1's l2: 0.000786381
[20]	training's l2: 0.000819882	valid_1's l2: 0.000783428
[21]	training's l2: 0.000815078	valid_1's l2: 0.000780578
[22]	training's l2: 0.000810349	valid_1's l2: 0.000777873
[23]	training's l2: 0.000806146	valid_1's l2: 0.000775079
[24]	training's l2: 0.000802095	valid_1's l2: 0.000773124
[25]	training's l2: 0.000798411	valid_1's l2: 0.000771392
[26]	training's l2: 0.000794794	valid_1's l2: 0.000769914
[27]	training's l2: 0.000791149	valid_1's l2: 0.000767891
[28]	training's l2: 0.000787123	valid_1's l2: 0.000765187
[29]	training's l2: 0.000783882	valid_1's l2: 0.000763368
[30]	training's l2: 0.000780155	valid_1's l2: 0.000761602
[31]	training's l2: 0.00077681	valid_1's l2: 0.000759839
[32]	training's l2: 0.000773286	valid_1's l2: 0.000757848
[33]	training's l2: 0.000770401	valid_1's l2: 0.000756884
[34]	training's l2: 0.000767748	valid_1's l2: 0.000755661
[35]	training's l2: 0.000765091	valid_1's l2: 0.000753911
[36]	training's l2: 0.000762306	valid_1's l2: 0.000752435
[37]	training's l2: 0.000759658	valid_1's l2: 0.000751338
[38]	training's l2: 0.000756929	valid_1's l2: 0.000750112
[39]	training's l2: 0.000753785	valid_1's l2: 0.000748258
[40]	training's l2: 0.000750727	valid_1's l2: 0.000746751
[41]	training's l2: 0.000748072	valid_1's l2: 0.000745837
[42]	training's l2: 0.000745378	valid_1's l2: 0.000744417
[43]	training's l2: 0.000742388	valid_1's l2: 0.000742043
[44]	training's l2: 0.000740286	valid_1's l2: 0.000741456
[45]	training's l2: 0.000737994	valid_1's l2: 0.000739685
[46]	training's l2: 0.000735315	valid_1's l2: 0.000738304
[47]	training's l2: 0.000733038	valid_1's l2: 0.000737559
[48]	training's l2: 0.000730221	valid_1's l2: 0.000736254
[49]	training's l2: 0.000728212	valid_1's l2: 0.000735975
[50]	training's l2: 0.000725225	valid_1's l2: 0.000734413
[51]	training's l2: 0.000722867	valid_1's l2: 0.000733517
[52]	training's l2: 0.000720735	valid_1's l2: 0.000732761
[53]	training's l2: 0.000717795	valid_1's l2: 0.000731127
[54]	training's l2: 0.0007161	valid_1's l2: 0.000730701
[55]	training's l2: 0.000713781	valid_1's l2: 0.000730063
[56]	training's l2: 0.000711847	valid_1's l2: 0.000729572
[57]	training's l2: 0.000709981	valid_1's l2: 0.000728907
[58]	training's l2: 0.000708031	valid_1's l2: 0.000727922
[59]	training's l2: 0.000706202	valid_1's l2: 0.000727682
[60]	training's l2: 0.000703893	valid_1's l2: 0.00072688
[61]	training's l2: 0.000702005	valid_1's l2: 0.000726489
[62]	training's l2: 0.000699971	valid_1's l2: 0.000726271
[63]	training's l2: 0.000698285	valid_1's l2: 0.00072585
[64]	training's l2: 0.000695995	valid_1's l2: 0.000724984
[65]	training's l2: 0.000694332	valid_1's l2: 0.000725128
[66]	training's l2: 0.00069204	valid_1's l2: 0.000724321
[67]	training's l2: 0.000690394	valid_1's l2: 0.000724153
[68]	training's l2: 0.000688733	valid_1's l2: 0.000723741
[69]	training's l2: 0.000686812	valid_1's l2: 0.000723392
[70]	training's l2: 0.000684956	valid_1's l2: 0.000723375
[71]	training's l2: 0.000683142	valid_1's l2: 0.000723194
[72]	training's l2: 0.000681176	valid_1's l2: 0.000722286
[73]	training's l2: 0.000679594	valid_1's l2: 0.000721564
[74]	training's l2: 0.000677829	valid_1's l2: 0.000721487
[75]	training's l2: 0.000676094	valid_1's l2: 0.000721458
[76]	training's l2: 0.000674212	valid_1's l2: 0.000720927
[77]	training's l2: 0.000672746	valid_1's l2: 0.000720675
[78]	training's l2: 0.000670795	valid_1's l2: 0.000720117
[79]	training's l2: 0.000668389	valid_1's l2: 0.000719277
[80]	training's l2: 0.000665925	valid_1's l2: 0.000718217
[81]	training's l2: 0.000664379	valid_1's l2: 0.000718016
[82]	training's l2: 0.000662662	valid_1's l2: 0.000717705
[83]	training's l2: 0.000660903	valid_1's l2: 0.000717411
[84]	training's l2: 0.000659411	valid_1's l2: 0.000717369
[85]	training's l2: 0.000657886	valid_1's l2: 0.000717373
[86]	training's l2: 0.000656324	valid_1's l2: 0.000717127
[87]	training's l2: 0.000654844	valid_1's l2: 0.000717029
[88]	training's l2: 0.000653165	valid_1's l2: 0.000716941
[89]	training's l2: 0.000651342	valid_1's l2: 0.000716213
[90]	training's l2: 0.000649665	valid_1's l2: 0.000716004
[91]	training's l2: 0.000648196	valid_1's l2: 0.000715845
[92]	training's l2: 0.000646701	valid_1's l2: 0.000715516
[93]	training's l2: 0.000644992	valid_1's l2: 0.000715561
[94]	training's l2: 0.000643107	valid_1's l2: 0.000715243
[95]	training's l2: 0.000641176	valid_1's l2: 0.000715217
[96]	training's l2: 0.000639548	valid_1's l2: 0.000714994
[97]	training's l2: 0.000638012	valid_1's l2: 0.000714695
[98]	training's l2: 0.000636457	valid_1's l2: 0.000714568
[99]	training's l2: 0.000635134	valid_1's l2: 0.00071441
[100]	training's l2: 0.000633707	valid_1's l2: 0.000714155
[101]	training's l2: 0.000632346	valid_1's l2: 0.000714386
[102]	training's l2: 0.000630886	valid_1's l2: 0.000713931
[103]	training's l2: 0.000629313	valid_1's l2: 0.000713775
[104]	training's l2: 0.000628036	valid_1's l2: 0.00071373
[105]	training's l2: 0.000626352	valid_1's l2: 0.000713521
[106]	training's l2: 0.000624966	valid_1's l2: 0.000713564
[107]	training's l2: 0.000623664	valid_1's l2: 0.000713495
[108]	training's l2: 0.000622071	valid_1's l2: 0.000713302
[109]	training's l2: 0.000620909	valid_1's l2: 0.000713122
[110]	training's l2: 0.000619424	valid_1's l2: 0.000712978
[111]	training's l2: 0.000617844	valid_1's l2: 0.000712595
[112]	training's l2: 0.00061636	valid_1's l2: 0.000712509
[113]	training's l2: 0.000614565	valid_1's l2: 0.000712059
[114]	training's l2: 0.000613195	valid_1's l2: 0.000711855
[115]	training's l2: 0.000611774	valid_1's l2: 0.000711879
[116]	training's l2: 0.000610476	valid_1's l2: 0.000711839
[117]	training's l2: 0.000609165	valid_1's l2: 0.000711608
[118]	training's l2: 0.000607844	valid_1's l2: 0.000711575
[119]	training's l2: 0.000606503	valid_1's l2: 0.000711549
[120]	training's l2: 0.000605161	valid_1's l2: 0.000711433
[121]	training's l2: 0.000603845	valid_1's l2: 0.000711468
[122]	training's l2: 0.00060234	valid_1's l2: 0.000711309
[123]	training's l2: 0.000601181	valid_1's l2: 0.000711372
[124]	training's l2: 0.000599861	valid_1's l2: 0.000711058
[125]	training's l2: 0.000598558	valid_1's l2: 0.000710949
[126]	training's l2: 0.00059728	valid_1's l2: 0.000710923
[127]	training's l2: 0.000596199	valid_1's l2: 0.000710599
[128]	training's l2: 0.000595015	valid_1's l2: 0.000710763
[129]	training's l2: 0.000593697	valid_1's l2: 0.000710668
[130]	training's l2: 0.000592315	valid_1's l2: 0.000710117
[131]	training's l2: 0.000591061	valid_1's l2: 0.000710279
[132]	training's l2: 0.000589502	valid_1's l2: 0.000710479
[133]	training's l2: 0.000588217	valid_1's l2: 0.000710434
[134]	training's l2: 0.00058694	valid_1's l2: 0.000710594
[135]	training's l2: 0.000585674	valid_1's l2: 0.000710587
[136]	training's l2: 0.00058444	valid_1's l2: 0.000710464
[137]	training's l2: 0.000583178	valid_1's l2: 0.00071071
[138]	training's l2: 0.000581723	valid_1's l2: 0.000710728
[139]	training's l2: 0.000580662	valid_1's l2: 0.000710752
[140]	training's l2: 0.000579456	valid_1's l2: 0.000710683
[141]	training's l2: 0.000578224	valid_1's l2: 0.000710801
[142]	training's l2: 0.000577074	valid_1's l2: 0.000710901
[143]	training's l2: 0.000575824	valid_1's l2: 0.000710934
[144]	training's l2: 0.000574506	valid_1's l2: 0.000710975
[145]	training's l2: 0.000573293	valid_1's l2: 0.000711098
[146]	training's l2: 0.000571958	valid_1's l2: 0.000711201
[147]	training's l2: 0.000570744	valid_1's l2: 0.000711446
[148]	training's l2: 0.000569411	valid_1's l2: 0.000711051
[149]	training's l2: 0.000568255	valid_1's l2: 0.000710974
[150]	training's l2: 0.00056707	valid_1's l2: 0.000710231
[151]	training's l2: 0.000565784	valid_1's l2: 0.000710122
[152]	training's l2: 0.000564643	valid_1's l2: 0.00071005
[153]	training's l2: 0.000563289	valid_1's l2: 0.000710237
[154]	training's l2: 0.000562128	valid_1's l2: 0.000710194
[155]	training's l2: 0.000561075	valid_1's l2: 0.000710158
[156]	training's l2: 0.000559904	valid_1's l2: 0.000710187
[157]	training's l2: 0.000559042	valid_1's l2: 0.000710246
[158]	training's l2: 0.00055793	valid_1's l2: 0.000710289
[159]	training's l2: 0.000556897	valid_1's l2: 0.000710206
[160]	training's l2: 0.000555809	valid_1's l2: 0.000710355
[161]	training's l2: 0.000554644	valid_1's l2: 0.000710248
[162]	training's l2: 0.000553546	valid_1's l2: 0.000710036
[163]	training's l2: 0.00055231	valid_1's l2: 0.000710051
[164]	training's l2: 0.00055118	valid_1's l2: 0.00071007
[165]	training's l2: 0.000550316	valid_1's l2: 0.000710091
[166]	training's l2: 0.000549241	valid_1's l2: 0.000710168
[167]	training's l2: 0.000548082	valid_1's l2: 0.000710212
[168]	training's l2: 0.0005469	valid_1's l2: 0.000710276
[169]	training's l2: 0.000545793	valid_1's l2: 0.000710365
[170]	training's l2: 0.000544734	valid_1's l2: 0.000710263
[171]	training's l2: 0.000543626	valid_1's l2: 0.000710165
[172]	training's l2: 0.000542629	valid_1's l2: 0.000710179
[173]	training's l2: 0.000541526	valid_1's l2: 0.000710299
[174]	training's l2: 0.000540379	valid_1's l2: 0.0007104
[175]	training's l2: 0.000539353	valid_1's l2: 0.000710223
[176]	training's l2: 0.000538452	valid_1's l2: 0.00071016
[177]	training's l2: 0.00053745	valid_1's l2: 0.000710198
[178]	training's l2: 0.000536383	valid_1's l2: 0.00071017
[179]	training's l2: 0.000535202	valid_1's l2: 0.000709653
[180]	training's l2: 0.000534314	valid_1's l2: 0.000709514
[181]	training's l2: 0.000533269	valid_1's l2: 0.000709691
[182]	training's l2: 0.00053228	valid_1's l2: 0.000710014
[183]	training's l2: 0.000531214	valid_1's l2: 0.00070985
[184]	training's l2: 0.000530173	valid_1's l2: 0.000709964
[185]	training's l2: 0.000529276	valid_1's l2: 0.000710068
[186]	training's l2: 0.00052828	valid_1's l2: 0.000710019
[187]	training's l2: 0.00052727	valid_1's l2: 0.00071007
[188]	training's l2: 0.000526231	valid_1's l2: 0.000709931
[189]	training's l2: 0.000525124	valid_1's l2: 0.000710025
[190]	training's l2: 0.000524284	valid_1's l2: 0.000709986
[191]	training's l2: 0.000523261	valid_1's l2: 0.000710006
[192]	training's l2: 0.000522087	valid_1's l2: 0.000709473
[193]	training's l2: 0.000521026	valid_1's l2: 0.000709552
[194]	training's l2: 0.000520177	valid_1's l2: 0.000709498
Did not meet early stopping. Best iteration is:
[194]	training's l2: 0.000520177	valid_1's l2: 0.000709498
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200039 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000984089	valid_1's l2: 0.00089283
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000966716	valid_1's l2: 0.000880382
[3]	training's l2: 0.000950565	valid_1's l2: 0.000867896
[4]	training's l2: 0.000937401	valid_1's l2: 0.000857465
[5]	training's l2: 0.000925311	valid_1's l2: 0.000849078
[6]	training's l2: 0.000914453	valid_1's l2: 0.000841598
[7]	training's l2: 0.000904625	valid_1's l2: 0.000834884
[8]	training's l2: 0.000894484	valid_1's l2: 0.000827793
[9]	training's l2: 0.000886309	valid_1's l2: 0.000822255
[10]	training's l2: 0.000876805	valid_1's l2: 0.000813815
[11]	training's l2: 0.000869065	valid_1's l2: 0.00080906
[12]	training's l2: 0.000862032	valid_1's l2: 0.000803833
[13]	training's l2: 0.000855981	valid_1's l2: 0.00079963
[14]	training's l2: 0.000850064	valid_1's l2: 0.000795676
[15]	training's l2: 0.000844929	valid_1's l2: 0.000791963
[16]	training's l2: 0.000839888	valid_1's l2: 0.000789102
[17]	training's l2: 0.000834545	valid_1's l2: 0.000785774
[18]	training's l2: 0.000830161	valid_1's l2: 0.000781771
[19]	training's l2: 0.000825172	valid_1's l2: 0.000778823
[20]	training's l2: 0.000821594	valid_1's l2: 0.000776231
[21]	training's l2: 0.000817761	valid_1's l2: 0.000774136
[22]	training's l2: 0.00081426	valid_1's l2: 0.000771567
[23]	training's l2: 0.000809741	valid_1's l2: 0.000769221
[24]	training's l2: 0.000806205	valid_1's l2: 0.000767915
[25]	training's l2: 0.000803122	valid_1's l2: 0.000766322
[26]	training's l2: 0.000799669	valid_1's l2: 0.000765208
[27]	training's l2: 0.00079592	valid_1's l2: 0.000763125
[28]	training's l2: 0.000793554	valid_1's l2: 0.000761497
[29]	training's l2: 0.000790962	valid_1's l2: 0.000760502
[30]	training's l2: 0.000787518	valid_1's l2: 0.000756387
[31]	training's l2: 0.000783908	valid_1's l2: 0.00075422
[32]	training's l2: 0.000781432	valid_1's l2: 0.000753108
[33]	training's l2: 0.000778371	valid_1's l2: 0.000751165
[34]	training's l2: 0.000775324	valid_1's l2: 0.000749886
[35]	training's l2: 0.000772658	valid_1's l2: 0.000748592
[36]	training's l2: 0.000770263	valid_1's l2: 0.000747275
[37]	training's l2: 0.000767058	valid_1's l2: 0.000745759
[38]	training's l2: 0.000763981	valid_1's l2: 0.000743427
[39]	training's l2: 0.000761925	valid_1's l2: 0.000742973
[40]	training's l2: 0.00075932	valid_1's l2: 0.00074215
[41]	training's l2: 0.000757232	valid_1's l2: 0.0007413
[42]	training's l2: 0.00075558	valid_1's l2: 0.000740905
[43]	training's l2: 0.000752381	valid_1's l2: 0.00074042
[44]	training's l2: 0.000749802	valid_1's l2: 0.000739322
[45]	training's l2: 0.000746647	valid_1's l2: 0.000737091
[46]	training's l2: 0.000745001	valid_1's l2: 0.000736422
[47]	training's l2: 0.000742256	valid_1's l2: 0.000735121
[48]	training's l2: 0.000740806	valid_1's l2: 0.00073447
[49]	training's l2: 0.00073912	valid_1's l2: 0.000734012
[50]	training's l2: 0.000737165	valid_1's l2: 0.000733484
[51]	training's l2: 0.000735396	valid_1's l2: 0.000732629
[52]	training's l2: 0.000733134	valid_1's l2: 0.000732064
[53]	training's l2: 0.000731043	valid_1's l2: 0.000731198
[54]	training's l2: 0.000729519	valid_1's l2: 0.00073103
[55]	training's l2: 0.000727326	valid_1's l2: 0.00072944
[56]	training's l2: 0.000725192	valid_1's l2: 0.000728717
[57]	training's l2: 0.000723398	valid_1's l2: 0.000728365
[58]	training's l2: 0.000722198	valid_1's l2: 0.000728139
[59]	training's l2: 0.000720391	valid_1's l2: 0.000727663
[60]	training's l2: 0.000718898	valid_1's l2: 0.000727436
[61]	training's l2: 0.000716863	valid_1's l2: 0.000727001
[62]	training's l2: 0.000715103	valid_1's l2: 0.000726033
[63]	training's l2: 0.000713538	valid_1's l2: 0.00072546
[64]	training's l2: 0.000712034	valid_1's l2: 0.000725889
[65]	training's l2: 0.00071035	valid_1's l2: 0.00072542
[66]	training's l2: 0.000708862	valid_1's l2: 0.000725271
[67]	training's l2: 0.00070731	valid_1's l2: 0.000724761
[68]	training's l2: 0.000705555	valid_1's l2: 0.00072482
[69]	training's l2: 0.000704267	valid_1's l2: 0.000724676
[70]	training's l2: 0.000702803	valid_1's l2: 0.000724474
[71]	training's l2: 0.00070158	valid_1's l2: 0.00072427
[72]	training's l2: 0.000700135	valid_1's l2: 0.000724027
[73]	training's l2: 0.000698795	valid_1's l2: 0.00072411
[74]	training's l2: 0.000697534	valid_1's l2: 0.000724104
[75]	training's l2: 0.000695989	valid_1's l2: 0.000723846
[76]	training's l2: 0.000694718	valid_1's l2: 0.000723567
[77]	training's l2: 0.000692705	valid_1's l2: 0.000722425
[78]	training's l2: 0.000691565	valid_1's l2: 0.000722145
[79]	training's l2: 0.000690307	valid_1's l2: 0.000722084
[80]	training's l2: 0.000688427	valid_1's l2: 0.000721143
[81]	training's l2: 0.000687191	valid_1's l2: 0.000721074
[82]	training's l2: 0.000685914	valid_1's l2: 0.000720845
[83]	training's l2: 0.00068456	valid_1's l2: 0.000720969
[84]	training's l2: 0.000683299	valid_1's l2: 0.000720795
[85]	training's l2: 0.000681701	valid_1's l2: 0.000719742
[86]	training's l2: 0.000680421	valid_1's l2: 0.000719839
[87]	training's l2: 0.000679201	valid_1's l2: 0.0007198
[88]	training's l2: 0.000677979	valid_1's l2: 0.000719229
[89]	training's l2: 0.000676471	valid_1's l2: 0.000719022
[90]	training's l2: 0.00067534	valid_1's l2: 0.000719069
[91]	training's l2: 0.000673629	valid_1's l2: 0.000719118
[92]	training's l2: 0.000672587	valid_1's l2: 0.000718914
[93]	training's l2: 0.000671541	valid_1's l2: 0.000718806
[94]	training's l2: 0.00067018	valid_1's l2: 0.000718838
[95]	training's l2: 0.000668893	valid_1's l2: 0.000718634
[96]	training's l2: 0.000667411	valid_1's l2: 0.000717656
[97]	training's l2: 0.000666295	valid_1's l2: 0.000716705
[98]	training's l2: 0.00066526	valid_1's l2: 0.000716404
[99]	training's l2: 0.000664136	valid_1's l2: 0.000716346
[100]	training's l2: 0.00066293	valid_1's l2: 0.000716008
[101]	training's l2: 0.000661857	valid_1's l2: 0.000715978
[102]	training's l2: 0.000660768	valid_1's l2: 0.000716147
[103]	training's l2: 0.000659171	valid_1's l2: 0.000715947
[104]	training's l2: 0.000658194	valid_1's l2: 0.000715774
[105]	training's l2: 0.000656833	valid_1's l2: 0.000715586
[106]	training's l2: 0.000655821	valid_1's l2: 0.000715632
[107]	training's l2: 0.000654882	valid_1's l2: 0.000715828
[108]	training's l2: 0.000653911	valid_1's l2: 0.000715717
[109]	training's l2: 0.000653034	valid_1's l2: 0.000715685
[110]	training's l2: 0.000651951	valid_1's l2: 0.00071553
[111]	training's l2: 0.000650845	valid_1's l2: 0.000715388
[112]	training's l2: 0.000649875	valid_1's l2: 0.000715415
[113]	training's l2: 0.000648733	valid_1's l2: 0.000715188
[114]	training's l2: 0.000647573	valid_1's l2: 0.000715126
[115]	training's l2: 0.000646708	valid_1's l2: 0.000715382
[116]	training's l2: 0.00064565	valid_1's l2: 0.000715133
[117]	training's l2: 0.000644738	valid_1's l2: 0.000715251
[118]	training's l2: 0.000643703	valid_1's l2: 0.000715237
[119]	training's l2: 0.000642673	valid_1's l2: 0.000715075
[120]	training's l2: 0.000641908	valid_1's l2: 0.000714972
[121]	training's l2: 0.00064103	valid_1's l2: 0.00071471
[122]	training's l2: 0.000640291	valid_1's l2: 0.000714805
[123]	training's l2: 0.000639467	valid_1's l2: 0.000714701
[124]	training's l2: 0.00063858	valid_1's l2: 0.000714596
[125]	training's l2: 0.000637554	valid_1's l2: 0.000714867
[126]	training's l2: 0.000636726	valid_1's l2: 0.000714902
[127]	training's l2: 0.000635694	valid_1's l2: 0.000714815
[128]	training's l2: 0.000634937	valid_1's l2: 0.000714815
[129]	training's l2: 0.000634065	valid_1's l2: 0.000714729
[130]	training's l2: 0.000632753	valid_1's l2: 0.00071435
[131]	training's l2: 0.000631899	valid_1's l2: 0.000714574
[132]	training's l2: 0.000630827	valid_1's l2: 0.000714409
[133]	training's l2: 0.000629909	valid_1's l2: 0.000714084
[134]	training's l2: 0.00062917	valid_1's l2: 0.000714066
[135]	training's l2: 0.0006282	valid_1's l2: 0.000713794
[136]	training's l2: 0.00062732	valid_1's l2: 0.000713748
[137]	training's l2: 0.000626219	valid_1's l2: 0.000713727
[138]	training's l2: 0.00062529	valid_1's l2: 0.000713639
[139]	training's l2: 0.000624301	valid_1's l2: 0.000713525
[140]	training's l2: 0.000623506	valid_1's l2: 0.000713503
[141]	training's l2: 0.000622551	valid_1's l2: 0.000713349
[142]	training's l2: 0.000621686	valid_1's l2: 0.000713471
[143]	training's l2: 0.000620918	valid_1's l2: 0.000713586
[144]	training's l2: 0.000620134	valid_1's l2: 0.000713354
[145]	training's l2: 0.000619181	valid_1's l2: 0.000713184
[146]	training's l2: 0.000618357	valid_1's l2: 0.000713
[147]	training's l2: 0.000617491	valid_1's l2: 0.00071297
[148]	training's l2: 0.000616612	valid_1's l2: 0.000712819
[149]	training's l2: 0.000615691	valid_1's l2: 0.000712761
[150]	training's l2: 0.000614965	valid_1's l2: 0.000712874
[151]	training's l2: 0.000614062	valid_1's l2: 0.000712794
[152]	training's l2: 0.00061321	valid_1's l2: 0.000712832
[153]	training's l2: 0.000612339	valid_1's l2: 0.000712915
[154]	training's l2: 0.00061127	valid_1's l2: 0.000712811
[155]	training's l2: 0.000610412	valid_1's l2: 0.000712499
[156]	training's l2: 0.000609568	valid_1's l2: 0.000712654
[157]	training's l2: 0.000608668	valid_1's l2: 0.000712686
[158]	training's l2: 0.000607616	valid_1's l2: 0.000712675
[159]	training's l2: 0.000606564	valid_1's l2: 0.000712734
[160]	training's l2: 0.000605847	valid_1's l2: 0.000712972
[161]	training's l2: 0.000605148	valid_1's l2: 0.000713002
[162]	training's l2: 0.000604376	valid_1's l2: 0.000712986
[163]	training's l2: 0.000603595	valid_1's l2: 0.000712828
[164]	training's l2: 0.00060287	valid_1's l2: 0.000712798
[165]	training's l2: 0.00060191	valid_1's l2: 0.000712487
[166]	training's l2: 0.000601127	valid_1's l2: 0.000712319
[167]	training's l2: 0.000600259	valid_1's l2: 0.000712091
[168]	training's l2: 0.000599174	valid_1's l2: 0.000711792
[169]	training's l2: 0.000598249	valid_1's l2: 0.000711831
[170]	training's l2: 0.000597544	valid_1's l2: 0.00071166
[171]	training's l2: 0.000596889	valid_1's l2: 0.000711525
[172]	training's l2: 0.000596103	valid_1's l2: 0.000711507
[173]	training's l2: 0.000595251	valid_1's l2: 0.000711473
[174]	training's l2: 0.000594468	valid_1's l2: 0.000711225
[175]	training's l2: 0.000593624	valid_1's l2: 0.000711145
[176]	training's l2: 0.000592916	valid_1's l2: 0.000711086
[177]	training's l2: 0.000592023	valid_1's l2: 0.000711062
[178]	training's l2: 0.000591375	valid_1's l2: 0.000711157
[179]	training's l2: 0.000590672	valid_1's l2: 0.00071118
[180]	training's l2: 0.000589956	valid_1's l2: 0.000711037
[181]	training's l2: 0.000589303	valid_1's l2: 0.000710876
[182]	training's l2: 0.000588468	valid_1's l2: 0.000710799
[183]	training's l2: 0.000587446	valid_1's l2: 0.000710901
[184]	training's l2: 0.000586494	valid_1's l2: 0.000710736
[185]	training's l2: 0.000585747	valid_1's l2: 0.000710714
[186]	training's l2: 0.000584885	valid_1's l2: 0.000710745
[187]	training's l2: 0.000584153	valid_1's l2: 0.000710707
[188]	training's l2: 0.00058341	valid_1's l2: 0.000710793
[189]	training's l2: 0.000582662	valid_1's l2: 0.000710811
[190]	training's l2: 0.000581846	valid_1's l2: 0.000710731
[191]	training's l2: 0.000580688	valid_1's l2: 0.000710434
[192]	training's l2: 0.000579982	valid_1's l2: 0.000710387
[193]	training's l2: 0.00057931	valid_1's l2: 0.000710529
[194]	training's l2: 0.000578676	valid_1's l2: 0.000710587
[195]	training's l2: 0.000577761	valid_1's l2: 0.000710496
Did not meet early stopping. Best iteration is:
[195]	training's l2: 0.000577761	valid_1's l2: 0.000710496
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216573 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.00098532	valid_1's l2: 0.000894846
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000969242	valid_1's l2: 0.000884772
[3]	training's l2: 0.000954301	valid_1's l2: 0.000874565
[4]	training's l2: 0.000940392	valid_1's l2: 0.000864907
[5]	training's l2: 0.000927676	valid_1's l2: 0.000855573
[6]	training's l2: 0.000916102	valid_1's l2: 0.000848449
[7]	training's l2: 0.000905688	valid_1's l2: 0.000841152
[8]	training's l2: 0.000895894	valid_1's l2: 0.000834868
[9]	training's l2: 0.000886665	valid_1's l2: 0.000829421
[10]	training's l2: 0.000878271	valid_1's l2: 0.000823771
[11]	training's l2: 0.000868961	valid_1's l2: 0.000819195
[12]	training's l2: 0.00085936	valid_1's l2: 0.000813797
[13]	training's l2: 0.000851335	valid_1's l2: 0.000810048
[14]	training's l2: 0.000842645	valid_1's l2: 0.000805373
[15]	training's l2: 0.000835346	valid_1's l2: 0.000801504
[16]	training's l2: 0.00082923	valid_1's l2: 0.000798219
[17]	training's l2: 0.000823422	valid_1's l2: 0.000795995
[18]	training's l2: 0.000816738	valid_1's l2: 0.000793169
[19]	training's l2: 0.000809573	valid_1's l2: 0.000788481
[20]	training's l2: 0.000803619	valid_1's l2: 0.000785247
[21]	training's l2: 0.000798744	valid_1's l2: 0.000783016
[22]	training's l2: 0.000793899	valid_1's l2: 0.000780903
[23]	training's l2: 0.000788424	valid_1's l2: 0.000778752
[24]	training's l2: 0.000783789	valid_1's l2: 0.000776135
[25]	training's l2: 0.000778992	valid_1's l2: 0.000774496
[26]	training's l2: 0.000775217	valid_1's l2: 0.000773652
[27]	training's l2: 0.000771249	valid_1's l2: 0.000771957
[28]	training's l2: 0.000767536	valid_1's l2: 0.000770414
[29]	training's l2: 0.000763746	valid_1's l2: 0.000768805
[30]	training's l2: 0.00076037	valid_1's l2: 0.000767536
[31]	training's l2: 0.000756489	valid_1's l2: 0.000765784
[32]	training's l2: 0.000752864	valid_1's l2: 0.000764345
[33]	training's l2: 0.00074944	valid_1's l2: 0.000763025
[34]	training's l2: 0.000745894	valid_1's l2: 0.000761858
[35]	training's l2: 0.000742837	valid_1's l2: 0.000760232
[36]	training's l2: 0.000739373	valid_1's l2: 0.000758978
[37]	training's l2: 0.000736119	valid_1's l2: 0.000757538
[38]	training's l2: 0.000732791	valid_1's l2: 0.000756642
[39]	training's l2: 0.000730008	valid_1's l2: 0.000755788
[40]	training's l2: 0.000727099	valid_1's l2: 0.00075428
[41]	training's l2: 0.000723621	valid_1's l2: 0.000752582
[42]	training's l2: 0.000721172	valid_1's l2: 0.000751892
[43]	training's l2: 0.000718824	valid_1's l2: 0.000751246
[44]	training's l2: 0.000715868	valid_1's l2: 0.000748945
[45]	training's l2: 0.000713046	valid_1's l2: 0.000747866
[46]	training's l2: 0.000710359	valid_1's l2: 0.000746437
[47]	training's l2: 0.000707351	valid_1's l2: 0.000744662
[48]	training's l2: 0.00070463	valid_1's l2: 0.000743416
[49]	training's l2: 0.000701899	valid_1's l2: 0.000742673
[50]	training's l2: 0.000699133	valid_1's l2: 0.000741022
[51]	training's l2: 0.000696282	valid_1's l2: 0.000739752
[52]	training's l2: 0.00069299	valid_1's l2: 0.00073797
[53]	training's l2: 0.000690842	valid_1's l2: 0.000737387
[54]	training's l2: 0.000688801	valid_1's l2: 0.000737056
[55]	training's l2: 0.000686456	valid_1's l2: 0.000735375
[56]	training's l2: 0.000684356	valid_1's l2: 0.000734711
[57]	training's l2: 0.000682162	valid_1's l2: 0.00073401
[58]	training's l2: 0.000679658	valid_1's l2: 0.000733376
[59]	training's l2: 0.000677411	valid_1's l2: 0.000732471
[60]	training's l2: 0.000674961	valid_1's l2: 0.000731538
[61]	training's l2: 0.000672836	valid_1's l2: 0.000731276
[62]	training's l2: 0.00067078	valid_1's l2: 0.000730859
[63]	training's l2: 0.000668645	valid_1's l2: 0.000730237
[64]	training's l2: 0.000666666	valid_1's l2: 0.000729607
[65]	training's l2: 0.000664384	valid_1's l2: 0.00072929
[66]	training's l2: 0.000661707	valid_1's l2: 0.000727657
[67]	training's l2: 0.000659189	valid_1's l2: 0.000727151
[68]	training's l2: 0.000657397	valid_1's l2: 0.000726669
[69]	training's l2: 0.0006555	valid_1's l2: 0.000726168
[70]	training's l2: 0.000653168	valid_1's l2: 0.000725043
[71]	training's l2: 0.000651283	valid_1's l2: 0.000724965
[72]	training's l2: 0.000649429	valid_1's l2: 0.000724348
[73]	training's l2: 0.000647714	valid_1's l2: 0.000723791
[74]	training's l2: 0.000645801	valid_1's l2: 0.000723484
[75]	training's l2: 0.000644099	valid_1's l2: 0.000723124
[76]	training's l2: 0.000641954	valid_1's l2: 0.000722232
[77]	training's l2: 0.000640233	valid_1's l2: 0.000722131
[78]	training's l2: 0.000638575	valid_1's l2: 0.000721535
[79]	training's l2: 0.000636773	valid_1's l2: 0.000721653
[80]	training's l2: 0.000635297	valid_1's l2: 0.000721392
[81]	training's l2: 0.000633215	valid_1's l2: 0.000720478
[82]	training's l2: 0.00063119	valid_1's l2: 0.000720279
[83]	training's l2: 0.000629509	valid_1's l2: 0.000720343
[84]	training's l2: 0.000627734	valid_1's l2: 0.000720159
[85]	training's l2: 0.00062614	valid_1's l2: 0.000719916
[86]	training's l2: 0.000624529	valid_1's l2: 0.000719488
[87]	training's l2: 0.000622249	valid_1's l2: 0.000718308
[88]	training's l2: 0.000620637	valid_1's l2: 0.000718462
[89]	training's l2: 0.000619198	valid_1's l2: 0.000718461
[90]	training's l2: 0.000617693	valid_1's l2: 0.000718357
[91]	training's l2: 0.000616235	valid_1's l2: 0.00071829
[92]	training's l2: 0.000614848	valid_1's l2: 0.000718304
[93]	training's l2: 0.000613217	valid_1's l2: 0.000718309
[94]	training's l2: 0.000611807	valid_1's l2: 0.000717882
[95]	training's l2: 0.000610282	valid_1's l2: 0.000717654
[96]	training's l2: 0.000608504	valid_1's l2: 0.00071738
[97]	training's l2: 0.000606642	valid_1's l2: 0.00071658
[98]	training's l2: 0.000605246	valid_1's l2: 0.000716388
[99]	training's l2: 0.000603855	valid_1's l2: 0.000716363
[100]	training's l2: 0.000602569	valid_1's l2: 0.00071623
[101]	training's l2: 0.000601085	valid_1's l2: 0.000716218
[102]	training's l2: 0.000599676	valid_1's l2: 0.000716227
[103]	training's l2: 0.000598339	valid_1's l2: 0.000716074
[104]	training's l2: 0.00059701	valid_1's l2: 0.000715865
[105]	training's l2: 0.000595561	valid_1's l2: 0.000715706
[106]	training's l2: 0.000594128	valid_1's l2: 0.000715031
[107]	training's l2: 0.00059271	valid_1's l2: 0.000715328
[108]	training's l2: 0.000591478	valid_1's l2: 0.000715187
[109]	training's l2: 0.000590113	valid_1's l2: 0.000715226
[110]	training's l2: 0.000588729	valid_1's l2: 0.000714988
[111]	training's l2: 0.000587409	valid_1's l2: 0.000714739
[112]	training's l2: 0.000585867	valid_1's l2: 0.00071439
[113]	training's l2: 0.000584558	valid_1's l2: 0.000714234
[114]	training's l2: 0.000583326	valid_1's l2: 0.000714037
[115]	training's l2: 0.000582084	valid_1's l2: 0.000713901
[116]	training's l2: 0.000580538	valid_1's l2: 0.000713171
[117]	training's l2: 0.000579201	valid_1's l2: 0.00071341
[118]	training's l2: 0.000577944	valid_1's l2: 0.000713568
[119]	training's l2: 0.000576489	valid_1's l2: 0.000713603
[120]	training's l2: 0.000575362	valid_1's l2: 0.000713707
[121]	training's l2: 0.000574228	valid_1's l2: 0.000713633
[122]	training's l2: 0.000572999	valid_1's l2: 0.00071363
[123]	training's l2: 0.000571697	valid_1's l2: 0.000713512
[124]	training's l2: 0.000570563	valid_1's l2: 0.000713566
[125]	training's l2: 0.000569363	valid_1's l2: 0.000713618
[126]	training's l2: 0.000568107	valid_1's l2: 0.000713925
[127]	training's l2: 0.000566892	valid_1's l2: 0.000713888
[128]	training's l2: 0.000565733	valid_1's l2: 0.000714083
[129]	training's l2: 0.000564689	valid_1's l2: 0.00071427
[130]	training's l2: 0.00056338	valid_1's l2: 0.000714377
[131]	training's l2: 0.000562192	valid_1's l2: 0.000714533
[132]	training's l2: 0.000560764	valid_1's l2: 0.000714041
[133]	training's l2: 0.000559394	valid_1's l2: 0.000713572
[134]	training's l2: 0.000558127	valid_1's l2: 0.000713112
[135]	training's l2: 0.000556952	valid_1's l2: 0.000713174
[136]	training's l2: 0.000555876	valid_1's l2: 0.000713303
[137]	training's l2: 0.000554856	valid_1's l2: 0.000713069
[138]	training's l2: 0.000553756	valid_1's l2: 0.00071352
[139]	training's l2: 0.00055259	valid_1's l2: 0.000713306
[140]	training's l2: 0.000551437	valid_1's l2: 0.000713417
[141]	training's l2: 0.000550318	valid_1's l2: 0.000713271
[142]	training's l2: 0.000549187	valid_1's l2: 0.000713503
[143]	training's l2: 0.000548075	valid_1's l2: 0.000713434
[144]	training's l2: 0.000546818	valid_1's l2: 0.000713645
[145]	training's l2: 0.000545651	valid_1's l2: 0.000713764
[146]	training's l2: 0.000544525	valid_1's l2: 0.000713922
[147]	training's l2: 0.00054348	valid_1's l2: 0.000713728
[148]	training's l2: 0.000542449	valid_1's l2: 0.000713814
[149]	training's l2: 0.00054129	valid_1's l2: 0.000713855
[150]	training's l2: 0.000540035	valid_1's l2: 0.000713831
[151]	training's l2: 0.00053884	valid_1's l2: 0.00071392
[152]	training's l2: 0.000537665	valid_1's l2: 0.000713835
[153]	training's l2: 0.000536555	valid_1's l2: 0.000713955
[154]	training's l2: 0.000535291	valid_1's l2: 0.000713896
[155]	training's l2: 0.000534228	valid_1's l2: 0.000713724
[156]	training's l2: 0.000533195	valid_1's l2: 0.000713664
[157]	training's l2: 0.000532207	valid_1's l2: 0.000713717
[158]	training's l2: 0.000531107	valid_1's l2: 0.000713236
[159]	training's l2: 0.000530029	valid_1's l2: 0.000713112
[160]	training's l2: 0.000529012	valid_1's l2: 0.000712972
[161]	training's l2: 0.000527897	valid_1's l2: 0.000712909
[162]	training's l2: 0.000526856	valid_1's l2: 0.000712991
[163]	training's l2: 0.000525947	valid_1's l2: 0.000713029
[164]	training's l2: 0.000524906	valid_1's l2: 0.000712939
[165]	training's l2: 0.000523861	valid_1's l2: 0.000712704
[166]	training's l2: 0.000522822	valid_1's l2: 0.000712682
[167]	training's l2: 0.000521869	valid_1's l2: 0.000712495
[168]	training's l2: 0.000520808	valid_1's l2: 0.000712554
[169]	training's l2: 0.000519808	valid_1's l2: 0.000712426
[170]	training's l2: 0.00051891	valid_1's l2: 0.000712334
[171]	training's l2: 0.000517754	valid_1's l2: 0.000711958
[172]	training's l2: 0.000516758	valid_1's l2: 0.000711943
[173]	training's l2: 0.000515832	valid_1's l2: 0.000711934
[174]	training's l2: 0.000514892	valid_1's l2: 0.000712046
[175]	training's l2: 0.000513857	valid_1's l2: 0.000712235
[176]	training's l2: 0.000512958	valid_1's l2: 0.000712351
[177]	training's l2: 0.000511898	valid_1's l2: 0.000712244
[178]	training's l2: 0.000511017	valid_1's l2: 0.000712388
[179]	training's l2: 0.000510063	valid_1's l2: 0.000712279
[180]	training's l2: 0.000509055	valid_1's l2: 0.000712228
[181]	training's l2: 0.000508213	valid_1's l2: 0.000712202
[182]	training's l2: 0.000507214	valid_1's l2: 0.000712194
[183]	training's l2: 0.000506198	valid_1's l2: 0.000711961
[184]	training's l2: 0.000505213	valid_1's l2: 0.00071204
[185]	training's l2: 0.00050424	valid_1's l2: 0.000712117
[186]	training's l2: 0.000503379	valid_1's l2: 0.000712259
[187]	training's l2: 0.000502535	valid_1's l2: 0.000712041
Did not meet early stopping. Best iteration is:
[187]	training's l2: 0.000502535	valid_1's l2: 0.000712041
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218147 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000975685	valid_1's l2: 0.000889729
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000952268	valid_1's l2: 0.000875943
[3]	training's l2: 0.000931669	valid_1's l2: 0.00086179
[4]	training's l2: 0.000913943	valid_1's l2: 0.000851782
[5]	training's l2: 0.000897805	valid_1's l2: 0.000842362
[6]	training's l2: 0.000885105	valid_1's l2: 0.000833873
[7]	training's l2: 0.000871698	valid_1's l2: 0.000826561
[8]	training's l2: 0.000856995	valid_1's l2: 0.000818651
[9]	training's l2: 0.00084352	valid_1's l2: 0.000810814
[10]	training's l2: 0.000832289	valid_1's l2: 0.000806758
[11]	training's l2: 0.000823174	valid_1's l2: 0.000801624
[12]	training's l2: 0.000812268	valid_1's l2: 0.000794275
[13]	training's l2: 0.000803689	valid_1's l2: 0.000789936
[14]	training's l2: 0.000797039	valid_1's l2: 0.00078568
[15]	training's l2: 0.000789067	valid_1's l2: 0.000782526
[16]	training's l2: 0.000783167	valid_1's l2: 0.000778538
[17]	training's l2: 0.000775143	valid_1's l2: 0.00077473
[18]	training's l2: 0.000767936	valid_1's l2: 0.000771452
[19]	training's l2: 0.000763513	valid_1's l2: 0.000769812
[20]	training's l2: 0.00075899	valid_1's l2: 0.00076879
[21]	training's l2: 0.000754669	valid_1's l2: 0.000766566
[22]	training's l2: 0.000749469	valid_1's l2: 0.000764536
[23]	training's l2: 0.000744557	valid_1's l2: 0.000761262
[24]	training's l2: 0.000740595	valid_1's l2: 0.000759656
[25]	training's l2: 0.000735701	valid_1's l2: 0.000757974
[26]	training's l2: 0.0007324	valid_1's l2: 0.000755755
[27]	training's l2: 0.000728885	valid_1's l2: 0.000754162
[28]	training's l2: 0.000725459	valid_1's l2: 0.000752167
[29]	training's l2: 0.000722223	valid_1's l2: 0.000751348
[30]	training's l2: 0.000718489	valid_1's l2: 0.00074888
[31]	training's l2: 0.000715033	valid_1's l2: 0.000747952
[32]	training's l2: 0.00071053	valid_1's l2: 0.000744374
[33]	training's l2: 0.000707279	valid_1's l2: 0.000742559
[34]	training's l2: 0.000704382	valid_1's l2: 0.000740858
[35]	training's l2: 0.000701509	valid_1's l2: 0.000739343
[36]	training's l2: 0.000698089	valid_1's l2: 0.000737939
[37]	training's l2: 0.000694233	valid_1's l2: 0.000735415
[38]	training's l2: 0.000691853	valid_1's l2: 0.000734232
[39]	training's l2: 0.00068957	valid_1's l2: 0.000734074
[40]	training's l2: 0.0006862	valid_1's l2: 0.000732142
[41]	training's l2: 0.000683807	valid_1's l2: 0.000731494
[42]	training's l2: 0.000681644	valid_1's l2: 0.000731238
[43]	training's l2: 0.00067858	valid_1's l2: 0.000730948
[44]	training's l2: 0.000676535	valid_1's l2: 0.000730146
[45]	training's l2: 0.000674121	valid_1's l2: 0.000729865
[46]	training's l2: 0.000671461	valid_1's l2: 0.000729229
[47]	training's l2: 0.000668607	valid_1's l2: 0.000727936
[48]	training's l2: 0.000666666	valid_1's l2: 0.000727909
[49]	training's l2: 0.000664433	valid_1's l2: 0.000727241
[50]	training's l2: 0.000662425	valid_1's l2: 0.000726888
[51]	training's l2: 0.000660655	valid_1's l2: 0.000726638
[52]	training's l2: 0.000658618	valid_1's l2: 0.000726451
[53]	training's l2: 0.00065666	valid_1's l2: 0.000725795
[54]	training's l2: 0.000654697	valid_1's l2: 0.000725705
[55]	training's l2: 0.00065298	valid_1's l2: 0.00072562
[56]	training's l2: 0.000650162	valid_1's l2: 0.000724291
[57]	training's l2: 0.000648399	valid_1's l2: 0.000723886
[58]	training's l2: 0.000646276	valid_1's l2: 0.000722499
[59]	training's l2: 0.000644121	valid_1's l2: 0.000722473
[60]	training's l2: 0.000642062	valid_1's l2: 0.000722535
[61]	training's l2: 0.000640149	valid_1's l2: 0.000722772
[62]	training's l2: 0.000638192	valid_1's l2: 0.000721874
[63]	training's l2: 0.000636567	valid_1's l2: 0.000721907
[64]	training's l2: 0.000634307	valid_1's l2: 0.000721065
[65]	training's l2: 0.000632415	valid_1's l2: 0.00072066
[66]	training's l2: 0.000630173	valid_1's l2: 0.00071985
[67]	training's l2: 0.000628383	valid_1's l2: 0.000719676
[68]	training's l2: 0.000626689	valid_1's l2: 0.00071995
[69]	training's l2: 0.000624884	valid_1's l2: 0.000720038
[70]	training's l2: 0.000623089	valid_1's l2: 0.000719905
[71]	training's l2: 0.000621248	valid_1's l2: 0.00071985
[72]	training's l2: 0.000619594	valid_1's l2: 0.000719389
[73]	training's l2: 0.000618007	valid_1's l2: 0.000719439
[74]	training's l2: 0.000616274	valid_1's l2: 0.000719679
[75]	training's l2: 0.000614543	valid_1's l2: 0.000719455
[76]	training's l2: 0.000613012	valid_1's l2: 0.000719397
[77]	training's l2: 0.000611265	valid_1's l2: 0.000719154
[78]	training's l2: 0.00060969	valid_1's l2: 0.00071928
[79]	training's l2: 0.000607834	valid_1's l2: 0.000718509
[80]	training's l2: 0.000606369	valid_1's l2: 0.000718716
[81]	training's l2: 0.000604775	valid_1's l2: 0.00071859
[82]	training's l2: 0.000603344	valid_1's l2: 0.000718487
[83]	training's l2: 0.0006018	valid_1's l2: 0.000717955
[84]	training's l2: 0.000600337	valid_1's l2: 0.000718136
[85]	training's l2: 0.000598726	valid_1's l2: 0.000717812
[86]	training's l2: 0.000597046	valid_1's l2: 0.000717484
[87]	training's l2: 0.000595334	valid_1's l2: 0.000716982
[88]	training's l2: 0.000593572	valid_1's l2: 0.000716928
[89]	training's l2: 0.000592101	valid_1's l2: 0.000716742
[90]	training's l2: 0.000590842	valid_1's l2: 0.000716573
[91]	training's l2: 0.000589598	valid_1's l2: 0.000716306
[92]	training's l2: 0.000587655	valid_1's l2: 0.000715733
[93]	training's l2: 0.00058613	valid_1's l2: 0.000715633
[94]	training's l2: 0.000584793	valid_1's l2: 0.000715578
[95]	training's l2: 0.000583277	valid_1's l2: 0.000715677
[96]	training's l2: 0.00058191	valid_1's l2: 0.000715551
Did not meet early stopping. Best iteration is:
[96]	training's l2: 0.00058191	valid_1's l2: 0.000715551
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204788 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000986736	valid_1's l2: 0.000895152
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000971905	valid_1's l2: 0.000883667
[3]	training's l2: 0.000957981	valid_1's l2: 0.000872953
[4]	training's l2: 0.000945083	valid_1's l2: 0.00086324
[5]	training's l2: 0.000933518	valid_1's l2: 0.000854513
[6]	training's l2: 0.000923695	valid_1's l2: 0.000847956
[7]	training's l2: 0.000914122	valid_1's l2: 0.000840838
[8]	training's l2: 0.00090578	valid_1's l2: 0.000834947
[9]	training's l2: 0.00089736	valid_1's l2: 0.000828586
[10]	training's l2: 0.000889994	valid_1's l2: 0.000824223
[11]	training's l2: 0.000882948	valid_1's l2: 0.000819036
[12]	training's l2: 0.000874882	valid_1's l2: 0.000813704
[13]	training's l2: 0.000867855	valid_1's l2: 0.000808645
[14]	training's l2: 0.000861624	valid_1's l2: 0.000804956
[15]	training's l2: 0.00085533	valid_1's l2: 0.000800595
[16]	training's l2: 0.000849373	valid_1's l2: 0.00079605
[17]	training's l2: 0.000844397	valid_1's l2: 0.000791959
[18]	training's l2: 0.000839979	valid_1's l2: 0.000789299
[19]	training's l2: 0.000836066	valid_1's l2: 0.000787318
[20]	training's l2: 0.000831871	valid_1's l2: 0.000784428
[21]	training's l2: 0.000828255	valid_1's l2: 0.00078256
[22]	training's l2: 0.000824327	valid_1's l2: 0.000779277
[23]	training's l2: 0.000819745	valid_1's l2: 0.00077673
[24]	training's l2: 0.000816726	valid_1's l2: 0.000775451
[25]	training's l2: 0.000813702	valid_1's l2: 0.000773978
[26]	training's l2: 0.000810517	valid_1's l2: 0.000772137
[27]	training's l2: 0.000807371	valid_1's l2: 0.000770625
[28]	training's l2: 0.000804172	valid_1's l2: 0.000768289
[29]	training's l2: 0.000801314	valid_1's l2: 0.00076712
[30]	training's l2: 0.000798069	valid_1's l2: 0.000765854
[31]	training's l2: 0.000795404	valid_1's l2: 0.000764466
[32]	training's l2: 0.000792921	valid_1's l2: 0.000763413
[33]	training's l2: 0.000790104	valid_1's l2: 0.000761213
[34]	training's l2: 0.000787619	valid_1's l2: 0.00076004
[35]	training's l2: 0.00078469	valid_1's l2: 0.000758697
[36]	training's l2: 0.000782295	valid_1's l2: 0.000757022
[37]	training's l2: 0.000780202	valid_1's l2: 0.000756633
[38]	training's l2: 0.00077752	valid_1's l2: 0.000755073
[39]	training's l2: 0.000775433	valid_1's l2: 0.000753553
[40]	training's l2: 0.000771972	valid_1's l2: 0.000750284
[41]	training's l2: 0.000769974	valid_1's l2: 0.000749887
[42]	training's l2: 0.000767587	valid_1's l2: 0.000748211
[43]	training's l2: 0.000765321	valid_1's l2: 0.000746167
[44]	training's l2: 0.000763343	valid_1's l2: 0.000745445
[45]	training's l2: 0.00076133	valid_1's l2: 0.00074462
[46]	training's l2: 0.000759456	valid_1's l2: 0.000744007
[47]	training's l2: 0.000757415	valid_1's l2: 0.00074265
[48]	training's l2: 0.000754935	valid_1's l2: 0.000741283
[49]	training's l2: 0.000753206	valid_1's l2: 0.000740781
[50]	training's l2: 0.000751386	valid_1's l2: 0.000740603
[51]	training's l2: 0.000749398	valid_1's l2: 0.000740297
[52]	training's l2: 0.000747287	valid_1's l2: 0.00073959
[53]	training's l2: 0.000745734	valid_1's l2: 0.000739242
[54]	training's l2: 0.000743132	valid_1's l2: 0.000737112
[55]	training's l2: 0.000740891	valid_1's l2: 0.000735742
[56]	training's l2: 0.000738878	valid_1's l2: 0.00073455
[57]	training's l2: 0.000736897	valid_1's l2: 0.000733928
[58]	training's l2: 0.000734821	valid_1's l2: 0.000733012
[59]	training's l2: 0.000733551	valid_1's l2: 0.000732516
[60]	training's l2: 0.0007322	valid_1's l2: 0.000732002
[61]	training's l2: 0.000730196	valid_1's l2: 0.000731403
[62]	training's l2: 0.000728685	valid_1's l2: 0.000731016
[63]	training's l2: 0.00072706	valid_1's l2: 0.000730328
[64]	training's l2: 0.000725545	valid_1's l2: 0.000730343
[65]	training's l2: 0.000724022	valid_1's l2: 0.000730083
[66]	training's l2: 0.000722131	valid_1's l2: 0.000728654
[67]	training's l2: 0.00072066	valid_1's l2: 0.000727917
[68]	training's l2: 0.000718933	valid_1's l2: 0.000727396
[69]	training's l2: 0.000717408	valid_1's l2: 0.000726912
[70]	training's l2: 0.000716142	valid_1's l2: 0.000726454
[71]	training's l2: 0.000714185	valid_1's l2: 0.000725426
[72]	training's l2: 0.000712617	valid_1's l2: 0.000725332
[73]	training's l2: 0.00071132	valid_1's l2: 0.000725268
[74]	training's l2: 0.000709598	valid_1's l2: 0.000724645
[75]	training's l2: 0.000708367	valid_1's l2: 0.000724593
[76]	training's l2: 0.00070667	valid_1's l2: 0.000723856
[77]	training's l2: 0.00070546	valid_1's l2: 0.000724017
[78]	training's l2: 0.000703951	valid_1's l2: 0.00072368
[79]	training's l2: 0.000702551	valid_1's l2: 0.000723046
[80]	training's l2: 0.000701369	valid_1's l2: 0.000722712
[81]	training's l2: 0.000700017	valid_1's l2: 0.000722857
[82]	training's l2: 0.00069865	valid_1's l2: 0.00072273
[83]	training's l2: 0.000697233	valid_1's l2: 0.000722551
[84]	training's l2: 0.000695488	valid_1's l2: 0.000721537
[85]	training's l2: 0.00069389	valid_1's l2: 0.000721368
[86]	training's l2: 0.000692586	valid_1's l2: 0.000720981
[87]	training's l2: 0.000691265	valid_1's l2: 0.000720716
[88]	training's l2: 0.000690153	valid_1's l2: 0.00072057
[89]	training's l2: 0.000688808	valid_1's l2: 0.000720525
[90]	training's l2: 0.000687652	valid_1's l2: 0.000720349
[91]	training's l2: 0.000686065	valid_1's l2: 0.000719535
[92]	training's l2: 0.000684621	valid_1's l2: 0.000719321
[93]	training's l2: 0.000683373	valid_1's l2: 0.000718806
[94]	training's l2: 0.000681962	valid_1's l2: 0.000718318
[95]	training's l2: 0.000680544	valid_1's l2: 0.000717914
[96]	training's l2: 0.000679344	valid_1's l2: 0.000717659
[97]	training's l2: 0.000678274	valid_1's l2: 0.000717709
[98]	training's l2: 0.000677087	valid_1's l2: 0.000717447
[99]	training's l2: 0.000675773	valid_1's l2: 0.000717452
[100]	training's l2: 0.00067429	valid_1's l2: 0.000717083
[101]	training's l2: 0.000673162	valid_1's l2: 0.00071728
[102]	training's l2: 0.000671943	valid_1's l2: 0.00071697
[103]	training's l2: 0.000670872	valid_1's l2: 0.000716969
[104]	training's l2: 0.000669579	valid_1's l2: 0.000716858
[105]	training's l2: 0.000668361	valid_1's l2: 0.000716459
[106]	training's l2: 0.000667119	valid_1's l2: 0.000716063
[107]	training's l2: 0.000665835	valid_1's l2: 0.00071604
[108]	training's l2: 0.000664521	valid_1's l2: 0.00071613
[109]	training's l2: 0.000663449	valid_1's l2: 0.000716126
[110]	training's l2: 0.000662459	valid_1's l2: 0.000715886
[111]	training's l2: 0.000661416	valid_1's l2: 0.000715565
[112]	training's l2: 0.000659981	valid_1's l2: 0.00071551
[113]	training's l2: 0.000658881	valid_1's l2: 0.000715334
[114]	training's l2: 0.000657597	valid_1's l2: 0.000715342
[115]	training's l2: 0.000656621	valid_1's l2: 0.000715323
[116]	training's l2: 0.00065567	valid_1's l2: 0.000715136
[117]	training's l2: 0.000654606	valid_1's l2: 0.000714828
[118]	training's l2: 0.000653455	valid_1's l2: 0.000714769
[119]	training's l2: 0.000652526	valid_1's l2: 0.000714603
[120]	training's l2: 0.000651453	valid_1's l2: 0.000714566
[121]	training's l2: 0.000650366	valid_1's l2: 0.000714229
[122]	training's l2: 0.000649372	valid_1's l2: 0.000713995
[123]	training's l2: 0.000648399	valid_1's l2: 0.000714099
[124]	training's l2: 0.000647393	valid_1's l2: 0.000714124
[125]	training's l2: 0.000646271	valid_1's l2: 0.000714194
[126]	training's l2: 0.000645164	valid_1's l2: 0.000714258
[127]	training's l2: 0.000643956	valid_1's l2: 0.000713981
[128]	training's l2: 0.00064288	valid_1's l2: 0.000713976
[129]	training's l2: 0.000641786	valid_1's l2: 0.000714057
[130]	training's l2: 0.00064076	valid_1's l2: 0.000714004
[131]	training's l2: 0.000639505	valid_1's l2: 0.000713729
[132]	training's l2: 0.000638631	valid_1's l2: 0.000713807
[133]	training's l2: 0.000637674	valid_1's l2: 0.00071383
[134]	training's l2: 0.000636483	valid_1's l2: 0.000713635
[135]	training's l2: 0.000635563	valid_1's l2: 0.0007139
[136]	training's l2: 0.000634552	valid_1's l2: 0.000713964
[137]	training's l2: 0.000633182	valid_1's l2: 0.000713564
[138]	training's l2: 0.000632243	valid_1's l2: 0.000713506
[139]	training's l2: 0.00063116	valid_1's l2: 0.000713068
[140]	training's l2: 0.000630053	valid_1's l2: 0.000712879
[141]	training's l2: 0.000629074	valid_1's l2: 0.000712535
[142]	training's l2: 0.00062812	valid_1's l2: 0.000712601
[143]	training's l2: 0.000626924	valid_1's l2: 0.000712337
[144]	training's l2: 0.000625957	valid_1's l2: 0.000711939
[145]	training's l2: 0.00062495	valid_1's l2: 0.000712085
[146]	training's l2: 0.000624047	valid_1's l2: 0.000711856
[147]	training's l2: 0.000622995	valid_1's l2: 0.000712095
[148]	training's l2: 0.00062197	valid_1's l2: 0.000711986
[149]	training's l2: 0.000621134	valid_1's l2: 0.000712077
[150]	training's l2: 0.000620083	valid_1's l2: 0.000712051
[151]	training's l2: 0.00061925	valid_1's l2: 0.000711903
[152]	training's l2: 0.000618082	valid_1's l2: 0.00071152
[153]	training's l2: 0.00061712	valid_1's l2: 0.000711504
[154]	training's l2: 0.000616245	valid_1's l2: 0.000711415
[155]	training's l2: 0.000615426	valid_1's l2: 0.000711381
[156]	training's l2: 0.000614568	valid_1's l2: 0.000711336
[157]	training's l2: 0.000613736	valid_1's l2: 0.000711171
[158]	training's l2: 0.000612922	valid_1's l2: 0.000711102
[159]	training's l2: 0.000611947	valid_1's l2: 0.00071122
[160]	training's l2: 0.000611004	valid_1's l2: 0.000711085
[161]	training's l2: 0.000610105	valid_1's l2: 0.00071093
[162]	training's l2: 0.000609169	valid_1's l2: 0.000710861
[163]	training's l2: 0.000608307	valid_1's l2: 0.000710749
[164]	training's l2: 0.000607478	valid_1's l2: 0.000710622
[165]	training's l2: 0.000606432	valid_1's l2: 0.000709982
[166]	training's l2: 0.000605465	valid_1's l2: 0.000709718
[167]	training's l2: 0.000604624	valid_1's l2: 0.000709724
[168]	training's l2: 0.000603781	valid_1's l2: 0.000709752
[169]	training's l2: 0.000602814	valid_1's l2: 0.000709777
[170]	training's l2: 0.000601828	valid_1's l2: 0.000709856
[171]	training's l2: 0.000600932	valid_1's l2: 0.000709995
[172]	training's l2: 0.000600069	valid_1's l2: 0.000709909
[173]	training's l2: 0.000599129	valid_1's l2: 0.000709758
[174]	training's l2: 0.00059819	valid_1's l2: 0.00070948
[175]	training's l2: 0.000597336	valid_1's l2: 0.000709624
[176]	training's l2: 0.000596518	valid_1's l2: 0.000709676
[177]	training's l2: 0.000595587	valid_1's l2: 0.000709696
[178]	training's l2: 0.00059484	valid_1's l2: 0.000709572
[179]	training's l2: 0.000594143	valid_1's l2: 0.000709712
[180]	training's l2: 0.00059343	valid_1's l2: 0.000709549
[181]	training's l2: 0.000592525	valid_1's l2: 0.000709328
[182]	training's l2: 0.000591737	valid_1's l2: 0.000709538
[183]	training's l2: 0.000590861	valid_1's l2: 0.000709456
[184]	training's l2: 0.000590035	valid_1's l2: 0.000709456
[185]	training's l2: 0.000589354	valid_1's l2: 0.000709438
[186]	training's l2: 0.000588528	valid_1's l2: 0.000709329
[187]	training's l2: 0.000587849	valid_1's l2: 0.000709411
[188]	training's l2: 0.000587039	valid_1's l2: 0.00070951
[189]	training's l2: 0.000586139	valid_1's l2: 0.000709414
[190]	training's l2: 0.000585327	valid_1's l2: 0.000709479
[191]	training's l2: 0.000584446	valid_1's l2: 0.000709641
Did not meet early stopping. Best iteration is:
[191]	training's l2: 0.000584446	valid_1's l2: 0.000709641
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.210388 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.00098055	valid_1's l2: 0.000891589
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000960452	valid_1's l2: 0.000879266
[3]	training's l2: 0.000942669	valid_1's l2: 0.000867873
[4]	training's l2: 0.000926433	valid_1's l2: 0.000857931
[5]	training's l2: 0.000910924	valid_1's l2: 0.000847916
[6]	training's l2: 0.000898012	valid_1's l2: 0.000839851
[7]	training's l2: 0.000885631	valid_1's l2: 0.000831552
[8]	training's l2: 0.000874288	valid_1's l2: 0.000825107
[9]	training's l2: 0.000863835	valid_1's l2: 0.000817736
[10]	training's l2: 0.000854729	valid_1's l2: 0.000812741
[11]	training's l2: 0.000845537	valid_1's l2: 0.000808166
[12]	training's l2: 0.000836731	valid_1's l2: 0.000804008
[13]	training's l2: 0.000828366	valid_1's l2: 0.000798712
[14]	training's l2: 0.000820022	valid_1's l2: 0.000793687
[15]	training's l2: 0.000812506	valid_1's l2: 0.000790483
[16]	training's l2: 0.000806113	valid_1's l2: 0.000787106
[17]	training's l2: 0.000800324	valid_1's l2: 0.000784262
[18]	training's l2: 0.000793742	valid_1's l2: 0.00078015
[19]	training's l2: 0.000787756	valid_1's l2: 0.000778112
[20]	training's l2: 0.000781206	valid_1's l2: 0.000775003
[21]	training's l2: 0.00077554	valid_1's l2: 0.000772257
[22]	training's l2: 0.000770387	valid_1's l2: 0.000770205
[23]	training's l2: 0.000765955	valid_1's l2: 0.000767823
[24]	training's l2: 0.000760928	valid_1's l2: 0.000766528
[25]	training's l2: 0.000756018	valid_1's l2: 0.000764899
[26]	training's l2: 0.000751072	valid_1's l2: 0.000763366
[27]	training's l2: 0.000745994	valid_1's l2: 0.00076074
[28]	training's l2: 0.000741837	valid_1's l2: 0.000759708
[29]	training's l2: 0.00073768	valid_1's l2: 0.000758007
[30]	training's l2: 0.000733578	valid_1's l2: 0.000756879
[31]	training's l2: 0.000729969	valid_1's l2: 0.000755737
[32]	training's l2: 0.000725545	valid_1's l2: 0.000753774
[33]	training's l2: 0.000721633	valid_1's l2: 0.000752117
[34]	training's l2: 0.000717874	valid_1's l2: 0.000750623
[35]	training's l2: 0.000714113	valid_1's l2: 0.000748988
[36]	training's l2: 0.000710146	valid_1's l2: 0.000747065
[37]	training's l2: 0.000706724	valid_1's l2: 0.000746096
[38]	training's l2: 0.000702299	valid_1's l2: 0.000744465
[39]	training's l2: 0.000699198	valid_1's l2: 0.000743376
[40]	training's l2: 0.000695569	valid_1's l2: 0.000741219
[41]	training's l2: 0.00069159	valid_1's l2: 0.000740474
[42]	training's l2: 0.000687943	valid_1's l2: 0.000738596
[43]	training's l2: 0.000684781	valid_1's l2: 0.000737685
[44]	training's l2: 0.000681843	valid_1's l2: 0.000736556
[45]	training's l2: 0.000678737	valid_1's l2: 0.000735767
[46]	training's l2: 0.000675261	valid_1's l2: 0.000733403
[47]	training's l2: 0.000671985	valid_1's l2: 0.000731931
[48]	training's l2: 0.000668592	valid_1's l2: 0.000731519
[49]	training's l2: 0.000665537	valid_1's l2: 0.000730988
[50]	training's l2: 0.000662528	valid_1's l2: 0.000730207
[51]	training's l2: 0.000659743	valid_1's l2: 0.000728871
[52]	training's l2: 0.000657102	valid_1's l2: 0.000728603
[53]	training's l2: 0.00065427	valid_1's l2: 0.000727401
[54]	training's l2: 0.000651138	valid_1's l2: 0.000726851
[55]	training's l2: 0.000648511	valid_1's l2: 0.000726534
[56]	training's l2: 0.000645931	valid_1's l2: 0.000726345
[57]	training's l2: 0.000643096	valid_1's l2: 0.000725427
[58]	training's l2: 0.000639361	valid_1's l2: 0.00072333
[59]	training's l2: 0.000635578	valid_1's l2: 0.000722975
[60]	training's l2: 0.00063335	valid_1's l2: 0.000722706
[61]	training's l2: 0.00063074	valid_1's l2: 0.000721784
[62]	training's l2: 0.000628398	valid_1's l2: 0.000721469
[63]	training's l2: 0.000626023	valid_1's l2: 0.000721365
[64]	training's l2: 0.000623618	valid_1's l2: 0.000721161
[65]	training's l2: 0.00062047	valid_1's l2: 0.000719888
[66]	training's l2: 0.000617855	valid_1's l2: 0.000719967
[67]	training's l2: 0.000615535	valid_1's l2: 0.000720062
[68]	training's l2: 0.000613259	valid_1's l2: 0.000719696
[69]	training's l2: 0.000610645	valid_1's l2: 0.000719322
[70]	training's l2: 0.000608116	valid_1's l2: 0.000718994
[71]	training's l2: 0.000605664	valid_1's l2: 0.00071829
[72]	training's l2: 0.000603502	valid_1's l2: 0.000718274
[73]	training's l2: 0.000601282	valid_1's l2: 0.000717982
[74]	training's l2: 0.000598877	valid_1's l2: 0.000717863
[75]	training's l2: 0.000596778	valid_1's l2: 0.000717732
[76]	training's l2: 0.00059447	valid_1's l2: 0.000717533
[77]	training's l2: 0.000592381	valid_1's l2: 0.000717917
[78]	training's l2: 0.00059024	valid_1's l2: 0.000717835
[79]	training's l2: 0.000588199	valid_1's l2: 0.000717747
[80]	training's l2: 0.000585849	valid_1's l2: 0.00071725
[81]	training's l2: 0.000583625	valid_1's l2: 0.000715911
[82]	training's l2: 0.000581548	valid_1's l2: 0.000715594
[83]	training's l2: 0.000579443	valid_1's l2: 0.000715985
[84]	training's l2: 0.000577434	valid_1's l2: 0.000716213
[85]	training's l2: 0.000575048	valid_1's l2: 0.000716314
[86]	training's l2: 0.000573179	valid_1's l2: 0.000716023
[87]	training's l2: 0.000571161	valid_1's l2: 0.000716274
[88]	training's l2: 0.000568947	valid_1's l2: 0.000716128
[89]	training's l2: 0.000567228	valid_1's l2: 0.000716017
[90]	training's l2: 0.000565386	valid_1's l2: 0.000716343
[91]	training's l2: 0.000563469	valid_1's l2: 0.000716075
[92]	training's l2: 0.000561638	valid_1's l2: 0.000715861
[93]	training's l2: 0.000559674	valid_1's l2: 0.000715766
[94]	training's l2: 0.000557974	valid_1's l2: 0.000715818
[95]	training's l2: 0.00055598	valid_1's l2: 0.000715667
[96]	training's l2: 0.000554033	valid_1's l2: 0.000715505
[97]	training's l2: 0.000551859	valid_1's l2: 0.000715239
[98]	training's l2: 0.000550256	valid_1's l2: 0.000715125
[99]	training's l2: 0.000548698	valid_1's l2: 0.000714992
[100]	training's l2: 0.000547104	valid_1's l2: 0.000715263
[101]	training's l2: 0.000545271	valid_1's l2: 0.000715378
[102]	training's l2: 0.000543157	valid_1's l2: 0.000715252
[103]	training's l2: 0.000541618	valid_1's l2: 0.000715503
[104]	training's l2: 0.000539798	valid_1's l2: 0.000714817
[105]	training's l2: 0.000538109	valid_1's l2: 0.000714986
[106]	training's l2: 0.000536145	valid_1's l2: 0.000714737
[107]	training's l2: 0.000534395	valid_1's l2: 0.000715061
[108]	training's l2: 0.00053257	valid_1's l2: 0.000714825
[109]	training's l2: 0.00053103	valid_1's l2: 0.000714865
[110]	training's l2: 0.000529495	valid_1's l2: 0.000714914
[111]	training's l2: 0.000527917	valid_1's l2: 0.000714689
[112]	training's l2: 0.00052647	valid_1's l2: 0.000714356
[113]	training's l2: 0.000524977	valid_1's l2: 0.000714403
[114]	training's l2: 0.000523222	valid_1's l2: 0.000714432
[115]	training's l2: 0.000521686	valid_1's l2: 0.00071443
[116]	training's l2: 0.000519952	valid_1's l2: 0.000714113
[117]	training's l2: 0.000518215	valid_1's l2: 0.000713739
[118]	training's l2: 0.000516718	valid_1's l2: 0.000713397
[119]	training's l2: 0.000515311	valid_1's l2: 0.000713083
[120]	training's l2: 0.000513857	valid_1's l2: 0.000713405
[121]	training's l2: 0.000512015	valid_1's l2: 0.000712835
[122]	training's l2: 0.00051069	valid_1's l2: 0.000712956
[123]	training's l2: 0.0005093	valid_1's l2: 0.000712909
[124]	training's l2: 0.000507724	valid_1's l2: 0.000712836
[125]	training's l2: 0.000505881	valid_1's l2: 0.000712411
[126]	training's l2: 0.000504093	valid_1's l2: 0.000712015
[127]	training's l2: 0.000502596	valid_1's l2: 0.000712046
[128]	training's l2: 0.000501081	valid_1's l2: 0.000712098
[129]	training's l2: 0.000499624	valid_1's l2: 0.000711923
[130]	training's l2: 0.000498226	valid_1's l2: 0.00071214
[131]	training's l2: 0.000496947	valid_1's l2: 0.000712239
[132]	training's l2: 0.000495441	valid_1's l2: 0.000712216
[133]	training's l2: 0.000493751	valid_1's l2: 0.000711654
[134]	training's l2: 0.000492574	valid_1's l2: 0.000711747
[135]	training's l2: 0.000491124	valid_1's l2: 0.000711743
[136]	training's l2: 0.000489741	valid_1's l2: 0.00071166
[137]	training's l2: 0.000488258	valid_1's l2: 0.000711627
[138]	training's l2: 0.000486859	valid_1's l2: 0.000712113
[139]	training's l2: 0.000485536	valid_1's l2: 0.000712243
[140]	training's l2: 0.000484006	valid_1's l2: 0.00071215
[141]	training's l2: 0.000482374	valid_1's l2: 0.000712309
[142]	training's l2: 0.000480981	valid_1's l2: 0.000712448
[143]	training's l2: 0.000479472	valid_1's l2: 0.000712482
[144]	training's l2: 0.000477915	valid_1's l2: 0.000712247
[145]	training's l2: 0.000476727	valid_1's l2: 0.000712261
[146]	training's l2: 0.000475424	valid_1's l2: 0.00071225
[147]	training's l2: 0.000473975	valid_1's l2: 0.000712211
[148]	training's l2: 0.000472684	valid_1's l2: 0.000712018
[149]	training's l2: 0.000471174	valid_1's l2: 0.000712103
[150]	training's l2: 0.000469754	valid_1's l2: 0.000712117
Did not meet early stopping. Best iteration is:
[150]	training's l2: 0.000469754	valid_1's l2: 0.000712117
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.204170 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000977632	valid_1's l2: 0.000889417
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.0009564	valid_1's l2: 0.000873747
[3]	training's l2: 0.000936697	valid_1's l2: 0.000859232
[4]	training's l2: 0.00092125	valid_1's l2: 0.000848558
[5]	training's l2: 0.000907121	valid_1's l2: 0.000838282
[6]	training's l2: 0.000893804	valid_1's l2: 0.000830163
[7]	training's l2: 0.0008824	valid_1's l2: 0.000823747
[8]	training's l2: 0.000870556	valid_1's l2: 0.000818274
[9]	training's l2: 0.0008603	valid_1's l2: 0.000813
[10]	training's l2: 0.000849827	valid_1's l2: 0.000805853
[11]	training's l2: 0.000840764	valid_1's l2: 0.00080013
[12]	training's l2: 0.000831911	valid_1's l2: 0.000794217
[13]	training's l2: 0.000824761	valid_1's l2: 0.000791037
[14]	training's l2: 0.000817994	valid_1's l2: 0.000786843
[15]	training's l2: 0.000811202	valid_1's l2: 0.00078269
[16]	training's l2: 0.000804516	valid_1's l2: 0.000778739
[17]	training's l2: 0.000799444	valid_1's l2: 0.000776302
[18]	training's l2: 0.000794968	valid_1's l2: 0.000773174
[19]	training's l2: 0.000790004	valid_1's l2: 0.000770603
[20]	training's l2: 0.000785128	valid_1's l2: 0.000768277
[21]	training's l2: 0.000780344	valid_1's l2: 0.000765996
[22]	training's l2: 0.000776375	valid_1's l2: 0.000764893
[23]	training's l2: 0.000771931	valid_1's l2: 0.000762701
[24]	training's l2: 0.000767852	valid_1's l2: 0.000760528
[25]	training's l2: 0.000763555	valid_1's l2: 0.00075791
[26]	training's l2: 0.000760291	valid_1's l2: 0.000757044
[27]	training's l2: 0.000755409	valid_1's l2: 0.000754052
[28]	training's l2: 0.000751038	valid_1's l2: 0.000751203
[29]	training's l2: 0.000746464	valid_1's l2: 0.000748189
[30]	training's l2: 0.000742891	valid_1's l2: 0.000747355
[31]	training's l2: 0.000739187	valid_1's l2: 0.000745738
[32]	training's l2: 0.00073538	valid_1's l2: 0.000742966
[33]	training's l2: 0.000732073	valid_1's l2: 0.000740637
[34]	training's l2: 0.000728369	valid_1's l2: 0.000739185
[35]	training's l2: 0.000725408	valid_1's l2: 0.000737822
[36]	training's l2: 0.000722551	valid_1's l2: 0.000736683
[37]	training's l2: 0.000718659	valid_1's l2: 0.000735549
[38]	training's l2: 0.000715666	valid_1's l2: 0.000734909
[39]	training's l2: 0.000712653	valid_1's l2: 0.000733505
[40]	training's l2: 0.000709744	valid_1's l2: 0.000732684
[41]	training's l2: 0.000707027	valid_1's l2: 0.000731573
[42]	training's l2: 0.000703742	valid_1's l2: 0.000730356
[43]	training's l2: 0.000701411	valid_1's l2: 0.000730083
[44]	training's l2: 0.00069931	valid_1's l2: 0.000729438
[45]	training's l2: 0.000696342	valid_1's l2: 0.000728844
[46]	training's l2: 0.000693928	valid_1's l2: 0.000727981
[47]	training's l2: 0.000691707	valid_1's l2: 0.00072734
[48]	training's l2: 0.000688373	valid_1's l2: 0.000724814
[49]	training's l2: 0.000686076	valid_1's l2: 0.000724159
[50]	training's l2: 0.000683867	valid_1's l2: 0.000723886
[51]	training's l2: 0.000681407	valid_1's l2: 0.000723295
[52]	training's l2: 0.000679372	valid_1's l2: 0.000723477
[53]	training's l2: 0.000677135	valid_1's l2: 0.000723464
[54]	training's l2: 0.000674829	valid_1's l2: 0.000723225
[55]	training's l2: 0.000672084	valid_1's l2: 0.000722909
[56]	training's l2: 0.000669553	valid_1's l2: 0.000722438
[57]	training's l2: 0.000667456	valid_1's l2: 0.000722627
[58]	training's l2: 0.000665582	valid_1's l2: 0.000722377
[59]	training's l2: 0.000663294	valid_1's l2: 0.000721282
[60]	training's l2: 0.000660999	valid_1's l2: 0.000721449
[61]	training's l2: 0.000658856	valid_1's l2: 0.000721224
[62]	training's l2: 0.000656894	valid_1's l2: 0.000720838
[63]	training's l2: 0.000654763	valid_1's l2: 0.000719816
[64]	training's l2: 0.000652668	valid_1's l2: 0.000719647
[65]	training's l2: 0.000650487	valid_1's l2: 0.000719359
[66]	training's l2: 0.000648433	valid_1's l2: 0.000719864
[67]	training's l2: 0.000646237	valid_1's l2: 0.000719767
[68]	training's l2: 0.000644494	valid_1's l2: 0.000719565
[69]	training's l2: 0.00064249	valid_1's l2: 0.000719376
[70]	training's l2: 0.000640452	valid_1's l2: 0.000718974
[71]	training's l2: 0.00063832	valid_1's l2: 0.000717813
[72]	training's l2: 0.000636626	valid_1's l2: 0.000717898
[73]	training's l2: 0.000634697	valid_1's l2: 0.000718265
[74]	training's l2: 0.0006325	valid_1's l2: 0.000718139
[75]	training's l2: 0.000629691	valid_1's l2: 0.000716872
[76]	training's l2: 0.000627648	valid_1's l2: 0.000716198
[77]	training's l2: 0.000625662	valid_1's l2: 0.00071648
[78]	training's l2: 0.000623869	valid_1's l2: 0.000716289
[79]	training's l2: 0.000622299	valid_1's l2: 0.000716453
[80]	training's l2: 0.000620861	valid_1's l2: 0.000716287
[81]	training's l2: 0.000619228	valid_1's l2: 0.000716346
[82]	training's l2: 0.000617605	valid_1's l2: 0.000716496
[83]	training's l2: 0.000616146	valid_1's l2: 0.000716433
[84]	training's l2: 0.000614716	valid_1's l2: 0.000716463
[85]	training's l2: 0.000612616	valid_1's l2: 0.000716113
[86]	training's l2: 0.000611021	valid_1's l2: 0.000716139
[87]	training's l2: 0.000609137	valid_1's l2: 0.000715508
[88]	training's l2: 0.000607661	valid_1's l2: 0.000715856
[89]	training's l2: 0.000606004	valid_1's l2: 0.000716104
[90]	training's l2: 0.000604491	valid_1's l2: 0.00071612
[91]	training's l2: 0.00060286	valid_1's l2: 0.000716491
[92]	training's l2: 0.000601656	valid_1's l2: 0.000716606
[93]	training's l2: 0.000600204	valid_1's l2: 0.000716755
[94]	training's l2: 0.000598363	valid_1's l2: 0.000716735
[95]	training's l2: 0.000596655	valid_1's l2: 0.000717122
[96]	training's l2: 0.000594833	valid_1's l2: 0.000716755
[97]	training's l2: 0.000593456	valid_1's l2: 0.00071673
[98]	training's l2: 0.000592061	valid_1's l2: 0.000716748
[99]	training's l2: 0.000590613	valid_1's l2: 0.000716488
[100]	training's l2: 0.000589269	valid_1's l2: 0.000715993
[101]	training's l2: 0.000587769	valid_1's l2: 0.000715531
[102]	training's l2: 0.000586078	valid_1's l2: 0.000715507
[103]	training's l2: 0.000584577	valid_1's l2: 0.000715895
[104]	training's l2: 0.000583297	valid_1's l2: 0.000716208
[105]	training's l2: 0.000581873	valid_1's l2: 0.000716025
[106]	training's l2: 0.000580641	valid_1's l2: 0.000715808
[107]	training's l2: 0.000579012	valid_1's l2: 0.00071579
[108]	training's l2: 0.000577517	valid_1's l2: 0.000715792
[109]	training's l2: 0.000575961	valid_1's l2: 0.000715658
[110]	training's l2: 0.000574106	valid_1's l2: 0.000715569
[111]	training's l2: 0.000572699	valid_1's l2: 0.0007155
[112]	training's l2: 0.000571236	valid_1's l2: 0.000715627
[113]	training's l2: 0.000570072	valid_1's l2: 0.000715712
[114]	training's l2: 0.000568302	valid_1's l2: 0.000715659
[115]	training's l2: 0.00056683	valid_1's l2: 0.00071576
[116]	training's l2: 0.000565445	valid_1's l2: 0.000715897
[117]	training's l2: 0.00056395	valid_1's l2: 0.00071607
[118]	training's l2: 0.00056271	valid_1's l2: 0.000715905
[119]	training's l2: 0.000561578	valid_1's l2: 0.000715693
[120]	training's l2: 0.000560116	valid_1's l2: 0.00071542
[121]	training's l2: 0.000558767	valid_1's l2: 0.000715339
[122]	training's l2: 0.000557319	valid_1's l2: 0.00071521
[123]	training's l2: 0.000555905	valid_1's l2: 0.000715192
[124]	training's l2: 0.000554706	valid_1's l2: 0.00071545
[125]	training's l2: 0.000553174	valid_1's l2: 0.000714995
[126]	training's l2: 0.000551956	valid_1's l2: 0.000714731
[127]	training's l2: 0.000550761	valid_1's l2: 0.000714963
[128]	training's l2: 0.000549504	valid_1's l2: 0.000715091
[129]	training's l2: 0.000548232	valid_1's l2: 0.00071499
[130]	training's l2: 0.000547052	valid_1's l2: 0.000714977
[131]	training's l2: 0.000545764	valid_1's l2: 0.000714888
[132]	training's l2: 0.000544613	valid_1's l2: 0.000715055
[133]	training's l2: 0.000543539	valid_1's l2: 0.00071529
[134]	training's l2: 0.000542263	valid_1's l2: 0.00071529
[135]	training's l2: 0.000541106	valid_1's l2: 0.000715302
[136]	training's l2: 0.000540014	valid_1's l2: 0.000715269
[137]	training's l2: 0.000538998	valid_1's l2: 0.00071532
[138]	training's l2: 0.000537783	valid_1's l2: 0.000715394
[139]	training's l2: 0.000536631	valid_1's l2: 0.00071545
[140]	training's l2: 0.000535596	valid_1's l2: 0.00071558
[141]	training's l2: 0.000534311	valid_1's l2: 0.000715658
[142]	training's l2: 0.00053309	valid_1's l2: 0.000715565
[143]	training's l2: 0.000531774	valid_1's l2: 0.000715863
[144]	training's l2: 0.000530431	valid_1's l2: 0.000715632
[145]	training's l2: 0.000529209	valid_1's l2: 0.000715797
[146]	training's l2: 0.000528028	valid_1's l2: 0.000715396
[147]	training's l2: 0.000526732	valid_1's l2: 0.000715332
[148]	training's l2: 0.000525682	valid_1's l2: 0.000715348
[149]	training's l2: 0.000524431	valid_1's l2: 0.000715139
[150]	training's l2: 0.000523373	valid_1's l2: 0.000715166
[151]	training's l2: 0.000522103	valid_1's l2: 0.000714943
[152]	training's l2: 0.000520784	valid_1's l2: 0.000714964
[153]	training's l2: 0.000519666	valid_1's l2: 0.000714815
[154]	training's l2: 0.000518386	valid_1's l2: 0.000714834
[155]	training's l2: 0.000517221	valid_1's l2: 0.000715032
[156]	training's l2: 0.000516065	valid_1's l2: 0.000715052
Early stopping, best iteration is:
[126]	training's l2: 0.000551956	valid_1's l2: 0.000714731
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200406 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230921	valid_1's l2: 0.00233286
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00227329	valid_1's l2: 0.00230357
[3]	training's l2: 0.00224013	valid_1's l2: 0.00227515
[4]	training's l2: 0.00221135	valid_1's l2: 0.00225309
[5]	training's l2: 0.002186	valid_1's l2: 0.00223183
[6]	training's l2: 0.00216322	valid_1's l2: 0.00221623
[7]	training's l2: 0.00214129	valid_1's l2: 0.0021994
[8]	training's l2: 0.00212238	valid_1's l2: 0.00218164
[9]	training's l2: 0.00210351	valid_1's l2: 0.00216656
[10]	training's l2: 0.00208644	valid_1's l2: 0.00215402
[11]	training's l2: 0.00207237	valid_1's l2: 0.00214492
[12]	training's l2: 0.00205811	valid_1's l2: 0.00213545
[13]	training's l2: 0.00204675	valid_1's l2: 0.00212611
[14]	training's l2: 0.00203619	valid_1's l2: 0.00211686
[15]	training's l2: 0.00202517	valid_1's l2: 0.0021108
[16]	training's l2: 0.00201536	valid_1's l2: 0.0021029
[17]	training's l2: 0.00200722	valid_1's l2: 0.00209648
[18]	training's l2: 0.00199838	valid_1's l2: 0.00209328
[19]	training's l2: 0.00199019	valid_1's l2: 0.00208931
[20]	training's l2: 0.00198306	valid_1's l2: 0.00208444
[21]	training's l2: 0.00197405	valid_1's l2: 0.00207951
[22]	training's l2: 0.00196684	valid_1's l2: 0.0020755
[23]	training's l2: 0.00195998	valid_1's l2: 0.00207225
[24]	training's l2: 0.00195418	valid_1's l2: 0.00206536
[25]	training's l2: 0.00194821	valid_1's l2: 0.00205966
[26]	training's l2: 0.0019425	valid_1's l2: 0.00205594
[27]	training's l2: 0.00193629	valid_1's l2: 0.00205266
[28]	training's l2: 0.00193161	valid_1's l2: 0.00204984
[29]	training's l2: 0.00192615	valid_1's l2: 0.00204636
[30]	training's l2: 0.00192057	valid_1's l2: 0.00204281
[31]	training's l2: 0.00191494	valid_1's l2: 0.00203926
[32]	training's l2: 0.0019106	valid_1's l2: 0.00203614
[33]	training's l2: 0.00190586	valid_1's l2: 0.00203368
[34]	training's l2: 0.00190138	valid_1's l2: 0.00202968
[35]	training's l2: 0.00189596	valid_1's l2: 0.00202673
[36]	training's l2: 0.00189112	valid_1's l2: 0.00202278
[37]	training's l2: 0.00188708	valid_1's l2: 0.00202123
[38]	training's l2: 0.0018828	valid_1's l2: 0.00201913
[39]	training's l2: 0.00187708	valid_1's l2: 0.00201538
[40]	training's l2: 0.0018732	valid_1's l2: 0.00201139
[41]	training's l2: 0.00186983	valid_1's l2: 0.00200859
[42]	training's l2: 0.00186471	valid_1's l2: 0.00200634
[43]	training's l2: 0.00186086	valid_1's l2: 0.00200308
[44]	training's l2: 0.00185734	valid_1's l2: 0.00200086
[45]	training's l2: 0.0018527	valid_1's l2: 0.00199858
[46]	training's l2: 0.00184904	valid_1's l2: 0.00199563
[47]	training's l2: 0.00184616	valid_1's l2: 0.00199496
[48]	training's l2: 0.00184293	valid_1's l2: 0.00199368
[49]	training's l2: 0.0018401	valid_1's l2: 0.00199322
[50]	training's l2: 0.00183676	valid_1's l2: 0.00199119
[51]	training's l2: 0.00183396	valid_1's l2: 0.00199007
[52]	training's l2: 0.00182983	valid_1's l2: 0.00198734
[53]	training's l2: 0.00182644	valid_1's l2: 0.00198604
[54]	training's l2: 0.00182332	valid_1's l2: 0.00198476
[55]	training's l2: 0.0018202	valid_1's l2: 0.00198274
[56]	training's l2: 0.00181688	valid_1's l2: 0.00198108
[57]	training's l2: 0.00181423	valid_1's l2: 0.00198016
[58]	training's l2: 0.00181159	valid_1's l2: 0.00197894
[59]	training's l2: 0.0018082	valid_1's l2: 0.00197702
[60]	training's l2: 0.00180504	valid_1's l2: 0.00197514
[61]	training's l2: 0.00180259	valid_1's l2: 0.00197456
[62]	training's l2: 0.00179999	valid_1's l2: 0.00197313
[63]	training's l2: 0.00179668	valid_1's l2: 0.00197281
[64]	training's l2: 0.00179407	valid_1's l2: 0.001972
[65]	training's l2: 0.00179141	valid_1's l2: 0.00197076
[66]	training's l2: 0.00178851	valid_1's l2: 0.00196923
[67]	training's l2: 0.00178576	valid_1's l2: 0.00196723
[68]	training's l2: 0.00178322	valid_1's l2: 0.00196599
[69]	training's l2: 0.00178106	valid_1's l2: 0.00196554
[70]	training's l2: 0.00177877	valid_1's l2: 0.00196444
[71]	training's l2: 0.00177624	valid_1's l2: 0.00196332
[72]	training's l2: 0.00177374	valid_1's l2: 0.0019621
[73]	training's l2: 0.00177137	valid_1's l2: 0.00196126
[74]	training's l2: 0.00176919	valid_1's l2: 0.00196044
[75]	training's l2: 0.00176633	valid_1's l2: 0.00195921
[76]	training's l2: 0.00176395	valid_1's l2: 0.00195869
[77]	training's l2: 0.00176182	valid_1's l2: 0.00195817
[78]	training's l2: 0.00175948	valid_1's l2: 0.00195756
[79]	training's l2: 0.00175709	valid_1's l2: 0.00195655
[80]	training's l2: 0.00175436	valid_1's l2: 0.00195542
[81]	training's l2: 0.00175232	valid_1's l2: 0.00195481
[82]	training's l2: 0.00175045	valid_1's l2: 0.00195393
[83]	training's l2: 0.00174848	valid_1's l2: 0.00195398
[84]	training's l2: 0.00174599	valid_1's l2: 0.00195346
[85]	training's l2: 0.00174403	valid_1's l2: 0.00195276
[86]	training's l2: 0.00174203	valid_1's l2: 0.00195281
[87]	training's l2: 0.00173912	valid_1's l2: 0.00195133
[88]	training's l2: 0.00173722	valid_1's l2: 0.00195055
[89]	training's l2: 0.00173525	valid_1's l2: 0.00194998
[90]	training's l2: 0.00173308	valid_1's l2: 0.00194946
[91]	training's l2: 0.00173082	valid_1's l2: 0.00194876
[92]	training's l2: 0.00172878	valid_1's l2: 0.00194782
[93]	training's l2: 0.00172698	valid_1's l2: 0.00194731
[94]	training's l2: 0.00172516	valid_1's l2: 0.00194741
[95]	training's l2: 0.00172256	valid_1's l2: 0.0019457
[96]	training's l2: 0.00172079	valid_1's l2: 0.00194599
[97]	training's l2: 0.0017189	valid_1's l2: 0.00194534
[98]	training's l2: 0.00171684	valid_1's l2: 0.00194545
[99]	training's l2: 0.00171482	valid_1's l2: 0.00194487
[100]	training's l2: 0.00171239	valid_1's l2: 0.00194369
[101]	training's l2: 0.00171024	valid_1's l2: 0.00194285
[102]	training's l2: 0.00170817	valid_1's l2: 0.00194201
[103]	training's l2: 0.00170625	valid_1's l2: 0.00194118
[104]	training's l2: 0.00170452	valid_1's l2: 0.00194109
[105]	training's l2: 0.00170276	valid_1's l2: 0.00194091
[106]	training's l2: 0.00170082	valid_1's l2: 0.00194107
[107]	training's l2: 0.00169896	valid_1's l2: 0.00194113
[108]	training's l2: 0.00169705	valid_1's l2: 0.00194009
[109]	training's l2: 0.00169545	valid_1's l2: 0.00193917
[110]	training's l2: 0.00169361	valid_1's l2: 0.0019385
[111]	training's l2: 0.00169187	valid_1's l2: 0.00193853
[112]	training's l2: 0.00169001	valid_1's l2: 0.00193769
[113]	training's l2: 0.0016882	valid_1's l2: 0.00193747
[114]	training's l2: 0.00168647	valid_1's l2: 0.00193684
[115]	training's l2: 0.00168502	valid_1's l2: 0.00193667
[116]	training's l2: 0.00168312	valid_1's l2: 0.00193631
[117]	training's l2: 0.00168132	valid_1's l2: 0.00193605
[118]	training's l2: 0.00167949	valid_1's l2: 0.00193622
[119]	training's l2: 0.00167757	valid_1's l2: 0.00193571
[120]	training's l2: 0.0016759	valid_1's l2: 0.00193511
[121]	training's l2: 0.00167419	valid_1's l2: 0.0019352
[122]	training's l2: 0.00167277	valid_1's l2: 0.00193467
[123]	training's l2: 0.00167103	valid_1's l2: 0.00193437
[124]	training's l2: 0.00166962	valid_1's l2: 0.00193433
[125]	training's l2: 0.00166767	valid_1's l2: 0.00193375
[126]	training's l2: 0.0016659	valid_1's l2: 0.00193394
[127]	training's l2: 0.00166427	valid_1's l2: 0.00193366
[128]	training's l2: 0.00166248	valid_1's l2: 0.00193351
[129]	training's l2: 0.00166087	valid_1's l2: 0.00193308
[130]	training's l2: 0.00165935	valid_1's l2: 0.00193331
[131]	training's l2: 0.0016576	valid_1's l2: 0.00193295
[132]	training's l2: 0.00165595	valid_1's l2: 0.00193236
[133]	training's l2: 0.00165451	valid_1's l2: 0.00193261
[134]	training's l2: 0.00165299	valid_1's l2: 0.00193285
[135]	training's l2: 0.00165092	valid_1's l2: 0.00193197
[136]	training's l2: 0.00164929	valid_1's l2: 0.00193216
[137]	training's l2: 0.00164795	valid_1's l2: 0.00193241
[138]	training's l2: 0.00164645	valid_1's l2: 0.00193243
[139]	training's l2: 0.00164494	valid_1's l2: 0.00193235
[140]	training's l2: 0.00164316	valid_1's l2: 0.00193173
[141]	training's l2: 0.00164153	valid_1's l2: 0.00193153
[142]	training's l2: 0.00164003	valid_1's l2: 0.00193143
[143]	training's l2: 0.00163849	valid_1's l2: 0.00193062
[144]	training's l2: 0.00163727	valid_1's l2: 0.00193062
[145]	training's l2: 0.0016359	valid_1's l2: 0.00193102
[146]	training's l2: 0.00163437	valid_1's l2: 0.00193095
[147]	training's l2: 0.00163298	valid_1's l2: 0.00193092
[148]	training's l2: 0.0016316	valid_1's l2: 0.00193075
[149]	training's l2: 0.00163003	valid_1's l2: 0.00193059
[150]	training's l2: 0.00162872	valid_1's l2: 0.00193046
[151]	training's l2: 0.00162735	valid_1's l2: 0.00193031
[152]	training's l2: 0.00162622	valid_1's l2: 0.00193022
[153]	training's l2: 0.00162488	valid_1's l2: 0.00193094
[154]	training's l2: 0.00162361	valid_1's l2: 0.00193072
[155]	training's l2: 0.00162242	valid_1's l2: 0.00193123
[156]	training's l2: 0.00162108	valid_1's l2: 0.00193144
[157]	training's l2: 0.00161967	valid_1's l2: 0.00193115
[158]	training's l2: 0.00161814	valid_1's l2: 0.00193086
[159]	training's l2: 0.00161673	valid_1's l2: 0.00193057
[160]	training's l2: 0.00161543	valid_1's l2: 0.00193027
[161]	training's l2: 0.00161391	valid_1's l2: 0.00192971
[162]	training's l2: 0.00161237	valid_1's l2: 0.0019289
[163]	training's l2: 0.00161093	valid_1's l2: 0.00192876
[164]	training's l2: 0.00160976	valid_1's l2: 0.00192933
[165]	training's l2: 0.00160846	valid_1's l2: 0.00192928
[166]	training's l2: 0.00160722	valid_1's l2: 0.00192922
[167]	training's l2: 0.00160572	valid_1's l2: 0.00192908
[168]	training's l2: 0.00160423	valid_1's l2: 0.00192874
[169]	training's l2: 0.00160281	valid_1's l2: 0.0019288
[170]	training's l2: 0.00160134	valid_1's l2: 0.00192856
[171]	training's l2: 0.00160009	valid_1's l2: 0.00192825
[172]	training's l2: 0.00159848	valid_1's l2: 0.00192811
[173]	training's l2: 0.00159697	valid_1's l2: 0.00192787
[174]	training's l2: 0.00159585	valid_1's l2: 0.00192754
[175]	training's l2: 0.00159471	valid_1's l2: 0.00192745
[176]	training's l2: 0.00159363	valid_1's l2: 0.00192703
[177]	training's l2: 0.00159219	valid_1's l2: 0.00192688
[178]	training's l2: 0.00159072	valid_1's l2: 0.00192668
[179]	training's l2: 0.00158946	valid_1's l2: 0.0019268
[180]	training's l2: 0.00158818	valid_1's l2: 0.00192671
[181]	training's l2: 0.00158682	valid_1's l2: 0.00192623
[182]	training's l2: 0.00158545	valid_1's l2: 0.00192583
[183]	training's l2: 0.0015842	valid_1's l2: 0.00192543
[184]	training's l2: 0.00158296	valid_1's l2: 0.00192536
[185]	training's l2: 0.00158172	valid_1's l2: 0.0019254
[186]	training's l2: 0.00158069	valid_1's l2: 0.00192552
[187]	training's l2: 0.00157908	valid_1's l2: 0.00192597
[188]	training's l2: 0.00157782	valid_1's l2: 0.00192633
[189]	training's l2: 0.00157671	valid_1's l2: 0.00192595
[190]	training's l2: 0.00157545	valid_1's l2: 0.00192588
[191]	training's l2: 0.00157409	valid_1's l2: 0.00192596
[192]	training's l2: 0.00157289	valid_1's l2: 0.00192596
[193]	training's l2: 0.00157147	valid_1's l2: 0.00192595
[194]	training's l2: 0.00157039	valid_1's l2: 0.00192593
[195]	training's l2: 0.00156894	valid_1's l2: 0.00192595
Did not meet early stopping. Best iteration is:
[195]	training's l2: 0.00156894	valid_1's l2: 0.00192595
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201970 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230477	valid_1's l2: 0.00232928
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00226299	valid_1's l2: 0.00229531
[3]	training's l2: 0.00222618	valid_1's l2: 0.00226675
[4]	training's l2: 0.00219177	valid_1's l2: 0.00224143
[5]	training's l2: 0.00216125	valid_1's l2: 0.00221827
[6]	training's l2: 0.00213345	valid_1's l2: 0.00219991
[7]	training's l2: 0.00210702	valid_1's l2: 0.00218266
[8]	training's l2: 0.00208414	valid_1's l2: 0.0021677
[9]	training's l2: 0.00206201	valid_1's l2: 0.00215282
[10]	training's l2: 0.00204094	valid_1's l2: 0.00214198
[11]	training's l2: 0.00202241	valid_1's l2: 0.0021312
[12]	training's l2: 0.00200475	valid_1's l2: 0.00212073
[13]	training's l2: 0.00198938	valid_1's l2: 0.00210855
[14]	training's l2: 0.00197353	valid_1's l2: 0.00209921
[15]	training's l2: 0.00195925	valid_1's l2: 0.0020903
[16]	training's l2: 0.00194533	valid_1's l2: 0.00208334
[17]	training's l2: 0.00193258	valid_1's l2: 0.00207877
[18]	training's l2: 0.00192121	valid_1's l2: 0.00207123
[19]	training's l2: 0.00190955	valid_1's l2: 0.0020664
[20]	training's l2: 0.00189942	valid_1's l2: 0.00206161
[21]	training's l2: 0.0018896	valid_1's l2: 0.00205699
[22]	training's l2: 0.00187928	valid_1's l2: 0.00205191
[23]	training's l2: 0.00187008	valid_1's l2: 0.00204606
[24]	training's l2: 0.00186053	valid_1's l2: 0.00204179
[25]	training's l2: 0.00185008	valid_1's l2: 0.00203555
[26]	training's l2: 0.00184205	valid_1's l2: 0.00203202
[27]	training's l2: 0.00183402	valid_1's l2: 0.00202785
[28]	training's l2: 0.0018253	valid_1's l2: 0.00202449
[29]	training's l2: 0.00181849	valid_1's l2: 0.00202236
[30]	training's l2: 0.00181137	valid_1's l2: 0.00202012
[31]	training's l2: 0.00180334	valid_1's l2: 0.0020179
[32]	training's l2: 0.00179539	valid_1's l2: 0.0020153
[33]	training's l2: 0.00178853	valid_1's l2: 0.00201219
[34]	training's l2: 0.00178149	valid_1's l2: 0.00201034
[35]	training's l2: 0.00177497	valid_1's l2: 0.00200854
[36]	training's l2: 0.00176874	valid_1's l2: 0.00200619
[37]	training's l2: 0.00176243	valid_1's l2: 0.00200336
[38]	training's l2: 0.00175624	valid_1's l2: 0.00200179
[39]	training's l2: 0.0017508	valid_1's l2: 0.00200045
[40]	training's l2: 0.00174433	valid_1's l2: 0.00199833
[41]	training's l2: 0.00173851	valid_1's l2: 0.00199583
[42]	training's l2: 0.00173308	valid_1's l2: 0.00199302
[43]	training's l2: 0.0017268	valid_1's l2: 0.00199148
[44]	training's l2: 0.00172174	valid_1's l2: 0.00199064
[45]	training's l2: 0.00171507	valid_1's l2: 0.0019882
[46]	training's l2: 0.00170988	valid_1's l2: 0.0019859
[47]	training's l2: 0.00170353	valid_1's l2: 0.00198318
[48]	training's l2: 0.00169818	valid_1's l2: 0.00198143
[49]	training's l2: 0.00169285	valid_1's l2: 0.00198008
[50]	training's l2: 0.00168779	valid_1's l2: 0.00197697
[51]	training's l2: 0.00168165	valid_1's l2: 0.00197412
[52]	training's l2: 0.00167672	valid_1's l2: 0.00197291
[53]	training's l2: 0.00167131	valid_1's l2: 0.00197181
[54]	training's l2: 0.00166635	valid_1's l2: 0.0019703
[55]	training's l2: 0.00166108	valid_1's l2: 0.00196837
[56]	training's l2: 0.00165649	valid_1's l2: 0.00196638
[57]	training's l2: 0.0016521	valid_1's l2: 0.00196491
[58]	training's l2: 0.0016475	valid_1's l2: 0.00196363
[59]	training's l2: 0.00164282	valid_1's l2: 0.00196288
[60]	training's l2: 0.00163786	valid_1's l2: 0.00196145
[61]	training's l2: 0.00163318	valid_1's l2: 0.00196134
[62]	training's l2: 0.00162841	valid_1's l2: 0.00196107
[63]	training's l2: 0.00162421	valid_1's l2: 0.00196064
[64]	training's l2: 0.00161941	valid_1's l2: 0.00195874
[65]	training's l2: 0.00161487	valid_1's l2: 0.0019578
[66]	training's l2: 0.00161058	valid_1's l2: 0.00195747
[67]	training's l2: 0.00160622	valid_1's l2: 0.00195697
[68]	training's l2: 0.001602	valid_1's l2: 0.00195657
[69]	training's l2: 0.00159785	valid_1's l2: 0.00195627
[70]	training's l2: 0.00159367	valid_1's l2: 0.00195572
[71]	training's l2: 0.00158952	valid_1's l2: 0.0019555
[72]	training's l2: 0.00158549	valid_1's l2: 0.00195558
[73]	training's l2: 0.00158133	valid_1's l2: 0.00195551
[74]	training's l2: 0.00157716	valid_1's l2: 0.00195562
[75]	training's l2: 0.0015728	valid_1's l2: 0.00195467
[76]	training's l2: 0.00156866	valid_1's l2: 0.00195431
[77]	training's l2: 0.00156495	valid_1's l2: 0.00195329
[78]	training's l2: 0.00156115	valid_1's l2: 0.00195301
[79]	training's l2: 0.00155735	valid_1's l2: 0.00195187
[80]	training's l2: 0.0015528	valid_1's l2: 0.00195048
[81]	training's l2: 0.00154852	valid_1's l2: 0.00194936
[82]	training's l2: 0.00154474	valid_1's l2: 0.00194862
[83]	training's l2: 0.0015412	valid_1's l2: 0.00194842
[84]	training's l2: 0.00153749	valid_1's l2: 0.00194837
[85]	training's l2: 0.00153359	valid_1's l2: 0.00194783
[86]	training's l2: 0.00152969	valid_1's l2: 0.00194721
[87]	training's l2: 0.00152602	valid_1's l2: 0.00194683
[88]	training's l2: 0.00152237	valid_1's l2: 0.00194663
[89]	training's l2: 0.00151844	valid_1's l2: 0.00194656
[90]	training's l2: 0.00151491	valid_1's l2: 0.0019462
[91]	training's l2: 0.00151135	valid_1's l2: 0.00194598
[92]	training's l2: 0.0015073	valid_1's l2: 0.00194488
[93]	training's l2: 0.0015037	valid_1's l2: 0.00194432
[94]	training's l2: 0.00150034	valid_1's l2: 0.00194387
[95]	training's l2: 0.00149681	valid_1's l2: 0.00194322
[96]	training's l2: 0.0014932	valid_1's l2: 0.00194235
[97]	training's l2: 0.00148948	valid_1's l2: 0.00194229
[98]	training's l2: 0.00148592	valid_1's l2: 0.00194188
[99]	training's l2: 0.0014825	valid_1's l2: 0.00194144
[100]	training's l2: 0.00147922	valid_1's l2: 0.00194098
[101]	training's l2: 0.00147571	valid_1's l2: 0.00193983
[102]	training's l2: 0.00147231	valid_1's l2: 0.00193978
[103]	training's l2: 0.00146898	valid_1's l2: 0.00193999
[104]	training's l2: 0.00146564	valid_1's l2: 0.00193965
[105]	training's l2: 0.00146218	valid_1's l2: 0.00193902
[106]	training's l2: 0.00145878	valid_1's l2: 0.00193879
[107]	training's l2: 0.0014554	valid_1's l2: 0.0019392
[108]	training's l2: 0.00145195	valid_1's l2: 0.00193864
[109]	training's l2: 0.00144842	valid_1's l2: 0.00193756
[110]	training's l2: 0.00144502	valid_1's l2: 0.00193745
[111]	training's l2: 0.00144196	valid_1's l2: 0.0019371
[112]	training's l2: 0.00143892	valid_1's l2: 0.00193671
[113]	training's l2: 0.00143565	valid_1's l2: 0.00193666
[114]	training's l2: 0.00143256	valid_1's l2: 0.0019359
[115]	training's l2: 0.00142933	valid_1's l2: 0.00193583
[116]	training's l2: 0.00142563	valid_1's l2: 0.00193546
[117]	training's l2: 0.00142241	valid_1's l2: 0.00193526
[118]	training's l2: 0.00141931	valid_1's l2: 0.00193484
[119]	training's l2: 0.00141613	valid_1's l2: 0.00193474
[120]	training's l2: 0.00141303	valid_1's l2: 0.00193509
[121]	training's l2: 0.00140989	valid_1's l2: 0.00193471
[122]	training's l2: 0.00140672	valid_1's l2: 0.00193488
[123]	training's l2: 0.00140354	valid_1's l2: 0.00193515
[124]	training's l2: 0.00140048	valid_1's l2: 0.00193488
[125]	training's l2: 0.00139746	valid_1's l2: 0.00193486
[126]	training's l2: 0.00139428	valid_1's l2: 0.0019344
[127]	training's l2: 0.00139131	valid_1's l2: 0.00193483
[128]	training's l2: 0.00138845	valid_1's l2: 0.00193489
[129]	training's l2: 0.00138548	valid_1's l2: 0.00193445
[130]	training's l2: 0.00138249	valid_1's l2: 0.00193454
[131]	training's l2: 0.00137936	valid_1's l2: 0.0019345
[132]	training's l2: 0.00137633	valid_1's l2: 0.00193406
[133]	training's l2: 0.00137344	valid_1's l2: 0.0019338
[134]	training's l2: 0.00137024	valid_1's l2: 0.00193326
[135]	training's l2: 0.00136717	valid_1's l2: 0.00193261
[136]	training's l2: 0.00136428	valid_1's l2: 0.00193234
[137]	training's l2: 0.00136143	valid_1's l2: 0.00193189
[138]	training's l2: 0.00135846	valid_1's l2: 0.0019317
[139]	training's l2: 0.00135549	valid_1's l2: 0.00193141
[140]	training's l2: 0.00135249	valid_1's l2: 0.00193152
[141]	training's l2: 0.00134966	valid_1's l2: 0.00193189
[142]	training's l2: 0.0013469	valid_1's l2: 0.00193157
[143]	training's l2: 0.00134407	valid_1's l2: 0.00193112
[144]	training's l2: 0.00134113	valid_1's l2: 0.00193147
[145]	training's l2: 0.00133805	valid_1's l2: 0.00193161
[146]	training's l2: 0.00133497	valid_1's l2: 0.00193155
[147]	training's l2: 0.00133213	valid_1's l2: 0.00193126
[148]	training's l2: 0.00132942	valid_1's l2: 0.00193161
[149]	training's l2: 0.00132657	valid_1's l2: 0.00193148
[150]	training's l2: 0.00132392	valid_1's l2: 0.00193107
[151]	training's l2: 0.00132118	valid_1's l2: 0.00193111
[152]	training's l2: 0.00131849	valid_1's l2: 0.00193098
[153]	training's l2: 0.00131576	valid_1's l2: 0.00193135
[154]	training's l2: 0.00131289	valid_1's l2: 0.00193127
[155]	training's l2: 0.00130988	valid_1's l2: 0.001931
[156]	training's l2: 0.00130725	valid_1's l2: 0.00193041
[157]	training's l2: 0.00130461	valid_1's l2: 0.00193065
[158]	training's l2: 0.00130201	valid_1's l2: 0.00193027
[159]	training's l2: 0.00129941	valid_1's l2: 0.00193031
[160]	training's l2: 0.00129671	valid_1's l2: 0.00192981
[161]	training's l2: 0.0012941	valid_1's l2: 0.00192954
[162]	training's l2: 0.00129148	valid_1's l2: 0.00192991
[163]	training's l2: 0.00128877	valid_1's l2: 0.00192987
[164]	training's l2: 0.00128606	valid_1's l2: 0.00192976
[165]	training's l2: 0.00128357	valid_1's l2: 0.00192955
[166]	training's l2: 0.00128089	valid_1's l2: 0.00192975
Did not meet early stopping. Best iteration is:
[166]	training's l2: 0.00128089	valid_1's l2: 0.00192975
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.200535 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00229476	valid_1's l2: 0.002321
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00225017	valid_1's l2: 0.00228507
[3]	training's l2: 0.00220924	valid_1's l2: 0.00224945
[4]	training's l2: 0.00217572	valid_1's l2: 0.00222358
[5]	training's l2: 0.00214781	valid_1's l2: 0.0022015
[6]	training's l2: 0.00212139	valid_1's l2: 0.00218008
[7]	training's l2: 0.00210061	valid_1's l2: 0.00216685
[8]	training's l2: 0.00208157	valid_1's l2: 0.00214892
[9]	training's l2: 0.00206441	valid_1's l2: 0.00213852
[10]	training's l2: 0.00205055	valid_1's l2: 0.00212475
[11]	training's l2: 0.00203688	valid_1's l2: 0.00211622
[12]	training's l2: 0.00202396	valid_1's l2: 0.00210648
[13]	training's l2: 0.00201289	valid_1's l2: 0.00209811
[14]	training's l2: 0.0020017	valid_1's l2: 0.00209159
[15]	training's l2: 0.00199223	valid_1's l2: 0.00208598
[16]	training's l2: 0.00198391	valid_1's l2: 0.00208011
[17]	training's l2: 0.00197655	valid_1's l2: 0.00207455
[18]	training's l2: 0.00196781	valid_1's l2: 0.00207013
[19]	training's l2: 0.00196106	valid_1's l2: 0.00206612
[20]	training's l2: 0.00195309	valid_1's l2: 0.00205702
[21]	training's l2: 0.00194612	valid_1's l2: 0.0020532
[22]	training's l2: 0.00193977	valid_1's l2: 0.00204861
[23]	training's l2: 0.00193394	valid_1's l2: 0.0020451
[24]	training's l2: 0.0019285	valid_1's l2: 0.00204139
[25]	training's l2: 0.00192359	valid_1's l2: 0.00203392
[26]	training's l2: 0.00191824	valid_1's l2: 0.00202891
[27]	training's l2: 0.00190986	valid_1's l2: 0.00202361
[28]	training's l2: 0.00190538	valid_1's l2: 0.00201965
[29]	training's l2: 0.00190022	valid_1's l2: 0.00201539
[30]	training's l2: 0.00189447	valid_1's l2: 0.00201359
[31]	training's l2: 0.0018907	valid_1's l2: 0.00201158
[32]	training's l2: 0.00188654	valid_1's l2: 0.00200772
[33]	training's l2: 0.00188029	valid_1's l2: 0.00200365
[34]	training's l2: 0.00187617	valid_1's l2: 0.00200297
[35]	training's l2: 0.00187158	valid_1's l2: 0.00200018
[36]	training's l2: 0.0018674	valid_1's l2: 0.00199639
[37]	training's l2: 0.00186348	valid_1's l2: 0.00199417
[38]	training's l2: 0.00185743	valid_1's l2: 0.00198992
[39]	training's l2: 0.00185403	valid_1's l2: 0.00198741
[40]	training's l2: 0.00185076	valid_1's l2: 0.0019855
[41]	training's l2: 0.00184729	valid_1's l2: 0.00198374
[42]	training's l2: 0.00184363	valid_1's l2: 0.00198212
[43]	training's l2: 0.00183928	valid_1's l2: 0.00197863
[44]	training's l2: 0.00183573	valid_1's l2: 0.00197658
[45]	training's l2: 0.00183284	valid_1's l2: 0.00197485
[46]	training's l2: 0.00183027	valid_1's l2: 0.00197333
[47]	training's l2: 0.00182688	valid_1's l2: 0.00197341
[48]	training's l2: 0.00182397	valid_1's l2: 0.00197105
[49]	training's l2: 0.00182123	valid_1's l2: 0.00197002
[50]	training's l2: 0.00181844	valid_1's l2: 0.00196886
[51]	training's l2: 0.00181568	valid_1's l2: 0.00196826
[52]	training's l2: 0.00181274	valid_1's l2: 0.00196739
[53]	training's l2: 0.00180993	valid_1's l2: 0.00196654
[54]	training's l2: 0.00180491	valid_1's l2: 0.00196365
[55]	training's l2: 0.00180217	valid_1's l2: 0.00196286
[56]	training's l2: 0.00180003	valid_1's l2: 0.00196231
[57]	training's l2: 0.00179739	valid_1's l2: 0.00196141
[58]	training's l2: 0.00179466	valid_1's l2: 0.00195978
[59]	training's l2: 0.00179103	valid_1's l2: 0.00195744
[60]	training's l2: 0.00178866	valid_1's l2: 0.00195736
[61]	training's l2: 0.00178494	valid_1's l2: 0.00195559
[62]	training's l2: 0.00178246	valid_1's l2: 0.00195505
[63]	training's l2: 0.00178007	valid_1's l2: 0.00195539
[64]	training's l2: 0.00177766	valid_1's l2: 0.0019543
[65]	training's l2: 0.00177576	valid_1's l2: 0.00195265
[66]	training's l2: 0.00177309	valid_1's l2: 0.00195113
[67]	training's l2: 0.00177111	valid_1's l2: 0.00194947
[68]	training's l2: 0.00176916	valid_1's l2: 0.00194839
[69]	training's l2: 0.00176677	valid_1's l2: 0.00194829
[70]	training's l2: 0.00176361	valid_1's l2: 0.00194627
[71]	training's l2: 0.00176166	valid_1's l2: 0.00194571
[72]	training's l2: 0.0017592	valid_1's l2: 0.00194487
[73]	training's l2: 0.00175715	valid_1's l2: 0.00194451
[74]	training's l2: 0.0017552	valid_1's l2: 0.0019438
[75]	training's l2: 0.00175324	valid_1's l2: 0.00194311
[76]	training's l2: 0.00175108	valid_1's l2: 0.00194307
[77]	training's l2: 0.00174875	valid_1's l2: 0.00194284
[78]	training's l2: 0.0017465	valid_1's l2: 0.00194311
[79]	training's l2: 0.00174403	valid_1's l2: 0.00194128
[80]	training's l2: 0.00174211	valid_1's l2: 0.00194129
[81]	training's l2: 0.00174013	valid_1's l2: 0.00194095
[82]	training's l2: 0.00173814	valid_1's l2: 0.00194072
[83]	training's l2: 0.00173623	valid_1's l2: 0.00194069
[84]	training's l2: 0.00173422	valid_1's l2: 0.0019408
[85]	training's l2: 0.00173203	valid_1's l2: 0.00194086
[86]	training's l2: 0.00173027	valid_1's l2: 0.00194071
[87]	training's l2: 0.00172871	valid_1's l2: 0.00194096
[88]	training's l2: 0.00172629	valid_1's l2: 0.00194032
[89]	training's l2: 0.00172436	valid_1's l2: 0.00193965
[90]	training's l2: 0.0017225	valid_1's l2: 0.0019382
[91]	training's l2: 0.0017207	valid_1's l2: 0.00193815
[92]	training's l2: 0.00171904	valid_1's l2: 0.00193719
[93]	training's l2: 0.00171727	valid_1's l2: 0.00193711
[94]	training's l2: 0.00171571	valid_1's l2: 0.00193708
[95]	training's l2: 0.00171416	valid_1's l2: 0.00193663
[96]	training's l2: 0.00171255	valid_1's l2: 0.00193643
[97]	training's l2: 0.00171086	valid_1's l2: 0.00193678
[98]	training's l2: 0.00170904	valid_1's l2: 0.00193594
[99]	training's l2: 0.00170726	valid_1's l2: 0.00193541
[100]	training's l2: 0.00170512	valid_1's l2: 0.00193629
[101]	training's l2: 0.00170337	valid_1's l2: 0.00193568
[102]	training's l2: 0.00170156	valid_1's l2: 0.0019354
[103]	training's l2: 0.00170014	valid_1's l2: 0.00193542
[104]	training's l2: 0.00169868	valid_1's l2: 0.00193476
[105]	training's l2: 0.0016969	valid_1's l2: 0.00193446
[106]	training's l2: 0.00169535	valid_1's l2: 0.00193471
[107]	training's l2: 0.00169374	valid_1's l2: 0.00193514
[108]	training's l2: 0.00169195	valid_1's l2: 0.00193523
[109]	training's l2: 0.00169007	valid_1's l2: 0.00193488
[110]	training's l2: 0.00168861	valid_1's l2: 0.00193488
[111]	training's l2: 0.00168663	valid_1's l2: 0.00193359
[112]	training's l2: 0.00168479	valid_1's l2: 0.00193321
[113]	training's l2: 0.00168308	valid_1's l2: 0.00193252
[114]	training's l2: 0.00168162	valid_1's l2: 0.00193293
[115]	training's l2: 0.00168001	valid_1's l2: 0.00193221
[116]	training's l2: 0.00167828	valid_1's l2: 0.00193268
[117]	training's l2: 0.00167679	valid_1's l2: 0.00193267
[118]	training's l2: 0.00167505	valid_1's l2: 0.00193271
[119]	training's l2: 0.00167352	valid_1's l2: 0.00193214
[120]	training's l2: 0.00167198	valid_1's l2: 0.00193205
[121]	training's l2: 0.00167057	valid_1's l2: 0.00193202
[122]	training's l2: 0.00166891	valid_1's l2: 0.00193184
[123]	training's l2: 0.00166731	valid_1's l2: 0.00193189
[124]	training's l2: 0.00166532	valid_1's l2: 0.00193115
[125]	training's l2: 0.00166346	valid_1's l2: 0.00193058
[126]	training's l2: 0.00166205	valid_1's l2: 0.00193065
[127]	training's l2: 0.0016606	valid_1's l2: 0.00193016
[128]	training's l2: 0.00165903	valid_1's l2: 0.00192999
[129]	training's l2: 0.00165752	valid_1's l2: 0.00192969
[130]	training's l2: 0.00165613	valid_1's l2: 0.00192969
[131]	training's l2: 0.00165456	valid_1's l2: 0.00192927
[132]	training's l2: 0.00165314	valid_1's l2: 0.0019296
[133]	training's l2: 0.00165168	valid_1's l2: 0.00192952
[134]	training's l2: 0.00165011	valid_1's l2: 0.00192918
[135]	training's l2: 0.0016486	valid_1's l2: 0.0019288
[136]	training's l2: 0.00164726	valid_1's l2: 0.00192887
[137]	training's l2: 0.00164591	valid_1's l2: 0.00192864
[138]	training's l2: 0.00164448	valid_1's l2: 0.00192843
[139]	training's l2: 0.00164314	valid_1's l2: 0.00192845
[140]	training's l2: 0.00164168	valid_1's l2: 0.00192849
[141]	training's l2: 0.00164059	valid_1's l2: 0.00192909
[142]	training's l2: 0.0016392	valid_1's l2: 0.00192892
[143]	training's l2: 0.00163776	valid_1's l2: 0.00192882
[144]	training's l2: 0.00163619	valid_1's l2: 0.00192843
[145]	training's l2: 0.00163484	valid_1's l2: 0.0019282
[146]	training's l2: 0.00163352	valid_1's l2: 0.00192818
[147]	training's l2: 0.0016322	valid_1's l2: 0.0019281
[148]	training's l2: 0.00163056	valid_1's l2: 0.00192781
[149]	training's l2: 0.00162923	valid_1's l2: 0.00192769
[150]	training's l2: 0.00162794	valid_1's l2: 0.00192774
[151]	training's l2: 0.0016268	valid_1's l2: 0.00192771
[152]	training's l2: 0.00162561	valid_1's l2: 0.00192761
[153]	training's l2: 0.00162391	valid_1's l2: 0.0019268
[154]	training's l2: 0.00162238	valid_1's l2: 0.00192693
[155]	training's l2: 0.00162114	valid_1's l2: 0.00192645
[156]	training's l2: 0.00162	valid_1's l2: 0.00192703
[157]	training's l2: 0.00161852	valid_1's l2: 0.00192728
[158]	training's l2: 0.00161722	valid_1's l2: 0.00192698
[159]	training's l2: 0.00161554	valid_1's l2: 0.00192684
[160]	training's l2: 0.00161412	valid_1's l2: 0.0019267
[161]	training's l2: 0.001613	valid_1's l2: 0.00192649
[162]	training's l2: 0.00161183	valid_1's l2: 0.00192662
[163]	training's l2: 0.0016108	valid_1's l2: 0.0019268
[164]	training's l2: 0.00160946	valid_1's l2: 0.00192653
[165]	training's l2: 0.00160824	valid_1's l2: 0.0019263
[166]	training's l2: 0.00160712	valid_1's l2: 0.00192675
[167]	training's l2: 0.00160607	valid_1's l2: 0.00192656
[168]	training's l2: 0.00160483	valid_1's l2: 0.00192632
[169]	training's l2: 0.00160346	valid_1's l2: 0.00192589
[170]	training's l2: 0.00160245	valid_1's l2: 0.00192612
[171]	training's l2: 0.00160157	valid_1's l2: 0.0019262
[172]	training's l2: 0.0016003	valid_1's l2: 0.00192588
[173]	training's l2: 0.00159908	valid_1's l2: 0.00192576
[174]	training's l2: 0.00159767	valid_1's l2: 0.00192583
[175]	training's l2: 0.00159643	valid_1's l2: 0.00192574
[176]	training's l2: 0.00159535	valid_1's l2: 0.00192577
[177]	training's l2: 0.00159418	valid_1's l2: 0.0019258
[178]	training's l2: 0.00159297	valid_1's l2: 0.00192585
[179]	training's l2: 0.00159177	valid_1's l2: 0.00192567
[180]	training's l2: 0.0015905	valid_1's l2: 0.0019256
[181]	training's l2: 0.00158921	valid_1's l2: 0.00192581
[182]	training's l2: 0.00158751	valid_1's l2: 0.0019261
[183]	training's l2: 0.00158646	valid_1's l2: 0.00192627
[184]	training's l2: 0.00158535	valid_1's l2: 0.00192622
[185]	training's l2: 0.00158408	valid_1's l2: 0.00192646
[186]	training's l2: 0.00158302	valid_1's l2: 0.00192732
[187]	training's l2: 0.00158159	valid_1's l2: 0.00192724
[188]	training's l2: 0.00158036	valid_1's l2: 0.00192692
Did not meet early stopping. Best iteration is:
[188]	training's l2: 0.00158036	valid_1's l2: 0.00192692
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198818 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230283	valid_1's l2: 0.00232782
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00226396	valid_1's l2: 0.00229612
[3]	training's l2: 0.00222943	valid_1's l2: 0.0022667
[4]	training's l2: 0.00219894	valid_1's l2: 0.00224261
[5]	training's l2: 0.002174	valid_1's l2: 0.00222174
[6]	training's l2: 0.00215103	valid_1's l2: 0.00220606
[7]	training's l2: 0.00213253	valid_1's l2: 0.00218698
[8]	training's l2: 0.00211354	valid_1's l2: 0.00217368
[9]	training's l2: 0.00209815	valid_1's l2: 0.00216219
[10]	training's l2: 0.00208433	valid_1's l2: 0.00214826
[11]	training's l2: 0.00207032	valid_1's l2: 0.00213938
[12]	training's l2: 0.00205873	valid_1's l2: 0.00213472
[13]	training's l2: 0.00204825	valid_1's l2: 0.00213067
[14]	training's l2: 0.00203814	valid_1's l2: 0.00212523
[15]	training's l2: 0.00202952	valid_1's l2: 0.00211619
[16]	training's l2: 0.00202176	valid_1's l2: 0.00211216
[17]	training's l2: 0.00201209	valid_1's l2: 0.00210673
[18]	training's l2: 0.00200538	valid_1's l2: 0.00209749
[19]	training's l2: 0.00199839	valid_1's l2: 0.0020924
[20]	training's l2: 0.00199168	valid_1's l2: 0.00208339
[21]	training's l2: 0.00198378	valid_1's l2: 0.00207614
[22]	training's l2: 0.00197775	valid_1's l2: 0.00207168
[23]	training's l2: 0.00197267	valid_1's l2: 0.00206548
[24]	training's l2: 0.00196657	valid_1's l2: 0.00206007
[25]	training's l2: 0.00196135	valid_1's l2: 0.00205691
[26]	training's l2: 0.00195702	valid_1's l2: 0.00205087
[27]	training's l2: 0.00194931	valid_1's l2: 0.00204643
[28]	training's l2: 0.0019449	valid_1's l2: 0.00204327
[29]	training's l2: 0.0019409	valid_1's l2: 0.0020382
[30]	training's l2: 0.00193466	valid_1's l2: 0.00203352
[31]	training's l2: 0.00193017	valid_1's l2: 0.00202831
[32]	training's l2: 0.00192689	valid_1's l2: 0.00202681
[33]	training's l2: 0.0019212	valid_1's l2: 0.00202443
[34]	training's l2: 0.00191715	valid_1's l2: 0.00201942
[35]	training's l2: 0.00191288	valid_1's l2: 0.00201713
[36]	training's l2: 0.00190939	valid_1's l2: 0.00201607
[37]	training's l2: 0.00190599	valid_1's l2: 0.00201363
[38]	training's l2: 0.00190263	valid_1's l2: 0.00201133
[39]	training's l2: 0.00189821	valid_1's l2: 0.00200904
[40]	training's l2: 0.00189446	valid_1's l2: 0.00200775
[41]	training's l2: 0.00189016	valid_1's l2: 0.00200426
[42]	training's l2: 0.00188669	valid_1's l2: 0.0020012
[43]	training's l2: 0.00188244	valid_1's l2: 0.00199911
[44]	training's l2: 0.00187953	valid_1's l2: 0.00199889
[45]	training's l2: 0.00187632	valid_1's l2: 0.00199701
[46]	training's l2: 0.00187375	valid_1's l2: 0.00199509
[47]	training's l2: 0.00187105	valid_1's l2: 0.00199373
[48]	training's l2: 0.00186849	valid_1's l2: 0.00199185
[49]	training's l2: 0.00186577	valid_1's l2: 0.0019895
[50]	training's l2: 0.00186343	valid_1's l2: 0.00198736
[51]	training's l2: 0.00186077	valid_1's l2: 0.00198579
[52]	training's l2: 0.00185818	valid_1's l2: 0.00198437
[53]	training's l2: 0.00185579	valid_1's l2: 0.00198366
[54]	training's l2: 0.00185315	valid_1's l2: 0.00198237
[55]	training's l2: 0.00184989	valid_1's l2: 0.00197993
[56]	training's l2: 0.0018477	valid_1's l2: 0.00197804
[57]	training's l2: 0.00184557	valid_1's l2: 0.00197731
[58]	training's l2: 0.00184383	valid_1's l2: 0.00197609
[59]	training's l2: 0.00184144	valid_1's l2: 0.00197393
[60]	training's l2: 0.0018397	valid_1's l2: 0.00197338
[61]	training's l2: 0.00183794	valid_1's l2: 0.00197248
[62]	training's l2: 0.0018355	valid_1's l2: 0.00197053
[63]	training's l2: 0.00183262	valid_1's l2: 0.00196945
[64]	training's l2: 0.00183062	valid_1's l2: 0.00196909
[65]	training's l2: 0.00182872	valid_1's l2: 0.00196895
[66]	training's l2: 0.00182673	valid_1's l2: 0.0019678
[67]	training's l2: 0.00182489	valid_1's l2: 0.00196716
[68]	training's l2: 0.00182324	valid_1's l2: 0.00196718
[69]	training's l2: 0.00182125	valid_1's l2: 0.00196636
[70]	training's l2: 0.00181968	valid_1's l2: 0.00196602
[71]	training's l2: 0.00181775	valid_1's l2: 0.00196571
[72]	training's l2: 0.00181606	valid_1's l2: 0.00196543
[73]	training's l2: 0.00181339	valid_1's l2: 0.00196448
[74]	training's l2: 0.00181138	valid_1's l2: 0.00196321
[75]	training's l2: 0.00180876	valid_1's l2: 0.00196119
[76]	training's l2: 0.00180673	valid_1's l2: 0.00196096
[77]	training's l2: 0.00180471	valid_1's l2: 0.00196005
[78]	training's l2: 0.00180283	valid_1's l2: 0.0019593
[79]	training's l2: 0.0018014	valid_1's l2: 0.00195857
[80]	training's l2: 0.00179993	valid_1's l2: 0.00195825
[81]	training's l2: 0.00179698	valid_1's l2: 0.00195642
[82]	training's l2: 0.00179526	valid_1's l2: 0.00195573
[83]	training's l2: 0.00179357	valid_1's l2: 0.00195493
[84]	training's l2: 0.00179183	valid_1's l2: 0.0019534
[85]	training's l2: 0.00179028	valid_1's l2: 0.0019532
[86]	training's l2: 0.0017886	valid_1's l2: 0.00195285
[87]	training's l2: 0.00178719	valid_1's l2: 0.00195207
[88]	training's l2: 0.00178566	valid_1's l2: 0.00195149
[89]	training's l2: 0.00178422	valid_1's l2: 0.00195043
[90]	training's l2: 0.00178216	valid_1's l2: 0.00194995
[91]	training's l2: 0.00178077	valid_1's l2: 0.00194968
[92]	training's l2: 0.00177937	valid_1's l2: 0.00194923
[93]	training's l2: 0.00177793	valid_1's l2: 0.00194944
[94]	training's l2: 0.00177623	valid_1's l2: 0.00194879
[95]	training's l2: 0.00177504	valid_1's l2: 0.0019486
[96]	training's l2: 0.00177266	valid_1's l2: 0.00194717
[97]	training's l2: 0.00177126	valid_1's l2: 0.00194684
[98]	training's l2: 0.00176985	valid_1's l2: 0.00194641
[99]	training's l2: 0.00176853	valid_1's l2: 0.00194599
[100]	training's l2: 0.00176682	valid_1's l2: 0.00194557
[101]	training's l2: 0.00176546	valid_1's l2: 0.00194572
[102]	training's l2: 0.00176399	valid_1's l2: 0.00194578
[103]	training's l2: 0.00176281	valid_1's l2: 0.00194503
[104]	training's l2: 0.00176129	valid_1's l2: 0.00194415
[105]	training's l2: 0.00176003	valid_1's l2: 0.00194421
[106]	training's l2: 0.00175855	valid_1's l2: 0.00194284
[107]	training's l2: 0.0017573	valid_1's l2: 0.00194243
[108]	training's l2: 0.0017559	valid_1's l2: 0.0019425
[109]	training's l2: 0.0017546	valid_1's l2: 0.00194237
[110]	training's l2: 0.00175336	valid_1's l2: 0.00194177
[111]	training's l2: 0.00175205	valid_1's l2: 0.0019409
[112]	training's l2: 0.00175089	valid_1's l2: 0.00194064
[113]	training's l2: 0.00174964	valid_1's l2: 0.00194085
[114]	training's l2: 0.00174802	valid_1's l2: 0.00194045
[115]	training's l2: 0.00174684	valid_1's l2: 0.00194062
[116]	training's l2: 0.00174538	valid_1's l2: 0.00193998
[117]	training's l2: 0.00174419	valid_1's l2: 0.00194009
[118]	training's l2: 0.00174234	valid_1's l2: 0.0019408
[119]	training's l2: 0.00174116	valid_1's l2: 0.00194074
[120]	training's l2: 0.00174006	valid_1's l2: 0.00194004
[121]	training's l2: 0.0017389	valid_1's l2: 0.00193958
[122]	training's l2: 0.00173741	valid_1's l2: 0.00193953
[123]	training's l2: 0.0017357	valid_1's l2: 0.0019392
[124]	training's l2: 0.00173473	valid_1's l2: 0.00193874
[125]	training's l2: 0.00173344	valid_1's l2: 0.00193871
[126]	training's l2: 0.00173224	valid_1's l2: 0.00193868
[127]	training's l2: 0.00173109	valid_1's l2: 0.00193776
[128]	training's l2: 0.00173006	valid_1's l2: 0.00193819
[129]	training's l2: 0.00172873	valid_1's l2: 0.00193781
[130]	training's l2: 0.00172743	valid_1's l2: 0.00193729
[131]	training's l2: 0.00172603	valid_1's l2: 0.00193659
[132]	training's l2: 0.00172472	valid_1's l2: 0.00193653
[133]	training's l2: 0.00172341	valid_1's l2: 0.00193683
[134]	training's l2: 0.00172234	valid_1's l2: 0.00193693
[135]	training's l2: 0.00172142	valid_1's l2: 0.00193651
[136]	training's l2: 0.00171998	valid_1's l2: 0.00193589
[137]	training's l2: 0.00171864	valid_1's l2: 0.00193523
[138]	training's l2: 0.00171739	valid_1's l2: 0.00193484
[139]	training's l2: 0.00171629	valid_1's l2: 0.00193543
[140]	training's l2: 0.00171527	valid_1's l2: 0.00193529
[141]	training's l2: 0.00171436	valid_1's l2: 0.00193502
[142]	training's l2: 0.00171314	valid_1's l2: 0.00193494
[143]	training's l2: 0.00171196	valid_1's l2: 0.00193439
[144]	training's l2: 0.00171109	valid_1's l2: 0.00193398
[145]	training's l2: 0.00170987	valid_1's l2: 0.00193415
[146]	training's l2: 0.00170852	valid_1's l2: 0.00193451
[147]	training's l2: 0.00170742	valid_1's l2: 0.0019343
[148]	training's l2: 0.0017064	valid_1's l2: 0.00193416
[149]	training's l2: 0.00170537	valid_1's l2: 0.00193408
[150]	training's l2: 0.00170412	valid_1's l2: 0.00193422
[151]	training's l2: 0.00170306	valid_1's l2: 0.00193435
[152]	training's l2: 0.00170167	valid_1's l2: 0.00193428
[153]	training's l2: 0.00170051	valid_1's l2: 0.0019343
[154]	training's l2: 0.00169935	valid_1's l2: 0.00193469
[155]	training's l2: 0.00169819	valid_1's l2: 0.00193469
[156]	training's l2: 0.00169706	valid_1's l2: 0.00193461
[157]	training's l2: 0.00169614	valid_1's l2: 0.0019348
[158]	training's l2: 0.00169509	valid_1's l2: 0.00193485
[159]	training's l2: 0.00169411	valid_1's l2: 0.00193447
[160]	training's l2: 0.00169308	valid_1's l2: 0.00193465
[161]	training's l2: 0.00169199	valid_1's l2: 0.00193422
[162]	training's l2: 0.00169092	valid_1's l2: 0.00193461
[163]	training's l2: 0.00168968	valid_1's l2: 0.00193408
[164]	training's l2: 0.00168867	valid_1's l2: 0.00193382
[165]	training's l2: 0.00168737	valid_1's l2: 0.00193432
[166]	training's l2: 0.00168629	valid_1's l2: 0.00193433
[167]	training's l2: 0.00168541	valid_1's l2: 0.00193392
[168]	training's l2: 0.00168408	valid_1's l2: 0.00193411
[169]	training's l2: 0.00168285	valid_1's l2: 0.00193365
[170]	training's l2: 0.0016818	valid_1's l2: 0.00193327
[171]	training's l2: 0.00168062	valid_1's l2: 0.00193255
[172]	training's l2: 0.00167964	valid_1's l2: 0.00193201
[173]	training's l2: 0.00167887	valid_1's l2: 0.00193252
[174]	training's l2: 0.00167754	valid_1's l2: 0.00193218
[175]	training's l2: 0.00167651	valid_1's l2: 0.00193185
[176]	training's l2: 0.0016753	valid_1's l2: 0.00193154
[177]	training's l2: 0.00167416	valid_1's l2: 0.00193102
[178]	training's l2: 0.00167294	valid_1's l2: 0.00193076
[179]	training's l2: 0.00167174	valid_1's l2: 0.0019302
[180]	training's l2: 0.00167059	valid_1's l2: 0.00193005
[181]	training's l2: 0.00166943	valid_1's l2: 0.00193025
[182]	training's l2: 0.00166849	valid_1's l2: 0.00193022
[183]	training's l2: 0.00166731	valid_1's l2: 0.00192994
Did not meet early stopping. Best iteration is:
[183]	training's l2: 0.00166731	valid_1's l2: 0.00192994
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266552 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230044	valid_1's l2: 0.00232466
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00225857	valid_1's l2: 0.00229057
[3]	training's l2: 0.00221987	valid_1's l2: 0.00226057
[4]	training's l2: 0.00218827	valid_1's l2: 0.00223463
[5]	training's l2: 0.00216057	valid_1's l2: 0.0022148
[6]	training's l2: 0.00213356	valid_1's l2: 0.00219412
[7]	training's l2: 0.00211156	valid_1's l2: 0.00217403
[8]	training's l2: 0.00209018	valid_1's l2: 0.00215975
[9]	training's l2: 0.00207234	valid_1's l2: 0.00214524
[10]	training's l2: 0.00205605	valid_1's l2: 0.00213436
[11]	training's l2: 0.00203987	valid_1's l2: 0.00212349
[12]	training's l2: 0.00202651	valid_1's l2: 0.00211293
[13]	training's l2: 0.00201309	valid_1's l2: 0.00210376
[14]	training's l2: 0.00200171	valid_1's l2: 0.00209657
[15]	training's l2: 0.00199171	valid_1's l2: 0.00208985
[16]	training's l2: 0.00198156	valid_1's l2: 0.00207928
[17]	training's l2: 0.00197203	valid_1's l2: 0.00207426
[18]	training's l2: 0.00196222	valid_1's l2: 0.00206962
[19]	training's l2: 0.0019549	valid_1's l2: 0.00206514
[20]	training's l2: 0.00194723	valid_1's l2: 0.00206075
[21]	training's l2: 0.00193895	valid_1's l2: 0.00205686
[22]	training's l2: 0.00193063	valid_1's l2: 0.00205353
[23]	training's l2: 0.00192344	valid_1's l2: 0.00204938
[24]	training's l2: 0.00191666	valid_1's l2: 0.00204554
[25]	training's l2: 0.00191024	valid_1's l2: 0.00204185
[26]	training's l2: 0.00190407	valid_1's l2: 0.0020369
[27]	training's l2: 0.00189863	valid_1's l2: 0.00203313
[28]	training's l2: 0.00189358	valid_1's l2: 0.0020275
[29]	training's l2: 0.00188812	valid_1's l2: 0.002025
[30]	training's l2: 0.00188327	valid_1's l2: 0.00202035
[31]	training's l2: 0.00187704	valid_1's l2: 0.0020178
[32]	training's l2: 0.00187016	valid_1's l2: 0.00201576
[33]	training's l2: 0.00186603	valid_1's l2: 0.00201426
[34]	training's l2: 0.00186135	valid_1's l2: 0.00201067
[35]	training's l2: 0.00185664	valid_1's l2: 0.00200656
[36]	training's l2: 0.00185014	valid_1's l2: 0.00200334
[37]	training's l2: 0.00184544	valid_1's l2: 0.002003
[38]	training's l2: 0.00184145	valid_1's l2: 0.00199927
[39]	training's l2: 0.00183634	valid_1's l2: 0.00199662
[40]	training's l2: 0.00183279	valid_1's l2: 0.0019956
[41]	training's l2: 0.00182901	valid_1's l2: 0.0019925
[42]	training's l2: 0.00182515	valid_1's l2: 0.00199116
[43]	training's l2: 0.00182112	valid_1's l2: 0.00198922
[44]	training's l2: 0.00181731	valid_1's l2: 0.00198722
[45]	training's l2: 0.00181374	valid_1's l2: 0.00198515
[46]	training's l2: 0.00181035	valid_1's l2: 0.00198353
[47]	training's l2: 0.00180655	valid_1's l2: 0.00198243
[48]	training's l2: 0.00180219	valid_1's l2: 0.00198025
[49]	training's l2: 0.00179935	valid_1's l2: 0.00197951
[50]	training's l2: 0.0017957	valid_1's l2: 0.00197861
[51]	training's l2: 0.00179263	valid_1's l2: 0.00197747
[52]	training's l2: 0.00178956	valid_1's l2: 0.00197537
[53]	training's l2: 0.00178522	valid_1's l2: 0.00197355
[54]	training's l2: 0.00178214	valid_1's l2: 0.00197149
[55]	training's l2: 0.001779	valid_1's l2: 0.00196965
[56]	training's l2: 0.00177601	valid_1's l2: 0.00196851
[57]	training's l2: 0.00177254	valid_1's l2: 0.00196687
[58]	training's l2: 0.00176969	valid_1's l2: 0.0019667
[59]	training's l2: 0.00176711	valid_1's l2: 0.00196585
[60]	training's l2: 0.00176342	valid_1's l2: 0.0019644
[61]	training's l2: 0.00176072	valid_1's l2: 0.00196292
[62]	training's l2: 0.001758	valid_1's l2: 0.00196156
[63]	training's l2: 0.00175474	valid_1's l2: 0.00195879
[64]	training's l2: 0.00175206	valid_1's l2: 0.00195799
[65]	training's l2: 0.00174935	valid_1's l2: 0.00195727
[66]	training's l2: 0.00174704	valid_1's l2: 0.00195634
[67]	training's l2: 0.00174441	valid_1's l2: 0.00195547
[68]	training's l2: 0.00174212	valid_1's l2: 0.00195509
[69]	training's l2: 0.00173961	valid_1's l2: 0.00195435
[70]	training's l2: 0.00173669	valid_1's l2: 0.00195401
[71]	training's l2: 0.00173399	valid_1's l2: 0.00195398
[72]	training's l2: 0.00173163	valid_1's l2: 0.00195314
[73]	training's l2: 0.00172939	valid_1's l2: 0.00195222
[74]	training's l2: 0.00172707	valid_1's l2: 0.00195144
[75]	training's l2: 0.00172336	valid_1's l2: 0.00194956
[76]	training's l2: 0.00172074	valid_1's l2: 0.00194912
[77]	training's l2: 0.0017184	valid_1's l2: 0.00194836
[78]	training's l2: 0.00171602	valid_1's l2: 0.00194748
[79]	training's l2: 0.00171265	valid_1's l2: 0.00194749
[80]	training's l2: 0.00171042	valid_1's l2: 0.00194663
[81]	training's l2: 0.00170798	valid_1's l2: 0.00194561
[82]	training's l2: 0.00170556	valid_1's l2: 0.00194544
[83]	training's l2: 0.00170349	valid_1's l2: 0.00194476
[84]	training's l2: 0.00170084	valid_1's l2: 0.001944
[85]	training's l2: 0.00169864	valid_1's l2: 0.00194264
[86]	training's l2: 0.00169587	valid_1's l2: 0.00194149
[87]	training's l2: 0.0016934	valid_1's l2: 0.00194058
[88]	training's l2: 0.00169134	valid_1's l2: 0.00194046
[89]	training's l2: 0.00168933	valid_1's l2: 0.00194018
[90]	training's l2: 0.00168728	valid_1's l2: 0.00194016
[91]	training's l2: 0.00168511	valid_1's l2: 0.00193995
[92]	training's l2: 0.00168276	valid_1's l2: 0.00193921
[93]	training's l2: 0.00168061	valid_1's l2: 0.00193842
[94]	training's l2: 0.0016786	valid_1's l2: 0.00193709
[95]	training's l2: 0.00167627	valid_1's l2: 0.0019368
[96]	training's l2: 0.00167432	valid_1's l2: 0.00193674
[97]	training's l2: 0.00167218	valid_1's l2: 0.00193605
[98]	training's l2: 0.00167015	valid_1's l2: 0.00193594
[99]	training's l2: 0.00166743	valid_1's l2: 0.00193581
[100]	training's l2: 0.00166511	valid_1's l2: 0.0019348
[101]	training's l2: 0.00166285	valid_1's l2: 0.00193425
[102]	training's l2: 0.00166097	valid_1's l2: 0.00193419
[103]	training's l2: 0.00165893	valid_1's l2: 0.00193453
[104]	training's l2: 0.00165699	valid_1's l2: 0.00193506
[105]	training's l2: 0.00165493	valid_1's l2: 0.0019347
[106]	training's l2: 0.00165307	valid_1's l2: 0.00193476
[107]	training's l2: 0.00165087	valid_1's l2: 0.00193384
[108]	training's l2: 0.00164891	valid_1's l2: 0.0019342
[109]	training's l2: 0.00164672	valid_1's l2: 0.00193353
[110]	training's l2: 0.00164487	valid_1's l2: 0.00193302
[111]	training's l2: 0.0016429	valid_1's l2: 0.00193254
[112]	training's l2: 0.00164113	valid_1's l2: 0.00193312
[113]	training's l2: 0.00163928	valid_1's l2: 0.00193321
[114]	training's l2: 0.0016375	valid_1's l2: 0.0019331
[115]	training's l2: 0.0016356	valid_1's l2: 0.00193307
[116]	training's l2: 0.00163333	valid_1's l2: 0.00193233
[117]	training's l2: 0.00163146	valid_1's l2: 0.0019319
[118]	training's l2: 0.00162944	valid_1's l2: 0.00193182
[119]	training's l2: 0.00162754	valid_1's l2: 0.00193212
[120]	training's l2: 0.00162582	valid_1's l2: 0.00193259
[121]	training's l2: 0.00162406	valid_1's l2: 0.00193267
[122]	training's l2: 0.0016222	valid_1's l2: 0.00193266
[123]	training's l2: 0.00162053	valid_1's l2: 0.0019326
[124]	training's l2: 0.0016187	valid_1's l2: 0.00193269
[125]	training's l2: 0.00161672	valid_1's l2: 0.00193227
[126]	training's l2: 0.00161498	valid_1's l2: 0.00193214
[127]	training's l2: 0.0016132	valid_1's l2: 0.00193202
[128]	training's l2: 0.00161155	valid_1's l2: 0.00193216
[129]	training's l2: 0.0016097	valid_1's l2: 0.00193241
[130]	training's l2: 0.00160794	valid_1's l2: 0.00193227
[131]	training's l2: 0.00160609	valid_1's l2: 0.00193208
[132]	training's l2: 0.00160421	valid_1's l2: 0.00193144
[133]	training's l2: 0.00160239	valid_1's l2: 0.00193114
[134]	training's l2: 0.00160043	valid_1's l2: 0.00193158
[135]	training's l2: 0.00159862	valid_1's l2: 0.00193115
[136]	training's l2: 0.00159705	valid_1's l2: 0.00193063
[137]	training's l2: 0.00159547	valid_1's l2: 0.00193031
[138]	training's l2: 0.0015939	valid_1's l2: 0.00193057
[139]	training's l2: 0.00159216	valid_1's l2: 0.00193024
[140]	training's l2: 0.00159029	valid_1's l2: 0.00192999
[141]	training's l2: 0.00158866	valid_1's l2: 0.00192967
[142]	training's l2: 0.00158702	valid_1's l2: 0.00193001
[143]	training's l2: 0.00158497	valid_1's l2: 0.00192954
[144]	training's l2: 0.00158328	valid_1's l2: 0.00192951
[145]	training's l2: 0.00158183	valid_1's l2: 0.00192999
[146]	training's l2: 0.00158025	valid_1's l2: 0.00193012
[147]	training's l2: 0.00157876	valid_1's l2: 0.00193031
[148]	training's l2: 0.00157698	valid_1's l2: 0.00193015
[149]	training's l2: 0.00157545	valid_1's l2: 0.0019299
[150]	training's l2: 0.00157379	valid_1's l2: 0.00192991
[151]	training's l2: 0.00157247	valid_1's l2: 0.00193028
[152]	training's l2: 0.00157103	valid_1's l2: 0.00193029
[153]	training's l2: 0.00156949	valid_1's l2: 0.00193055
[154]	training's l2: 0.00156799	valid_1's l2: 0.00193049
[155]	training's l2: 0.00156627	valid_1's l2: 0.00193034
[156]	training's l2: 0.00156475	valid_1's l2: 0.0019302
[157]	training's l2: 0.00156328	valid_1's l2: 0.0019298
[158]	training's l2: 0.00156172	valid_1's l2: 0.00192935
[159]	training's l2: 0.00155992	valid_1's l2: 0.00192925
[160]	training's l2: 0.00155851	valid_1's l2: 0.00192916
[161]	training's l2: 0.00155706	valid_1's l2: 0.00192937
[162]	training's l2: 0.00155556	valid_1's l2: 0.00192978
[163]	training's l2: 0.001554	valid_1's l2: 0.00192959
[164]	training's l2: 0.00155257	valid_1's l2: 0.00192966
[165]	training's l2: 0.00155082	valid_1's l2: 0.00192948
[166]	training's l2: 0.00154926	valid_1's l2: 0.0019294
[167]	training's l2: 0.0015476	valid_1's l2: 0.00192919
[168]	training's l2: 0.00154611	valid_1's l2: 0.00192949
[169]	training's l2: 0.0015445	valid_1's l2: 0.00192907
[170]	training's l2: 0.0015428	valid_1's l2: 0.00192902
[171]	training's l2: 0.00154105	valid_1's l2: 0.00192949
[172]	training's l2: 0.0015398	valid_1's l2: 0.00193
[173]	training's l2: 0.00153825	valid_1's l2: 0.00192937
[174]	training's l2: 0.00153673	valid_1's l2: 0.0019291
[175]	training's l2: 0.00153506	valid_1's l2: 0.00192875
[176]	training's l2: 0.00153358	valid_1's l2: 0.00192846
[177]	training's l2: 0.00153196	valid_1's l2: 0.00192881
[178]	training's l2: 0.00153053	valid_1's l2: 0.00192925
[179]	training's l2: 0.0015292	valid_1's l2: 0.00192932
[180]	training's l2: 0.00152783	valid_1's l2: 0.0019295
[181]	training's l2: 0.00152642	valid_1's l2: 0.00192975
[182]	training's l2: 0.00152476	valid_1's l2: 0.00193014
[183]	training's l2: 0.00152323	valid_1's l2: 0.00192987
Did not meet early stopping. Best iteration is:
[183]	training's l2: 0.00152323	valid_1's l2: 0.00192987
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207719 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230499	valid_1's l2: 0.00232887
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00226708	valid_1's l2: 0.00229821
[3]	training's l2: 0.00223211	valid_1's l2: 0.00226792
[4]	training's l2: 0.00220148	valid_1's l2: 0.00224401
[5]	training's l2: 0.00217535	valid_1's l2: 0.00222241
[6]	training's l2: 0.00215257	valid_1's l2: 0.00220693
[7]	training's l2: 0.00213037	valid_1's l2: 0.00218939
[8]	training's l2: 0.00211263	valid_1's l2: 0.00217276
[9]	training's l2: 0.00209477	valid_1's l2: 0.00216137
[10]	training's l2: 0.00207893	valid_1's l2: 0.00215018
[11]	training's l2: 0.00206553	valid_1's l2: 0.00214128
[12]	training's l2: 0.00205363	valid_1's l2: 0.00213122
[13]	training's l2: 0.0020416	valid_1's l2: 0.0021229
[14]	training's l2: 0.00203117	valid_1's l2: 0.00211337
[15]	training's l2: 0.0020208	valid_1's l2: 0.0021079
[16]	training's l2: 0.00201199	valid_1's l2: 0.00209999
[17]	training's l2: 0.00200171	valid_1's l2: 0.0020948
[18]	training's l2: 0.00199308	valid_1's l2: 0.00208807
[19]	training's l2: 0.00198497	valid_1's l2: 0.00208435
[20]	training's l2: 0.00197611	valid_1's l2: 0.00207694
[21]	training's l2: 0.00196987	valid_1's l2: 0.00207208
[22]	training's l2: 0.00196322	valid_1's l2: 0.00206755
[23]	training's l2: 0.00195691	valid_1's l2: 0.00206376
[24]	training's l2: 0.00195052	valid_1's l2: 0.0020605
[25]	training's l2: 0.00194462	valid_1's l2: 0.00205496
[26]	training's l2: 0.00193914	valid_1's l2: 0.00205171
[27]	training's l2: 0.00193417	valid_1's l2: 0.00204929
[28]	training's l2: 0.00192873	valid_1's l2: 0.00204301
[29]	training's l2: 0.00192423	valid_1's l2: 0.00203712
[30]	training's l2: 0.00191975	valid_1's l2: 0.00203568
[31]	training's l2: 0.00191539	valid_1's l2: 0.00203112
[32]	training's l2: 0.00191132	valid_1's l2: 0.0020286
[33]	training's l2: 0.00190774	valid_1's l2: 0.00202697
[34]	training's l2: 0.00190149	valid_1's l2: 0.00202266
[35]	training's l2: 0.00189749	valid_1's l2: 0.00201719
[36]	training's l2: 0.00189227	valid_1's l2: 0.00201556
[37]	training's l2: 0.00188862	valid_1's l2: 0.00201257
[38]	training's l2: 0.00188334	valid_1's l2: 0.00200868
[39]	training's l2: 0.0018794	valid_1's l2: 0.00200564
[40]	training's l2: 0.00187602	valid_1's l2: 0.00200356
[41]	training's l2: 0.00187148	valid_1's l2: 0.00200155
[42]	training's l2: 0.00186803	valid_1's l2: 0.00199867
[43]	training's l2: 0.00186455	valid_1's l2: 0.00199572
[44]	training's l2: 0.00186069	valid_1's l2: 0.00199429
[45]	training's l2: 0.00185799	valid_1's l2: 0.00199372
[46]	training's l2: 0.00185345	valid_1's l2: 0.00199124
[47]	training's l2: 0.00185055	valid_1's l2: 0.00198908
[48]	training's l2: 0.00184798	valid_1's l2: 0.00198803
[49]	training's l2: 0.00184491	valid_1's l2: 0.00198641
[50]	training's l2: 0.00184207	valid_1's l2: 0.00198436
[51]	training's l2: 0.00183961	valid_1's l2: 0.00198364
[52]	training's l2: 0.00183676	valid_1's l2: 0.00198163
[53]	training's l2: 0.00183431	valid_1's l2: 0.00198067
[54]	training's l2: 0.00183172	valid_1's l2: 0.00197979
[55]	training's l2: 0.0018281	valid_1's l2: 0.00197771
[56]	training's l2: 0.00182517	valid_1's l2: 0.00197579
[57]	training's l2: 0.00182266	valid_1's l2: 0.00197454
[58]	training's l2: 0.00181996	valid_1's l2: 0.00197382
[59]	training's l2: 0.00181765	valid_1's l2: 0.00197327
[60]	training's l2: 0.00181493	valid_1's l2: 0.00197245
[61]	training's l2: 0.00181205	valid_1's l2: 0.00197166
[62]	training's l2: 0.00180976	valid_1's l2: 0.00197048
[63]	training's l2: 0.00180722	valid_1's l2: 0.00196822
[64]	training's l2: 0.00180518	valid_1's l2: 0.00196762
[65]	training's l2: 0.00180194	valid_1's l2: 0.00196519
[66]	training's l2: 0.00179971	valid_1's l2: 0.00196344
[67]	training's l2: 0.00179695	valid_1's l2: 0.00196212
[68]	training's l2: 0.00179462	valid_1's l2: 0.0019612
[69]	training's l2: 0.00179162	valid_1's l2: 0.00195949
[70]	training's l2: 0.0017893	valid_1's l2: 0.00195924
[71]	training's l2: 0.00178746	valid_1's l2: 0.00195821
[72]	training's l2: 0.00178547	valid_1's l2: 0.00195761
[73]	training's l2: 0.00178346	valid_1's l2: 0.00195645
[74]	training's l2: 0.00178172	valid_1's l2: 0.00195554
[75]	training's l2: 0.00177856	valid_1's l2: 0.00195361
[76]	training's l2: 0.00177637	valid_1's l2: 0.00195321
[77]	training's l2: 0.00177415	valid_1's l2: 0.00195355
[78]	training's l2: 0.00177235	valid_1's l2: 0.00195362
[79]	training's l2: 0.00177066	valid_1's l2: 0.00195335
[80]	training's l2: 0.00176785	valid_1's l2: 0.00195233
[81]	training's l2: 0.00176616	valid_1's l2: 0.00195195
[82]	training's l2: 0.00176398	valid_1's l2: 0.00195065
[83]	training's l2: 0.00176232	valid_1's l2: 0.00195049
[84]	training's l2: 0.00176042	valid_1's l2: 0.00194957
[85]	training's l2: 0.00175815	valid_1's l2: 0.00194798
[86]	training's l2: 0.00175639	valid_1's l2: 0.00194741
[87]	training's l2: 0.00175464	valid_1's l2: 0.00194671
[88]	training's l2: 0.00175274	valid_1's l2: 0.00194654
[89]	training's l2: 0.00175107	valid_1's l2: 0.00194599
[90]	training's l2: 0.00174932	valid_1's l2: 0.00194575
[91]	training's l2: 0.00174729	valid_1's l2: 0.00194578
[92]	training's l2: 0.0017448	valid_1's l2: 0.00194476
[93]	training's l2: 0.00174295	valid_1's l2: 0.0019445
[94]	training's l2: 0.00174111	valid_1's l2: 0.00194364
[95]	training's l2: 0.00173957	valid_1's l2: 0.0019438
[96]	training's l2: 0.00173773	valid_1's l2: 0.00194362
[97]	training's l2: 0.00173604	valid_1's l2: 0.00194295
[98]	training's l2: 0.0017345	valid_1's l2: 0.00194213
[99]	training's l2: 0.00173291	valid_1's l2: 0.00194124
[100]	training's l2: 0.00173128	valid_1's l2: 0.0019414
[101]	training's l2: 0.00172982	valid_1's l2: 0.00194132
[102]	training's l2: 0.00172769	valid_1's l2: 0.00194044
[103]	training's l2: 0.00172611	valid_1's l2: 0.00194042
[104]	training's l2: 0.00172447	valid_1's l2: 0.00194011
[105]	training's l2: 0.00172248	valid_1's l2: 0.00194074
[106]	training's l2: 0.0017207	valid_1's l2: 0.00193953
[107]	training's l2: 0.00171916	valid_1's l2: 0.00193895
[108]	training's l2: 0.00171757	valid_1's l2: 0.00193955
[109]	training's l2: 0.00171613	valid_1's l2: 0.00193923
[110]	training's l2: 0.0017145	valid_1's l2: 0.00193809
[111]	training's l2: 0.00171322	valid_1's l2: 0.00193817
[112]	training's l2: 0.0017116	valid_1's l2: 0.00193803
[113]	training's l2: 0.00171013	valid_1's l2: 0.00193733
[114]	training's l2: 0.00170853	valid_1's l2: 0.00193696
[115]	training's l2: 0.00170692	valid_1's l2: 0.00193666
[116]	training's l2: 0.00170524	valid_1's l2: 0.001936
[117]	training's l2: 0.00170381	valid_1's l2: 0.00193582
[118]	training's l2: 0.00170229	valid_1's l2: 0.00193566
[119]	training's l2: 0.0017007	valid_1's l2: 0.00193554
[120]	training's l2: 0.00169919	valid_1's l2: 0.00193547
[121]	training's l2: 0.00169784	valid_1's l2: 0.00193536
[122]	training's l2: 0.00169634	valid_1's l2: 0.00193531
[123]	training's l2: 0.00169496	valid_1's l2: 0.00193461
[124]	training's l2: 0.00169364	valid_1's l2: 0.00193481
[125]	training's l2: 0.00169229	valid_1's l2: 0.0019349
[126]	training's l2: 0.00169085	valid_1's l2: 0.00193496
[127]	training's l2: 0.00168932	valid_1's l2: 0.001935
[128]	training's l2: 0.00168797	valid_1's l2: 0.00193488
[129]	training's l2: 0.00168658	valid_1's l2: 0.00193493
[130]	training's l2: 0.00168503	valid_1's l2: 0.00193408
[131]	training's l2: 0.00168357	valid_1's l2: 0.00193332
[132]	training's l2: 0.00168212	valid_1's l2: 0.00193353
[133]	training's l2: 0.00168074	valid_1's l2: 0.00193317
[134]	training's l2: 0.00167935	valid_1's l2: 0.001933
[135]	training's l2: 0.00167802	valid_1's l2: 0.00193273
[136]	training's l2: 0.00167686	valid_1's l2: 0.0019331
[137]	training's l2: 0.00167559	valid_1's l2: 0.00193313
[138]	training's l2: 0.00167441	valid_1's l2: 0.00193361
[139]	training's l2: 0.00167287	valid_1's l2: 0.00193336
[140]	training's l2: 0.0016713	valid_1's l2: 0.00193286
[141]	training's l2: 0.00166999	valid_1's l2: 0.00193231
[142]	training's l2: 0.00166856	valid_1's l2: 0.00193243
[143]	training's l2: 0.00166711	valid_1's l2: 0.00193288
[144]	training's l2: 0.00166579	valid_1's l2: 0.00193291
[145]	training's l2: 0.00166445	valid_1's l2: 0.0019331
[146]	training's l2: 0.00166283	valid_1's l2: 0.00193245
[147]	training's l2: 0.00166155	valid_1's l2: 0.00193259
[148]	training's l2: 0.00166026	valid_1's l2: 0.00193189
[149]	training's l2: 0.00165901	valid_1's l2: 0.00193179
[150]	training's l2: 0.00165775	valid_1's l2: 0.00193165
[151]	training's l2: 0.00165661	valid_1's l2: 0.00193176
[152]	training's l2: 0.0016554	valid_1's l2: 0.00193176
[153]	training's l2: 0.00165411	valid_1's l2: 0.0019318
[154]	training's l2: 0.00165297	valid_1's l2: 0.00193193
[155]	training's l2: 0.00165178	valid_1's l2: 0.00193223
[156]	training's l2: 0.00165058	valid_1's l2: 0.00193192
[157]	training's l2: 0.0016494	valid_1's l2: 0.0019316
[158]	training's l2: 0.00164837	valid_1's l2: 0.00193129
[159]	training's l2: 0.0016472	valid_1's l2: 0.00193067
[160]	training's l2: 0.00164593	valid_1's l2: 0.0019303
[161]	training's l2: 0.00164463	valid_1's l2: 0.00193027
[162]	training's l2: 0.00164337	valid_1's l2: 0.00192989
[163]	training's l2: 0.00164216	valid_1's l2: 0.00193005
[164]	training's l2: 0.00164112	valid_1's l2: 0.00192984
[165]	training's l2: 0.00163998	valid_1's l2: 0.00192999
[166]	training's l2: 0.00163902	valid_1's l2: 0.00192995
[167]	training's l2: 0.00163783	valid_1's l2: 0.00192961
[168]	training's l2: 0.00163677	valid_1's l2: 0.00192902
[169]	training's l2: 0.00163596	valid_1's l2: 0.00192897
[170]	training's l2: 0.00163481	valid_1's l2: 0.00192899
[171]	training's l2: 0.00163371	valid_1's l2: 0.00192905
[172]	training's l2: 0.00163245	valid_1's l2: 0.00192886
[173]	training's l2: 0.00163122	valid_1's l2: 0.00192901
[174]	training's l2: 0.00163009	valid_1's l2: 0.00192895
[175]	training's l2: 0.00162898	valid_1's l2: 0.00192917
[176]	training's l2: 0.00162805	valid_1's l2: 0.00192934
[177]	training's l2: 0.00162705	valid_1's l2: 0.00192931
[178]	training's l2: 0.00162574	valid_1's l2: 0.00192912
[179]	training's l2: 0.00162447	valid_1's l2: 0.00192919
[180]	training's l2: 0.00162322	valid_1's l2: 0.00192916
[181]	training's l2: 0.00162194	valid_1's l2: 0.00192932
[182]	training's l2: 0.00162082	valid_1's l2: 0.00192901
[183]	training's l2: 0.00161972	valid_1's l2: 0.00192888
[184]	training's l2: 0.00161834	valid_1's l2: 0.00192902
[185]	training's l2: 0.00161721	valid_1's l2: 0.00192856
[186]	training's l2: 0.00161599	valid_1's l2: 0.00192872
[187]	training's l2: 0.00161518	valid_1's l2: 0.0019289
[188]	training's l2: 0.00161411	valid_1's l2: 0.00192883
[189]	training's l2: 0.00161284	valid_1's l2: 0.00192868
[190]	training's l2: 0.0016119	valid_1's l2: 0.00192863
[191]	training's l2: 0.00161084	valid_1's l2: 0.00192831
[192]	training's l2: 0.00160992	valid_1's l2: 0.00192827
[193]	training's l2: 0.00160896	valid_1's l2: 0.00192825
Did not meet early stopping. Best iteration is:
[193]	training's l2: 0.00160896	valid_1's l2: 0.00192825
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219630 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230734	valid_1's l2: 0.00233179
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00227082	valid_1's l2: 0.00230108
[3]	training's l2: 0.00223724	valid_1's l2: 0.00227161
[4]	training's l2: 0.00220745	valid_1's l2: 0.00224723
[5]	training's l2: 0.00218267	valid_1's l2: 0.00222779
[6]	training's l2: 0.00216118	valid_1's l2: 0.00220865
[7]	training's l2: 0.00213942	valid_1's l2: 0.00219267
[8]	training's l2: 0.00212062	valid_1's l2: 0.00217839
[9]	training's l2: 0.00210294	valid_1's l2: 0.00216619
[10]	training's l2: 0.00208743	valid_1's l2: 0.00215545
[11]	training's l2: 0.00207461	valid_1's l2: 0.00214323
[12]	training's l2: 0.00206258	valid_1's l2: 0.0021327
[13]	training's l2: 0.00205051	valid_1's l2: 0.00212687
[14]	training's l2: 0.00204002	valid_1's l2: 0.00212113
[15]	training's l2: 0.00203038	valid_1's l2: 0.00211214
[16]	training's l2: 0.00201995	valid_1's l2: 0.00210483
[17]	training's l2: 0.00201173	valid_1's l2: 0.00210045
[18]	training's l2: 0.00200152	valid_1's l2: 0.00209277
[19]	training's l2: 0.00199401	valid_1's l2: 0.00208571
[20]	training's l2: 0.00198664	valid_1's l2: 0.00208214
[21]	training's l2: 0.00198034	valid_1's l2: 0.00207705
[22]	training's l2: 0.00197362	valid_1's l2: 0.00207029
[23]	training's l2: 0.00196624	valid_1's l2: 0.00206397
[24]	training's l2: 0.00196133	valid_1's l2: 0.00206058
[25]	training's l2: 0.0019556	valid_1's l2: 0.00205759
[26]	training's l2: 0.00194964	valid_1's l2: 0.0020536
[27]	training's l2: 0.00194501	valid_1's l2: 0.00205039
[28]	training's l2: 0.00194012	valid_1's l2: 0.00204485
[29]	training's l2: 0.00193577	valid_1's l2: 0.00203937
[30]	training's l2: 0.00193143	valid_1's l2: 0.00203658
[31]	training's l2: 0.00192721	valid_1's l2: 0.00203242
[32]	training's l2: 0.00192356	valid_1's l2: 0.00203075
[33]	training's l2: 0.0019169	valid_1's l2: 0.00202725
[34]	training's l2: 0.0019134	valid_1's l2: 0.00202591
[35]	training's l2: 0.00190767	valid_1's l2: 0.00202245
[36]	training's l2: 0.00190397	valid_1's l2: 0.00201839
[37]	training's l2: 0.00189963	valid_1's l2: 0.00201741
[38]	training's l2: 0.00189581	valid_1's l2: 0.00201207
[39]	training's l2: 0.00189263	valid_1's l2: 0.00201116
[40]	training's l2: 0.00188807	valid_1's l2: 0.00200802
[41]	training's l2: 0.00188389	valid_1's l2: 0.00200632
[42]	training's l2: 0.00188006	valid_1's l2: 0.00200408
[43]	training's l2: 0.00187679	valid_1's l2: 0.00200158
[44]	training's l2: 0.00187191	valid_1's l2: 0.00199897
[45]	training's l2: 0.00186927	valid_1's l2: 0.0019968
[46]	training's l2: 0.00186597	valid_1's l2: 0.00199533
[47]	training's l2: 0.0018628	valid_1's l2: 0.00199327
[48]	training's l2: 0.00185864	valid_1's l2: 0.0019901
[49]	training's l2: 0.00185622	valid_1's l2: 0.00198916
[50]	training's l2: 0.00185342	valid_1's l2: 0.00198804
[51]	training's l2: 0.00185086	valid_1's l2: 0.00198679
[52]	training's l2: 0.00184815	valid_1's l2: 0.00198452
[53]	training's l2: 0.00184518	valid_1's l2: 0.00198182
[54]	training's l2: 0.00184266	valid_1's l2: 0.00198019
[55]	training's l2: 0.00184022	valid_1's l2: 0.0019788
[56]	training's l2: 0.00183656	valid_1's l2: 0.00197733
[57]	training's l2: 0.00183439	valid_1's l2: 0.00197607
[58]	training's l2: 0.00183187	valid_1's l2: 0.00197522
[59]	training's l2: 0.00182833	valid_1's l2: 0.00197312
[60]	training's l2: 0.0018258	valid_1's l2: 0.00197155
[61]	training's l2: 0.00182315	valid_1's l2: 0.00197106
[62]	training's l2: 0.00182079	valid_1's l2: 0.00197064
[63]	training's l2: 0.00181855	valid_1's l2: 0.00196886
[64]	training's l2: 0.00181617	valid_1's l2: 0.00196773
[65]	training's l2: 0.00181386	valid_1's l2: 0.00196764
[66]	training's l2: 0.00181158	valid_1's l2: 0.00196724
[67]	training's l2: 0.00180943	valid_1's l2: 0.00196698
[68]	training's l2: 0.00180743	valid_1's l2: 0.00196646
[69]	training's l2: 0.00180512	valid_1's l2: 0.00196539
[70]	training's l2: 0.001803	valid_1's l2: 0.00196474
[71]	training's l2: 0.00180024	valid_1's l2: 0.00196456
[72]	training's l2: 0.00179837	valid_1's l2: 0.00196381
[73]	training's l2: 0.00179597	valid_1's l2: 0.00196273
[74]	training's l2: 0.00179369	valid_1's l2: 0.00196174
[75]	training's l2: 0.00179162	valid_1's l2: 0.00196088
[76]	training's l2: 0.00178971	valid_1's l2: 0.00196008
[77]	training's l2: 0.00178791	valid_1's l2: 0.00195954
[78]	training's l2: 0.00178582	valid_1's l2: 0.00195882
[79]	training's l2: 0.00178333	valid_1's l2: 0.00195854
[80]	training's l2: 0.00178117	valid_1's l2: 0.00195819
[81]	training's l2: 0.00177847	valid_1's l2: 0.00195625
[82]	training's l2: 0.00177675	valid_1's l2: 0.0019561
[83]	training's l2: 0.00177501	valid_1's l2: 0.00195602
[84]	training's l2: 0.00177335	valid_1's l2: 0.00195534
[85]	training's l2: 0.00177152	valid_1's l2: 0.0019551
[86]	training's l2: 0.00176842	valid_1's l2: 0.00195335
[87]	training's l2: 0.00176653	valid_1's l2: 0.00195203
[88]	training's l2: 0.00176415	valid_1's l2: 0.00195051
[89]	training's l2: 0.00176236	valid_1's l2: 0.00195012
[90]	training's l2: 0.00176065	valid_1's l2: 0.00194907
[91]	training's l2: 0.00175874	valid_1's l2: 0.00194932
[92]	training's l2: 0.00175667	valid_1's l2: 0.0019486
[93]	training's l2: 0.00175504	valid_1's l2: 0.00194839
[94]	training's l2: 0.00175342	valid_1's l2: 0.00194759
[95]	training's l2: 0.00175172	valid_1's l2: 0.00194726
[96]	training's l2: 0.0017501	valid_1's l2: 0.00194716
[97]	training's l2: 0.0017485	valid_1's l2: 0.00194701
[98]	training's l2: 0.00174619	valid_1's l2: 0.00194593
[99]	training's l2: 0.00174472	valid_1's l2: 0.0019457
[100]	training's l2: 0.00174299	valid_1's l2: 0.00194506
[101]	training's l2: 0.00174138	valid_1's l2: 0.00194547
[102]	training's l2: 0.00173934	valid_1's l2: 0.00194526
[103]	training's l2: 0.00173773	valid_1's l2: 0.00194503
[104]	training's l2: 0.00173596	valid_1's l2: 0.00194438
[105]	training's l2: 0.00173456	valid_1's l2: 0.00194338
[106]	training's l2: 0.00173315	valid_1's l2: 0.00194353
[107]	training's l2: 0.00173159	valid_1's l2: 0.00194317
[108]	training's l2: 0.00172997	valid_1's l2: 0.001943
[109]	training's l2: 0.00172823	valid_1's l2: 0.00194248
[110]	training's l2: 0.00172687	valid_1's l2: 0.00194187
[111]	training's l2: 0.00172518	valid_1's l2: 0.00194227
[112]	training's l2: 0.00172362	valid_1's l2: 0.00194184
[113]	training's l2: 0.00172181	valid_1's l2: 0.00194111
[114]	training's l2: 0.00172038	valid_1's l2: 0.00194136
[115]	training's l2: 0.00171886	valid_1's l2: 0.00194112
[116]	training's l2: 0.00171718	valid_1's l2: 0.00194056
[117]	training's l2: 0.00171568	valid_1's l2: 0.00194095
[118]	training's l2: 0.00171408	valid_1's l2: 0.00194087
[119]	training's l2: 0.00171246	valid_1's l2: 0.00194052
[120]	training's l2: 0.00171111	valid_1's l2: 0.00194066
[121]	training's l2: 0.00170976	valid_1's l2: 0.00194003
[122]	training's l2: 0.00170773	valid_1's l2: 0.00193828
[123]	training's l2: 0.00170626	valid_1's l2: 0.00193779
[124]	training's l2: 0.00170467	valid_1's l2: 0.00193759
[125]	training's l2: 0.00170329	valid_1's l2: 0.00193798
[126]	training's l2: 0.00170176	valid_1's l2: 0.00193799
[127]	training's l2: 0.00170037	valid_1's l2: 0.00193771
[128]	training's l2: 0.00169865	valid_1's l2: 0.00193738
[129]	training's l2: 0.0016974	valid_1's l2: 0.00193777
[130]	training's l2: 0.00169574	valid_1's l2: 0.00193729
[131]	training's l2: 0.00169424	valid_1's l2: 0.00193698
[132]	training's l2: 0.00169289	valid_1's l2: 0.00193693
[133]	training's l2: 0.00169157	valid_1's l2: 0.00193681
[134]	training's l2: 0.00168999	valid_1's l2: 0.0019369
[135]	training's l2: 0.00168867	valid_1's l2: 0.00193709
[136]	training's l2: 0.00168738	valid_1's l2: 0.00193693
[137]	training's l2: 0.00168611	valid_1's l2: 0.00193648
[138]	training's l2: 0.00168485	valid_1's l2: 0.00193611
[139]	training's l2: 0.0016835	valid_1's l2: 0.00193616
[140]	training's l2: 0.00168193	valid_1's l2: 0.00193472
[141]	training's l2: 0.00168047	valid_1's l2: 0.00193459
[142]	training's l2: 0.00167909	valid_1's l2: 0.00193463
[143]	training's l2: 0.00167792	valid_1's l2: 0.00193454
[144]	training's l2: 0.00167659	valid_1's l2: 0.00193405
[145]	training's l2: 0.00167519	valid_1's l2: 0.00193394
[146]	training's l2: 0.00167377	valid_1's l2: 0.00193351
[147]	training's l2: 0.00167233	valid_1's l2: 0.00193312
[148]	training's l2: 0.0016711	valid_1's l2: 0.00193315
[149]	training's l2: 0.0016696	valid_1's l2: 0.00193293
[150]	training's l2: 0.00166809	valid_1's l2: 0.00193326
[151]	training's l2: 0.00166685	valid_1's l2: 0.00193321
[152]	training's l2: 0.00166576	valid_1's l2: 0.00193295
[153]	training's l2: 0.00166449	valid_1's l2: 0.00193242
[154]	training's l2: 0.00166311	valid_1's l2: 0.00193176
[155]	training's l2: 0.00166186	valid_1's l2: 0.00193166
[156]	training's l2: 0.00166069	valid_1's l2: 0.00193162
[157]	training's l2: 0.00165971	valid_1's l2: 0.00193165
[158]	training's l2: 0.00165877	valid_1's l2: 0.00193127
[159]	training's l2: 0.00165752	valid_1's l2: 0.00193117
[160]	training's l2: 0.00165635	valid_1's l2: 0.00193119
[161]	training's l2: 0.00165533	valid_1's l2: 0.00193078
[162]	training's l2: 0.00165402	valid_1's l2: 0.00193101
[163]	training's l2: 0.00165268	valid_1's l2: 0.00193079
[164]	training's l2: 0.00165117	valid_1's l2: 0.00193041
[165]	training's l2: 0.00165008	valid_1's l2: 0.00193065
[166]	training's l2: 0.00164885	valid_1's l2: 0.00193102
[167]	training's l2: 0.00164747	valid_1's l2: 0.0019309
[168]	training's l2: 0.00164656	valid_1's l2: 0.00193076
[169]	training's l2: 0.00164528	valid_1's l2: 0.00193085
[170]	training's l2: 0.0016441	valid_1's l2: 0.00193087
[171]	training's l2: 0.00164264	valid_1's l2: 0.00193097
[172]	training's l2: 0.00164163	valid_1's l2: 0.00193074
[173]	training's l2: 0.0016405	valid_1's l2: 0.00193069
[174]	training's l2: 0.0016392	valid_1's l2: 0.00193033
[175]	training's l2: 0.00163784	valid_1's l2: 0.00193072
[176]	training's l2: 0.00163651	valid_1's l2: 0.00193059
[177]	training's l2: 0.00163538	valid_1's l2: 0.00193044
Did not meet early stopping. Best iteration is:
[177]	training's l2: 0.00163538	valid_1's l2: 0.00193044
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215097 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.0023081	valid_1's l2: 0.00233254
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.002269	valid_1's l2: 0.00230067
[3]	training's l2: 0.00223382	valid_1's l2: 0.00227424
[4]	training's l2: 0.00219989	valid_1's l2: 0.00225045
[5]	training's l2: 0.00217069	valid_1's l2: 0.00222975
[6]	training's l2: 0.00214345	valid_1's l2: 0.00220934
[7]	training's l2: 0.00211772	valid_1's l2: 0.00219286
[8]	training's l2: 0.00209432	valid_1's l2: 0.00217765
[9]	training's l2: 0.00207265	valid_1's l2: 0.00216299
[10]	training's l2: 0.00205206	valid_1's l2: 0.00214913
[11]	training's l2: 0.00203236	valid_1's l2: 0.00213638
[12]	training's l2: 0.00201473	valid_1's l2: 0.00212618
[13]	training's l2: 0.00199784	valid_1's l2: 0.00211728
[14]	training's l2: 0.00198161	valid_1's l2: 0.00210785
[15]	training's l2: 0.00196732	valid_1's l2: 0.00209848
[16]	training's l2: 0.00195267	valid_1's l2: 0.00209025
[17]	training's l2: 0.00193929	valid_1's l2: 0.00208282
[18]	training's l2: 0.00192594	valid_1's l2: 0.00207582
[19]	training's l2: 0.00191362	valid_1's l2: 0.00206954
[20]	training's l2: 0.00190261	valid_1's l2: 0.00206404
[21]	training's l2: 0.0018913	valid_1's l2: 0.00205957
[22]	training's l2: 0.00188141	valid_1's l2: 0.00205369
[23]	training's l2: 0.00187118	valid_1's l2: 0.00204999
[24]	training's l2: 0.00186216	valid_1's l2: 0.0020451
[25]	training's l2: 0.00185302	valid_1's l2: 0.00204237
[26]	training's l2: 0.00184367	valid_1's l2: 0.00203894
[27]	training's l2: 0.00183525	valid_1's l2: 0.00203365
[28]	training's l2: 0.00182678	valid_1's l2: 0.00202991
[29]	training's l2: 0.001818	valid_1's l2: 0.00202619
[30]	training's l2: 0.00181033	valid_1's l2: 0.00202333
[31]	training's l2: 0.00180284	valid_1's l2: 0.00202118
[32]	training's l2: 0.00179513	valid_1's l2: 0.00201869
[33]	training's l2: 0.00178664	valid_1's l2: 0.0020156
[34]	training's l2: 0.0017794	valid_1's l2: 0.00201191
[35]	training's l2: 0.0017725	valid_1's l2: 0.00200977
[36]	training's l2: 0.00176499	valid_1's l2: 0.00200732
[37]	training's l2: 0.00175818	valid_1's l2: 0.00200438
[38]	training's l2: 0.00175208	valid_1's l2: 0.00200269
[39]	training's l2: 0.00174557	valid_1's l2: 0.00200026
[40]	training's l2: 0.00173912	valid_1's l2: 0.0019971
[41]	training's l2: 0.00173237	valid_1's l2: 0.00199531
[42]	training's l2: 0.00172669	valid_1's l2: 0.00199403
[43]	training's l2: 0.00172084	valid_1's l2: 0.00199254
[44]	training's l2: 0.00171416	valid_1's l2: 0.00199082
[45]	training's l2: 0.00170843	valid_1's l2: 0.00198838
[46]	training's l2: 0.001702	valid_1's l2: 0.00198744
[47]	training's l2: 0.00169664	valid_1's l2: 0.001985
[48]	training's l2: 0.00169135	valid_1's l2: 0.0019822
[49]	training's l2: 0.00168507	valid_1's l2: 0.00198074
[50]	training's l2: 0.00167966	valid_1's l2: 0.00197924
[51]	training's l2: 0.00167434	valid_1's l2: 0.00197804
[52]	training's l2: 0.00166809	valid_1's l2: 0.00197566
[53]	training's l2: 0.00166282	valid_1's l2: 0.00197454
[54]	training's l2: 0.00165753	valid_1's l2: 0.00197305
[55]	training's l2: 0.00165163	valid_1's l2: 0.00197069
[56]	training's l2: 0.00164656	valid_1's l2: 0.00197021
[57]	training's l2: 0.00164152	valid_1's l2: 0.00196905
[58]	training's l2: 0.00163576	valid_1's l2: 0.00196775
[59]	training's l2: 0.00163098	valid_1's l2: 0.00196614
[60]	training's l2: 0.00162584	valid_1's l2: 0.00196485
[61]	training's l2: 0.00162095	valid_1's l2: 0.00196433
[62]	training's l2: 0.00161591	valid_1's l2: 0.00196275
[63]	training's l2: 0.00161136	valid_1's l2: 0.00196124
[64]	training's l2: 0.00160683	valid_1's l2: 0.00195943
[65]	training's l2: 0.00160244	valid_1's l2: 0.00195804
[66]	training's l2: 0.00159766	valid_1's l2: 0.0019574
[67]	training's l2: 0.00159312	valid_1's l2: 0.00195603
[68]	training's l2: 0.00158846	valid_1's l2: 0.00195458
[69]	training's l2: 0.00158372	valid_1's l2: 0.00195323
[70]	training's l2: 0.0015795	valid_1's l2: 0.00195243
[71]	training's l2: 0.00157502	valid_1's l2: 0.00195159
[72]	training's l2: 0.00157048	valid_1's l2: 0.00195138
[73]	training's l2: 0.00156578	valid_1's l2: 0.00195003
[74]	training's l2: 0.00156125	valid_1's l2: 0.00194949
[75]	training's l2: 0.00155695	valid_1's l2: 0.00194895
[76]	training's l2: 0.00155268	valid_1's l2: 0.00194846
[77]	training's l2: 0.0015483	valid_1's l2: 0.00194721
[78]	training's l2: 0.00154415	valid_1's l2: 0.0019471
[79]	training's l2: 0.00153965	valid_1's l2: 0.00194687
[80]	training's l2: 0.00153556	valid_1's l2: 0.00194643
[81]	training's l2: 0.00153153	valid_1's l2: 0.00194581
[82]	training's l2: 0.00152753	valid_1's l2: 0.00194527
[83]	training's l2: 0.00152381	valid_1's l2: 0.00194509
[84]	training's l2: 0.00151902	valid_1's l2: 0.00194367
[85]	training's l2: 0.00151495	valid_1's l2: 0.00194367
[86]	training's l2: 0.00151124	valid_1's l2: 0.00194332
[87]	training's l2: 0.00150752	valid_1's l2: 0.00194256
[88]	training's l2: 0.00150378	valid_1's l2: 0.00194217
[89]	training's l2: 0.00149968	valid_1's l2: 0.00194183
[90]	training's l2: 0.00149543	valid_1's l2: 0.00194084
[91]	training's l2: 0.00149157	valid_1's l2: 0.00194121
[92]	training's l2: 0.00148787	valid_1's l2: 0.00194135
[93]	training's l2: 0.00148416	valid_1's l2: 0.00194099
[94]	training's l2: 0.00148051	valid_1's l2: 0.00194064
[95]	training's l2: 0.00147693	valid_1's l2: 0.00194007
[96]	training's l2: 0.00147316	valid_1's l2: 0.00193927
[97]	training's l2: 0.00146892	valid_1's l2: 0.00193804
[98]	training's l2: 0.00146504	valid_1's l2: 0.00193782
[99]	training's l2: 0.00146126	valid_1's l2: 0.001937
[100]	training's l2: 0.00145787	valid_1's l2: 0.00193638
[101]	training's l2: 0.00145448	valid_1's l2: 0.00193644
[102]	training's l2: 0.00145071	valid_1's l2: 0.00193656
[103]	training's l2: 0.00144724	valid_1's l2: 0.00193617
[104]	training's l2: 0.00144323	valid_1's l2: 0.00193531
[105]	training's l2: 0.00143961	valid_1's l2: 0.00193459
[106]	training's l2: 0.00143595	valid_1's l2: 0.00193388
[107]	training's l2: 0.00143253	valid_1's l2: 0.00193386
[108]	training's l2: 0.00142877	valid_1's l2: 0.00193372
[109]	training's l2: 0.00142512	valid_1's l2: 0.00193326
[110]	training's l2: 0.00142176	valid_1's l2: 0.00193268
[111]	training's l2: 0.00141831	valid_1's l2: 0.00193226
[112]	training's l2: 0.00141464	valid_1's l2: 0.00193209
[113]	training's l2: 0.00141145	valid_1's l2: 0.00193222
[114]	training's l2: 0.00140775	valid_1's l2: 0.00193147
[115]	training's l2: 0.00140412	valid_1's l2: 0.00193077
[116]	training's l2: 0.00140064	valid_1's l2: 0.00193073
[117]	training's l2: 0.00139735	valid_1's l2: 0.00193093
[118]	training's l2: 0.00139435	valid_1's l2: 0.0019305
[119]	training's l2: 0.00139094	valid_1's l2: 0.0019297
[120]	training's l2: 0.00138771	valid_1's l2: 0.00192943
[121]	training's l2: 0.00138405	valid_1's l2: 0.0019294
[122]	training's l2: 0.00138082	valid_1's l2: 0.00192943
[123]	training's l2: 0.00137741	valid_1's l2: 0.00192945
[124]	training's l2: 0.00137426	valid_1's l2: 0.00192959
[125]	training's l2: 0.00137099	valid_1's l2: 0.00192948
[126]	training's l2: 0.00136753	valid_1's l2: 0.00192878
[127]	training's l2: 0.00136442	valid_1's l2: 0.00192863
[128]	training's l2: 0.00136153	valid_1's l2: 0.0019287
[129]	training's l2: 0.00135848	valid_1's l2: 0.0019286
[130]	training's l2: 0.001355	valid_1's l2: 0.00192837
[131]	training's l2: 0.00135172	valid_1's l2: 0.00192863
[132]	training's l2: 0.00134867	valid_1's l2: 0.00192809
[133]	training's l2: 0.0013454	valid_1's l2: 0.00192791
[134]	training's l2: 0.00134202	valid_1's l2: 0.00192731
[135]	training's l2: 0.00133906	valid_1's l2: 0.00192757
[136]	training's l2: 0.00133595	valid_1's l2: 0.00192749
[137]	training's l2: 0.00133265	valid_1's l2: 0.00192704
[138]	training's l2: 0.00132974	valid_1's l2: 0.00192714
[139]	training's l2: 0.00132665	valid_1's l2: 0.00192669
[140]	training's l2: 0.00132338	valid_1's l2: 0.00192648
[141]	training's l2: 0.00132058	valid_1's l2: 0.00192647
[142]	training's l2: 0.00131731	valid_1's l2: 0.00192598
[143]	training's l2: 0.00131449	valid_1's l2: 0.00192545
[144]	training's l2: 0.00131152	valid_1's l2: 0.00192542
[145]	training's l2: 0.0013084	valid_1's l2: 0.00192509
[146]	training's l2: 0.00130566	valid_1's l2: 0.0019252
[147]	training's l2: 0.00130288	valid_1's l2: 0.00192527
[148]	training's l2: 0.00129992	valid_1's l2: 0.00192506
[149]	training's l2: 0.00129727	valid_1's l2: 0.00192533
[150]	training's l2: 0.0012945	valid_1's l2: 0.00192519
[151]	training's l2: 0.00129174	valid_1's l2: 0.00192506
[152]	training's l2: 0.00128888	valid_1's l2: 0.00192499
[153]	training's l2: 0.0012861	valid_1's l2: 0.00192467
[154]	training's l2: 0.00128333	valid_1's l2: 0.00192414
[155]	training's l2: 0.00128057	valid_1's l2: 0.00192387
[156]	training's l2: 0.00127766	valid_1's l2: 0.00192402
[157]	training's l2: 0.00127476	valid_1's l2: 0.0019238
[158]	training's l2: 0.0012723	valid_1's l2: 0.00192382
[159]	training's l2: 0.00126931	valid_1's l2: 0.00192397
[160]	training's l2: 0.00126652	valid_1's l2: 0.00192449
[161]	training's l2: 0.00126414	valid_1's l2: 0.00192437
[162]	training's l2: 0.00126138	valid_1's l2: 0.00192491
[163]	training's l2: 0.00125864	valid_1's l2: 0.00192534
[164]	training's l2: 0.00125597	valid_1's l2: 0.00192517
[165]	training's l2: 0.0012533	valid_1's l2: 0.00192555
[166]	training's l2: 0.00125073	valid_1's l2: 0.00192568
[167]	training's l2: 0.00124815	valid_1's l2: 0.00192521
[168]	training's l2: 0.00124543	valid_1's l2: 0.00192456
[169]	training's l2: 0.0012429	valid_1's l2: 0.00192441
[170]	training's l2: 0.00124024	valid_1's l2: 0.00192419
[171]	training's l2: 0.0012374	valid_1's l2: 0.00192401
[172]	training's l2: 0.00123454	valid_1's l2: 0.00192479
[173]	training's l2: 0.00123203	valid_1's l2: 0.00192454
[174]	training's l2: 0.00122917	valid_1's l2: 0.00192484
[175]	training's l2: 0.00122658	valid_1's l2: 0.00192498
[176]	training's l2: 0.00122405	valid_1's l2: 0.00192505
[177]	training's l2: 0.00122147	valid_1's l2: 0.00192522
[178]	training's l2: 0.00121886	valid_1's l2: 0.00192499
[179]	training's l2: 0.00121646	valid_1's l2: 0.00192511
[180]	training's l2: 0.00121401	valid_1's l2: 0.00192523
[181]	training's l2: 0.00121185	valid_1's l2: 0.00192551
[182]	training's l2: 0.00120935	valid_1's l2: 0.00192547
[183]	training's l2: 0.00120712	valid_1's l2: 0.00192542
[184]	training's l2: 0.00120473	valid_1's l2: 0.00192557
[185]	training's l2: 0.00120222	valid_1's l2: 0.00192588
Did not meet early stopping. Best iteration is:
[185]	training's l2: 0.00120222	valid_1's l2: 0.00192588
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.230374 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230698	valid_1's l2: 0.00233058
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00226921	valid_1's l2: 0.00229949
[3]	training's l2: 0.00223514	valid_1's l2: 0.00227242
[4]	training's l2: 0.00220546	valid_1's l2: 0.00224694
[5]	training's l2: 0.00217729	valid_1's l2: 0.00222471
[6]	training's l2: 0.00215415	valid_1's l2: 0.00220722
[7]	training's l2: 0.00213135	valid_1's l2: 0.0021913
[8]	training's l2: 0.00211271	valid_1's l2: 0.00217604
[9]	training's l2: 0.00209535	valid_1's l2: 0.00216403
[10]	training's l2: 0.00207762	valid_1's l2: 0.00215056
[11]	training's l2: 0.00206194	valid_1's l2: 0.00213917
[12]	training's l2: 0.00204837	valid_1's l2: 0.00212694
[13]	training's l2: 0.00203479	valid_1's l2: 0.00211705
[14]	training's l2: 0.00202379	valid_1's l2: 0.00210919
[15]	training's l2: 0.00201281	valid_1's l2: 0.00210363
[16]	training's l2: 0.00200396	valid_1's l2: 0.00210025
[17]	training's l2: 0.00199446	valid_1's l2: 0.00209439
[18]	training's l2: 0.00198538	valid_1's l2: 0.0020872
[19]	training's l2: 0.00197789	valid_1's l2: 0.00208137
[20]	training's l2: 0.0019707	valid_1's l2: 0.00207488
[21]	training's l2: 0.00196302	valid_1's l2: 0.00207101
[22]	training's l2: 0.00195615	valid_1's l2: 0.00206657
[23]	training's l2: 0.00194889	valid_1's l2: 0.00206288
[24]	training's l2: 0.00194141	valid_1's l2: 0.00205966
[25]	training's l2: 0.00193494	valid_1's l2: 0.00205613
[26]	training's l2: 0.00192923	valid_1's l2: 0.0020526
[27]	training's l2: 0.00192379	valid_1's l2: 0.00205032
[28]	training's l2: 0.00191732	valid_1's l2: 0.00204646
[29]	training's l2: 0.00191207	valid_1's l2: 0.00204435
[30]	training's l2: 0.001906	valid_1's l2: 0.00203998
[31]	training's l2: 0.00190129	valid_1's l2: 0.00203673
[32]	training's l2: 0.00189642	valid_1's l2: 0.0020318
[33]	training's l2: 0.00189207	valid_1's l2: 0.00203011
[34]	training's l2: 0.00188785	valid_1's l2: 0.0020285
[35]	training's l2: 0.00188332	valid_1's l2: 0.00202442
[36]	training's l2: 0.00187689	valid_1's l2: 0.00202105
[37]	training's l2: 0.00187279	valid_1's l2: 0.0020168
[38]	training's l2: 0.00186899	valid_1's l2: 0.00201335
[39]	training's l2: 0.00186323	valid_1's l2: 0.00201049
[40]	training's l2: 0.00185945	valid_1's l2: 0.00200958
[41]	training's l2: 0.00185594	valid_1's l2: 0.00200816
[42]	training's l2: 0.00185176	valid_1's l2: 0.00200541
[43]	training's l2: 0.00184636	valid_1's l2: 0.00200116
[44]	training's l2: 0.00184274	valid_1's l2: 0.00200032
[45]	training's l2: 0.00183842	valid_1's l2: 0.00199826
[46]	training's l2: 0.00183505	valid_1's l2: 0.00199675
[47]	training's l2: 0.00183165	valid_1's l2: 0.00199506
[48]	training's l2: 0.00182823	valid_1's l2: 0.00199195
[49]	training's l2: 0.00182498	valid_1's l2: 0.00198973
[50]	training's l2: 0.00182045	valid_1's l2: 0.00198748
[51]	training's l2: 0.00181756	valid_1's l2: 0.00198703
[52]	training's l2: 0.00181457	valid_1's l2: 0.00198596
[53]	training's l2: 0.00181144	valid_1's l2: 0.00198406
[54]	training's l2: 0.0018085	valid_1's l2: 0.00198251
[55]	training's l2: 0.00180454	valid_1's l2: 0.00198082
[56]	training's l2: 0.00180117	valid_1's l2: 0.00197952
[57]	training's l2: 0.00179793	valid_1's l2: 0.00197832
[58]	training's l2: 0.00179485	valid_1's l2: 0.00197668
[59]	training's l2: 0.0017924	valid_1's l2: 0.00197602
[60]	training's l2: 0.00178948	valid_1's l2: 0.00197476
[61]	training's l2: 0.00178673	valid_1's l2: 0.00197387
[62]	training's l2: 0.00178394	valid_1's l2: 0.00197343
[63]	training's l2: 0.00178157	valid_1's l2: 0.00197311
[64]	training's l2: 0.0017784	valid_1's l2: 0.00197157
[65]	training's l2: 0.00177535	valid_1's l2: 0.00196991
[66]	training's l2: 0.00177257	valid_1's l2: 0.00196946
[67]	training's l2: 0.00177014	valid_1's l2: 0.00196933
[68]	training's l2: 0.00176763	valid_1's l2: 0.00196831
[69]	training's l2: 0.0017653	valid_1's l2: 0.00196742
[70]	training's l2: 0.00176268	valid_1's l2: 0.00196703
[71]	training's l2: 0.00176019	valid_1's l2: 0.00196662
[72]	training's l2: 0.00175809	valid_1's l2: 0.00196597
[73]	training's l2: 0.00175566	valid_1's l2: 0.00196555
[74]	training's l2: 0.00175301	valid_1's l2: 0.00196607
[75]	training's l2: 0.00175065	valid_1's l2: 0.00196548
[76]	training's l2: 0.00174834	valid_1's l2: 0.00196478
[77]	training's l2: 0.00174614	valid_1's l2: 0.0019641
[78]	training's l2: 0.00174316	valid_1's l2: 0.00196297
[79]	training's l2: 0.00174105	valid_1's l2: 0.00196182
[80]	training's l2: 0.00173864	valid_1's l2: 0.00196128
[81]	training's l2: 0.00173642	valid_1's l2: 0.00196111
[82]	training's l2: 0.00173421	valid_1's l2: 0.00196145
[83]	training's l2: 0.00173197	valid_1's l2: 0.00196053
[84]	training's l2: 0.00172957	valid_1's l2: 0.00195921
[85]	training's l2: 0.00172762	valid_1's l2: 0.00195869
[86]	training's l2: 0.00172525	valid_1's l2: 0.00195805
[87]	training's l2: 0.00172332	valid_1's l2: 0.0019572
[88]	training's l2: 0.0017211	valid_1's l2: 0.00195715
[89]	training's l2: 0.00171786	valid_1's l2: 0.00195621
[90]	training's l2: 0.00171589	valid_1's l2: 0.0019562
[91]	training's l2: 0.00171397	valid_1's l2: 0.00195534
[92]	training's l2: 0.00171178	valid_1's l2: 0.00195444
[93]	training's l2: 0.00170951	valid_1's l2: 0.00195352
[94]	training's l2: 0.00170735	valid_1's l2: 0.00195261
[95]	training's l2: 0.00170441	valid_1's l2: 0.00195182
[96]	training's l2: 0.00170247	valid_1's l2: 0.0019514
[97]	training's l2: 0.00170051	valid_1's l2: 0.00195072
[98]	training's l2: 0.00169818	valid_1's l2: 0.00195089
[99]	training's l2: 0.0016961	valid_1's l2: 0.0019505
[100]	training's l2: 0.00169367	valid_1's l2: 0.00194905
[101]	training's l2: 0.00169168	valid_1's l2: 0.00194895
[102]	training's l2: 0.00168956	valid_1's l2: 0.00194873
[103]	training's l2: 0.00168756	valid_1's l2: 0.00194866
[104]	training's l2: 0.0016857	valid_1's l2: 0.00194843
[105]	training's l2: 0.0016839	valid_1's l2: 0.00194868
[106]	training's l2: 0.00168223	valid_1's l2: 0.00194784
[107]	training's l2: 0.00168027	valid_1's l2: 0.00194786
[108]	training's l2: 0.00167859	valid_1's l2: 0.0019479
[109]	training's l2: 0.00167669	valid_1's l2: 0.00194811
[110]	training's l2: 0.00167481	valid_1's l2: 0.00194745
[111]	training's l2: 0.00167282	valid_1's l2: 0.00194741
[112]	training's l2: 0.00167086	valid_1's l2: 0.00194723
[113]	training's l2: 0.00166912	valid_1's l2: 0.00194685
[114]	training's l2: 0.00166742	valid_1's l2: 0.00194654
[115]	training's l2: 0.00166563	valid_1's l2: 0.00194644
[116]	training's l2: 0.00166373	valid_1's l2: 0.00194679
[117]	training's l2: 0.00166197	valid_1's l2: 0.00194643
[118]	training's l2: 0.00166004	valid_1's l2: 0.00194605
[119]	training's l2: 0.00165825	valid_1's l2: 0.00194583
[120]	training's l2: 0.00165643	valid_1's l2: 0.00194571
[121]	training's l2: 0.00165482	valid_1's l2: 0.00194601
[122]	training's l2: 0.00165282	valid_1's l2: 0.00194523
[123]	training's l2: 0.00165095	valid_1's l2: 0.00194519
[124]	training's l2: 0.0016493	valid_1's l2: 0.0019452
[125]	training's l2: 0.00164748	valid_1's l2: 0.00194517
[126]	training's l2: 0.00164578	valid_1's l2: 0.00194547
[127]	training's l2: 0.00164405	valid_1's l2: 0.00194532
[128]	training's l2: 0.00164253	valid_1's l2: 0.00194546
[129]	training's l2: 0.00164092	valid_1's l2: 0.00194534
[130]	training's l2: 0.00163919	valid_1's l2: 0.00194471
[131]	training's l2: 0.00163765	valid_1's l2: 0.00194427
[132]	training's l2: 0.001636	valid_1's l2: 0.00194359
[133]	training's l2: 0.00163429	valid_1's l2: 0.00194326
[134]	training's l2: 0.00163295	valid_1's l2: 0.00194318
[135]	training's l2: 0.00163148	valid_1's l2: 0.00194314
[136]	training's l2: 0.00162989	valid_1's l2: 0.00194302
[137]	training's l2: 0.00162821	valid_1's l2: 0.00194287
[138]	training's l2: 0.00162664	valid_1's l2: 0.00194268
[139]	training's l2: 0.00162518	valid_1's l2: 0.00194233
[140]	training's l2: 0.00162366	valid_1's l2: 0.00194232
[141]	training's l2: 0.00162217	valid_1's l2: 0.00194182
[142]	training's l2: 0.00162054	valid_1's l2: 0.00194187
[143]	training's l2: 0.00161901	valid_1's l2: 0.00194235
[144]	training's l2: 0.00161734	valid_1's l2: 0.00194207
[145]	training's l2: 0.00161572	valid_1's l2: 0.00194182
[146]	training's l2: 0.00161419	valid_1's l2: 0.00194167
[147]	training's l2: 0.00161284	valid_1's l2: 0.00194111
[148]	training's l2: 0.00161131	valid_1's l2: 0.00194138
[149]	training's l2: 0.0016098	valid_1's l2: 0.00194155
[150]	training's l2: 0.00160833	valid_1's l2: 0.00194143
[151]	training's l2: 0.00160692	valid_1's l2: 0.00194137
[152]	training's l2: 0.00160515	valid_1's l2: 0.00194088
[153]	training's l2: 0.0016037	valid_1's l2: 0.00194087
[154]	training's l2: 0.00160219	valid_1's l2: 0.00194025
[155]	training's l2: 0.0016007	valid_1's l2: 0.00194005
[156]	training's l2: 0.00159901	valid_1's l2: 0.00193976
[157]	training's l2: 0.0015973	valid_1's l2: 0.00193906
[158]	training's l2: 0.00159588	valid_1's l2: 0.00193948
[159]	training's l2: 0.00159464	valid_1's l2: 0.00193953
[160]	training's l2: 0.00159336	valid_1's l2: 0.0019393
[161]	training's l2: 0.0015919	valid_1's l2: 0.00193902
[162]	training's l2: 0.00159041	valid_1's l2: 0.00193897
[163]	training's l2: 0.00158885	valid_1's l2: 0.00193896
[164]	training's l2: 0.00158733	valid_1's l2: 0.00193944
[165]	training's l2: 0.00158582	valid_1's l2: 0.00193938
[166]	training's l2: 0.00158429	valid_1's l2: 0.00193939
[167]	training's l2: 0.00158275	valid_1's l2: 0.00193913
[168]	training's l2: 0.00158136	valid_1's l2: 0.00193927
[169]	training's l2: 0.00157989	valid_1's l2: 0.00193931
[170]	training's l2: 0.00157859	valid_1's l2: 0.00193845
[171]	training's l2: 0.00157732	valid_1's l2: 0.00193858
[172]	training's l2: 0.00157567	valid_1's l2: 0.00193845
[173]	training's l2: 0.00157425	valid_1's l2: 0.0019382
[174]	training's l2: 0.00157303	valid_1's l2: 0.00193822
[175]	training's l2: 0.00157164	valid_1's l2: 0.00193829
[176]	training's l2: 0.00157052	valid_1's l2: 0.0019386
[177]	training's l2: 0.00156906	valid_1's l2: 0.00193798
[178]	training's l2: 0.00156788	valid_1's l2: 0.00193772
[179]	training's l2: 0.00156667	valid_1's l2: 0.00193751
[180]	training's l2: 0.00156546	valid_1's l2: 0.00193722
[181]	training's l2: 0.00156405	valid_1's l2: 0.00193684
[182]	training's l2: 0.00156288	valid_1's l2: 0.0019367
[183]	training's l2: 0.00156152	valid_1's l2: 0.00193664
[184]	training's l2: 0.00155993	valid_1's l2: 0.00193632
[185]	training's l2: 0.0015584	valid_1's l2: 0.0019365
[186]	training's l2: 0.00155679	valid_1's l2: 0.00193631
[187]	training's l2: 0.0015555	valid_1's l2: 0.0019362
[188]	training's l2: 0.00155431	valid_1's l2: 0.00193631
[189]	training's l2: 0.00155306	valid_1's l2: 0.00193615
[190]	training's l2: 0.0015515	valid_1's l2: 0.00193571
[191]	training's l2: 0.00155002	valid_1's l2: 0.00193539
[192]	training's l2: 0.00154853	valid_1's l2: 0.00193553
[193]	training's l2: 0.00154727	valid_1's l2: 0.00193536
Did not meet early stopping. Best iteration is:
[193]	training's l2: 0.00154727	valid_1's l2: 0.00193536
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.236823 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00231203	valid_1's l2: 0.00233532
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00227612	valid_1's l2: 0.00230572
[3]	training's l2: 0.00224361	valid_1's l2: 0.00228106
[4]	training's l2: 0.00221213	valid_1's l2: 0.00225739
[5]	training's l2: 0.00218452	valid_1's l2: 0.00223542
[6]	training's l2: 0.00215934	valid_1's l2: 0.00221754
[7]	training's l2: 0.00213497	valid_1's l2: 0.00220143
[8]	training's l2: 0.00211293	valid_1's l2: 0.00218701
[9]	training's l2: 0.00209165	valid_1's l2: 0.00217181
[10]	training's l2: 0.00207134	valid_1's l2: 0.00215966
[11]	training's l2: 0.00205319	valid_1's l2: 0.00214824
[12]	training's l2: 0.00203637	valid_1's l2: 0.00213739
[13]	training's l2: 0.0020195	valid_1's l2: 0.00212783
[14]	training's l2: 0.00200368	valid_1's l2: 0.00211866
[15]	training's l2: 0.00198939	valid_1's l2: 0.00211039
[16]	training's l2: 0.0019753	valid_1's l2: 0.00210307
[17]	training's l2: 0.00196321	valid_1's l2: 0.00209499
[18]	training's l2: 0.00195162	valid_1's l2: 0.00208758
[19]	training's l2: 0.00193954	valid_1's l2: 0.00208102
[20]	training's l2: 0.00192911	valid_1's l2: 0.00207661
[21]	training's l2: 0.00191783	valid_1's l2: 0.00207061
[22]	training's l2: 0.00190699	valid_1's l2: 0.00206571
[23]	training's l2: 0.00189707	valid_1's l2: 0.00206093
[24]	training's l2: 0.0018883	valid_1's l2: 0.00205552
[25]	training's l2: 0.0018793	valid_1's l2: 0.00205169
[26]	training's l2: 0.00187053	valid_1's l2: 0.002047
[27]	training's l2: 0.00186234	valid_1's l2: 0.00204275
[28]	training's l2: 0.0018549	valid_1's l2: 0.00203861
[29]	training's l2: 0.00184694	valid_1's l2: 0.00203533
[30]	training's l2: 0.00183979	valid_1's l2: 0.00203272
[31]	training's l2: 0.00183254	valid_1's l2: 0.00203013
[32]	training's l2: 0.00182523	valid_1's l2: 0.00202667
[33]	training's l2: 0.00181761	valid_1's l2: 0.0020237
[34]	training's l2: 0.00181103	valid_1's l2: 0.00202144
[35]	training's l2: 0.00180331	valid_1's l2: 0.00201803
[36]	training's l2: 0.00179708	valid_1's l2: 0.00201618
[37]	training's l2: 0.00179074	valid_1's l2: 0.0020144
[38]	training's l2: 0.00178434	valid_1's l2: 0.0020119
[39]	training's l2: 0.00177831	valid_1's l2: 0.00200924
[40]	training's l2: 0.00177234	valid_1's l2: 0.0020053
[41]	training's l2: 0.00176641	valid_1's l2: 0.00200303
[42]	training's l2: 0.00176062	valid_1's l2: 0.00200048
[43]	training's l2: 0.00175478	valid_1's l2: 0.0019988
[44]	training's l2: 0.00174816	valid_1's l2: 0.00199625
[45]	training's l2: 0.00174271	valid_1's l2: 0.00199465
[46]	training's l2: 0.00173705	valid_1's l2: 0.00199299
[47]	training's l2: 0.00173223	valid_1's l2: 0.00199177
[48]	training's l2: 0.00172734	valid_1's l2: 0.00199051
[49]	training's l2: 0.00172212	valid_1's l2: 0.00198843
[50]	training's l2: 0.00171689	valid_1's l2: 0.00198668
[51]	training's l2: 0.00171192	valid_1's l2: 0.00198419
[52]	training's l2: 0.00170575	valid_1's l2: 0.00198215
[53]	training's l2: 0.00170079	valid_1's l2: 0.0019806
[54]	training's l2: 0.00169544	valid_1's l2: 0.00197951
[55]	training's l2: 0.00169073	valid_1's l2: 0.00197806
[56]	training's l2: 0.00168525	valid_1's l2: 0.00197606
[57]	training's l2: 0.00168053	valid_1's l2: 0.00197367
[58]	training's l2: 0.00167598	valid_1's l2: 0.00197361
[59]	training's l2: 0.00167029	valid_1's l2: 0.00197172
[60]	training's l2: 0.00166562	valid_1's l2: 0.00196997
[61]	training's l2: 0.00166042	valid_1's l2: 0.0019683
[62]	training's l2: 0.00165623	valid_1's l2: 0.00196767
[63]	training's l2: 0.00165178	valid_1's l2: 0.00196667
[64]	training's l2: 0.00164683	valid_1's l2: 0.00196541
[65]	training's l2: 0.00164215	valid_1's l2: 0.00196412
[66]	training's l2: 0.00163779	valid_1's l2: 0.00196338
[67]	training's l2: 0.00163324	valid_1's l2: 0.00196319
[68]	training's l2: 0.00162916	valid_1's l2: 0.00196299
[69]	training's l2: 0.00162485	valid_1's l2: 0.00196227
[70]	training's l2: 0.00162102	valid_1's l2: 0.00196175
[71]	training's l2: 0.00161725	valid_1's l2: 0.00196052
[72]	training's l2: 0.00161302	valid_1's l2: 0.00195899
[73]	training's l2: 0.00160883	valid_1's l2: 0.00195859
[74]	training's l2: 0.0016048	valid_1's l2: 0.00195762
[75]	training's l2: 0.00160052	valid_1's l2: 0.00195682
[76]	training's l2: 0.00159653	valid_1's l2: 0.00195621
[77]	training's l2: 0.00159247	valid_1's l2: 0.00195668
[78]	training's l2: 0.00158851	valid_1's l2: 0.00195609
[79]	training's l2: 0.00158459	valid_1's l2: 0.00195553
[80]	training's l2: 0.0015808	valid_1's l2: 0.00195475
[81]	training's l2: 0.00157675	valid_1's l2: 0.00195457
[82]	training's l2: 0.00157287	valid_1's l2: 0.00195364
[83]	training's l2: 0.00156906	valid_1's l2: 0.00195319
[84]	training's l2: 0.00156536	valid_1's l2: 0.00195284
[85]	training's l2: 0.00156161	valid_1's l2: 0.0019522
[86]	training's l2: 0.00155787	valid_1's l2: 0.0019516
[87]	training's l2: 0.00155405	valid_1's l2: 0.00195037
[88]	training's l2: 0.00155038	valid_1's l2: 0.00195063
[89]	training's l2: 0.00154668	valid_1's l2: 0.00195008
[90]	training's l2: 0.00154301	valid_1's l2: 0.00194886
[91]	training's l2: 0.00153947	valid_1's l2: 0.00194873
[92]	training's l2: 0.00153584	valid_1's l2: 0.00194838
[93]	training's l2: 0.00153224	valid_1's l2: 0.0019477
[94]	training's l2: 0.00152864	valid_1's l2: 0.00194684
[95]	training's l2: 0.00152494	valid_1's l2: 0.00194589
[96]	training's l2: 0.00152147	valid_1's l2: 0.001946
[97]	training's l2: 0.00151765	valid_1's l2: 0.00194514
[98]	training's l2: 0.00151359	valid_1's l2: 0.00194486
[99]	training's l2: 0.00151032	valid_1's l2: 0.00194447
[100]	training's l2: 0.00150696	valid_1's l2: 0.00194323
[101]	training's l2: 0.00150305	valid_1's l2: 0.00194227
[102]	training's l2: 0.00149965	valid_1's l2: 0.00194229
[103]	training's l2: 0.00149599	valid_1's l2: 0.0019423
[104]	training's l2: 0.00149279	valid_1's l2: 0.00194257
[105]	training's l2: 0.00148948	valid_1's l2: 0.00194265
[106]	training's l2: 0.00148618	valid_1's l2: 0.00194222
[107]	training's l2: 0.0014827	valid_1's l2: 0.00194155
[108]	training's l2: 0.00147954	valid_1's l2: 0.00194098
[109]	training's l2: 0.00147588	valid_1's l2: 0.00193997
[110]	training's l2: 0.00147257	valid_1's l2: 0.0019396
[111]	training's l2: 0.00146937	valid_1's l2: 0.00193923
[112]	training's l2: 0.001466	valid_1's l2: 0.0019391
[113]	training's l2: 0.00146285	valid_1's l2: 0.00193902
[114]	training's l2: 0.00145949	valid_1's l2: 0.00193897
[115]	training's l2: 0.00145605	valid_1's l2: 0.00193831
[116]	training's l2: 0.00145278	valid_1's l2: 0.00193802
[117]	training's l2: 0.00144952	valid_1's l2: 0.00193814
[118]	training's l2: 0.00144655	valid_1's l2: 0.00193802
[119]	training's l2: 0.00144336	valid_1's l2: 0.00193737
[120]	training's l2: 0.00144028	valid_1's l2: 0.00193716
[121]	training's l2: 0.00143725	valid_1's l2: 0.00193698
[122]	training's l2: 0.00143403	valid_1's l2: 0.00193725
[123]	training's l2: 0.00143064	valid_1's l2: 0.00193717
[124]	training's l2: 0.00142754	valid_1's l2: 0.00193674
[125]	training's l2: 0.00142449	valid_1's l2: 0.00193647
[126]	training's l2: 0.0014216	valid_1's l2: 0.00193639
[127]	training's l2: 0.00141835	valid_1's l2: 0.00193639
[128]	training's l2: 0.00141506	valid_1's l2: 0.00193602
[129]	training's l2: 0.00141192	valid_1's l2: 0.00193571
[130]	training's l2: 0.00140892	valid_1's l2: 0.00193542
[131]	training's l2: 0.00140593	valid_1's l2: 0.00193587
[132]	training's l2: 0.00140309	valid_1's l2: 0.001936
[133]	training's l2: 0.00140013	valid_1's l2: 0.00193548
[134]	training's l2: 0.00139721	valid_1's l2: 0.00193486
[135]	training's l2: 0.00139436	valid_1's l2: 0.00193478
[136]	training's l2: 0.00139146	valid_1's l2: 0.00193485
[137]	training's l2: 0.00138854	valid_1's l2: 0.00193482
[138]	training's l2: 0.00138547	valid_1's l2: 0.00193426
[139]	training's l2: 0.00138274	valid_1's l2: 0.00193396
[140]	training's l2: 0.00137984	valid_1's l2: 0.00193353
[141]	training's l2: 0.00137721	valid_1's l2: 0.00193353
[142]	training's l2: 0.00137431	valid_1's l2: 0.00193352
[143]	training's l2: 0.00137158	valid_1's l2: 0.00193307
[144]	training's l2: 0.00136881	valid_1's l2: 0.00193295
[145]	training's l2: 0.00136594	valid_1's l2: 0.00193221
[146]	training's l2: 0.00136326	valid_1's l2: 0.00193241
[147]	training's l2: 0.00136032	valid_1's l2: 0.00193171
[148]	training's l2: 0.00135763	valid_1's l2: 0.00193147
[149]	training's l2: 0.00135481	valid_1's l2: 0.00193124
[150]	training's l2: 0.00135211	valid_1's l2: 0.00193123
[151]	training's l2: 0.00134938	valid_1's l2: 0.00193164
[152]	training's l2: 0.00134661	valid_1's l2: 0.00193181
[153]	training's l2: 0.0013439	valid_1's l2: 0.0019315
[154]	training's l2: 0.0013412	valid_1's l2: 0.00193079
[155]	training's l2: 0.00133845	valid_1's l2: 0.00193042
[156]	training's l2: 0.00133588	valid_1's l2: 0.00193008
[157]	training's l2: 0.00133303	valid_1's l2: 0.0019298
[158]	training's l2: 0.00133041	valid_1's l2: 0.00192957
[159]	training's l2: 0.00132782	valid_1's l2: 0.00192906
[160]	training's l2: 0.00132505	valid_1's l2: 0.00192879
[161]	training's l2: 0.00132231	valid_1's l2: 0.00192886
[162]	training's l2: 0.00131973	valid_1's l2: 0.00192863
[163]	training's l2: 0.0013172	valid_1's l2: 0.00192814
[164]	training's l2: 0.00131472	valid_1's l2: 0.00192865
[165]	training's l2: 0.00131216	valid_1's l2: 0.00192865
[166]	training's l2: 0.00130948	valid_1's l2: 0.00192815
[167]	training's l2: 0.00130693	valid_1's l2: 0.00192782
[168]	training's l2: 0.00130447	valid_1's l2: 0.00192755
[169]	training's l2: 0.00130181	valid_1's l2: 0.00192764
[170]	training's l2: 0.00129922	valid_1's l2: 0.00192746
[171]	training's l2: 0.00129676	valid_1's l2: 0.00192751
[172]	training's l2: 0.00129412	valid_1's l2: 0.00192807
[173]	training's l2: 0.00129182	valid_1's l2: 0.00192802
[174]	training's l2: 0.00128922	valid_1's l2: 0.00192818
[175]	training's l2: 0.00128682	valid_1's l2: 0.00192794
[176]	training's l2: 0.00128437	valid_1's l2: 0.00192766
[177]	training's l2: 0.00128197	valid_1's l2: 0.00192747
[178]	training's l2: 0.0012795	valid_1's l2: 0.00192795
[179]	training's l2: 0.00127718	valid_1's l2: 0.00192783
[180]	training's l2: 0.00127461	valid_1's l2: 0.00192734
[181]	training's l2: 0.00127223	valid_1's l2: 0.00192717
[182]	training's l2: 0.00126988	valid_1's l2: 0.00192693
[183]	training's l2: 0.00126735	valid_1's l2: 0.00192699
[184]	training's l2: 0.00126494	valid_1's l2: 0.00192703
[185]	training's l2: 0.00126264	valid_1's l2: 0.00192699
Did not meet early stopping. Best iteration is:
[185]	training's l2: 0.00126264	valid_1's l2: 0.00192699
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.232300 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00397948	valid_1's l2: 0.00368679
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00389957	valid_1's l2: 0.00363164
[3]	training's l2: 0.00382805	valid_1's l2: 0.00358084
[4]	training's l2: 0.00376642	valid_1's l2: 0.00354096
[5]	training's l2: 0.00370967	valid_1's l2: 0.00350656
[6]	training's l2: 0.00365308	valid_1's l2: 0.00347305
[7]	training's l2: 0.00360499	valid_1's l2: 0.00344276
[8]	training's l2: 0.00355967	valid_1's l2: 0.00341672
[9]	training's l2: 0.00351687	valid_1's l2: 0.00338992
[10]	training's l2: 0.00347749	valid_1's l2: 0.00336853
[11]	training's l2: 0.00344087	valid_1's l2: 0.00335006
[12]	training's l2: 0.00340486	valid_1's l2: 0.00333235
[13]	training's l2: 0.00337212	valid_1's l2: 0.00331749
[14]	training's l2: 0.00334127	valid_1's l2: 0.00330275
[15]	training's l2: 0.0033111	valid_1's l2: 0.00328771
[16]	training's l2: 0.00328478	valid_1's l2: 0.00327573
[17]	training's l2: 0.00325765	valid_1's l2: 0.00326469
[18]	training's l2: 0.00323287	valid_1's l2: 0.00325437
[19]	training's l2: 0.00321063	valid_1's l2: 0.00324809
[20]	training's l2: 0.00318969	valid_1's l2: 0.00324074
[21]	training's l2: 0.00316969	valid_1's l2: 0.00323439
[22]	training's l2: 0.00314786	valid_1's l2: 0.0032271
[23]	training's l2: 0.00312912	valid_1's l2: 0.00322244
[24]	training's l2: 0.00311161	valid_1's l2: 0.00321922
[25]	training's l2: 0.00309505	valid_1's l2: 0.00321307
[26]	training's l2: 0.00307866	valid_1's l2: 0.00320922
[27]	training's l2: 0.00306255	valid_1's l2: 0.00320458
[28]	training's l2: 0.00304617	valid_1's l2: 0.00319947
[29]	training's l2: 0.00303178	valid_1's l2: 0.00319573
[30]	training's l2: 0.00301709	valid_1's l2: 0.00319126
[31]	training's l2: 0.00300338	valid_1's l2: 0.00318742
[32]	training's l2: 0.00298925	valid_1's l2: 0.0031832
[33]	training's l2: 0.00297453	valid_1's l2: 0.0031781
[34]	training's l2: 0.00296059	valid_1's l2: 0.00317462
[35]	training's l2: 0.00294843	valid_1's l2: 0.00317082
[36]	training's l2: 0.00293553	valid_1's l2: 0.00316941
[37]	training's l2: 0.00292398	valid_1's l2: 0.00316568
[38]	training's l2: 0.00291312	valid_1's l2: 0.00316527
[39]	training's l2: 0.0029028	valid_1's l2: 0.00316241
[40]	training's l2: 0.00289022	valid_1's l2: 0.00316
[41]	training's l2: 0.00287964	valid_1's l2: 0.0031573
[42]	training's l2: 0.00286979	valid_1's l2: 0.00315588
[43]	training's l2: 0.00285896	valid_1's l2: 0.00315507
[44]	training's l2: 0.00284863	valid_1's l2: 0.00315423
[45]	training's l2: 0.00283687	valid_1's l2: 0.00315138
[46]	training's l2: 0.00282633	valid_1's l2: 0.00314967
[47]	training's l2: 0.0028166	valid_1's l2: 0.00314742
[48]	training's l2: 0.00280542	valid_1's l2: 0.00314599
[49]	training's l2: 0.00279574	valid_1's l2: 0.00314385
[50]	training's l2: 0.002785	valid_1's l2: 0.00314242
[51]	training's l2: 0.00277564	valid_1's l2: 0.00314077
[52]	training's l2: 0.00276574	valid_1's l2: 0.0031391
[53]	training's l2: 0.00275574	valid_1's l2: 0.00313575
[54]	training's l2: 0.00274696	valid_1's l2: 0.00313444
[55]	training's l2: 0.00273647	valid_1's l2: 0.00313162
[56]	training's l2: 0.00272776	valid_1's l2: 0.00312984
[57]	training's l2: 0.00271841	valid_1's l2: 0.00312922
[58]	training's l2: 0.00270815	valid_1's l2: 0.00312463
[59]	training's l2: 0.00269936	valid_1's l2: 0.00312262
[60]	training's l2: 0.00269088	valid_1's l2: 0.00312142
[61]	training's l2: 0.00268106	valid_1's l2: 0.00311852
[62]	training's l2: 0.00267285	valid_1's l2: 0.0031184
[63]	training's l2: 0.00266359	valid_1's l2: 0.00311826
[64]	training's l2: 0.00265533	valid_1's l2: 0.00311673
[65]	training's l2: 0.00264705	valid_1's l2: 0.00311731
[66]	training's l2: 0.00263864	valid_1's l2: 0.00311662
[67]	training's l2: 0.00262917	valid_1's l2: 0.00311312
[68]	training's l2: 0.00262054	valid_1's l2: 0.0031122
[69]	training's l2: 0.00261257	valid_1's l2: 0.00311167
[70]	training's l2: 0.0026042	valid_1's l2: 0.00311001
[71]	training's l2: 0.00259656	valid_1's l2: 0.00310973
[72]	training's l2: 0.0025878	valid_1's l2: 0.00310931
[73]	training's l2: 0.00258087	valid_1's l2: 0.00310828
[74]	training's l2: 0.00257306	valid_1's l2: 0.00310817
[75]	training's l2: 0.00256486	valid_1's l2: 0.00310669
[76]	training's l2: 0.00255727	valid_1's l2: 0.00310581
[77]	training's l2: 0.00254966	valid_1's l2: 0.00310484
[78]	training's l2: 0.0025419	valid_1's l2: 0.00310455
[79]	training's l2: 0.00253535	valid_1's l2: 0.00310417
[80]	training's l2: 0.00252772	valid_1's l2: 0.00310345
[81]	training's l2: 0.00251996	valid_1's l2: 0.00310296
[82]	training's l2: 0.00251298	valid_1's l2: 0.00310158
[83]	training's l2: 0.00250558	valid_1's l2: 0.00310038
[84]	training's l2: 0.00249836	valid_1's l2: 0.00309933
[85]	training's l2: 0.00249135	valid_1's l2: 0.00309889
[86]	training's l2: 0.00248478	valid_1's l2: 0.00309954
[87]	training's l2: 0.00247763	valid_1's l2: 0.00309901
[88]	training's l2: 0.00247021	valid_1's l2: 0.00309809
[89]	training's l2: 0.00246303	valid_1's l2: 0.00309729
[90]	training's l2: 0.00245628	valid_1's l2: 0.00309585
[91]	training's l2: 0.002449	valid_1's l2: 0.00309507
[92]	training's l2: 0.00244231	valid_1's l2: 0.00309488
[93]	training's l2: 0.002435	valid_1's l2: 0.00309547
[94]	training's l2: 0.00242852	valid_1's l2: 0.00309394
[95]	training's l2: 0.00242227	valid_1's l2: 0.00309384
[96]	training's l2: 0.00241593	valid_1's l2: 0.0030934
[97]	training's l2: 0.00240948	valid_1's l2: 0.00309316
[98]	training's l2: 0.00240287	valid_1's l2: 0.00309302
[99]	training's l2: 0.00239634	valid_1's l2: 0.00309183
[100]	training's l2: 0.00238998	valid_1's l2: 0.00309096
[101]	training's l2: 0.00238355	valid_1's l2: 0.00309036
[102]	training's l2: 0.00237738	valid_1's l2: 0.00309066
[103]	training's l2: 0.00237065	valid_1's l2: 0.00309059
[104]	training's l2: 0.00236414	valid_1's l2: 0.00309054
[105]	training's l2: 0.00235805	valid_1's l2: 0.00309072
[106]	training's l2: 0.00235222	valid_1's l2: 0.00308962
[107]	training's l2: 0.0023459	valid_1's l2: 0.00308894
[108]	training's l2: 0.00233966	valid_1's l2: 0.00308952
[109]	training's l2: 0.00233353	valid_1's l2: 0.00308885
[110]	training's l2: 0.002327	valid_1's l2: 0.00308748
[111]	training's l2: 0.00232069	valid_1's l2: 0.00308832
[112]	training's l2: 0.00231464	valid_1's l2: 0.00308783
[113]	training's l2: 0.0023089	valid_1's l2: 0.00308836
[114]	training's l2: 0.00230296	valid_1's l2: 0.00308764
[115]	training's l2: 0.00229688	valid_1's l2: 0.00308654
[116]	training's l2: 0.0022908	valid_1's l2: 0.00308668
[117]	training's l2: 0.00228443	valid_1's l2: 0.00308731
[118]	training's l2: 0.00227876	valid_1's l2: 0.00308771
[119]	training's l2: 0.00227366	valid_1's l2: 0.00308713
[120]	training's l2: 0.00226829	valid_1's l2: 0.00308738
[121]	training's l2: 0.00226309	valid_1's l2: 0.00308639
[122]	training's l2: 0.00225674	valid_1's l2: 0.00308638
[123]	training's l2: 0.00225125	valid_1's l2: 0.00308717
[124]	training's l2: 0.0022459	valid_1's l2: 0.00308645
[125]	training's l2: 0.00224026	valid_1's l2: 0.00308653
[126]	training's l2: 0.00223409	valid_1's l2: 0.00308519
[127]	training's l2: 0.00222863	valid_1's l2: 0.0030853
[128]	training's l2: 0.00222275	valid_1's l2: 0.00308638
[129]	training's l2: 0.00221685	valid_1's l2: 0.00308719
[130]	training's l2: 0.00221088	valid_1's l2: 0.0030876
[131]	training's l2: 0.00220513	valid_1's l2: 0.00308678
[132]	training's l2: 0.00219931	valid_1's l2: 0.00308658
[133]	training's l2: 0.00219384	valid_1's l2: 0.00308633
[134]	training's l2: 0.00218861	valid_1's l2: 0.00308603
[135]	training's l2: 0.00218311	valid_1's l2: 0.00308654
[136]	training's l2: 0.00217788	valid_1's l2: 0.003087
[137]	training's l2: 0.002172	valid_1's l2: 0.00308662
[138]	training's l2: 0.00216679	valid_1's l2: 0.00308678
[139]	training's l2: 0.00216163	valid_1's l2: 0.00308702
[140]	training's l2: 0.00215596	valid_1's l2: 0.00308686
[141]	training's l2: 0.00215062	valid_1's l2: 0.00308755
[142]	training's l2: 0.0021456	valid_1's l2: 0.00308729
[143]	training's l2: 0.00214011	valid_1's l2: 0.00308804
[144]	training's l2: 0.00213525	valid_1's l2: 0.00308832
[145]	training's l2: 0.00213039	valid_1's l2: 0.00308799
[146]	training's l2: 0.0021258	valid_1's l2: 0.00308809
[147]	training's l2: 0.00212069	valid_1's l2: 0.00308741
[148]	training's l2: 0.00211534	valid_1's l2: 0.00308783
[149]	training's l2: 0.00210975	valid_1's l2: 0.00308689
[150]	training's l2: 0.00210482	valid_1's l2: 0.00308748
[151]	training's l2: 0.00209915	valid_1's l2: 0.0030886
[152]	training's l2: 0.00209407	valid_1's l2: 0.00308867
[153]	training's l2: 0.00208947	valid_1's l2: 0.00308909
[154]	training's l2: 0.00208429	valid_1's l2: 0.00308906
[155]	training's l2: 0.00207928	valid_1's l2: 0.00308914
[156]	training's l2: 0.00207442	valid_1's l2: 0.00308777
Early stopping, best iteration is:
[126]	training's l2: 0.00223409	valid_1's l2: 0.00308519
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.239361 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00395542	valid_1's l2: 0.00366946
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00385881	valid_1's l2: 0.00360117
[3]	training's l2: 0.00378204	valid_1's l2: 0.00354745
[4]	training's l2: 0.00371262	valid_1's l2: 0.00350252
[5]	training's l2: 0.0036511	valid_1's l2: 0.00346544
[6]	training's l2: 0.0035955	valid_1's l2: 0.00343283
[7]	training's l2: 0.00354627	valid_1's l2: 0.00340542
[8]	training's l2: 0.00350027	valid_1's l2: 0.00337999
[9]	training's l2: 0.00346208	valid_1's l2: 0.00336296
[10]	training's l2: 0.0034193	valid_1's l2: 0.00333815
[11]	training's l2: 0.00338644	valid_1's l2: 0.00332293
[12]	training's l2: 0.00335677	valid_1's l2: 0.0033116
[13]	training's l2: 0.00332759	valid_1's l2: 0.00330096
[14]	training's l2: 0.00329849	valid_1's l2: 0.00329068
[15]	training's l2: 0.00327342	valid_1's l2: 0.00327716
[16]	training's l2: 0.00324876	valid_1's l2: 0.003266
[17]	training's l2: 0.00322771	valid_1's l2: 0.0032575
[18]	training's l2: 0.00320461	valid_1's l2: 0.00324825
[19]	training's l2: 0.00318701	valid_1's l2: 0.00324168
[20]	training's l2: 0.00316948	valid_1's l2: 0.00323707
[21]	training's l2: 0.0031539	valid_1's l2: 0.00323344
[22]	training's l2: 0.00313712	valid_1's l2: 0.00322716
[23]	training's l2: 0.00312411	valid_1's l2: 0.00322337
[24]	training's l2: 0.00310961	valid_1's l2: 0.00321751
[25]	training's l2: 0.00309333	valid_1's l2: 0.00320865
[26]	training's l2: 0.00307944	valid_1's l2: 0.00320579
[27]	training's l2: 0.00306688	valid_1's l2: 0.0032018
[28]	training's l2: 0.0030533	valid_1's l2: 0.00319623
[29]	training's l2: 0.00304047	valid_1's l2: 0.00319189
[30]	training's l2: 0.0030296	valid_1's l2: 0.00319048
[31]	training's l2: 0.00301648	valid_1's l2: 0.00318519
[32]	training's l2: 0.00300601	valid_1's l2: 0.0031839
[33]	training's l2: 0.002995	valid_1's l2: 0.00317893
[34]	training's l2: 0.0029849	valid_1's l2: 0.00317624
[35]	training's l2: 0.00297208	valid_1's l2: 0.00316967
[36]	training's l2: 0.0029619	valid_1's l2: 0.00316718
[37]	training's l2: 0.00295257	valid_1's l2: 0.00316614
[38]	training's l2: 0.00294337	valid_1's l2: 0.0031636
[39]	training's l2: 0.00293384	valid_1's l2: 0.00316125
[40]	training's l2: 0.00292377	valid_1's l2: 0.00315675
[41]	training's l2: 0.00291338	valid_1's l2: 0.00315062
[42]	training's l2: 0.00290504	valid_1's l2: 0.0031488
[43]	training's l2: 0.00289476	valid_1's l2: 0.0031445
[44]	training's l2: 0.00288619	valid_1's l2: 0.00314191
[45]	training's l2: 0.00287803	valid_1's l2: 0.00314066
[46]	training's l2: 0.00287053	valid_1's l2: 0.00313882
[47]	training's l2: 0.00286144	valid_1's l2: 0.00313677
[48]	training's l2: 0.00285151	valid_1's l2: 0.00313188
[49]	training's l2: 0.00284436	valid_1's l2: 0.00313227
[50]	training's l2: 0.00283771	valid_1's l2: 0.00313045
[51]	training's l2: 0.00283021	valid_1's l2: 0.00312918
[52]	training's l2: 0.00282166	valid_1's l2: 0.00312687
[53]	training's l2: 0.00281435	valid_1's l2: 0.00312523
[54]	training's l2: 0.00280676	valid_1's l2: 0.00312535
[55]	training's l2: 0.00279816	valid_1's l2: 0.00312313
[56]	training's l2: 0.00279074	valid_1's l2: 0.00312009
[57]	training's l2: 0.00278439	valid_1's l2: 0.00311991
[58]	training's l2: 0.00277659	valid_1's l2: 0.00311662
[59]	training's l2: 0.00276971	valid_1's l2: 0.00311515
[60]	training's l2: 0.00276319	valid_1's l2: 0.00311408
[61]	training's l2: 0.00275672	valid_1's l2: 0.00311329
[62]	training's l2: 0.00274995	valid_1's l2: 0.0031114
[63]	training's l2: 0.00274283	valid_1's l2: 0.00310976
[64]	training's l2: 0.00273492	valid_1's l2: 0.00310671
[65]	training's l2: 0.00272753	valid_1's l2: 0.00310529
[66]	training's l2: 0.00272174	valid_1's l2: 0.00310362
[67]	training's l2: 0.00271422	valid_1's l2: 0.00310246
[68]	training's l2: 0.00270782	valid_1's l2: 0.00310118
[69]	training's l2: 0.00270186	valid_1's l2: 0.00310051
[70]	training's l2: 0.00269642	valid_1's l2: 0.0031004
[71]	training's l2: 0.00269103	valid_1's l2: 0.00310019
[72]	training's l2: 0.00268509	valid_1's l2: 0.00309981
[73]	training's l2: 0.0026788	valid_1's l2: 0.0030987
[74]	training's l2: 0.0026722	valid_1's l2: 0.00309859
[75]	training's l2: 0.00266636	valid_1's l2: 0.00309912
[76]	training's l2: 0.00266068	valid_1's l2: 0.00309814
[77]	training's l2: 0.00265548	valid_1's l2: 0.00309847
[78]	training's l2: 0.00264931	valid_1's l2: 0.00309865
[79]	training's l2: 0.00264314	valid_1's l2: 0.00309843
[80]	training's l2: 0.00263726	valid_1's l2: 0.00309793
[81]	training's l2: 0.00263219	valid_1's l2: 0.00309803
[82]	training's l2: 0.00262678	valid_1's l2: 0.00309666
[83]	training's l2: 0.00262073	valid_1's l2: 0.00309429
[84]	training's l2: 0.00261541	valid_1's l2: 0.00309529
[85]	training's l2: 0.00260932	valid_1's l2: 0.00309497
[86]	training's l2: 0.00260425	valid_1's l2: 0.003095
[87]	training's l2: 0.00259881	valid_1's l2: 0.00309405
[88]	training's l2: 0.00259278	valid_1's l2: 0.00309352
[89]	training's l2: 0.00258696	valid_1's l2: 0.00309427
[90]	training's l2: 0.00258117	valid_1's l2: 0.00309417
[91]	training's l2: 0.00257637	valid_1's l2: 0.00309528
[92]	training's l2: 0.00257115	valid_1's l2: 0.00309455
[93]	training's l2: 0.00256611	valid_1's l2: 0.0030945
[94]	training's l2: 0.00256093	valid_1's l2: 0.0030936
[95]	training's l2: 0.00255605	valid_1's l2: 0.0030925
[96]	training's l2: 0.00255124	valid_1's l2: 0.00309207
[97]	training's l2: 0.0025459	valid_1's l2: 0.00309205
[98]	training's l2: 0.00254106	valid_1's l2: 0.00309173
[99]	training's l2: 0.00253648	valid_1's l2: 0.00309085
[100]	training's l2: 0.00253128	valid_1's l2: 0.00309115
[101]	training's l2: 0.00252653	valid_1's l2: 0.00309185
[102]	training's l2: 0.00252155	valid_1's l2: 0.00309089
[103]	training's l2: 0.00251641	valid_1's l2: 0.00309066
[104]	training's l2: 0.00251155	valid_1's l2: 0.00308994
[105]	training's l2: 0.00250748	valid_1's l2: 0.00309047
[106]	training's l2: 0.00250283	valid_1's l2: 0.00309136
[107]	training's l2: 0.00249817	valid_1's l2: 0.00309129
[108]	training's l2: 0.00249301	valid_1's l2: 0.00309083
[109]	training's l2: 0.00248825	valid_1's l2: 0.00309039
[110]	training's l2: 0.00248311	valid_1's l2: 0.00309106
[111]	training's l2: 0.00247804	valid_1's l2: 0.00309037
[112]	training's l2: 0.00247291	valid_1's l2: 0.00309011
[113]	training's l2: 0.00246888	valid_1's l2: 0.00308972
[114]	training's l2: 0.00246415	valid_1's l2: 0.00308981
[115]	training's l2: 0.00246004	valid_1's l2: 0.00308992
[116]	training's l2: 0.00245545	valid_1's l2: 0.00308974
[117]	training's l2: 0.00245119	valid_1's l2: 0.00308947
[118]	training's l2: 0.00244645	valid_1's l2: 0.00308813
[119]	training's l2: 0.0024421	valid_1's l2: 0.00308805
[120]	training's l2: 0.00243669	valid_1's l2: 0.00308718
[121]	training's l2: 0.00243163	valid_1's l2: 0.00308734
[122]	training's l2: 0.00242735	valid_1's l2: 0.00308646
[123]	training's l2: 0.00242324	valid_1's l2: 0.00308668
[124]	training's l2: 0.00241883	valid_1's l2: 0.00308667
[125]	training's l2: 0.00241375	valid_1's l2: 0.00308515
[126]	training's l2: 0.00240911	valid_1's l2: 0.0030848
[127]	training's l2: 0.00240457	valid_1's l2: 0.0030845
[128]	training's l2: 0.00239976	valid_1's l2: 0.00308438
[129]	training's l2: 0.00239629	valid_1's l2: 0.00308387
[130]	training's l2: 0.00239153	valid_1's l2: 0.00308396
[131]	training's l2: 0.00238647	valid_1's l2: 0.00308391
[132]	training's l2: 0.00238197	valid_1's l2: 0.00308366
[133]	training's l2: 0.00237806	valid_1's l2: 0.00308347
[134]	training's l2: 0.00237366	valid_1's l2: 0.0030827
[135]	training's l2: 0.00236926	valid_1's l2: 0.00308268
[136]	training's l2: 0.00236485	valid_1's l2: 0.0030834
[137]	training's l2: 0.00236118	valid_1's l2: 0.00308383
[138]	training's l2: 0.00235727	valid_1's l2: 0.00308348
[139]	training's l2: 0.00235354	valid_1's l2: 0.00308318
[140]	training's l2: 0.00234922	valid_1's l2: 0.00308306
[141]	training's l2: 0.0023448	valid_1's l2: 0.00308247
[142]	training's l2: 0.00234063	valid_1's l2: 0.00308276
[143]	training's l2: 0.00233641	valid_1's l2: 0.0030826
[144]	training's l2: 0.00233335	valid_1's l2: 0.00308232
[145]	training's l2: 0.00232983	valid_1's l2: 0.00308296
[146]	training's l2: 0.00232503	valid_1's l2: 0.00308357
[147]	training's l2: 0.0023215	valid_1's l2: 0.00308384
[148]	training's l2: 0.00231784	valid_1's l2: 0.00308372
[149]	training's l2: 0.00231404	valid_1's l2: 0.00308331
[150]	training's l2: 0.00230988	valid_1's l2: 0.00308314
[151]	training's l2: 0.00230582	valid_1's l2: 0.00308226
[152]	training's l2: 0.00230179	valid_1's l2: 0.00308167
[153]	training's l2: 0.00229743	valid_1's l2: 0.00308125
[154]	training's l2: 0.00229382	valid_1's l2: 0.00308079
[155]	training's l2: 0.00229003	valid_1's l2: 0.00308076
[156]	training's l2: 0.0022859	valid_1's l2: 0.00308114
[157]	training's l2: 0.00228182	valid_1's l2: 0.00308054
[158]	training's l2: 0.00227829	valid_1's l2: 0.00308122
[159]	training's l2: 0.00227477	valid_1's l2: 0.00308102
[160]	training's l2: 0.00227127	valid_1's l2: 0.00308079
[161]	training's l2: 0.00226682	valid_1's l2: 0.00308066
[162]	training's l2: 0.00226361	valid_1's l2: 0.00308114
[163]	training's l2: 0.00225995	valid_1's l2: 0.00308124
[164]	training's l2: 0.00225578	valid_1's l2: 0.00308137
[165]	training's l2: 0.00225218	valid_1's l2: 0.00308123
[166]	training's l2: 0.00224844	valid_1's l2: 0.00308051
[167]	training's l2: 0.00224384	valid_1's l2: 0.00308135
[168]	training's l2: 0.00224024	valid_1's l2: 0.0030818
[169]	training's l2: 0.00223602	valid_1's l2: 0.00308111
[170]	training's l2: 0.00223265	valid_1's l2: 0.00308138
[171]	training's l2: 0.00222903	valid_1's l2: 0.00308124
[172]	training's l2: 0.00222554	valid_1's l2: 0.00308068
[173]	training's l2: 0.00222169	valid_1's l2: 0.00308156
Did not meet early stopping. Best iteration is:
[173]	training's l2: 0.00222169	valid_1's l2: 0.00308156
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.252703 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00397272	valid_1's l2: 0.00368146
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00388989	valid_1's l2: 0.0036225
[3]	training's l2: 0.00381345	valid_1's l2: 0.00356974
[4]	training's l2: 0.00374523	valid_1's l2: 0.0035264
[5]	training's l2: 0.00368644	valid_1's l2: 0.00348979
[6]	training's l2: 0.00363311	valid_1's l2: 0.00345805
[7]	training's l2: 0.00358454	valid_1's l2: 0.0034283
[8]	training's l2: 0.00353872	valid_1's l2: 0.0034034
[9]	training's l2: 0.00349691	valid_1's l2: 0.00338225
[10]	training's l2: 0.00345497	valid_1's l2: 0.00335773
[11]	training's l2: 0.00341976	valid_1's l2: 0.00334003
[12]	training's l2: 0.0033855	valid_1's l2: 0.00332363
[13]	training's l2: 0.00335149	valid_1's l2: 0.00330679
[14]	training's l2: 0.00332077	valid_1's l2: 0.00329103
[15]	training's l2: 0.00329298	valid_1's l2: 0.00327788
[16]	training's l2: 0.00326486	valid_1's l2: 0.00326569
[17]	training's l2: 0.00324099	valid_1's l2: 0.00325504
[18]	training's l2: 0.00321796	valid_1's l2: 0.0032464
[19]	training's l2: 0.00319784	valid_1's l2: 0.00323827
[20]	training's l2: 0.0031748	valid_1's l2: 0.00322937
[21]	training's l2: 0.00315585	valid_1's l2: 0.00322119
[22]	training's l2: 0.00313602	valid_1's l2: 0.00321323
[23]	training's l2: 0.0031164	valid_1's l2: 0.00320773
[24]	training's l2: 0.00309796	valid_1's l2: 0.00320141
[25]	training's l2: 0.00308042	valid_1's l2: 0.00319567
[26]	training's l2: 0.00306456	valid_1's l2: 0.00319316
[27]	training's l2: 0.0030498	valid_1's l2: 0.00319008
[28]	training's l2: 0.0030358	valid_1's l2: 0.00318819
[29]	training's l2: 0.00302054	valid_1's l2: 0.00318323
[30]	training's l2: 0.00300708	valid_1's l2: 0.00318152
[31]	training's l2: 0.00299383	valid_1's l2: 0.00317877
[32]	training's l2: 0.00298115	valid_1's l2: 0.00317703
[33]	training's l2: 0.00296958	valid_1's l2: 0.00317384
[34]	training's l2: 0.00295658	valid_1's l2: 0.00316923
[35]	training's l2: 0.00294354	valid_1's l2: 0.00316509
[36]	training's l2: 0.00293073	valid_1's l2: 0.0031611
[37]	training's l2: 0.00291903	valid_1's l2: 0.00315791
[38]	training's l2: 0.00290721	valid_1's l2: 0.00315526
[39]	training's l2: 0.00289717	valid_1's l2: 0.0031529
[40]	training's l2: 0.00288645	valid_1's l2: 0.00315067
[41]	training's l2: 0.00287618	valid_1's l2: 0.00314893
[42]	training's l2: 0.00286508	valid_1's l2: 0.00314602
[43]	training's l2: 0.00285579	valid_1's l2: 0.00314393
[44]	training's l2: 0.00284489	valid_1's l2: 0.00314049
[45]	training's l2: 0.00283572	valid_1's l2: 0.00313884
[46]	training's l2: 0.00282548	valid_1's l2: 0.00313807
[47]	training's l2: 0.00281612	valid_1's l2: 0.00313811
[48]	training's l2: 0.00280575	valid_1's l2: 0.0031349
[49]	training's l2: 0.00279619	valid_1's l2: 0.0031327
[50]	training's l2: 0.00278581	valid_1's l2: 0.00312897
[51]	training's l2: 0.00277638	valid_1's l2: 0.00312741
[52]	training's l2: 0.00276613	valid_1's l2: 0.00312315
[53]	training's l2: 0.00275626	valid_1's l2: 0.00312251
[54]	training's l2: 0.00274771	valid_1's l2: 0.00312165
[55]	training's l2: 0.00273709	valid_1's l2: 0.0031188
[56]	training's l2: 0.00272851	valid_1's l2: 0.00311857
[57]	training's l2: 0.00271881	valid_1's l2: 0.00311569
[58]	training's l2: 0.0027103	valid_1's l2: 0.0031141
[59]	training's l2: 0.0027018	valid_1's l2: 0.00311341
[60]	training's l2: 0.00269382	valid_1's l2: 0.00311287
[61]	training's l2: 0.0026846	valid_1's l2: 0.00310882
[62]	training's l2: 0.00267545	valid_1's l2: 0.00310706
[63]	training's l2: 0.00266686	valid_1's l2: 0.00310662
[64]	training's l2: 0.00265926	valid_1's l2: 0.00310611
[65]	training's l2: 0.00265151	valid_1's l2: 0.0031039
[66]	training's l2: 0.00264286	valid_1's l2: 0.00310185
[67]	training's l2: 0.00263423	valid_1's l2: 0.00309974
[68]	training's l2: 0.00262633	valid_1's l2: 0.00309905
[69]	training's l2: 0.00261861	valid_1's l2: 0.00309764
[70]	training's l2: 0.00261072	valid_1's l2: 0.00309624
[71]	training's l2: 0.00260351	valid_1's l2: 0.00309558
[72]	training's l2: 0.00259636	valid_1's l2: 0.00309498
[73]	training's l2: 0.00258846	valid_1's l2: 0.00309369
[74]	training's l2: 0.00258106	valid_1's l2: 0.00309366
[75]	training's l2: 0.002574	valid_1's l2: 0.00309403
[76]	training's l2: 0.00256649	valid_1's l2: 0.00309203
[77]	training's l2: 0.00256051	valid_1's l2: 0.00309223
[78]	training's l2: 0.00255343	valid_1's l2: 0.00309163
[79]	training's l2: 0.00254618	valid_1's l2: 0.0030902
[80]	training's l2: 0.00253822	valid_1's l2: 0.00308837
[81]	training's l2: 0.00252983	valid_1's l2: 0.00308602
[82]	training's l2: 0.00252291	valid_1's l2: 0.00308578
[83]	training's l2: 0.00251679	valid_1's l2: 0.00308441
[84]	training's l2: 0.00251001	valid_1's l2: 0.00308502
[85]	training's l2: 0.00250269	valid_1's l2: 0.00308378
[86]	training's l2: 0.00249564	valid_1's l2: 0.00308295
[87]	training's l2: 0.00248963	valid_1's l2: 0.0030832
[88]	training's l2: 0.00248281	valid_1's l2: 0.00308314
[89]	training's l2: 0.00247593	valid_1's l2: 0.00308148
[90]	training's l2: 0.0024693	valid_1's l2: 0.0030814
[91]	training's l2: 0.00246241	valid_1's l2: 0.0030808
[92]	training's l2: 0.00245684	valid_1's l2: 0.00307996
[93]	training's l2: 0.00245095	valid_1's l2: 0.00308004
[94]	training's l2: 0.00244441	valid_1's l2: 0.00307998
[95]	training's l2: 0.00243811	valid_1's l2: 0.00307967
[96]	training's l2: 0.00243165	valid_1's l2: 0.00307613
[97]	training's l2: 0.00242467	valid_1's l2: 0.00307499
[98]	training's l2: 0.00241847	valid_1's l2: 0.00307479
[99]	training's l2: 0.00241201	valid_1's l2: 0.00307484
[100]	training's l2: 0.00240514	valid_1's l2: 0.00307381
[101]	training's l2: 0.00239812	valid_1's l2: 0.00307291
[102]	training's l2: 0.00239251	valid_1's l2: 0.0030731
[103]	training's l2: 0.00238626	valid_1's l2: 0.00307263
[104]	training's l2: 0.00238044	valid_1's l2: 0.00307212
[105]	training's l2: 0.00237448	valid_1's l2: 0.0030721
[106]	training's l2: 0.00236889	valid_1's l2: 0.00307088
[107]	training's l2: 0.00236296	valid_1's l2: 0.00307179
[108]	training's l2: 0.0023569	valid_1's l2: 0.00307011
[109]	training's l2: 0.00235107	valid_1's l2: 0.00307058
[110]	training's l2: 0.00234506	valid_1's l2: 0.00307109
[111]	training's l2: 0.00233988	valid_1's l2: 0.00307102
[112]	training's l2: 0.00233407	valid_1's l2: 0.0030705
[113]	training's l2: 0.00232891	valid_1's l2: 0.00307093
[114]	training's l2: 0.00232327	valid_1's l2: 0.0030701
[115]	training's l2: 0.0023175	valid_1's l2: 0.00307036
[116]	training's l2: 0.00231188	valid_1's l2: 0.00307022
[117]	training's l2: 0.00230646	valid_1's l2: 0.00307021
[118]	training's l2: 0.00230118	valid_1's l2: 0.00306948
[119]	training's l2: 0.00229588	valid_1's l2: 0.00306907
[120]	training's l2: 0.00229053	valid_1's l2: 0.00306934
[121]	training's l2: 0.0022844	valid_1's l2: 0.0030685
[122]	training's l2: 0.00227869	valid_1's l2: 0.00306819
[123]	training's l2: 0.00227336	valid_1's l2: 0.00306796
[124]	training's l2: 0.00226772	valid_1's l2: 0.00306841
[125]	training's l2: 0.00226223	valid_1's l2: 0.00306751
[126]	training's l2: 0.00225706	valid_1's l2: 0.00306706
[127]	training's l2: 0.00225218	valid_1's l2: 0.0030667
[128]	training's l2: 0.00224604	valid_1's l2: 0.00306777
[129]	training's l2: 0.00224142	valid_1's l2: 0.00306727
[130]	training's l2: 0.00223568	valid_1's l2: 0.00306733
[131]	training's l2: 0.00223028	valid_1's l2: 0.00306696
[132]	training's l2: 0.00222516	valid_1's l2: 0.00306628
[133]	training's l2: 0.00222067	valid_1's l2: 0.0030666
[134]	training's l2: 0.00221535	valid_1's l2: 0.00306693
[135]	training's l2: 0.00221119	valid_1's l2: 0.00306677
[136]	training's l2: 0.00220613	valid_1's l2: 0.00306673
[137]	training's l2: 0.00220048	valid_1's l2: 0.0030665
[138]	training's l2: 0.00219571	valid_1's l2: 0.00306597
[139]	training's l2: 0.00219118	valid_1's l2: 0.00306599
[140]	training's l2: 0.00218614	valid_1's l2: 0.00306663
[141]	training's l2: 0.00218185	valid_1's l2: 0.00306676
[142]	training's l2: 0.00217744	valid_1's l2: 0.00306739
[143]	training's l2: 0.00217259	valid_1's l2: 0.00306748
[144]	training's l2: 0.00216861	valid_1's l2: 0.00306746
[145]	training's l2: 0.00216385	valid_1's l2: 0.00306879
[146]	training's l2: 0.00215917	valid_1's l2: 0.00306918
[147]	training's l2: 0.00215352	valid_1's l2: 0.00306974
[148]	training's l2: 0.0021491	valid_1's l2: 0.00306919
[149]	training's l2: 0.00214479	valid_1's l2: 0.00306885
[150]	training's l2: 0.00214028	valid_1's l2: 0.00306859
[151]	training's l2: 0.00213611	valid_1's l2: 0.00306854
[152]	training's l2: 0.00213086	valid_1's l2: 0.00306823
Did not meet early stopping. Best iteration is:
[152]	training's l2: 0.00213086	valid_1's l2: 0.00306823
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264628 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00397373	valid_1's l2: 0.00368389
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.0038921	valid_1's l2: 0.00362808
[3]	training's l2: 0.00381618	valid_1's l2: 0.00357741
[4]	training's l2: 0.00375222	valid_1's l2: 0.00353878
[5]	training's l2: 0.00369431	valid_1's l2: 0.00350208
[6]	training's l2: 0.00363785	valid_1's l2: 0.0034665
[7]	training's l2: 0.00359057	valid_1's l2: 0.00343725
[8]	training's l2: 0.00354509	valid_1's l2: 0.00341156
[9]	training's l2: 0.00350297	valid_1's l2: 0.00338682
[10]	training's l2: 0.003461	valid_1's l2: 0.00336268
[11]	training's l2: 0.00342261	valid_1's l2: 0.00334305
[12]	training's l2: 0.0033878	valid_1's l2: 0.00332596
[13]	training's l2: 0.00335611	valid_1's l2: 0.00330967
[14]	training's l2: 0.00332667	valid_1's l2: 0.00329585
[15]	training's l2: 0.00330041	valid_1's l2: 0.00328647
[16]	training's l2: 0.00327366	valid_1's l2: 0.00327624
[17]	training's l2: 0.0032497	valid_1's l2: 0.00326656
[18]	training's l2: 0.00322378	valid_1's l2: 0.00325728
[19]	training's l2: 0.00320276	valid_1's l2: 0.00324879
[20]	training's l2: 0.00318234	valid_1's l2: 0.00323902
[21]	training's l2: 0.00316068	valid_1's l2: 0.0032299
[22]	training's l2: 0.00314062	valid_1's l2: 0.00322386
[23]	training's l2: 0.00312233	valid_1's l2: 0.00321993
[24]	training's l2: 0.0031037	valid_1's l2: 0.00321528
[25]	training's l2: 0.00308593	valid_1's l2: 0.00321104
[26]	training's l2: 0.0030713	valid_1's l2: 0.0032064
[27]	training's l2: 0.00305604	valid_1's l2: 0.0032011
[28]	training's l2: 0.0030414	valid_1's l2: 0.00319753
[29]	training's l2: 0.00302767	valid_1's l2: 0.00319492
[30]	training's l2: 0.00301453	valid_1's l2: 0.0031901
[31]	training's l2: 0.00300103	valid_1's l2: 0.00318725
[32]	training's l2: 0.00298901	valid_1's l2: 0.00318364
[33]	training's l2: 0.00297587	valid_1's l2: 0.00318129
[34]	training's l2: 0.00296486	valid_1's l2: 0.00318125
[35]	training's l2: 0.00295191	valid_1's l2: 0.00317728
[36]	training's l2: 0.00293979	valid_1's l2: 0.00317504
[37]	training's l2: 0.00292644	valid_1's l2: 0.00317135
[38]	training's l2: 0.00291434	valid_1's l2: 0.00316898
[39]	training's l2: 0.00290334	valid_1's l2: 0.00316735
[40]	training's l2: 0.00289146	valid_1's l2: 0.0031643
[41]	training's l2: 0.00288099	valid_1's l2: 0.00316265
[42]	training's l2: 0.00286924	valid_1's l2: 0.00315937
[43]	training's l2: 0.00285804	valid_1's l2: 0.00315716
[44]	training's l2: 0.00284772	valid_1's l2: 0.00315623
[45]	training's l2: 0.00283815	valid_1's l2: 0.00315419
[46]	training's l2: 0.00282796	valid_1's l2: 0.00315106
[47]	training's l2: 0.00281901	valid_1's l2: 0.00315022
[48]	training's l2: 0.00281035	valid_1's l2: 0.00314822
[49]	training's l2: 0.00280119	valid_1's l2: 0.0031464
[50]	training's l2: 0.00279169	valid_1's l2: 0.0031437
[51]	training's l2: 0.00278103	valid_1's l2: 0.00313936
[52]	training's l2: 0.00277261	valid_1's l2: 0.00313932
[53]	training's l2: 0.00276352	valid_1's l2: 0.00313608
[54]	training's l2: 0.00275499	valid_1's l2: 0.00313377
[55]	training's l2: 0.002747	valid_1's l2: 0.00313226
[56]	training's l2: 0.00273815	valid_1's l2: 0.0031303
[57]	training's l2: 0.0027283	valid_1's l2: 0.00312733
[58]	training's l2: 0.00272076	valid_1's l2: 0.00312588
[59]	training's l2: 0.00271226	valid_1's l2: 0.00312572
[60]	training's l2: 0.00270457	valid_1's l2: 0.00312292
[61]	training's l2: 0.0026956	valid_1's l2: 0.00311873
[62]	training's l2: 0.00268819	valid_1's l2: 0.00311866
[63]	training's l2: 0.00267908	valid_1's l2: 0.00311553
[64]	training's l2: 0.00267177	valid_1's l2: 0.00311447
[65]	training's l2: 0.00266493	valid_1's l2: 0.00311315
[66]	training's l2: 0.00265784	valid_1's l2: 0.0031127
[67]	training's l2: 0.00264943	valid_1's l2: 0.00310999
[68]	training's l2: 0.00264263	valid_1's l2: 0.00310922
[69]	training's l2: 0.00263426	valid_1's l2: 0.00310616
[70]	training's l2: 0.00262797	valid_1's l2: 0.00310586
[71]	training's l2: 0.00262023	valid_1's l2: 0.00310498
[72]	training's l2: 0.00261284	valid_1's l2: 0.00310396
[73]	training's l2: 0.00260647	valid_1's l2: 0.0031039
[74]	training's l2: 0.00259969	valid_1's l2: 0.00310344
[75]	training's l2: 0.00259326	valid_1's l2: 0.0031027
[76]	training's l2: 0.00258588	valid_1's l2: 0.00310149
[77]	training's l2: 0.00257917	valid_1's l2: 0.00310137
[78]	training's l2: 0.00257291	valid_1's l2: 0.00310103
[79]	training's l2: 0.00256613	valid_1's l2: 0.00309753
[80]	training's l2: 0.00255941	valid_1's l2: 0.00309519
[81]	training's l2: 0.00255255	valid_1's l2: 0.00309525
[82]	training's l2: 0.00254604	valid_1's l2: 0.00309424
[83]	training's l2: 0.0025391	valid_1's l2: 0.00309486
[84]	training's l2: 0.00253319	valid_1's l2: 0.00309476
[85]	training's l2: 0.00252685	valid_1's l2: 0.00309485
[86]	training's l2: 0.00252116	valid_1's l2: 0.00309446
[87]	training's l2: 0.00251527	valid_1's l2: 0.00309455
[88]	training's l2: 0.00250848	valid_1's l2: 0.00309329
[89]	training's l2: 0.00250237	valid_1's l2: 0.00309314
[90]	training's l2: 0.00249674	valid_1's l2: 0.00309304
[91]	training's l2: 0.00249078	valid_1's l2: 0.0030921
[92]	training's l2: 0.00248467	valid_1's l2: 0.00309156
[93]	training's l2: 0.00247898	valid_1's l2: 0.0030911
[94]	training's l2: 0.00247309	valid_1's l2: 0.00309112
[95]	training's l2: 0.0024664	valid_1's l2: 0.00308999
[96]	training's l2: 0.00246059	valid_1's l2: 0.00308924
[97]	training's l2: 0.00245478	valid_1's l2: 0.00309005
[98]	training's l2: 0.00244894	valid_1's l2: 0.00308867
[99]	training's l2: 0.00244306	valid_1's l2: 0.00308846
[100]	training's l2: 0.00243715	valid_1's l2: 0.00308796
[101]	training's l2: 0.00243178	valid_1's l2: 0.00308718
[102]	training's l2: 0.00242587	valid_1's l2: 0.0030876
[103]	training's l2: 0.00241993	valid_1's l2: 0.0030878
[104]	training's l2: 0.00241437	valid_1's l2: 0.00308659
[105]	training's l2: 0.00240758	valid_1's l2: 0.00308495
[106]	training's l2: 0.00240194	valid_1's l2: 0.00308546
[107]	training's l2: 0.00239649	valid_1's l2: 0.0030848
[108]	training's l2: 0.0023909	valid_1's l2: 0.00308472
[109]	training's l2: 0.00238547	valid_1's l2: 0.00308381
[110]	training's l2: 0.00237933	valid_1's l2: 0.0030835
[111]	training's l2: 0.00237401	valid_1's l2: 0.00308366
[112]	training's l2: 0.00236901	valid_1's l2: 0.00308386
[113]	training's l2: 0.0023638	valid_1's l2: 0.00308293
[114]	training's l2: 0.00235906	valid_1's l2: 0.00308305
[115]	training's l2: 0.0023539	valid_1's l2: 0.00308142
[116]	training's l2: 0.00234817	valid_1's l2: 0.00308047
[117]	training's l2: 0.00234279	valid_1's l2: 0.00307958
[118]	training's l2: 0.00233713	valid_1's l2: 0.00307826
[119]	training's l2: 0.00233192	valid_1's l2: 0.00307834
[120]	training's l2: 0.00232698	valid_1's l2: 0.00307816
[121]	training's l2: 0.00232136	valid_1's l2: 0.00307835
[122]	training's l2: 0.0023162	valid_1's l2: 0.00307787
[123]	training's l2: 0.00231096	valid_1's l2: 0.00307824
[124]	training's l2: 0.00230555	valid_1's l2: 0.0030783
[125]	training's l2: 0.00230029	valid_1's l2: 0.00307767
[126]	training's l2: 0.00229597	valid_1's l2: 0.00307755
[127]	training's l2: 0.00229089	valid_1's l2: 0.00307763
[128]	training's l2: 0.00228558	valid_1's l2: 0.0030774
[129]	training's l2: 0.00228047	valid_1's l2: 0.00307689
[130]	training's l2: 0.00227583	valid_1's l2: 0.00307679
[131]	training's l2: 0.00227161	valid_1's l2: 0.00307686
[132]	training's l2: 0.00226692	valid_1's l2: 0.00307711
[133]	training's l2: 0.00226198	valid_1's l2: 0.00307616
[134]	training's l2: 0.00225743	valid_1's l2: 0.00307664
[135]	training's l2: 0.00225242	valid_1's l2: 0.00307604
[136]	training's l2: 0.00224752	valid_1's l2: 0.00307567
[137]	training's l2: 0.00224291	valid_1's l2: 0.00307504
[138]	training's l2: 0.00223828	valid_1's l2: 0.0030749
[139]	training's l2: 0.00223319	valid_1's l2: 0.00307429
[140]	training's l2: 0.00222876	valid_1's l2: 0.00307407
[141]	training's l2: 0.00222404	valid_1's l2: 0.00307401
[142]	training's l2: 0.0022193	valid_1's l2: 0.00307448
[143]	training's l2: 0.00221478	valid_1's l2: 0.00307414
[144]	training's l2: 0.00221014	valid_1's l2: 0.00307459
[145]	training's l2: 0.00220605	valid_1's l2: 0.00307534
[146]	training's l2: 0.00220137	valid_1's l2: 0.00307562
[147]	training's l2: 0.00219709	valid_1's l2: 0.00307533
[148]	training's l2: 0.00219291	valid_1's l2: 0.00307504
[149]	training's l2: 0.0021888	valid_1's l2: 0.00307556
[150]	training's l2: 0.00218461	valid_1's l2: 0.00307607
[151]	training's l2: 0.00217991	valid_1's l2: 0.00307512
[152]	training's l2: 0.00217499	valid_1's l2: 0.00307473
[153]	training's l2: 0.00217126	valid_1's l2: 0.00307537
[154]	training's l2: 0.00216683	valid_1's l2: 0.00307534
[155]	training's l2: 0.00216207	valid_1's l2: 0.0030751
[156]	training's l2: 0.00215814	valid_1's l2: 0.00307542
[157]	training's l2: 0.00215404	valid_1's l2: 0.0030752
[158]	training's l2: 0.00214952	valid_1's l2: 0.00307543
[159]	training's l2: 0.00214512	valid_1's l2: 0.00307571
[160]	training's l2: 0.00214046	valid_1's l2: 0.00307592
[161]	training's l2: 0.00213636	valid_1's l2: 0.00307695
[162]	training's l2: 0.00213211	valid_1's l2: 0.00307727
[163]	training's l2: 0.00212761	valid_1's l2: 0.00307793
[164]	training's l2: 0.00212387	valid_1's l2: 0.00307843
[165]	training's l2: 0.00212028	valid_1's l2: 0.00307844
[166]	training's l2: 0.00211611	valid_1's l2: 0.00307812
[167]	training's l2: 0.0021123	valid_1's l2: 0.00307767
[168]	training's l2: 0.00210847	valid_1's l2: 0.00307809
[169]	training's l2: 0.00210422	valid_1's l2: 0.00307893
[170]	training's l2: 0.00210024	valid_1's l2: 0.00307823
[171]	training's l2: 0.0020963	valid_1's l2: 0.00307826
Early stopping, best iteration is:
[141]	training's l2: 0.00222404	valid_1's l2: 0.00307401
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241574 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00397528	valid_1's l2: 0.00368233
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00389253	valid_1's l2: 0.00362372
[3]	training's l2: 0.00381754	valid_1's l2: 0.00357298
[4]	training's l2: 0.00375434	valid_1's l2: 0.00352978
[5]	training's l2: 0.00369683	valid_1's l2: 0.00349503
[6]	training's l2: 0.00364067	valid_1's l2: 0.00346075
[7]	training's l2: 0.00359224	valid_1's l2: 0.00343169
[8]	training's l2: 0.0035469	valid_1's l2: 0.00340537
[9]	training's l2: 0.00350474	valid_1's l2: 0.00338081
[10]	training's l2: 0.00346435	valid_1's l2: 0.00335939
[11]	training's l2: 0.0034259	valid_1's l2: 0.00333871
[12]	training's l2: 0.00339113	valid_1's l2: 0.00332237
[13]	training's l2: 0.00335733	valid_1's l2: 0.00330601
[14]	training's l2: 0.00332823	valid_1's l2: 0.003295
[15]	training's l2: 0.00330022	valid_1's l2: 0.00328287
[16]	training's l2: 0.00327354	valid_1's l2: 0.00327064
[17]	training's l2: 0.00324592	valid_1's l2: 0.00325603
[18]	training's l2: 0.00322207	valid_1's l2: 0.00324724
[19]	training's l2: 0.00320102	valid_1's l2: 0.00323976
[20]	training's l2: 0.00318064	valid_1's l2: 0.00323256
[21]	training's l2: 0.00316137	valid_1's l2: 0.00322648
[22]	training's l2: 0.00314063	valid_1's l2: 0.00322005
[23]	training's l2: 0.00312273	valid_1's l2: 0.00321472
[24]	training's l2: 0.0031029	valid_1's l2: 0.00320705
[25]	training's l2: 0.00308583	valid_1's l2: 0.00320167
[26]	training's l2: 0.00307119	valid_1's l2: 0.00319976
[27]	training's l2: 0.00305446	valid_1's l2: 0.00319464
[28]	training's l2: 0.00303861	valid_1's l2: 0.00319067
[29]	training's l2: 0.00302533	valid_1's l2: 0.00318759
[30]	training's l2: 0.00301215	valid_1's l2: 0.00318583
[31]	training's l2: 0.00299948	valid_1's l2: 0.0031818
[32]	training's l2: 0.00298621	valid_1's l2: 0.00317903
[33]	training's l2: 0.0029742	valid_1's l2: 0.00317635
[34]	training's l2: 0.00296185	valid_1's l2: 0.00317436
[35]	training's l2: 0.00294915	valid_1's l2: 0.00316799
[36]	training's l2: 0.00293613	valid_1's l2: 0.0031644
[37]	training's l2: 0.00292535	valid_1's l2: 0.00316172
[38]	training's l2: 0.00291373	valid_1's l2: 0.00315706
[39]	training's l2: 0.00290372	valid_1's l2: 0.00315483
[40]	training's l2: 0.00289358	valid_1's l2: 0.00315407
[41]	training's l2: 0.00288377	valid_1's l2: 0.00315247
[42]	training's l2: 0.00287306	valid_1's l2: 0.00314946
[43]	training's l2: 0.00286391	valid_1's l2: 0.00314838
[44]	training's l2: 0.00285288	valid_1's l2: 0.00314607
[45]	training's l2: 0.00284383	valid_1's l2: 0.00314381
[46]	training's l2: 0.00283341	valid_1's l2: 0.00314134
[47]	training's l2: 0.00282311	valid_1's l2: 0.00313884
[48]	training's l2: 0.00281234	valid_1's l2: 0.00313432
[49]	training's l2: 0.00280274	valid_1's l2: 0.0031332
[50]	training's l2: 0.00279387	valid_1's l2: 0.00313128
[51]	training's l2: 0.00278381	valid_1's l2: 0.00312923
[52]	training's l2: 0.00277305	valid_1's l2: 0.00312541
[53]	training's l2: 0.00276431	valid_1's l2: 0.00312583
[54]	training's l2: 0.0027555	valid_1's l2: 0.00312531
[55]	training's l2: 0.00274648	valid_1's l2: 0.00312229
[56]	training's l2: 0.0027368	valid_1's l2: 0.00311848
[57]	training's l2: 0.00272774	valid_1's l2: 0.00311789
[58]	training's l2: 0.00271903	valid_1's l2: 0.00311525
[59]	training's l2: 0.00270954	valid_1's l2: 0.00311271
[60]	training's l2: 0.00270135	valid_1's l2: 0.003112
[61]	training's l2: 0.00269342	valid_1's l2: 0.00311082
[62]	training's l2: 0.00268447	valid_1's l2: 0.0031092
[63]	training's l2: 0.00267569	valid_1's l2: 0.00310721
[64]	training's l2: 0.00266729	valid_1's l2: 0.00310634
[65]	training's l2: 0.00266003	valid_1's l2: 0.00310672
[66]	training's l2: 0.00265205	valid_1's l2: 0.00310533
[67]	training's l2: 0.00264434	valid_1's l2: 0.00310426
[68]	training's l2: 0.0026369	valid_1's l2: 0.00310418
[69]	training's l2: 0.00262905	valid_1's l2: 0.00310269
[70]	training's l2: 0.00262205	valid_1's l2: 0.00310211
[71]	training's l2: 0.00261484	valid_1's l2: 0.0031012
[72]	training's l2: 0.00260659	valid_1's l2: 0.0030986
[73]	training's l2: 0.00259974	valid_1's l2: 0.00309749
[74]	training's l2: 0.00259209	valid_1's l2: 0.00309596
[75]	training's l2: 0.00258453	valid_1's l2: 0.00309515
[76]	training's l2: 0.0025778	valid_1's l2: 0.00309454
[77]	training's l2: 0.00257032	valid_1's l2: 0.0030944
[78]	training's l2: 0.00256265	valid_1's l2: 0.0030931
[79]	training's l2: 0.00255594	valid_1's l2: 0.00309189
[80]	training's l2: 0.00254863	valid_1's l2: 0.00308981
[81]	training's l2: 0.00254152	valid_1's l2: 0.0030895
[82]	training's l2: 0.00253471	valid_1's l2: 0.0030903
[83]	training's l2: 0.00252812	valid_1's l2: 0.00308989
[84]	training's l2: 0.00252112	valid_1's l2: 0.00308946
[85]	training's l2: 0.00251461	valid_1's l2: 0.00308914
[86]	training's l2: 0.00250826	valid_1's l2: 0.00308795
[87]	training's l2: 0.00250177	valid_1's l2: 0.00308813
[88]	training's l2: 0.00249503	valid_1's l2: 0.00308749
[89]	training's l2: 0.00248797	valid_1's l2: 0.00308688
[90]	training's l2: 0.0024812	valid_1's l2: 0.00308544
[91]	training's l2: 0.00247468	valid_1's l2: 0.00308399
[92]	training's l2: 0.0024683	valid_1's l2: 0.00308333
[93]	training's l2: 0.00246188	valid_1's l2: 0.00308341
[94]	training's l2: 0.00245547	valid_1's l2: 0.00308239
[95]	training's l2: 0.0024493	valid_1's l2: 0.00308308
[96]	training's l2: 0.00244297	valid_1's l2: 0.00308255
[97]	training's l2: 0.00243664	valid_1's l2: 0.00308189
[98]	training's l2: 0.00243046	valid_1's l2: 0.00308164
[99]	training's l2: 0.00242395	valid_1's l2: 0.00308053
[100]	training's l2: 0.00241834	valid_1's l2: 0.0030792
[101]	training's l2: 0.0024122	valid_1's l2: 0.00307878
[102]	training's l2: 0.0024061	valid_1's l2: 0.00307847
[103]	training's l2: 0.00240017	valid_1's l2: 0.00307797
[104]	training's l2: 0.00239387	valid_1's l2: 0.00307679
[105]	training's l2: 0.00238789	valid_1's l2: 0.00307698
[106]	training's l2: 0.00238164	valid_1's l2: 0.00307611
[107]	training's l2: 0.00237593	valid_1's l2: 0.00307689
[108]	training's l2: 0.00237016	valid_1's l2: 0.00307721
[109]	training's l2: 0.00236366	valid_1's l2: 0.00307558
[110]	training's l2: 0.00235778	valid_1's l2: 0.0030751
[111]	training's l2: 0.00235173	valid_1's l2: 0.00307511
[112]	training's l2: 0.00234589	valid_1's l2: 0.0030756
[113]	training's l2: 0.00234041	valid_1's l2: 0.003075
[114]	training's l2: 0.00233484	valid_1's l2: 0.00307559
[115]	training's l2: 0.00232893	valid_1's l2: 0.00307549
[116]	training's l2: 0.0023233	valid_1's l2: 0.00307549
[117]	training's l2: 0.00231759	valid_1's l2: 0.00307596
[118]	training's l2: 0.00231211	valid_1's l2: 0.00307521
[119]	training's l2: 0.00230621	valid_1's l2: 0.00307505
[120]	training's l2: 0.00230069	valid_1's l2: 0.00307366
[121]	training's l2: 0.00229485	valid_1's l2: 0.00307446
[122]	training's l2: 0.00228923	valid_1's l2: 0.00307469
[123]	training's l2: 0.00228392	valid_1's l2: 0.0030745
[124]	training's l2: 0.00227796	valid_1's l2: 0.00307498
[125]	training's l2: 0.00227269	valid_1's l2: 0.00307353
[126]	training's l2: 0.00226695	valid_1's l2: 0.00307349
[127]	training's l2: 0.00226195	valid_1's l2: 0.00307305
[128]	training's l2: 0.00225605	valid_1's l2: 0.0030733
[129]	training's l2: 0.00225077	valid_1's l2: 0.00307386
[130]	training's l2: 0.00224491	valid_1's l2: 0.00307442
[131]	training's l2: 0.00223966	valid_1's l2: 0.00307427
[132]	training's l2: 0.0022346	valid_1's l2: 0.00307402
[133]	training's l2: 0.00222915	valid_1's l2: 0.00307361
[134]	training's l2: 0.00222377	valid_1's l2: 0.0030731
[135]	training's l2: 0.00221848	valid_1's l2: 0.00307359
[136]	training's l2: 0.00221354	valid_1's l2: 0.00307345
[137]	training's l2: 0.00220867	valid_1's l2: 0.00307371
[138]	training's l2: 0.00220352	valid_1's l2: 0.00307278
[139]	training's l2: 0.00219801	valid_1's l2: 0.00307294
[140]	training's l2: 0.00219318	valid_1's l2: 0.00307364
[141]	training's l2: 0.00218784	valid_1's l2: 0.00307366
[142]	training's l2: 0.00218252	valid_1's l2: 0.00307352
[143]	training's l2: 0.00217763	valid_1's l2: 0.003074
[144]	training's l2: 0.00217207	valid_1's l2: 0.00307422
[145]	training's l2: 0.00216753	valid_1's l2: 0.003074
[146]	training's l2: 0.00216264	valid_1's l2: 0.00307437
[147]	training's l2: 0.00215751	valid_1's l2: 0.00307442
[148]	training's l2: 0.0021525	valid_1's l2: 0.00307431
[149]	training's l2: 0.00214754	valid_1's l2: 0.00307429
[150]	training's l2: 0.00214318	valid_1's l2: 0.00307481
[151]	training's l2: 0.00213835	valid_1's l2: 0.00307483
[152]	training's l2: 0.00213423	valid_1's l2: 0.00307513
[153]	training's l2: 0.00212934	valid_1's l2: 0.00307565
[154]	training's l2: 0.00212482	valid_1's l2: 0.00307539
[155]	training's l2: 0.0021201	valid_1's l2: 0.00307456
[156]	training's l2: 0.00211486	valid_1's l2: 0.00307415
[157]	training's l2: 0.00211	valid_1's l2: 0.00307385
[158]	training's l2: 0.0021053	valid_1's l2: 0.00307402
[159]	training's l2: 0.00210084	valid_1's l2: 0.00307409
[160]	training's l2: 0.00209619	valid_1's l2: 0.00307401
[161]	training's l2: 0.00209164	valid_1's l2: 0.00307475
[162]	training's l2: 0.0020871	valid_1's l2: 0.00307489
[163]	training's l2: 0.00208222	valid_1's l2: 0.00307551
[164]	training's l2: 0.00207729	valid_1's l2: 0.0030752
[165]	training's l2: 0.0020725	valid_1's l2: 0.00307589
[166]	training's l2: 0.00206787	valid_1's l2: 0.00307598
[167]	training's l2: 0.00206335	valid_1's l2: 0.00307473
[168]	training's l2: 0.00205868	valid_1's l2: 0.00307484
Early stopping, best iteration is:
[138]	training's l2: 0.00220352	valid_1's l2: 0.00307278
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282267 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00397624	valid_1's l2: 0.00368574
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00389396	valid_1's l2: 0.00362813
[3]	training's l2: 0.00381945	valid_1's l2: 0.00357947
[4]	training's l2: 0.00375657	valid_1's l2: 0.00353997
[5]	training's l2: 0.00369856	valid_1's l2: 0.003506
[6]	training's l2: 0.00364164	valid_1's l2: 0.00347235
[7]	training's l2: 0.00359261	valid_1's l2: 0.00344168
[8]	training's l2: 0.00354764	valid_1's l2: 0.00341666
[9]	training's l2: 0.00350504	valid_1's l2: 0.00339349
[10]	training's l2: 0.00346505	valid_1's l2: 0.00337254
[11]	training's l2: 0.00342658	valid_1's l2: 0.00335102
[12]	training's l2: 0.00339213	valid_1's l2: 0.00333519
[13]	training's l2: 0.00335796	valid_1's l2: 0.00331663
[14]	training's l2: 0.00332849	valid_1's l2: 0.00330271
[15]	training's l2: 0.00330043	valid_1's l2: 0.00329101
[16]	training's l2: 0.00327332	valid_1's l2: 0.00327946
[17]	training's l2: 0.00324539	valid_1's l2: 0.00326579
[18]	training's l2: 0.00322167	valid_1's l2: 0.00325524
[19]	training's l2: 0.00319837	valid_1's l2: 0.00324727
[20]	training's l2: 0.00317615	valid_1's l2: 0.00324
[21]	training's l2: 0.00315448	valid_1's l2: 0.00323416
[22]	training's l2: 0.00313371	valid_1's l2: 0.00322505
[23]	training's l2: 0.00311552	valid_1's l2: 0.00321872
[24]	training's l2: 0.00309583	valid_1's l2: 0.00321336
[25]	training's l2: 0.00307678	valid_1's l2: 0.00320711
[26]	training's l2: 0.00306053	valid_1's l2: 0.00320281
[27]	training's l2: 0.00304323	valid_1's l2: 0.00319897
[28]	training's l2: 0.00302711	valid_1's l2: 0.00319293
[29]	training's l2: 0.00301153	valid_1's l2: 0.00318808
[30]	training's l2: 0.0029976	valid_1's l2: 0.00318504
[31]	training's l2: 0.00298406	valid_1's l2: 0.00318229
[32]	training's l2: 0.00296962	valid_1's l2: 0.00317938
[33]	training's l2: 0.00295731	valid_1's l2: 0.00317566
[34]	training's l2: 0.00294524	valid_1's l2: 0.00317402
[35]	training's l2: 0.00293142	valid_1's l2: 0.00316841
[36]	training's l2: 0.00292043	valid_1's l2: 0.00316606
[37]	training's l2: 0.00290934	valid_1's l2: 0.00316339
[38]	training's l2: 0.00289621	valid_1's l2: 0.00316055
[39]	training's l2: 0.00288561	valid_1's l2: 0.00315788
[40]	training's l2: 0.00287426	valid_1's l2: 0.00315736
[41]	training's l2: 0.00286411	valid_1's l2: 0.00315548
[42]	training's l2: 0.00285332	valid_1's l2: 0.0031534
[43]	training's l2: 0.00284145	valid_1's l2: 0.00315009
[44]	training's l2: 0.00283093	valid_1's l2: 0.0031491
[45]	training's l2: 0.00282074	valid_1's l2: 0.00314673
[46]	training's l2: 0.00280887	valid_1's l2: 0.00314188
[47]	training's l2: 0.00279935	valid_1's l2: 0.00313982
[48]	training's l2: 0.00278869	valid_1's l2: 0.00313588
[49]	training's l2: 0.00277946	valid_1's l2: 0.00313395
[50]	training's l2: 0.0027694	valid_1's l2: 0.00313177
[51]	training's l2: 0.00275916	valid_1's l2: 0.00312871
[52]	training's l2: 0.00274882	valid_1's l2: 0.00312758
[53]	training's l2: 0.00273879	valid_1's l2: 0.00312509
[54]	training's l2: 0.00272972	valid_1's l2: 0.00312237
[55]	training's l2: 0.00271871	valid_1's l2: 0.0031198
[56]	training's l2: 0.00270943	valid_1's l2: 0.00311905
[57]	training's l2: 0.00270085	valid_1's l2: 0.00311913
[58]	training's l2: 0.00269203	valid_1's l2: 0.00311874
[59]	training's l2: 0.00268259	valid_1's l2: 0.00311476
[60]	training's l2: 0.00267417	valid_1's l2: 0.003113
[61]	training's l2: 0.00266622	valid_1's l2: 0.00311339
[62]	training's l2: 0.00265837	valid_1's l2: 0.00311292
[63]	training's l2: 0.00264901	valid_1's l2: 0.00310973
[64]	training's l2: 0.00264083	valid_1's l2: 0.00311007
[65]	training's l2: 0.00263351	valid_1's l2: 0.00311
[66]	training's l2: 0.00262542	valid_1's l2: 0.00310965
[67]	training's l2: 0.00261606	valid_1's l2: 0.00310765
[68]	training's l2: 0.00260809	valid_1's l2: 0.00310618
[69]	training's l2: 0.00260067	valid_1's l2: 0.00310622
[70]	training's l2: 0.00259353	valid_1's l2: 0.00310663
[71]	training's l2: 0.002585	valid_1's l2: 0.00310316
[72]	training's l2: 0.00257725	valid_1's l2: 0.00310182
[73]	training's l2: 0.00256888	valid_1's l2: 0.00310057
[74]	training's l2: 0.00256157	valid_1's l2: 0.00309868
[75]	training's l2: 0.00255454	valid_1's l2: 0.00309855
[76]	training's l2: 0.0025474	valid_1's l2: 0.0030968
[77]	training's l2: 0.00253992	valid_1's l2: 0.00309566
[78]	training's l2: 0.00253334	valid_1's l2: 0.00309511
[79]	training's l2: 0.00252598	valid_1's l2: 0.00309473
[80]	training's l2: 0.00251897	valid_1's l2: 0.00309476
[81]	training's l2: 0.00251221	valid_1's l2: 0.00309422
[82]	training's l2: 0.00250501	valid_1's l2: 0.00309428
[83]	training's l2: 0.00249807	valid_1's l2: 0.0030949
[84]	training's l2: 0.0024912	valid_1's l2: 0.003094
[85]	training's l2: 0.0024846	valid_1's l2: 0.00309395
[86]	training's l2: 0.00247769	valid_1's l2: 0.00309407
[87]	training's l2: 0.00247051	valid_1's l2: 0.00309398
[88]	training's l2: 0.00246381	valid_1's l2: 0.00309323
[89]	training's l2: 0.002457	valid_1's l2: 0.00309316
[90]	training's l2: 0.00245049	valid_1's l2: 0.0030936
[91]	training's l2: 0.00244405	valid_1's l2: 0.00309409
[92]	training's l2: 0.00243759	valid_1's l2: 0.00309493
[93]	training's l2: 0.0024315	valid_1's l2: 0.00309474
[94]	training's l2: 0.00242514	valid_1's l2: 0.00309552
[95]	training's l2: 0.00241907	valid_1's l2: 0.00309493
[96]	training's l2: 0.0024123	valid_1's l2: 0.00309489
[97]	training's l2: 0.0024059	valid_1's l2: 0.00309348
[98]	training's l2: 0.00239986	valid_1's l2: 0.00309377
[99]	training's l2: 0.00239369	valid_1's l2: 0.00309361
[100]	training's l2: 0.00238765	valid_1's l2: 0.00309427
[101]	training's l2: 0.00238059	valid_1's l2: 0.00309288
[102]	training's l2: 0.00237453	valid_1's l2: 0.00309249
[103]	training's l2: 0.00236822	valid_1's l2: 0.0030914
[104]	training's l2: 0.0023618	valid_1's l2: 0.00309068
[105]	training's l2: 0.00235575	valid_1's l2: 0.00308975
[106]	training's l2: 0.00234993	valid_1's l2: 0.00308981
[107]	training's l2: 0.00234374	valid_1's l2: 0.00308875
[108]	training's l2: 0.0023384	valid_1's l2: 0.00308851
[109]	training's l2: 0.00233242	valid_1's l2: 0.00308861
[110]	training's l2: 0.00232576	valid_1's l2: 0.00308779
[111]	training's l2: 0.00231993	valid_1's l2: 0.0030873
[112]	training's l2: 0.00231416	valid_1's l2: 0.00308715
[113]	training's l2: 0.0023079	valid_1's l2: 0.00308709
[114]	training's l2: 0.00230158	valid_1's l2: 0.00308628
[115]	training's l2: 0.00229572	valid_1's l2: 0.00308692
[116]	training's l2: 0.00229014	valid_1's l2: 0.00308599
[117]	training's l2: 0.00228415	valid_1's l2: 0.00308492
[118]	training's l2: 0.0022785	valid_1's l2: 0.00308471
[119]	training's l2: 0.00227288	valid_1's l2: 0.00308339
Did not meet early stopping. Best iteration is:
[119]	training's l2: 0.00227288	valid_1's l2: 0.00308339
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.284065 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00395999	valid_1's l2: 0.00367949
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00386328	valid_1's l2: 0.00361532
[3]	training's l2: 0.00377723	valid_1's l2: 0.00356226
[4]	training's l2: 0.00369842	valid_1's l2: 0.00351767
[5]	training's l2: 0.00362939	valid_1's l2: 0.00347916
[6]	training's l2: 0.00356709	valid_1's l2: 0.00344669
[7]	training's l2: 0.00350883	valid_1's l2: 0.00341589
[8]	training's l2: 0.00345699	valid_1's l2: 0.00338953
[9]	training's l2: 0.00340837	valid_1's l2: 0.00336649
[10]	training's l2: 0.0033608	valid_1's l2: 0.00334582
[11]	training's l2: 0.00331994	valid_1's l2: 0.00332905
[12]	training's l2: 0.00328224	valid_1's l2: 0.00331242
[13]	training's l2: 0.00324467	valid_1's l2: 0.00329594
[14]	training's l2: 0.00320756	valid_1's l2: 0.00327774
[15]	training's l2: 0.00317677	valid_1's l2: 0.00326705
[16]	training's l2: 0.00314821	valid_1's l2: 0.00325779
[17]	training's l2: 0.00312104	valid_1's l2: 0.00324923
[18]	training's l2: 0.00309634	valid_1's l2: 0.00324233
[19]	training's l2: 0.00306956	valid_1's l2: 0.00323043
[20]	training's l2: 0.00304737	valid_1's l2: 0.00322184
[21]	training's l2: 0.00302559	valid_1's l2: 0.00321564
[22]	training's l2: 0.00300332	valid_1's l2: 0.00320835
[23]	training's l2: 0.00298261	valid_1's l2: 0.00320246
[24]	training's l2: 0.00296215	valid_1's l2: 0.00319761
[25]	training's l2: 0.00294252	valid_1's l2: 0.00319364
[26]	training's l2: 0.00292515	valid_1's l2: 0.00318761
[27]	training's l2: 0.00290811	valid_1's l2: 0.00318404
[28]	training's l2: 0.00289157	valid_1's l2: 0.00317814
[29]	training's l2: 0.00287701	valid_1's l2: 0.00317616
[30]	training's l2: 0.00286145	valid_1's l2: 0.00317177
[31]	training's l2: 0.00284752	valid_1's l2: 0.00317017
[32]	training's l2: 0.00283418	valid_1's l2: 0.0031673
[33]	training's l2: 0.00282031	valid_1's l2: 0.00316535
[34]	training's l2: 0.00280782	valid_1's l2: 0.00316214
[35]	training's l2: 0.00279539	valid_1's l2: 0.00315875
[36]	training's l2: 0.00278162	valid_1's l2: 0.00315514
[37]	training's l2: 0.00276882	valid_1's l2: 0.00315214
[38]	training's l2: 0.00275658	valid_1's l2: 0.00315148
[39]	training's l2: 0.00274492	valid_1's l2: 0.00314762
[40]	training's l2: 0.00273337	valid_1's l2: 0.00314524
[41]	training's l2: 0.00272122	valid_1's l2: 0.00314207
[42]	training's l2: 0.00271059	valid_1's l2: 0.00314026
[43]	training's l2: 0.00269903	valid_1's l2: 0.00313772
[44]	training's l2: 0.00268829	valid_1's l2: 0.00313611
[45]	training's l2: 0.00267705	valid_1's l2: 0.00313396
[46]	training's l2: 0.00266606	valid_1's l2: 0.00313165
[47]	training's l2: 0.00265593	valid_1's l2: 0.00313081
[48]	training's l2: 0.00264576	valid_1's l2: 0.00312872
[49]	training's l2: 0.00263508	valid_1's l2: 0.00312629
[50]	training's l2: 0.00262482	valid_1's l2: 0.00312578
[51]	training's l2: 0.00261315	valid_1's l2: 0.00312071
[52]	training's l2: 0.00260295	valid_1's l2: 0.00311802
[53]	training's l2: 0.00259311	valid_1's l2: 0.00311688
[54]	training's l2: 0.00258319	valid_1's l2: 0.00311422
[55]	training's l2: 0.00257356	valid_1's l2: 0.0031114
[56]	training's l2: 0.00256422	valid_1's l2: 0.00311078
[57]	training's l2: 0.00255542	valid_1's l2: 0.00310995
[58]	training's l2: 0.00254488	valid_1's l2: 0.00310627
[59]	training's l2: 0.00253594	valid_1's l2: 0.00310514
[60]	training's l2: 0.00252611	valid_1's l2: 0.00310307
[61]	training's l2: 0.00251658	valid_1's l2: 0.00310133
[62]	training's l2: 0.00250823	valid_1's l2: 0.00310039
[63]	training's l2: 0.0024982	valid_1's l2: 0.00309745
[64]	training's l2: 0.00248975	valid_1's l2: 0.003097
[65]	training's l2: 0.00248203	valid_1's l2: 0.00309589
[66]	training's l2: 0.00247385	valid_1's l2: 0.00309509
[67]	training's l2: 0.00246575	valid_1's l2: 0.00309528
[68]	training's l2: 0.00245742	valid_1's l2: 0.00309415
[69]	training's l2: 0.00244926	valid_1's l2: 0.00309361
[70]	training's l2: 0.00244149	valid_1's l2: 0.00309249
[71]	training's l2: 0.00243315	valid_1's l2: 0.00309035
[72]	training's l2: 0.00242521	valid_1's l2: 0.00308911
[73]	training's l2: 0.00241741	valid_1's l2: 0.00308788
[74]	training's l2: 0.0024093	valid_1's l2: 0.00308749
[75]	training's l2: 0.00240118	valid_1's l2: 0.00308504
[76]	training's l2: 0.0023936	valid_1's l2: 0.00308453
[77]	training's l2: 0.00238588	valid_1's l2: 0.00308462
[78]	training's l2: 0.00237934	valid_1's l2: 0.00308443
[79]	training's l2: 0.00237164	valid_1's l2: 0.00308304
[80]	training's l2: 0.00236436	valid_1's l2: 0.003082
[81]	training's l2: 0.002357	valid_1's l2: 0.00308169
[82]	training's l2: 0.0023498	valid_1's l2: 0.00308044
[83]	training's l2: 0.00234227	valid_1's l2: 0.00308
[84]	training's l2: 0.0023341	valid_1's l2: 0.00307864
[85]	training's l2: 0.00232677	valid_1's l2: 0.00307891
[86]	training's l2: 0.00231956	valid_1's l2: 0.00307863
[87]	training's l2: 0.00231301	valid_1's l2: 0.00307912
[88]	training's l2: 0.00230561	valid_1's l2: 0.0030784
[89]	training's l2: 0.00229806	valid_1's l2: 0.00307714
[90]	training's l2: 0.00229043	valid_1's l2: 0.00307616
[91]	training's l2: 0.00228361	valid_1's l2: 0.00307578
[92]	training's l2: 0.00227672	valid_1's l2: 0.00307521
[93]	training's l2: 0.00226975	valid_1's l2: 0.00307467
[94]	training's l2: 0.00226333	valid_1's l2: 0.00307523
[95]	training's l2: 0.00225631	valid_1's l2: 0.00307526
[96]	training's l2: 0.00224929	valid_1's l2: 0.00307417
[97]	training's l2: 0.00224324	valid_1's l2: 0.00307332
[98]	training's l2: 0.00223671	valid_1's l2: 0.00307239
[99]	training's l2: 0.00223112	valid_1's l2: 0.00307126
[100]	training's l2: 0.0022246	valid_1's l2: 0.00307062
[101]	training's l2: 0.00221846	valid_1's l2: 0.00307032
[102]	training's l2: 0.00221223	valid_1's l2: 0.00306991
[103]	training's l2: 0.00220562	valid_1's l2: 0.00306858
[104]	training's l2: 0.00220012	valid_1's l2: 0.00306832
[105]	training's l2: 0.00219384	valid_1's l2: 0.00306819
[106]	training's l2: 0.0021877	valid_1's l2: 0.00306772
[107]	training's l2: 0.00218171	valid_1's l2: 0.00306776
[108]	training's l2: 0.00217499	valid_1's l2: 0.00306689
[109]	training's l2: 0.00216987	valid_1's l2: 0.00306642
[110]	training's l2: 0.00216392	valid_1's l2: 0.00306622
[111]	training's l2: 0.00215832	valid_1's l2: 0.00306672
[112]	training's l2: 0.002152	valid_1's l2: 0.00306584
[113]	training's l2: 0.0021465	valid_1's l2: 0.00306534
[114]	training's l2: 0.00213974	valid_1's l2: 0.00306333
[115]	training's l2: 0.00213445	valid_1's l2: 0.00306324
[116]	training's l2: 0.00212837	valid_1's l2: 0.00306249
[117]	training's l2: 0.00212171	valid_1's l2: 0.00306016
[118]	training's l2: 0.00211603	valid_1's l2: 0.00305983
[119]	training's l2: 0.00210996	valid_1's l2: 0.00306013
[120]	training's l2: 0.00210366	valid_1's l2: 0.00305928
[121]	training's l2: 0.00209757	valid_1's l2: 0.00305943
[122]	training's l2: 0.00209147	valid_1's l2: 0.00305804
[123]	training's l2: 0.00208629	valid_1's l2: 0.00305854
[124]	training's l2: 0.00208026	valid_1's l2: 0.00305888
[125]	training's l2: 0.00207558	valid_1's l2: 0.00305915
[126]	training's l2: 0.00206953	valid_1's l2: 0.00305933
[127]	training's l2: 0.00206351	valid_1's l2: 0.00305947
[128]	training's l2: 0.0020573	valid_1's l2: 0.00305903
[129]	training's l2: 0.0020524	valid_1's l2: 0.00305841
[130]	training's l2: 0.0020474	valid_1's l2: 0.00305797
[131]	training's l2: 0.00204273	valid_1's l2: 0.00305866
[132]	training's l2: 0.00203704	valid_1's l2: 0.00305856
[133]	training's l2: 0.00203164	valid_1's l2: 0.00305903
[134]	training's l2: 0.00202594	valid_1's l2: 0.00305884
[135]	training's l2: 0.00202097	valid_1's l2: 0.0030591
[136]	training's l2: 0.00201576	valid_1's l2: 0.0030591
[137]	training's l2: 0.00201079	valid_1's l2: 0.00305902
[138]	training's l2: 0.00200505	valid_1's l2: 0.00305857
[139]	training's l2: 0.0019997	valid_1's l2: 0.00305826
[140]	training's l2: 0.00199504	valid_1's l2: 0.00305804
[141]	training's l2: 0.00199022	valid_1's l2: 0.00305819
[142]	training's l2: 0.00198501	valid_1's l2: 0.00305858
[143]	training's l2: 0.0019802	valid_1's l2: 0.00305843
Did not meet early stopping. Best iteration is:
[143]	training's l2: 0.0019802	valid_1's l2: 0.00305843
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.283879 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00394415	valid_1's l2: 0.00366274
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00383583	valid_1's l2: 0.00358556
[3]	training's l2: 0.00374749	valid_1's l2: 0.00352722
[4]	training's l2: 0.00366884	valid_1's l2: 0.00347945
[5]	training's l2: 0.00359776	valid_1's l2: 0.00343609
[6]	training's l2: 0.00353617	valid_1's l2: 0.00340122
[7]	training's l2: 0.00347985	valid_1's l2: 0.00336983
[8]	training's l2: 0.00342346	valid_1's l2: 0.00334418
[9]	training's l2: 0.00337757	valid_1's l2: 0.00332063
[10]	training's l2: 0.00333371	valid_1's l2: 0.00329873
[11]	training's l2: 0.00329259	valid_1's l2: 0.0032827
[12]	training's l2: 0.00325504	valid_1's l2: 0.00326982
[13]	training's l2: 0.0032227	valid_1's l2: 0.00325793
[14]	training's l2: 0.00319023	valid_1's l2: 0.00324526
[15]	training's l2: 0.00315789	valid_1's l2: 0.00323293
[16]	training's l2: 0.003129	valid_1's l2: 0.00322289
[17]	training's l2: 0.00310416	valid_1's l2: 0.0032148
[18]	training's l2: 0.00308031	valid_1's l2: 0.00320898
[19]	training's l2: 0.00305698	valid_1's l2: 0.00320124
[20]	training's l2: 0.00303548	valid_1's l2: 0.00319667
[21]	training's l2: 0.00301519	valid_1's l2: 0.00319083
[22]	training's l2: 0.00299478	valid_1's l2: 0.00318724
[23]	training's l2: 0.00297502	valid_1's l2: 0.00317967
[24]	training's l2: 0.00295616	valid_1's l2: 0.00317458
[25]	training's l2: 0.00293766	valid_1's l2: 0.00317265
[26]	training's l2: 0.00292129	valid_1's l2: 0.00317039
[27]	training's l2: 0.00290509	valid_1's l2: 0.0031692
[28]	training's l2: 0.00288739	valid_1's l2: 0.00316438
[29]	training's l2: 0.00287028	valid_1's l2: 0.00315923
[30]	training's l2: 0.00285596	valid_1's l2: 0.00315458
[31]	training's l2: 0.00283978	valid_1's l2: 0.0031529
[32]	training's l2: 0.00282463	valid_1's l2: 0.00315086
[33]	training's l2: 0.00280798	valid_1's l2: 0.00314777
[34]	training's l2: 0.00279515	valid_1's l2: 0.00314512
[35]	training's l2: 0.00278136	valid_1's l2: 0.00314204
[36]	training's l2: 0.00276766	valid_1's l2: 0.00314001
[37]	training's l2: 0.00275489	valid_1's l2: 0.0031398
[38]	training's l2: 0.00274175	valid_1's l2: 0.00313688
[39]	training's l2: 0.00272765	valid_1's l2: 0.00313259
[40]	training's l2: 0.00271476	valid_1's l2: 0.00313228
[41]	training's l2: 0.00270154	valid_1's l2: 0.00312841
[42]	training's l2: 0.00268942	valid_1's l2: 0.00312542
[43]	training's l2: 0.00267752	valid_1's l2: 0.00312276
[44]	training's l2: 0.00266371	valid_1's l2: 0.00312105
[45]	training's l2: 0.00265006	valid_1's l2: 0.00311352
[46]	training's l2: 0.00263746	valid_1's l2: 0.00311395
[47]	training's l2: 0.00262548	valid_1's l2: 0.00311365
[48]	training's l2: 0.00261351	valid_1's l2: 0.00311344
[49]	training's l2: 0.00260182	valid_1's l2: 0.00311354
[50]	training's l2: 0.00259067	valid_1's l2: 0.00311119
[51]	training's l2: 0.00257946	valid_1's l2: 0.00310973
[52]	training's l2: 0.00256865	valid_1's l2: 0.00310838
[53]	training's l2: 0.00255799	valid_1's l2: 0.00310914
[54]	training's l2: 0.00254784	valid_1's l2: 0.00310914
[55]	training's l2: 0.00253704	valid_1's l2: 0.00310878
[56]	training's l2: 0.00252579	valid_1's l2: 0.0031066
[57]	training's l2: 0.0025149	valid_1's l2: 0.00310382
[58]	training's l2: 0.00250504	valid_1's l2: 0.00310487
[59]	training's l2: 0.00249519	valid_1's l2: 0.00310402
[60]	training's l2: 0.00248516	valid_1's l2: 0.00310298
[61]	training's l2: 0.0024738	valid_1's l2: 0.00310097
[62]	training's l2: 0.00246359	valid_1's l2: 0.00309971
[63]	training's l2: 0.00245357	valid_1's l2: 0.00310079
[64]	training's l2: 0.00244327	valid_1's l2: 0.00309991
[65]	training's l2: 0.00243411	valid_1's l2: 0.00310009
[66]	training's l2: 0.0024244	valid_1's l2: 0.00309963
[67]	training's l2: 0.00241526	valid_1's l2: 0.00309895
[68]	training's l2: 0.00240605	valid_1's l2: 0.00309888
[69]	training's l2: 0.0023969	valid_1's l2: 0.00309872
[70]	training's l2: 0.00238795	valid_1's l2: 0.00309917
[71]	training's l2: 0.00237931	valid_1's l2: 0.00309881
[72]	training's l2: 0.00237101	valid_1's l2: 0.00309893
[73]	training's l2: 0.00236181	valid_1's l2: 0.00309987
[74]	training's l2: 0.00235342	valid_1's l2: 0.00310049
[75]	training's l2: 0.00234515	valid_1's l2: 0.00310004
[76]	training's l2: 0.00233691	valid_1's l2: 0.00309967
[77]	training's l2: 0.00232794	valid_1's l2: 0.00309712
[78]	training's l2: 0.00231927	valid_1's l2: 0.0030967
[79]	training's l2: 0.00231111	valid_1's l2: 0.00309643
[80]	training's l2: 0.00230225	valid_1's l2: 0.0030945
[81]	training's l2: 0.00229548	valid_1's l2: 0.0030946
[82]	training's l2: 0.00228701	valid_1's l2: 0.00309476
[83]	training's l2: 0.00227888	valid_1's l2: 0.00309421
[84]	training's l2: 0.00227082	valid_1's l2: 0.00309438
[85]	training's l2: 0.00226314	valid_1's l2: 0.00309547
[86]	training's l2: 0.00225493	valid_1's l2: 0.00309581
[87]	training's l2: 0.00224614	valid_1's l2: 0.00309587
[88]	training's l2: 0.00223929	valid_1's l2: 0.00309554
[89]	training's l2: 0.00223081	valid_1's l2: 0.0030954
[90]	training's l2: 0.00222365	valid_1's l2: 0.00309392
[91]	training's l2: 0.00221604	valid_1's l2: 0.00309332
[92]	training's l2: 0.00220841	valid_1's l2: 0.00309303
[93]	training's l2: 0.00220031	valid_1's l2: 0.00309324
[94]	training's l2: 0.00219225	valid_1's l2: 0.00309156
[95]	training's l2: 0.0021846	valid_1's l2: 0.0030905
[96]	training's l2: 0.0021769	valid_1's l2: 0.00308945
[97]	training's l2: 0.0021701	valid_1's l2: 0.00308902
[98]	training's l2: 0.00216265	valid_1's l2: 0.0030882
[99]	training's l2: 0.00215606	valid_1's l2: 0.00308883
Did not meet early stopping. Best iteration is:
[99]	training's l2: 0.00215606	valid_1's l2: 0.00308883
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302653 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00398535	valid_1's l2: 0.00369494
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00391185	valid_1's l2: 0.00364834
[3]	training's l2: 0.00384326	valid_1's l2: 0.00360399
[4]	training's l2: 0.00378327	valid_1's l2: 0.00356584
[5]	training's l2: 0.00372434	valid_1's l2: 0.00353207
[6]	training's l2: 0.00367293	valid_1's l2: 0.00350209
[7]	training's l2: 0.00362306	valid_1's l2: 0.00347587
[8]	training's l2: 0.00357704	valid_1's l2: 0.0034503
[9]	training's l2: 0.00353278	valid_1's l2: 0.00342802
[10]	training's l2: 0.00349188	valid_1's l2: 0.00340812
[11]	training's l2: 0.00345404	valid_1's l2: 0.00338957
[12]	training's l2: 0.00341576	valid_1's l2: 0.00337295
[13]	training's l2: 0.00338116	valid_1's l2: 0.00335824
[14]	training's l2: 0.00334923	valid_1's l2: 0.00334365
[15]	training's l2: 0.00331999	valid_1's l2: 0.00333083
[16]	training's l2: 0.00329234	valid_1's l2: 0.00331907
[17]	training's l2: 0.00326548	valid_1's l2: 0.00330784
[18]	training's l2: 0.00324099	valid_1's l2: 0.00329673
[19]	training's l2: 0.00321585	valid_1's l2: 0.00328456
[20]	training's l2: 0.00319075	valid_1's l2: 0.00327523
[21]	training's l2: 0.00316978	valid_1's l2: 0.00326529
[22]	training's l2: 0.00314702	valid_1's l2: 0.00325673
[23]	training's l2: 0.00312726	valid_1's l2: 0.00324917
[24]	training's l2: 0.00310942	valid_1's l2: 0.003242
[25]	training's l2: 0.0030923	valid_1's l2: 0.00323523
[26]	training's l2: 0.00307613	valid_1's l2: 0.00322936
[27]	training's l2: 0.00305821	valid_1's l2: 0.00322316
[28]	training's l2: 0.00304107	valid_1's l2: 0.00321826
[29]	training's l2: 0.0030254	valid_1's l2: 0.00321165
[30]	training's l2: 0.00301084	valid_1's l2: 0.00320524
[31]	training's l2: 0.00299797	valid_1's l2: 0.00320171
[32]	training's l2: 0.00298447	valid_1's l2: 0.00319785
[33]	training's l2: 0.00296976	valid_1's l2: 0.00319427
[34]	training's l2: 0.00295697	valid_1's l2: 0.00319136
[35]	training's l2: 0.00294458	valid_1's l2: 0.00318727
[36]	training's l2: 0.00293203	valid_1's l2: 0.00318338
[37]	training's l2: 0.00292024	valid_1's l2: 0.00317947
[38]	training's l2: 0.00290914	valid_1's l2: 0.00317717
[39]	training's l2: 0.00289824	valid_1's l2: 0.00317547
[40]	training's l2: 0.00288618	valid_1's l2: 0.00317265
[41]	training's l2: 0.00287598	valid_1's l2: 0.00317096
[42]	training's l2: 0.00286586	valid_1's l2: 0.0031673
[43]	training's l2: 0.00285662	valid_1's l2: 0.00316544
[44]	training's l2: 0.00284652	valid_1's l2: 0.00316286
[45]	training's l2: 0.00283635	valid_1's l2: 0.00315945
[46]	training's l2: 0.00282721	valid_1's l2: 0.00315668
[47]	training's l2: 0.00281799	valid_1's l2: 0.00315484
[48]	training's l2: 0.00280932	valid_1's l2: 0.00315424
[49]	training's l2: 0.00280073	valid_1's l2: 0.00315153
[50]	training's l2: 0.00279173	valid_1's l2: 0.00314977
[51]	training's l2: 0.00278222	valid_1's l2: 0.00314743
[52]	training's l2: 0.00277372	valid_1's l2: 0.00314559
[53]	training's l2: 0.00276491	valid_1's l2: 0.00314261
[54]	training's l2: 0.00275635	valid_1's l2: 0.00314058
[55]	training's l2: 0.00274771	valid_1's l2: 0.00313805
[56]	training's l2: 0.00273954	valid_1's l2: 0.00313657
[57]	training's l2: 0.00273068	valid_1's l2: 0.00313309
[58]	training's l2: 0.00272331	valid_1's l2: 0.00313257
[59]	training's l2: 0.00271531	valid_1's l2: 0.00313054
[60]	training's l2: 0.0027072	valid_1's l2: 0.00312854
[61]	training's l2: 0.00269999	valid_1's l2: 0.00312695
[62]	training's l2: 0.0026909	valid_1's l2: 0.00312312
[63]	training's l2: 0.00268334	valid_1's l2: 0.00312241
[64]	training's l2: 0.00267596	valid_1's l2: 0.00311948
[65]	training's l2: 0.00266889	valid_1's l2: 0.00311956
[66]	training's l2: 0.0026619	valid_1's l2: 0.00311857
[67]	training's l2: 0.00265486	valid_1's l2: 0.00311809
[68]	training's l2: 0.00264678	valid_1's l2: 0.00311597
[69]	training's l2: 0.00263881	valid_1's l2: 0.00311254
[70]	training's l2: 0.00263149	valid_1's l2: 0.00311229
[71]	training's l2: 0.00262469	valid_1's l2: 0.00311107
[72]	training's l2: 0.00261754	valid_1's l2: 0.00310848
[73]	training's l2: 0.00261096	valid_1's l2: 0.00310811
[74]	training's l2: 0.00260429	valid_1's l2: 0.00310792
[75]	training's l2: 0.00259764	valid_1's l2: 0.00310596
[76]	training's l2: 0.0025914	valid_1's l2: 0.00310545
[77]	training's l2: 0.00258513	valid_1's l2: 0.00310477
[78]	training's l2: 0.00257752	valid_1's l2: 0.00310195
[79]	training's l2: 0.00257124	valid_1's l2: 0.0031009
[80]	training's l2: 0.00256402	valid_1's l2: 0.00309935
[81]	training's l2: 0.00255777	valid_1's l2: 0.00309864
[82]	training's l2: 0.00255068	valid_1's l2: 0.00309671
[83]	training's l2: 0.00254469	valid_1's l2: 0.00309515
[84]	training's l2: 0.00253816	valid_1's l2: 0.00309382
[85]	training's l2: 0.0025321	valid_1's l2: 0.00309347
[86]	training's l2: 0.00252614	valid_1's l2: 0.00309225
[87]	training's l2: 0.00251982	valid_1's l2: 0.00309024
[88]	training's l2: 0.0025138	valid_1's l2: 0.00308952
[89]	training's l2: 0.0025079	valid_1's l2: 0.00308829
[90]	training's l2: 0.00250194	valid_1's l2: 0.00308792
[91]	training's l2: 0.00249541	valid_1's l2: 0.00308494
[92]	training's l2: 0.00249007	valid_1's l2: 0.00308551
[93]	training's l2: 0.00248389	valid_1's l2: 0.00308368
[94]	training's l2: 0.00247818	valid_1's l2: 0.00308368
[95]	training's l2: 0.00247234	valid_1's l2: 0.00308312
[96]	training's l2: 0.00246683	valid_1's l2: 0.00308384
[97]	training's l2: 0.00246135	valid_1's l2: 0.00308277
[98]	training's l2: 0.00245587	valid_1's l2: 0.00308237
[99]	training's l2: 0.00245042	valid_1's l2: 0.00308151
[100]	training's l2: 0.0024446	valid_1's l2: 0.00308061
[101]	training's l2: 0.00243911	valid_1's l2: 0.00308016
[102]	training's l2: 0.00243383	valid_1's l2: 0.00307986
[103]	training's l2: 0.00242802	valid_1's l2: 0.00307827
[104]	training's l2: 0.0024227	valid_1's l2: 0.0030778
[105]	training's l2: 0.00241746	valid_1's l2: 0.00307857
[106]	training's l2: 0.0024121	valid_1's l2: 0.0030782
[107]	training's l2: 0.00240698	valid_1's l2: 0.00307797
[108]	training's l2: 0.00240194	valid_1's l2: 0.00307839
[109]	training's l2: 0.00239717	valid_1's l2: 0.00307861
[110]	training's l2: 0.00239252	valid_1's l2: 0.00307895
[111]	training's l2: 0.0023876	valid_1's l2: 0.00307881
[112]	training's l2: 0.00238297	valid_1's l2: 0.00307941
[113]	training's l2: 0.002378	valid_1's l2: 0.00307923
[114]	training's l2: 0.00237303	valid_1's l2: 0.00307899
[115]	training's l2: 0.00236739	valid_1's l2: 0.00307787
[116]	training's l2: 0.00236223	valid_1's l2: 0.00307612
[117]	training's l2: 0.00235743	valid_1's l2: 0.00307569
[118]	training's l2: 0.00235234	valid_1's l2: 0.00307493
[119]	training's l2: 0.00234753	valid_1's l2: 0.00307434
[120]	training's l2: 0.00234224	valid_1's l2: 0.00307384
[121]	training's l2: 0.00233741	valid_1's l2: 0.00307391
[122]	training's l2: 0.00233248	valid_1's l2: 0.00307375
[123]	training's l2: 0.00232768	valid_1's l2: 0.00307248
[124]	training's l2: 0.00232265	valid_1's l2: 0.00307256
[125]	training's l2: 0.00231748	valid_1's l2: 0.00307134
Did not meet early stopping. Best iteration is:
[125]	training's l2: 0.00231748	valid_1's l2: 0.00307134
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270705 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00394405	valid_1's l2: 0.00366403
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00383662	valid_1's l2: 0.00359211
[3]	training's l2: 0.00374849	valid_1's l2: 0.00353784
[4]	training's l2: 0.00366774	valid_1's l2: 0.00349274
[5]	training's l2: 0.00359377	valid_1's l2: 0.00344841
[6]	training's l2: 0.00352727	valid_1's l2: 0.0034101
[7]	training's l2: 0.00346993	valid_1's l2: 0.00338083
[8]	training's l2: 0.00341499	valid_1's l2: 0.00335616
[9]	training's l2: 0.00336162	valid_1's l2: 0.00333
[10]	training's l2: 0.00331341	valid_1's l2: 0.00330902
[11]	training's l2: 0.00327286	valid_1's l2: 0.00329203
[12]	training's l2: 0.00323614	valid_1's l2: 0.00327836
[13]	training's l2: 0.00319802	valid_1's l2: 0.00326407
[14]	training's l2: 0.00316747	valid_1's l2: 0.00325487
[15]	training's l2: 0.00313943	valid_1's l2: 0.00324457
[16]	training's l2: 0.00311179	valid_1's l2: 0.00323736
[17]	training's l2: 0.0030823	valid_1's l2: 0.00322928
[18]	training's l2: 0.00305601	valid_1's l2: 0.0032229
[19]	training's l2: 0.00303183	valid_1's l2: 0.0032165
[20]	training's l2: 0.00300802	valid_1's l2: 0.00321242
[21]	training's l2: 0.00298907	valid_1's l2: 0.00320759
[22]	training's l2: 0.00296778	valid_1's l2: 0.00320178
[23]	training's l2: 0.0029476	valid_1's l2: 0.00319784
[24]	training's l2: 0.00292818	valid_1's l2: 0.00319508
[25]	training's l2: 0.00291	valid_1's l2: 0.00319335
[26]	training's l2: 0.00289121	valid_1's l2: 0.00318676
[27]	training's l2: 0.00287444	valid_1's l2: 0.00318495
[28]	training's l2: 0.00285757	valid_1's l2: 0.00317923
[29]	training's l2: 0.00284193	valid_1's l2: 0.00317479
[30]	training's l2: 0.0028272	valid_1's l2: 0.00317509
[31]	training's l2: 0.00281188	valid_1's l2: 0.00317121
[32]	training's l2: 0.00279817	valid_1's l2: 0.00316936
[33]	training's l2: 0.00278461	valid_1's l2: 0.00316479
[34]	training's l2: 0.00277061	valid_1's l2: 0.00316173
[35]	training's l2: 0.0027572	valid_1's l2: 0.00315894
[36]	training's l2: 0.00274439	valid_1's l2: 0.00315583
[37]	training's l2: 0.00273181	valid_1's l2: 0.003153
[38]	training's l2: 0.00271998	valid_1's l2: 0.00315117
[39]	training's l2: 0.00270528	valid_1's l2: 0.00314515
[40]	training's l2: 0.00269378	valid_1's l2: 0.00314464
[41]	training's l2: 0.002682	valid_1's l2: 0.00314162
[42]	training's l2: 0.00266794	valid_1's l2: 0.00313863
[43]	training's l2: 0.00265452	valid_1's l2: 0.00313145
[44]	training's l2: 0.00264236	valid_1's l2: 0.00313151
[45]	training's l2: 0.00263156	valid_1's l2: 0.00312884
[46]	training's l2: 0.00262166	valid_1's l2: 0.00312785
[47]	training's l2: 0.00261036	valid_1's l2: 0.00312673
[48]	training's l2: 0.00259971	valid_1's l2: 0.00312534
[49]	training's l2: 0.00258853	valid_1's l2: 0.00312295
[50]	training's l2: 0.00257865	valid_1's l2: 0.00311965
[51]	training's l2: 0.00256873	valid_1's l2: 0.00311886
[52]	training's l2: 0.00255906	valid_1's l2: 0.00311906
[53]	training's l2: 0.00254854	valid_1's l2: 0.0031181
[54]	training's l2: 0.00253996	valid_1's l2: 0.00311798
[55]	training's l2: 0.00253002	valid_1's l2: 0.00311737
[56]	training's l2: 0.00252077	valid_1's l2: 0.00311569
[57]	training's l2: 0.00251187	valid_1's l2: 0.00311658
[58]	training's l2: 0.00250277	valid_1's l2: 0.00311477
[59]	training's l2: 0.00249251	valid_1's l2: 0.00311407
[60]	training's l2: 0.00248387	valid_1's l2: 0.00311346
[61]	training's l2: 0.00247549	valid_1's l2: 0.00311306
[62]	training's l2: 0.0024666	valid_1's l2: 0.0031108
[63]	training's l2: 0.00245807	valid_1's l2: 0.00311176
[64]	training's l2: 0.00244927	valid_1's l2: 0.00311229
[65]	training's l2: 0.00244119	valid_1's l2: 0.00311342
[66]	training's l2: 0.00243255	valid_1's l2: 0.0031139
[67]	training's l2: 0.0024245	valid_1's l2: 0.00311297
[68]	training's l2: 0.00241582	valid_1's l2: 0.00311158
[69]	training's l2: 0.00240696	valid_1's l2: 0.0031096
[70]	training's l2: 0.00239813	valid_1's l2: 0.00310894
[71]	training's l2: 0.00238965	valid_1's l2: 0.00310832
[72]	training's l2: 0.00238209	valid_1's l2: 0.00310771
[73]	training's l2: 0.00237412	valid_1's l2: 0.00310682
[74]	training's l2: 0.0023659	valid_1's l2: 0.00310569
[75]	training's l2: 0.00235801	valid_1's l2: 0.00310576
[76]	training's l2: 0.00234995	valid_1's l2: 0.0031064
[77]	training's l2: 0.00234143	valid_1's l2: 0.00310466
[78]	training's l2: 0.00233373	valid_1's l2: 0.00310516
[79]	training's l2: 0.00232659	valid_1's l2: 0.00310525
[80]	training's l2: 0.0023188	valid_1's l2: 0.00310494
[81]	training's l2: 0.00231145	valid_1's l2: 0.00310353
[82]	training's l2: 0.00230288	valid_1's l2: 0.00310158
[83]	training's l2: 0.00229622	valid_1's l2: 0.00310256
[84]	training's l2: 0.00228884	valid_1's l2: 0.00310325
[85]	training's l2: 0.00228158	valid_1's l2: 0.00310348
[86]	training's l2: 0.00227403	valid_1's l2: 0.00310341
[87]	training's l2: 0.00226713	valid_1's l2: 0.00310405
[88]	training's l2: 0.0022601	valid_1's l2: 0.00310341
[89]	training's l2: 0.0022531	valid_1's l2: 0.00310333
[90]	training's l2: 0.00224626	valid_1's l2: 0.00310404
[91]	training's l2: 0.00223941	valid_1's l2: 0.00310347
[92]	training's l2: 0.00223187	valid_1's l2: 0.00310234
[93]	training's l2: 0.00222482	valid_1's l2: 0.00310152
[94]	training's l2: 0.00221779	valid_1's l2: 0.00310168
[95]	training's l2: 0.00221078	valid_1's l2: 0.00310157
[96]	training's l2: 0.00220385	valid_1's l2: 0.00310128
[97]	training's l2: 0.00219687	valid_1's l2: 0.00310087
[98]	training's l2: 0.00219026	valid_1's l2: 0.00310071
[99]	training's l2: 0.00218296	valid_1's l2: 0.00310158
[100]	training's l2: 0.00217576	valid_1's l2: 0.00310219
[101]	training's l2: 0.00216945	valid_1's l2: 0.00310351
[102]	training's l2: 0.00216288	valid_1's l2: 0.00310372
[103]	training's l2: 0.00215633	valid_1's l2: 0.00310327
[104]	training's l2: 0.00215041	valid_1's l2: 0.00310374
[105]	training's l2: 0.0021437	valid_1's l2: 0.00310461
[106]	training's l2: 0.0021379	valid_1's l2: 0.00310428
[107]	training's l2: 0.00213139	valid_1's l2: 0.00310532
[108]	training's l2: 0.00212484	valid_1's l2: 0.00310517
[109]	training's l2: 0.00211804	valid_1's l2: 0.0031057
[110]	training's l2: 0.00211088	valid_1's l2: 0.00310579
[111]	training's l2: 0.00210441	valid_1's l2: 0.00310572
[112]	training's l2: 0.00209827	valid_1's l2: 0.0031052
[113]	training's l2: 0.00209196	valid_1's l2: 0.00310503
[114]	training's l2: 0.00208625	valid_1's l2: 0.00310496
[115]	training's l2: 0.00208052	valid_1's l2: 0.00310467
[116]	training's l2: 0.00207433	valid_1's l2: 0.00310463
[117]	training's l2: 0.00206845	valid_1's l2: 0.00310537
[118]	training's l2: 0.00206202	valid_1's l2: 0.00310598
[119]	training's l2: 0.00205635	valid_1's l2: 0.00310568
[120]	training's l2: 0.0020506	valid_1's l2: 0.0031061
[121]	training's l2: 0.00204445	valid_1's l2: 0.00310687
[122]	training's l2: 0.00203927	valid_1's l2: 0.00310748
[123]	training's l2: 0.00203307	valid_1's l2: 0.00310697
[124]	training's l2: 0.00202704	valid_1's l2: 0.00310781
[125]	training's l2: 0.00202103	valid_1's l2: 0.00310719
[126]	training's l2: 0.00201492	valid_1's l2: 0.00310655
[127]	training's l2: 0.00200845	valid_1's l2: 0.00310442
[128]	training's l2: 0.00200267	valid_1's l2: 0.00310333
Early stopping, best iteration is:
[98]	training's l2: 0.00219026	valid_1's l2: 0.00310071
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260797 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.697073	valid_1's l2: 0.699182
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.674685	valid_1's l2: 0.677392
[3]	training's l2: 0.653994	valid_1's l2: 0.657513
[4]	training's l2: 0.635576	valid_1's l2: 0.640336
[5]	training's l2: 0.618395	valid_1's l2: 0.623975
[6]	training's l2: 0.60299	valid_1's l2: 0.609443
[7]	training's l2: 0.589185	valid_1's l2: 0.596164
[8]	training's l2: 0.576082	valid_1's l2: 0.584414
[9]	training's l2: 0.564453	valid_1's l2: 0.573407
[10]	training's l2: 0.5534	valid_1's l2: 0.563737
[11]	training's l2: 0.54372	valid_1's l2: 0.554806
[12]	training's l2: 0.534797	valid_1's l2: 0.547089
[13]	training's l2: 0.52635	valid_1's l2: 0.540312
[14]	training's l2: 0.518418	valid_1's l2: 0.534091
[15]	training's l2: 0.511052	valid_1's l2: 0.527723
[16]	training's l2: 0.504162	valid_1's l2: 0.522527
[17]	training's l2: 0.497541	valid_1's l2: 0.517314
[18]	training's l2: 0.491782	valid_1's l2: 0.511856
[19]	training's l2: 0.486156	valid_1's l2: 0.508007
[20]	training's l2: 0.481002	valid_1's l2: 0.504123
[21]	training's l2: 0.476151	valid_1's l2: 0.500692
[22]	training's l2: 0.471695	valid_1's l2: 0.49782
[23]	training's l2: 0.46713	valid_1's l2: 0.494446
[24]	training's l2: 0.46276	valid_1's l2: 0.491173
[25]	training's l2: 0.458714	valid_1's l2: 0.488525
[26]	training's l2: 0.454969	valid_1's l2: 0.485836
[27]	training's l2: 0.450907	valid_1's l2: 0.483196
[28]	training's l2: 0.44739	valid_1's l2: 0.480953
[29]	training's l2: 0.443738	valid_1's l2: 0.478602
[30]	training's l2: 0.440816	valid_1's l2: 0.477308
[31]	training's l2: 0.438027	valid_1's l2: 0.475612
[32]	training's l2: 0.435061	valid_1's l2: 0.474031
[33]	training's l2: 0.432065	valid_1's l2: 0.472248
[34]	training's l2: 0.42937	valid_1's l2: 0.470353
[35]	training's l2: 0.426544	valid_1's l2: 0.468958
[36]	training's l2: 0.42366	valid_1's l2: 0.467263
[37]	training's l2: 0.42107	valid_1's l2: 0.466059
[38]	training's l2: 0.418176	valid_1's l2: 0.463945
[39]	training's l2: 0.415903	valid_1's l2: 0.462793
[40]	training's l2: 0.413881	valid_1's l2: 0.461784
[41]	training's l2: 0.411297	valid_1's l2: 0.460381
[42]	training's l2: 0.408665	valid_1's l2: 0.45892
[43]	training's l2: 0.406465	valid_1's l2: 0.458065
[44]	training's l2: 0.40453	valid_1's l2: 0.457223
[45]	training's l2: 0.402581	valid_1's l2: 0.456258
[46]	training's l2: 0.400831	valid_1's l2: 0.455511
[47]	training's l2: 0.398803	valid_1's l2: 0.454551
[48]	training's l2: 0.397051	valid_1's l2: 0.453916
[49]	training's l2: 0.395307	valid_1's l2: 0.453372
[50]	training's l2: 0.393225	valid_1's l2: 0.452565
[51]	training's l2: 0.39165	valid_1's l2: 0.451961
[52]	training's l2: 0.389868	valid_1's l2: 0.451597
[53]	training's l2: 0.388056	valid_1's l2: 0.450732
[54]	training's l2: 0.386592	valid_1's l2: 0.450262
[55]	training's l2: 0.385043	valid_1's l2: 0.449875
[56]	training's l2: 0.383459	valid_1's l2: 0.449402
[57]	training's l2: 0.381923	valid_1's l2: 0.449232
[58]	training's l2: 0.380313	valid_1's l2: 0.448717
[59]	training's l2: 0.378469	valid_1's l2: 0.448061
[60]	training's l2: 0.377063	valid_1's l2: 0.447874
[61]	training's l2: 0.375046	valid_1's l2: 0.446573
[62]	training's l2: 0.373469	valid_1's l2: 0.445846
[63]	training's l2: 0.371696	valid_1's l2: 0.444637
[64]	training's l2: 0.370369	valid_1's l2: 0.44439
[65]	training's l2: 0.368805	valid_1's l2: 0.444032
[66]	training's l2: 0.367249	valid_1's l2: 0.443288
[67]	training's l2: 0.365896	valid_1's l2: 0.442883
[68]	training's l2: 0.3644	valid_1's l2: 0.442144
[69]	training's l2: 0.362746	valid_1's l2: 0.44143
[70]	training's l2: 0.361383	valid_1's l2: 0.441039
[71]	training's l2: 0.360209	valid_1's l2: 0.440908
[72]	training's l2: 0.358866	valid_1's l2: 0.440507
[73]	training's l2: 0.357688	valid_1's l2: 0.440083
[74]	training's l2: 0.356469	valid_1's l2: 0.439672
[75]	training's l2: 0.354831	valid_1's l2: 0.438855
[76]	training's l2: 0.353291	valid_1's l2: 0.438431
[77]	training's l2: 0.352188	valid_1's l2: 0.43844
[78]	training's l2: 0.351087	valid_1's l2: 0.438072
[79]	training's l2: 0.349279	valid_1's l2: 0.437107
[80]	training's l2: 0.348159	valid_1's l2: 0.436945
[81]	training's l2: 0.347131	valid_1's l2: 0.436887
[82]	training's l2: 0.346091	valid_1's l2: 0.436833
[83]	training's l2: 0.344602	valid_1's l2: 0.436208
[84]	training's l2: 0.343511	valid_1's l2: 0.435851
[85]	training's l2: 0.3422	valid_1's l2: 0.435624
[86]	training's l2: 0.340966	valid_1's l2: 0.435262
[87]	training's l2: 0.339994	valid_1's l2: 0.434976
[88]	training's l2: 0.338927	valid_1's l2: 0.43485
[89]	training's l2: 0.337933	valid_1's l2: 0.434554
[90]	training's l2: 0.336612	valid_1's l2: 0.43382
[91]	training's l2: 0.335476	valid_1's l2: 0.433683
[92]	training's l2: 0.334527	valid_1's l2: 0.433596
[93]	training's l2: 0.333602	valid_1's l2: 0.433382
[94]	training's l2: 0.332717	valid_1's l2: 0.433258
[95]	training's l2: 0.331567	valid_1's l2: 0.433054
[96]	training's l2: 0.330617	valid_1's l2: 0.432943
[97]	training's l2: 0.329626	valid_1's l2: 0.432872
[98]	training's l2: 0.328608	valid_1's l2: 0.432732
[99]	training's l2: 0.327694	valid_1's l2: 0.432525
[100]	training's l2: 0.326773	valid_1's l2: 0.43258
[101]	training's l2: 0.325958	valid_1's l2: 0.432455
[102]	training's l2: 0.324849	valid_1's l2: 0.432351
[103]	training's l2: 0.323876	valid_1's l2: 0.432076
[104]	training's l2: 0.322977	valid_1's l2: 0.431754
[105]	training's l2: 0.321955	valid_1's l2: 0.431346
[106]	training's l2: 0.32094	valid_1's l2: 0.431088
[107]	training's l2: 0.320029	valid_1's l2: 0.43091
[108]	training's l2: 0.319152	valid_1's l2: 0.430939
[109]	training's l2: 0.318117	valid_1's l2: 0.430718
[110]	training's l2: 0.31722	valid_1's l2: 0.430705
[111]	training's l2: 0.316374	valid_1's l2: 0.430802
[112]	training's l2: 0.315591	valid_1's l2: 0.430732
[113]	training's l2: 0.314827	valid_1's l2: 0.43069
[114]	training's l2: 0.313912	valid_1's l2: 0.430661
[115]	training's l2: 0.313044	valid_1's l2: 0.430276
[116]	training's l2: 0.312156	valid_1's l2: 0.429938
[117]	training's l2: 0.311293	valid_1's l2: 0.429651
[118]	training's l2: 0.310445	valid_1's l2: 0.429534
[119]	training's l2: 0.309709	valid_1's l2: 0.429364
[120]	training's l2: 0.308955	valid_1's l2: 0.42936
[121]	training's l2: 0.308162	valid_1's l2: 0.429396
[122]	training's l2: 0.307387	valid_1's l2: 0.429421
[123]	training's l2: 0.306665	valid_1's l2: 0.429515
[124]	training's l2: 0.305854	valid_1's l2: 0.429483
[125]	training's l2: 0.305083	valid_1's l2: 0.42943
[126]	training's l2: 0.304325	valid_1's l2: 0.42929
[127]	training's l2: 0.303617	valid_1's l2: 0.429229
[128]	training's l2: 0.302913	valid_1's l2: 0.42935
[129]	training's l2: 0.302172	valid_1's l2: 0.429359
[130]	training's l2: 0.301349	valid_1's l2: 0.42907
[131]	training's l2: 0.300544	valid_1's l2: 0.428699
[132]	training's l2: 0.299813	valid_1's l2: 0.428582
[133]	training's l2: 0.299024	valid_1's l2: 0.428418
[134]	training's l2: 0.298239	valid_1's l2: 0.428445
[135]	training's l2: 0.297452	valid_1's l2: 0.428446
[136]	training's l2: 0.296761	valid_1's l2: 0.428375
[137]	training's l2: 0.296003	valid_1's l2: 0.428313
[138]	training's l2: 0.295232	valid_1's l2: 0.428508
[139]	training's l2: 0.294546	valid_1's l2: 0.428362
[140]	training's l2: 0.293863	valid_1's l2: 0.428322
[141]	training's l2: 0.293181	valid_1's l2: 0.428339
[142]	training's l2: 0.292585	valid_1's l2: 0.428526
[143]	training's l2: 0.291834	valid_1's l2: 0.42843
[144]	training's l2: 0.291096	valid_1's l2: 0.428269
[145]	training's l2: 0.290351	valid_1's l2: 0.428158
[146]	training's l2: 0.289642	valid_1's l2: 0.428216
[147]	training's l2: 0.289044	valid_1's l2: 0.428223
[148]	training's l2: 0.288453	valid_1's l2: 0.428417
[149]	training's l2: 0.287695	valid_1's l2: 0.428311
[150]	training's l2: 0.286914	valid_1's l2: 0.428009
[151]	training's l2: 0.286238	valid_1's l2: 0.427934
[152]	training's l2: 0.285607	valid_1's l2: 0.427822
[153]	training's l2: 0.284965	valid_1's l2: 0.427689
[154]	training's l2: 0.284377	valid_1's l2: 0.427678
[155]	training's l2: 0.283747	valid_1's l2: 0.42764
[156]	training's l2: 0.283163	valid_1's l2: 0.427683
[157]	training's l2: 0.282542	valid_1's l2: 0.427727
[158]	training's l2: 0.281911	valid_1's l2: 0.427807
[159]	training's l2: 0.281123	valid_1's l2: 0.427783
[160]	training's l2: 0.280488	valid_1's l2: 0.42774
[161]	training's l2: 0.279737	valid_1's l2: 0.427533
[162]	training's l2: 0.279131	valid_1's l2: 0.427613
[163]	training's l2: 0.278484	valid_1's l2: 0.427484
[164]	training's l2: 0.277857	valid_1's l2: 0.427455
[165]	training's l2: 0.277336	valid_1's l2: 0.427402
[166]	training's l2: 0.276675	valid_1's l2: 0.427359
[167]	training's l2: 0.276043	valid_1's l2: 0.427557
[168]	training's l2: 0.275348	valid_1's l2: 0.427261
[169]	training's l2: 0.274769	valid_1's l2: 0.427196
[170]	training's l2: 0.274198	valid_1's l2: 0.427259
[171]	training's l2: 0.273556	valid_1's l2: 0.427277
[172]	training's l2: 0.272987	valid_1's l2: 0.427175
[173]	training's l2: 0.272449	valid_1's l2: 0.42709
[174]	training's l2: 0.271862	valid_1's l2: 0.42697
[175]	training's l2: 0.271269	valid_1's l2: 0.427079
[176]	training's l2: 0.270616	valid_1's l2: 0.426831
[177]	training's l2: 0.270034	valid_1's l2: 0.426832
[178]	training's l2: 0.269359	valid_1's l2: 0.42704
[179]	training's l2: 0.268738	valid_1's l2: 0.427103
[180]	training's l2: 0.268094	valid_1's l2: 0.427072
[181]	training's l2: 0.267462	valid_1's l2: 0.42707
[182]	training's l2: 0.266918	valid_1's l2: 0.427065
[183]	training's l2: 0.266378	valid_1's l2: 0.42703
[184]	training's l2: 0.265769	valid_1's l2: 0.427098
[185]	training's l2: 0.265196	valid_1's l2: 0.427055
[186]	training's l2: 0.264638	valid_1's l2: 0.427138
[187]	training's l2: 0.264023	valid_1's l2: 0.427223
[188]	training's l2: 0.263418	valid_1's l2: 0.427121
[189]	training's l2: 0.262766	valid_1's l2: 0.427044
Did not meet early stopping. Best iteration is:
[189]	training's l2: 0.262766	valid_1's l2: 0.427044
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285240 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.690559	valid_1's l2: 0.692438
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.663571	valid_1's l2: 0.665369
[3]	training's l2: 0.639025	valid_1's l2: 0.64117
[4]	training's l2: 0.618229	valid_1's l2: 0.620578
[5]	training's l2: 0.599789	valid_1's l2: 0.602917
[6]	training's l2: 0.582342	valid_1's l2: 0.586931
[7]	training's l2: 0.5681	valid_1's l2: 0.573604
[8]	training's l2: 0.554762	valid_1's l2: 0.561908
[9]	training's l2: 0.543109	valid_1's l2: 0.551705
[10]	training's l2: 0.53259	valid_1's l2: 0.542198
[11]	training's l2: 0.523026	valid_1's l2: 0.533563
[12]	training's l2: 0.513841	valid_1's l2: 0.526879
[13]	training's l2: 0.505517	valid_1's l2: 0.520019
[14]	training's l2: 0.498552	valid_1's l2: 0.514325
[15]	training's l2: 0.491778	valid_1's l2: 0.509045
[16]	training's l2: 0.485618	valid_1's l2: 0.503804
[17]	training's l2: 0.479636	valid_1's l2: 0.498911
[18]	training's l2: 0.474069	valid_1's l2: 0.49482
[19]	training's l2: 0.469246	valid_1's l2: 0.491769
[20]	training's l2: 0.464058	valid_1's l2: 0.488642
[21]	training's l2: 0.459445	valid_1's l2: 0.485764
[22]	training's l2: 0.454714	valid_1's l2: 0.482295
[23]	training's l2: 0.450982	valid_1's l2: 0.479723
[24]	training's l2: 0.447258	valid_1's l2: 0.477102
[25]	training's l2: 0.443642	valid_1's l2: 0.475268
[26]	training's l2: 0.440195	valid_1's l2: 0.472843
[27]	training's l2: 0.437078	valid_1's l2: 0.471087
[28]	training's l2: 0.434227	valid_1's l2: 0.469809
[29]	training's l2: 0.431168	valid_1's l2: 0.468177
[30]	training's l2: 0.427363	valid_1's l2: 0.46601
[31]	training's l2: 0.424687	valid_1's l2: 0.464529
[32]	training's l2: 0.422163	valid_1's l2: 0.463297
[33]	training's l2: 0.419923	valid_1's l2: 0.462456
[34]	training's l2: 0.417107	valid_1's l2: 0.461185
[35]	training's l2: 0.413993	valid_1's l2: 0.45954
[36]	training's l2: 0.411409	valid_1's l2: 0.458389
[37]	training's l2: 0.408764	valid_1's l2: 0.457375
[38]	training's l2: 0.40688	valid_1's l2: 0.456511
[39]	training's l2: 0.405058	valid_1's l2: 0.455926
[40]	training's l2: 0.403291	valid_1's l2: 0.455352
[41]	training's l2: 0.401267	valid_1's l2: 0.454571
[42]	training's l2: 0.399325	valid_1's l2: 0.453958
[43]	training's l2: 0.396095	valid_1's l2: 0.451959
[44]	training's l2: 0.394038	valid_1's l2: 0.450609
[45]	training's l2: 0.391883	valid_1's l2: 0.449973
[46]	training's l2: 0.39024	valid_1's l2: 0.44969
[47]	training's l2: 0.388558	valid_1's l2: 0.44932
[48]	training's l2: 0.386572	valid_1's l2: 0.448343
[49]	training's l2: 0.384895	valid_1's l2: 0.44742
[50]	training's l2: 0.383323	valid_1's l2: 0.446899
[51]	training's l2: 0.381485	valid_1's l2: 0.445924
[52]	training's l2: 0.379744	valid_1's l2: 0.445244
[53]	training's l2: 0.378016	valid_1's l2: 0.444818
[54]	training's l2: 0.376603	valid_1's l2: 0.444552
[55]	training's l2: 0.375055	valid_1's l2: 0.443894
[56]	training's l2: 0.373355	valid_1's l2: 0.443162
[57]	training's l2: 0.371959	valid_1's l2: 0.442884
[58]	training's l2: 0.370472	valid_1's l2: 0.442397
[59]	training's l2: 0.36913	valid_1's l2: 0.442027
[60]	training's l2: 0.366803	valid_1's l2: 0.440639
[61]	training's l2: 0.365506	valid_1's l2: 0.440103
[62]	training's l2: 0.363946	valid_1's l2: 0.439311
[63]	training's l2: 0.362717	valid_1's l2: 0.438808
[64]	training's l2: 0.36077	valid_1's l2: 0.437548
[65]	training's l2: 0.359454	valid_1's l2: 0.437466
[66]	training's l2: 0.358323	valid_1's l2: 0.437395
[67]	training's l2: 0.356716	valid_1's l2: 0.436772
[68]	training's l2: 0.355487	valid_1's l2: 0.436394
[69]	training's l2: 0.354241	valid_1's l2: 0.436226
[70]	training's l2: 0.352961	valid_1's l2: 0.435997
[71]	training's l2: 0.351923	valid_1's l2: 0.435803
[72]	training's l2: 0.350875	valid_1's l2: 0.435925
[73]	training's l2: 0.349408	valid_1's l2: 0.435379
[74]	training's l2: 0.348313	valid_1's l2: 0.434976
[75]	training's l2: 0.347126	valid_1's l2: 0.434828
[76]	training's l2: 0.346099	valid_1's l2: 0.434499
[77]	training's l2: 0.344782	valid_1's l2: 0.433974
[78]	training's l2: 0.343834	valid_1's l2: 0.433992
[79]	training's l2: 0.342732	valid_1's l2: 0.433718
[80]	training's l2: 0.341826	valid_1's l2: 0.433607
[81]	training's l2: 0.340909	valid_1's l2: 0.433362
[82]	training's l2: 0.339919	valid_1's l2: 0.43338
[83]	training's l2: 0.338535	valid_1's l2: 0.432575
[84]	training's l2: 0.337591	valid_1's l2: 0.432637
[85]	training's l2: 0.336659	valid_1's l2: 0.432463
[86]	training's l2: 0.335803	valid_1's l2: 0.432409
[87]	training's l2: 0.334739	valid_1's l2: 0.432003
[88]	training's l2: 0.33368	valid_1's l2: 0.432024
[89]	training's l2: 0.332676	valid_1's l2: 0.432097
[90]	training's l2: 0.331679	valid_1's l2: 0.431701
[91]	training's l2: 0.330839	valid_1's l2: 0.431646
[92]	training's l2: 0.329892	valid_1's l2: 0.431329
[93]	training's l2: 0.328931	valid_1's l2: 0.431256
[94]	training's l2: 0.327883	valid_1's l2: 0.430964
[95]	training's l2: 0.327073	valid_1's l2: 0.430921
[96]	training's l2: 0.32602	valid_1's l2: 0.430634
[97]	training's l2: 0.325182	valid_1's l2: 0.430451
[98]	training's l2: 0.324378	valid_1's l2: 0.430477
[99]	training's l2: 0.323571	valid_1's l2: 0.430407
[100]	training's l2: 0.322752	valid_1's l2: 0.430737
[101]	training's l2: 0.321758	valid_1's l2: 0.43047
[102]	training's l2: 0.320833	valid_1's l2: 0.430429
[103]	training's l2: 0.31974	valid_1's l2: 0.430333
[104]	training's l2: 0.318829	valid_1's l2: 0.429888
[105]	training's l2: 0.317817	valid_1's l2: 0.429279
[106]	training's l2: 0.316931	valid_1's l2: 0.42907
[107]	training's l2: 0.316146	valid_1's l2: 0.429226
[108]	training's l2: 0.315322	valid_1's l2: 0.429437
[109]	training's l2: 0.314615	valid_1's l2: 0.429394
[110]	training's l2: 0.313737	valid_1's l2: 0.429012
[111]	training's l2: 0.313008	valid_1's l2: 0.429352
[112]	training's l2: 0.312323	valid_1's l2: 0.429362
[113]	training's l2: 0.311639	valid_1's l2: 0.429395
[114]	training's l2: 0.310728	valid_1's l2: 0.429303
[115]	training's l2: 0.309856	valid_1's l2: 0.429227
[116]	training's l2: 0.309071	valid_1's l2: 0.429335
[117]	training's l2: 0.308298	valid_1's l2: 0.429644
[118]	training's l2: 0.307547	valid_1's l2: 0.429354
[119]	training's l2: 0.306755	valid_1's l2: 0.429251
[120]	training's l2: 0.305899	valid_1's l2: 0.428928
[121]	training's l2: 0.305163	valid_1's l2: 0.428897
[122]	training's l2: 0.304409	valid_1's l2: 0.428872
[123]	training's l2: 0.303581	valid_1's l2: 0.428859
[124]	training's l2: 0.302835	valid_1's l2: 0.428748
[125]	training's l2: 0.302127	valid_1's l2: 0.428682
[126]	training's l2: 0.30149	valid_1's l2: 0.428677
[127]	training's l2: 0.300754	valid_1's l2: 0.428775
[128]	training's l2: 0.300041	valid_1's l2: 0.428672
[129]	training's l2: 0.299386	valid_1's l2: 0.428962
[130]	training's l2: 0.298672	valid_1's l2: 0.428915
[131]	training's l2: 0.297987	valid_1's l2: 0.428871
[132]	training's l2: 0.297329	valid_1's l2: 0.42886
[133]	training's l2: 0.296637	valid_1's l2: 0.42894
[134]	training's l2: 0.296011	valid_1's l2: 0.428981
[135]	training's l2: 0.295232	valid_1's l2: 0.428892
[136]	training's l2: 0.294607	valid_1's l2: 0.428865
[137]	training's l2: 0.293728	valid_1's l2: 0.428553
[138]	training's l2: 0.293144	valid_1's l2: 0.428508
[139]	training's l2: 0.292554	valid_1's l2: 0.428563
[140]	training's l2: 0.292079	valid_1's l2: 0.428521
[141]	training's l2: 0.2914	valid_1's l2: 0.428605
[142]	training's l2: 0.290753	valid_1's l2: 0.428619
[143]	training's l2: 0.290114	valid_1's l2: 0.428399
[144]	training's l2: 0.289389	valid_1's l2: 0.42837
[145]	training's l2: 0.288769	valid_1's l2: 0.428464
[146]	training's l2: 0.288098	valid_1's l2: 0.42804
[147]	training's l2: 0.28756	valid_1's l2: 0.428054
[148]	training's l2: 0.286835	valid_1's l2: 0.428008
[149]	training's l2: 0.286229	valid_1's l2: 0.428037
[150]	training's l2: 0.285616	valid_1's l2: 0.427955
[151]	training's l2: 0.284977	valid_1's l2: 0.427882
[152]	training's l2: 0.284412	valid_1's l2: 0.427999
[153]	training's l2: 0.28384	valid_1's l2: 0.428089
[154]	training's l2: 0.283167	valid_1's l2: 0.427952
[155]	training's l2: 0.282598	valid_1's l2: 0.427963
[156]	training's l2: 0.281947	valid_1's l2: 0.427987
[157]	training's l2: 0.281291	valid_1's l2: 0.427886
[158]	training's l2: 0.280777	valid_1's l2: 0.427949
[159]	training's l2: 0.279937	valid_1's l2: 0.427927
[160]	training's l2: 0.279184	valid_1's l2: 0.427937
[161]	training's l2: 0.278555	valid_1's l2: 0.428063
[162]	training's l2: 0.277953	valid_1's l2: 0.42807
[163]	training's l2: 0.277309	valid_1's l2: 0.428232
[164]	training's l2: 0.276669	valid_1's l2: 0.428129
[165]	training's l2: 0.276045	valid_1's l2: 0.428022
[166]	training's l2: 0.27551	valid_1's l2: 0.428085
[167]	training's l2: 0.274856	valid_1's l2: 0.428008
[168]	training's l2: 0.274224	valid_1's l2: 0.427764
[169]	training's l2: 0.273689	valid_1's l2: 0.427818
[170]	training's l2: 0.273016	valid_1's l2: 0.4279
[171]	training's l2: 0.27243	valid_1's l2: 0.427948
[172]	training's l2: 0.271766	valid_1's l2: 0.427861
[173]	training's l2: 0.271264	valid_1's l2: 0.428011
[174]	training's l2: 0.270665	valid_1's l2: 0.427843
[175]	training's l2: 0.270044	valid_1's l2: 0.42786
[176]	training's l2: 0.269549	valid_1's l2: 0.427855
[177]	training's l2: 0.269012	valid_1's l2: 0.427862
[178]	training's l2: 0.268395	valid_1's l2: 0.427908
[179]	training's l2: 0.267668	valid_1's l2: 0.427915
[180]	training's l2: 0.26715	valid_1's l2: 0.427864
[181]	training's l2: 0.26669	valid_1's l2: 0.427823
[182]	training's l2: 0.266062	valid_1's l2: 0.427812
[183]	training's l2: 0.265674	valid_1's l2: 0.427727
[184]	training's l2: 0.265074	valid_1's l2: 0.42776
Did not meet early stopping. Best iteration is:
[184]	training's l2: 0.265074	valid_1's l2: 0.42776
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233274 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.693383	valid_1's l2: 0.696508
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.667932	valid_1's l2: 0.672092
[3]	training's l2: 0.644655	valid_1's l2: 0.650186
[4]	training's l2: 0.623945	valid_1's l2: 0.631147
[5]	training's l2: 0.605458	valid_1's l2: 0.613808
[6]	training's l2: 0.589014	valid_1's l2: 0.598668
[7]	training's l2: 0.573479	valid_1's l2: 0.584957
[8]	training's l2: 0.559456	valid_1's l2: 0.572491
[9]	training's l2: 0.546462	valid_1's l2: 0.561414
[10]	training's l2: 0.53435	valid_1's l2: 0.551237
[11]	training's l2: 0.523799	valid_1's l2: 0.542966
[12]	training's l2: 0.513921	valid_1's l2: 0.535316
[13]	training's l2: 0.505031	valid_1's l2: 0.528687
[14]	training's l2: 0.496482	valid_1's l2: 0.522228
[15]	training's l2: 0.488688	valid_1's l2: 0.516207
[16]	training's l2: 0.481206	valid_1's l2: 0.51103
[17]	training's l2: 0.474552	valid_1's l2: 0.506137
[18]	training's l2: 0.468543	valid_1's l2: 0.501653
[19]	training's l2: 0.462436	valid_1's l2: 0.497738
[20]	training's l2: 0.456899	valid_1's l2: 0.494065
[21]	training's l2: 0.451609	valid_1's l2: 0.490481
[22]	training's l2: 0.446682	valid_1's l2: 0.487301
[23]	training's l2: 0.441756	valid_1's l2: 0.484351
[24]	training's l2: 0.437323	valid_1's l2: 0.481725
[25]	training's l2: 0.432901	valid_1's l2: 0.479071
[26]	training's l2: 0.428862	valid_1's l2: 0.477543
[27]	training's l2: 0.424518	valid_1's l2: 0.475564
[28]	training's l2: 0.421003	valid_1's l2: 0.474023
[29]	training's l2: 0.417483	valid_1's l2: 0.472371
[30]	training's l2: 0.413913	valid_1's l2: 0.470634
[31]	training's l2: 0.410562	valid_1's l2: 0.468645
[32]	training's l2: 0.407502	valid_1's l2: 0.467135
[33]	training's l2: 0.404401	valid_1's l2: 0.465777
[34]	training's l2: 0.40105	valid_1's l2: 0.463912
[35]	training's l2: 0.397896	valid_1's l2: 0.462594
[36]	training's l2: 0.394784	valid_1's l2: 0.461563
[37]	training's l2: 0.392109	valid_1's l2: 0.460272
[38]	training's l2: 0.389543	valid_1's l2: 0.459469
[39]	training's l2: 0.386739	valid_1's l2: 0.458433
[40]	training's l2: 0.384323	valid_1's l2: 0.457727
[41]	training's l2: 0.381533	valid_1's l2: 0.456354
[42]	training's l2: 0.378686	valid_1's l2: 0.455204
[43]	training's l2: 0.376315	valid_1's l2: 0.45452
[44]	training's l2: 0.373786	valid_1's l2: 0.45336
[45]	training's l2: 0.371709	valid_1's l2: 0.45249
[46]	training's l2: 0.369492	valid_1's l2: 0.451817
[47]	training's l2: 0.367515	valid_1's l2: 0.451149
[48]	training's l2: 0.365073	valid_1's l2: 0.450436
[49]	training's l2: 0.362987	valid_1's l2: 0.449848
[50]	training's l2: 0.360772	valid_1's l2: 0.449159
[51]	training's l2: 0.358997	valid_1's l2: 0.448679
[52]	training's l2: 0.357225	valid_1's l2: 0.448474
[53]	training's l2: 0.354843	valid_1's l2: 0.447018
[54]	training's l2: 0.352633	valid_1's l2: 0.44626
[55]	training's l2: 0.349854	valid_1's l2: 0.444453
[56]	training's l2: 0.347423	valid_1's l2: 0.443251
[57]	training's l2: 0.345552	valid_1's l2: 0.442909
[58]	training's l2: 0.343847	valid_1's l2: 0.442843
[59]	training's l2: 0.342094	valid_1's l2: 0.44265
[60]	training's l2: 0.340493	valid_1's l2: 0.442405
[61]	training's l2: 0.338199	valid_1's l2: 0.441283
[62]	training's l2: 0.336349	valid_1's l2: 0.440684
[63]	training's l2: 0.334742	valid_1's l2: 0.440328
[64]	training's l2: 0.332758	valid_1's l2: 0.439351
[65]	training's l2: 0.33134	valid_1's l2: 0.43897
[66]	training's l2: 0.329752	valid_1's l2: 0.438462
[67]	training's l2: 0.327993	valid_1's l2: 0.43795
[68]	training's l2: 0.326398	valid_1's l2: 0.437487
[69]	training's l2: 0.324464	valid_1's l2: 0.436583
[70]	training's l2: 0.323051	valid_1's l2: 0.436405
[71]	training's l2: 0.321462	valid_1's l2: 0.435803
[72]	training's l2: 0.320085	valid_1's l2: 0.435674
[73]	training's l2: 0.3185	valid_1's l2: 0.435191
[74]	training's l2: 0.316757	valid_1's l2: 0.434715
[75]	training's l2: 0.315499	valid_1's l2: 0.434632
[76]	training's l2: 0.314154	valid_1's l2: 0.434272
[77]	training's l2: 0.312897	valid_1's l2: 0.434199
[78]	training's l2: 0.311367	valid_1's l2: 0.433685
[79]	training's l2: 0.310109	valid_1's l2: 0.433601
[80]	training's l2: 0.308762	valid_1's l2: 0.433548
[81]	training's l2: 0.307328	valid_1's l2: 0.433512
[82]	training's l2: 0.305867	valid_1's l2: 0.433116
[83]	training's l2: 0.304635	valid_1's l2: 0.43301
[84]	training's l2: 0.303291	valid_1's l2: 0.432675
[85]	training's l2: 0.301982	valid_1's l2: 0.432265
[86]	training's l2: 0.300575	valid_1's l2: 0.431841
[87]	training's l2: 0.299295	valid_1's l2: 0.431738
[88]	training's l2: 0.298081	valid_1's l2: 0.431556
[89]	training's l2: 0.296824	valid_1's l2: 0.431349
[90]	training's l2: 0.295709	valid_1's l2: 0.431139
[91]	training's l2: 0.294469	valid_1's l2: 0.430938
[92]	training's l2: 0.293215	valid_1's l2: 0.430949
[93]	training's l2: 0.291866	valid_1's l2: 0.430778
[94]	training's l2: 0.290812	valid_1's l2: 0.430659
[95]	training's l2: 0.289735	valid_1's l2: 0.430591
[96]	training's l2: 0.288409	valid_1's l2: 0.430222
[97]	training's l2: 0.287297	valid_1's l2: 0.430028
[98]	training's l2: 0.286063	valid_1's l2: 0.4299
[99]	training's l2: 0.28493	valid_1's l2: 0.429722
[100]	training's l2: 0.28383	valid_1's l2: 0.429519
[101]	training's l2: 0.282743	valid_1's l2: 0.429615
[102]	training's l2: 0.281702	valid_1's l2: 0.429492
[103]	training's l2: 0.280727	valid_1's l2: 0.429367
[104]	training's l2: 0.279733	valid_1's l2: 0.429442
[105]	training's l2: 0.278684	valid_1's l2: 0.429453
[106]	training's l2: 0.277634	valid_1's l2: 0.429431
[107]	training's l2: 0.276462	valid_1's l2: 0.429214
[108]	training's l2: 0.275302	valid_1's l2: 0.428966
[109]	training's l2: 0.274197	valid_1's l2: 0.428703
[110]	training's l2: 0.273144	valid_1's l2: 0.428592
[111]	training's l2: 0.27224	valid_1's l2: 0.428863
[112]	training's l2: 0.271108	valid_1's l2: 0.428635
[113]	training's l2: 0.270173	valid_1's l2: 0.428451
[114]	training's l2: 0.269287	valid_1's l2: 0.42839
[115]	training's l2: 0.268317	valid_1's l2: 0.428253
[116]	training's l2: 0.267326	valid_1's l2: 0.427952
[117]	training's l2: 0.266387	valid_1's l2: 0.427929
[118]	training's l2: 0.265272	valid_1's l2: 0.427423
[119]	training's l2: 0.264224	valid_1's l2: 0.427204
[120]	training's l2: 0.26331	valid_1's l2: 0.427182
[121]	training's l2: 0.262362	valid_1's l2: 0.427061
[122]	training's l2: 0.261515	valid_1's l2: 0.426826
[123]	training's l2: 0.260716	valid_1's l2: 0.426849
[124]	training's l2: 0.25985	valid_1's l2: 0.426861
[125]	training's l2: 0.258863	valid_1's l2: 0.426985
[126]	training's l2: 0.257942	valid_1's l2: 0.426925
[127]	training's l2: 0.257148	valid_1's l2: 0.427035
[128]	training's l2: 0.256063	valid_1's l2: 0.427151
[129]	training's l2: 0.255254	valid_1's l2: 0.427113
[130]	training's l2: 0.254436	valid_1's l2: 0.427225
[131]	training's l2: 0.253556	valid_1's l2: 0.427167
[132]	training's l2: 0.252623	valid_1's l2: 0.42742
[133]	training's l2: 0.251897	valid_1's l2: 0.427382
[134]	training's l2: 0.250969	valid_1's l2: 0.427244
[135]	training's l2: 0.250038	valid_1's l2: 0.427247
[136]	training's l2: 0.249021	valid_1's l2: 0.427217
[137]	training's l2: 0.248219	valid_1's l2: 0.42723
[138]	training's l2: 0.24745	valid_1's l2: 0.427308
[139]	training's l2: 0.246565	valid_1's l2: 0.427333
[140]	training's l2: 0.2457	valid_1's l2: 0.427139
[141]	training's l2: 0.244954	valid_1's l2: 0.42717
[142]	training's l2: 0.244086	valid_1's l2: 0.427028
[143]	training's l2: 0.243183	valid_1's l2: 0.426751
[144]	training's l2: 0.242419	valid_1's l2: 0.426685
[145]	training's l2: 0.241577	valid_1's l2: 0.426522
[146]	training's l2: 0.240733	valid_1's l2: 0.426326
[147]	training's l2: 0.240038	valid_1's l2: 0.426327
[148]	training's l2: 0.239264	valid_1's l2: 0.426268
[149]	training's l2: 0.238431	valid_1's l2: 0.426251
[150]	training's l2: 0.237647	valid_1's l2: 0.426262
[151]	training's l2: 0.236923	valid_1's l2: 0.426256
[152]	training's l2: 0.236258	valid_1's l2: 0.426473
[153]	training's l2: 0.235557	valid_1's l2: 0.426441
[154]	training's l2: 0.234764	valid_1's l2: 0.426454
[155]	training's l2: 0.23406	valid_1's l2: 0.426537
[156]	training's l2: 0.233351	valid_1's l2: 0.426407
[157]	training's l2: 0.232754	valid_1's l2: 0.426381
[158]	training's l2: 0.232045	valid_1's l2: 0.426373
[159]	training's l2: 0.231257	valid_1's l2: 0.426428
[160]	training's l2: 0.230632	valid_1's l2: 0.426423
[161]	training's l2: 0.229955	valid_1's l2: 0.426629
[162]	training's l2: 0.229243	valid_1's l2: 0.426613
Did not meet early stopping. Best iteration is:
[162]	training's l2: 0.229243	valid_1's l2: 0.426613
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181059 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.680997	valid_1's l2: 0.685274
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.645766	valid_1's l2: 0.651959
[3]	training's l2: 0.616354	valid_1's l2: 0.625433
[4]	training's l2: 0.590442	valid_1's l2: 0.60219
[5]	training's l2: 0.568195	valid_1's l2: 0.583437
[6]	training's l2: 0.548944	valid_1's l2: 0.566068
[7]	training's l2: 0.531782	valid_1's l2: 0.55179
[8]	training's l2: 0.516602	valid_1's l2: 0.540054
[9]	training's l2: 0.503341	valid_1's l2: 0.529432
[10]	training's l2: 0.490721	valid_1's l2: 0.519555
[11]	training's l2: 0.480327	valid_1's l2: 0.511858
[12]	training's l2: 0.470348	valid_1's l2: 0.505455
[13]	training's l2: 0.461547	valid_1's l2: 0.49922
[14]	training's l2: 0.453463	valid_1's l2: 0.493889
[15]	training's l2: 0.445654	valid_1's l2: 0.488926
[16]	training's l2: 0.438688	valid_1's l2: 0.484995
[17]	training's l2: 0.432557	valid_1's l2: 0.481926
[18]	training's l2: 0.426341	valid_1's l2: 0.477924
[19]	training's l2: 0.421088	valid_1's l2: 0.474997
[20]	training's l2: 0.415635	valid_1's l2: 0.471949
[21]	training's l2: 0.411068	valid_1's l2: 0.47061
[22]	training's l2: 0.405831	valid_1's l2: 0.467364
[23]	training's l2: 0.40174	valid_1's l2: 0.46556
[24]	training's l2: 0.397183	valid_1's l2: 0.463763
[25]	training's l2: 0.39294	valid_1's l2: 0.462731
[26]	training's l2: 0.387992	valid_1's l2: 0.459977
[27]	training's l2: 0.383311	valid_1's l2: 0.458186
[28]	training's l2: 0.379861	valid_1's l2: 0.45692
[29]	training's l2: 0.37582	valid_1's l2: 0.454989
[30]	training's l2: 0.372585	valid_1's l2: 0.453775
[31]	training's l2: 0.369527	valid_1's l2: 0.452974
[32]	training's l2: 0.366508	valid_1's l2: 0.452251
[33]	training's l2: 0.36315	valid_1's l2: 0.451284
[34]	training's l2: 0.360211	valid_1's l2: 0.450075
[35]	training's l2: 0.357476	valid_1's l2: 0.449483
[36]	training's l2: 0.354265	valid_1's l2: 0.44836
[37]	training's l2: 0.351531	valid_1's l2: 0.447633
[38]	training's l2: 0.348699	valid_1's l2: 0.447276
[39]	training's l2: 0.345714	valid_1's l2: 0.445773
[40]	training's l2: 0.342995	valid_1's l2: 0.44518
[41]	training's l2: 0.340322	valid_1's l2: 0.44394
[42]	training's l2: 0.337837	valid_1's l2: 0.443492
[43]	training's l2: 0.335687	valid_1's l2: 0.443377
[44]	training's l2: 0.332329	valid_1's l2: 0.440963
[45]	training's l2: 0.330004	valid_1's l2: 0.440798
[46]	training's l2: 0.327457	valid_1's l2: 0.440214
[47]	training's l2: 0.324503	valid_1's l2: 0.438961
[48]	training's l2: 0.322097	valid_1's l2: 0.438562
[49]	training's l2: 0.319994	valid_1's l2: 0.438248
[50]	training's l2: 0.317365	valid_1's l2: 0.437083
[51]	training's l2: 0.315387	valid_1's l2: 0.437329
[52]	training's l2: 0.313067	valid_1's l2: 0.436428
[53]	training's l2: 0.31096	valid_1's l2: 0.43595
[54]	training's l2: 0.30865	valid_1's l2: 0.435189
[55]	training's l2: 0.306864	valid_1's l2: 0.435236
[56]	training's l2: 0.305022	valid_1's l2: 0.435345
[57]	training's l2: 0.303175	valid_1's l2: 0.434972
[58]	training's l2: 0.30095	valid_1's l2: 0.43409
[59]	training's l2: 0.299198	valid_1's l2: 0.434269
[60]	training's l2: 0.297163	valid_1's l2: 0.433589
[61]	training's l2: 0.295213	valid_1's l2: 0.432974
[62]	training's l2: 0.293506	valid_1's l2: 0.432933
[63]	training's l2: 0.29189	valid_1's l2: 0.432774
[64]	training's l2: 0.290232	valid_1's l2: 0.432867
[65]	training's l2: 0.288378	valid_1's l2: 0.432227
[66]	training's l2: 0.286688	valid_1's l2: 0.432129
[67]	training's l2: 0.285159	valid_1's l2: 0.432399
[68]	training's l2: 0.283608	valid_1's l2: 0.43243
[69]	training's l2: 0.281921	valid_1's l2: 0.431887
[70]	training's l2: 0.280398	valid_1's l2: 0.431647
[71]	training's l2: 0.278789	valid_1's l2: 0.431096
[72]	training's l2: 0.27721	valid_1's l2: 0.430924
[73]	training's l2: 0.275734	valid_1's l2: 0.430975
[74]	training's l2: 0.274309	valid_1's l2: 0.430687
[75]	training's l2: 0.272909	valid_1's l2: 0.430335
[76]	training's l2: 0.271364	valid_1's l2: 0.430383
[77]	training's l2: 0.269845	valid_1's l2: 0.430386
[78]	training's l2: 0.268191	valid_1's l2: 0.43025
[79]	training's l2: 0.266904	valid_1's l2: 0.430187
[80]	training's l2: 0.265499	valid_1's l2: 0.430469
[81]	training's l2: 0.263976	valid_1's l2: 0.430401
[82]	training's l2: 0.262605	valid_1's l2: 0.430314
[83]	training's l2: 0.261443	valid_1's l2: 0.430289
[84]	training's l2: 0.260093	valid_1's l2: 0.430515
[85]	training's l2: 0.258521	valid_1's l2: 0.429882
[86]	training's l2: 0.257261	valid_1's l2: 0.429873
[87]	training's l2: 0.256067	valid_1's l2: 0.430007
[88]	training's l2: 0.254877	valid_1's l2: 0.430211
[89]	training's l2: 0.25375	valid_1's l2: 0.430026
[90]	training's l2: 0.252337	valid_1's l2: 0.429837
[91]	training's l2: 0.251186	valid_1's l2: 0.429776
[92]	training's l2: 0.250012	valid_1's l2: 0.429762
[93]	training's l2: 0.248885	valid_1's l2: 0.429852
[94]	training's l2: 0.247874	valid_1's l2: 0.429946
[95]	training's l2: 0.246571	valid_1's l2: 0.429808
[96]	training's l2: 0.24528	valid_1's l2: 0.430008
[97]	training's l2: 0.244163	valid_1's l2: 0.430422
[98]	training's l2: 0.243133	valid_1's l2: 0.430475
[99]	training's l2: 0.242006	valid_1's l2: 0.430348
[100]	training's l2: 0.240662	valid_1's l2: 0.430082
[101]	training's l2: 0.239371	valid_1's l2: 0.430258
[102]	training's l2: 0.238328	valid_1's l2: 0.430193
[103]	training's l2: 0.237304	valid_1's l2: 0.430409
[104]	training's l2: 0.2362	valid_1's l2: 0.43041
[105]	training's l2: 0.234994	valid_1's l2: 0.430104
[106]	training's l2: 0.233951	valid_1's l2: 0.43003
[107]	training's l2: 0.232859	valid_1's l2: 0.4301
[108]	training's l2: 0.231617	valid_1's l2: 0.430067
[109]	training's l2: 0.230604	valid_1's l2: 0.430055
[110]	training's l2: 0.229668	valid_1's l2: 0.4297
[111]	training's l2: 0.228471	valid_1's l2: 0.429737
[112]	training's l2: 0.22732	valid_1's l2: 0.429726
[113]	training's l2: 0.226294	valid_1's l2: 0.429669
[114]	training's l2: 0.225245	valid_1's l2: 0.429712
[115]	training's l2: 0.224307	valid_1's l2: 0.429684
[116]	training's l2: 0.223267	valid_1's l2: 0.429683
[117]	training's l2: 0.222227	valid_1's l2: 0.429542
Did not meet early stopping. Best iteration is:
[117]	training's l2: 0.222227	valid_1's l2: 0.429542
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184156 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.692411	valid_1's l2: 0.69465
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.666334	valid_1's l2: 0.66942
[3]	training's l2: 0.64266	valid_1's l2: 0.646856
[4]	training's l2: 0.621784	valid_1's l2: 0.627548
[5]	training's l2: 0.603186	valid_1's l2: 0.609705
[6]	training's l2: 0.586884	valid_1's l2: 0.594081
[7]	training's l2: 0.572327	valid_1's l2: 0.580494
[8]	training's l2: 0.558393	valid_1's l2: 0.568009
[9]	training's l2: 0.546201	valid_1's l2: 0.556869
[10]	training's l2: 0.53537	valid_1's l2: 0.54729
[11]	training's l2: 0.525003	valid_1's l2: 0.538198
[12]	training's l2: 0.516104	valid_1's l2: 0.530677
[13]	training's l2: 0.507176	valid_1's l2: 0.523698
[14]	training's l2: 0.499359	valid_1's l2: 0.518121
[15]	training's l2: 0.492313	valid_1's l2: 0.512704
[16]	training's l2: 0.485448	valid_1's l2: 0.507679
[17]	training's l2: 0.478731	valid_1's l2: 0.503211
[18]	training's l2: 0.472722	valid_1's l2: 0.498177
[19]	training's l2: 0.467572	valid_1's l2: 0.494182
[20]	training's l2: 0.462378	valid_1's l2: 0.490537
[21]	training's l2: 0.457361	valid_1's l2: 0.487289
[22]	training's l2: 0.453085	valid_1's l2: 0.484532
[23]	training's l2: 0.448985	valid_1's l2: 0.481912
[24]	training's l2: 0.444846	valid_1's l2: 0.478962
[25]	training's l2: 0.441218	valid_1's l2: 0.476676
[26]	training's l2: 0.437775	valid_1's l2: 0.47447
[27]	training's l2: 0.434509	valid_1's l2: 0.472642
[28]	training's l2: 0.430942	valid_1's l2: 0.470937
[29]	training's l2: 0.428031	valid_1's l2: 0.469611
[30]	training's l2: 0.42451	valid_1's l2: 0.467855
[31]	training's l2: 0.421258	valid_1's l2: 0.466305
[32]	training's l2: 0.418358	valid_1's l2: 0.465219
[33]	training's l2: 0.415252	valid_1's l2: 0.463693
[34]	training's l2: 0.411808	valid_1's l2: 0.46192
[35]	training's l2: 0.409253	valid_1's l2: 0.460955
[36]	training's l2: 0.406547	valid_1's l2: 0.459448
[37]	training's l2: 0.404219	valid_1's l2: 0.458609
[38]	training's l2: 0.401569	valid_1's l2: 0.457049
[39]	training's l2: 0.399393	valid_1's l2: 0.456249
[40]	training's l2: 0.397161	valid_1's l2: 0.455585
[41]	training's l2: 0.395089	valid_1's l2: 0.454313
[42]	training's l2: 0.393107	valid_1's l2: 0.453573
[43]	training's l2: 0.391068	valid_1's l2: 0.452643
[44]	training's l2: 0.388829	valid_1's l2: 0.451755
[45]	training's l2: 0.386125	valid_1's l2: 0.450204
[46]	training's l2: 0.384362	valid_1's l2: 0.449775
[47]	training's l2: 0.382386	valid_1's l2: 0.449124
[48]	training's l2: 0.380231	valid_1's l2: 0.448138
[49]	training's l2: 0.378525	valid_1's l2: 0.447887
[50]	training's l2: 0.376076	valid_1's l2: 0.446803
[51]	training's l2: 0.374145	valid_1's l2: 0.446217
[52]	training's l2: 0.372351	valid_1's l2: 0.445546
[53]	training's l2: 0.370452	valid_1's l2: 0.4447
[54]	training's l2: 0.368818	valid_1's l2: 0.44416
[55]	training's l2: 0.367203	valid_1's l2: 0.443608
[56]	training's l2: 0.365368	valid_1's l2: 0.442702
[57]	training's l2: 0.363806	valid_1's l2: 0.442371
[58]	training's l2: 0.361386	valid_1's l2: 0.441229
[59]	training's l2: 0.359885	valid_1's l2: 0.440604
[60]	training's l2: 0.357955	valid_1's l2: 0.439754
[61]	training's l2: 0.356202	valid_1's l2: 0.438843
[62]	training's l2: 0.354947	valid_1's l2: 0.438774
[63]	training's l2: 0.353621	valid_1's l2: 0.438689
[64]	training's l2: 0.351926	valid_1's l2: 0.437645
[65]	training's l2: 0.350629	valid_1's l2: 0.437548
[66]	training's l2: 0.348711	valid_1's l2: 0.437011
[67]	training's l2: 0.34733	valid_1's l2: 0.436516
[68]	training's l2: 0.345989	valid_1's l2: 0.436302
[69]	training's l2: 0.344674	valid_1's l2: 0.436148
[70]	training's l2: 0.343445	valid_1's l2: 0.435867
[71]	training's l2: 0.342	valid_1's l2: 0.435447
[72]	training's l2: 0.340755	valid_1's l2: 0.435251
[73]	training's l2: 0.339546	valid_1's l2: 0.435182
[74]	training's l2: 0.338311	valid_1's l2: 0.435153
[75]	training's l2: 0.336745	valid_1's l2: 0.43484
[76]	training's l2: 0.335321	valid_1's l2: 0.434348
[77]	training's l2: 0.334106	valid_1's l2: 0.434045
[78]	training's l2: 0.332985	valid_1's l2: 0.43404
[79]	training's l2: 0.331898	valid_1's l2: 0.433945
[80]	training's l2: 0.330487	valid_1's l2: 0.433591
[81]	training's l2: 0.32922	valid_1's l2: 0.433204
[82]	training's l2: 0.328072	valid_1's l2: 0.432822
[83]	training's l2: 0.32699	valid_1's l2: 0.43266
[84]	training's l2: 0.325916	valid_1's l2: 0.432536
[85]	training's l2: 0.324764	valid_1's l2: 0.432581
[86]	training's l2: 0.32367	valid_1's l2: 0.432573
[87]	training's l2: 0.32267	valid_1's l2: 0.43245
[88]	training's l2: 0.321636	valid_1's l2: 0.432076
[89]	training's l2: 0.3205	valid_1's l2: 0.432035
[90]	training's l2: 0.319456	valid_1's l2: 0.431912
[91]	training's l2: 0.318454	valid_1's l2: 0.431754
[92]	training's l2: 0.317448	valid_1's l2: 0.431651
[93]	training's l2: 0.316405	valid_1's l2: 0.4315
[94]	training's l2: 0.315094	valid_1's l2: 0.430981
[95]	training's l2: 0.313882	valid_1's l2: 0.430499
[96]	training's l2: 0.312686	valid_1's l2: 0.430346
[97]	training's l2: 0.31171	valid_1's l2: 0.430384
[98]	training's l2: 0.310544	valid_1's l2: 0.430095
[99]	training's l2: 0.309607	valid_1's l2: 0.429896
[100]	training's l2: 0.308652	valid_1's l2: 0.429598
[101]	training's l2: 0.307664	valid_1's l2: 0.429604
[102]	training's l2: 0.306784	valid_1's l2: 0.429492
[103]	training's l2: 0.305926	valid_1's l2: 0.429512
[104]	training's l2: 0.304934	valid_1's l2: 0.42921
[105]	training's l2: 0.304082	valid_1's l2: 0.429259
[106]	training's l2: 0.30321	valid_1's l2: 0.429348
[107]	training's l2: 0.302404	valid_1's l2: 0.42937
[108]	training's l2: 0.301343	valid_1's l2: 0.4291
[109]	training's l2: 0.300492	valid_1's l2: 0.429053
[110]	training's l2: 0.29969	valid_1's l2: 0.42923
[111]	training's l2: 0.298889	valid_1's l2: 0.429333
[112]	training's l2: 0.298028	valid_1's l2: 0.429171
[113]	training's l2: 0.297233	valid_1's l2: 0.429226
[114]	training's l2: 0.296382	valid_1's l2: 0.429151
[115]	training's l2: 0.295613	valid_1's l2: 0.429183
[116]	training's l2: 0.294722	valid_1's l2: 0.429111
[117]	training's l2: 0.293875	valid_1's l2: 0.429083
[118]	training's l2: 0.292923	valid_1's l2: 0.428764
[119]	training's l2: 0.292102	valid_1's l2: 0.428832
[120]	training's l2: 0.29132	valid_1's l2: 0.428616
[121]	training's l2: 0.290465	valid_1's l2: 0.428501
[122]	training's l2: 0.289698	valid_1's l2: 0.428543
[123]	training's l2: 0.28879	valid_1's l2: 0.428316
[124]	training's l2: 0.28798	valid_1's l2: 0.42836
[125]	training's l2: 0.287138	valid_1's l2: 0.42842
[126]	training's l2: 0.286161	valid_1's l2: 0.428352
[127]	training's l2: 0.285408	valid_1's l2: 0.428451
[128]	training's l2: 0.284654	valid_1's l2: 0.42836
[129]	training's l2: 0.283906	valid_1's l2: 0.428457
[130]	training's l2: 0.283174	valid_1's l2: 0.428515
[131]	training's l2: 0.282314	valid_1's l2: 0.42866
[132]	training's l2: 0.281633	valid_1's l2: 0.428673
[133]	training's l2: 0.280839	valid_1's l2: 0.428531
[134]	training's l2: 0.279918	valid_1's l2: 0.428559
[135]	training's l2: 0.279116	valid_1's l2: 0.428197
[136]	training's l2: 0.278334	valid_1's l2: 0.428221
[137]	training's l2: 0.277579	valid_1's l2: 0.428269
[138]	training's l2: 0.276766	valid_1's l2: 0.428202
[139]	training's l2: 0.275985	valid_1's l2: 0.428005
[140]	training's l2: 0.275233	valid_1's l2: 0.428048
[141]	training's l2: 0.274452	valid_1's l2: 0.428067
[142]	training's l2: 0.273823	valid_1's l2: 0.4281
[143]	training's l2: 0.273106	valid_1's l2: 0.427997
[144]	training's l2: 0.272358	valid_1's l2: 0.427851
[145]	training's l2: 0.271578	valid_1's l2: 0.427926
[146]	training's l2: 0.270877	valid_1's l2: 0.42783
[147]	training's l2: 0.270252	valid_1's l2: 0.427982
[148]	training's l2: 0.26942	valid_1's l2: 0.427807
[149]	training's l2: 0.26868	valid_1's l2: 0.427863
[150]	training's l2: 0.268029	valid_1's l2: 0.427809
[151]	training's l2: 0.267288	valid_1's l2: 0.427752
[152]	training's l2: 0.266552	valid_1's l2: 0.427715
[153]	training's l2: 0.265788	valid_1's l2: 0.427431
[154]	training's l2: 0.265014	valid_1's l2: 0.427575
[155]	training's l2: 0.264394	valid_1's l2: 0.427599
[156]	training's l2: 0.263642	valid_1's l2: 0.427349
[157]	training's l2: 0.262914	valid_1's l2: 0.427209
[158]	training's l2: 0.26221	valid_1's l2: 0.427169
[159]	training's l2: 0.261582	valid_1's l2: 0.427276
[160]	training's l2: 0.260818	valid_1's l2: 0.427278
[161]	training's l2: 0.260183	valid_1's l2: 0.427355
[162]	training's l2: 0.25942	valid_1's l2: 0.42731
[163]	training's l2: 0.258813	valid_1's l2: 0.427404
[164]	training's l2: 0.258085	valid_1's l2: 0.427451
[165]	training's l2: 0.257426	valid_1's l2: 0.427343
[166]	training's l2: 0.256863	valid_1's l2: 0.427304
[167]	training's l2: 0.256308	valid_1's l2: 0.427305
[168]	training's l2: 0.255657	valid_1's l2: 0.427278
[169]	training's l2: 0.255029	valid_1's l2: 0.427222
[170]	training's l2: 0.254398	valid_1's l2: 0.427199
[171]	training's l2: 0.253714	valid_1's l2: 0.427208
[172]	training's l2: 0.253106	valid_1's l2: 0.427295
[173]	training's l2: 0.252576	valid_1's l2: 0.427385
[174]	training's l2: 0.252045	valid_1's l2: 0.427413
[175]	training's l2: 0.251321	valid_1's l2: 0.427515
[176]	training's l2: 0.25059	valid_1's l2: 0.427612
[177]	training's l2: 0.249954	valid_1's l2: 0.427586
[178]	training's l2: 0.249325	valid_1's l2: 0.427485
[179]	training's l2: 0.248722	valid_1's l2: 0.427621
[180]	training's l2: 0.247995	valid_1's l2: 0.427667
[181]	training's l2: 0.24745	valid_1's l2: 0.427718
[182]	training's l2: 0.246769	valid_1's l2: 0.427756
[183]	training's l2: 0.246189	valid_1's l2: 0.427735
[184]	training's l2: 0.245428	valid_1's l2: 0.427815
[185]	training's l2: 0.24485	valid_1's l2: 0.427836
[186]	training's l2: 0.244198	valid_1's l2: 0.428009
[187]	training's l2: 0.243628	valid_1's l2: 0.427938
[188]	training's l2: 0.243111	valid_1's l2: 0.42795
Early stopping, best iteration is:
[158]	training's l2: 0.26221	valid_1's l2: 0.427169
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262924 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.695977	valid_1's l2: 0.698596
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.672654	valid_1's l2: 0.676182
[3]	training's l2: 0.651112	valid_1's l2: 0.655783
[4]	training's l2: 0.63177	valid_1's l2: 0.637991
[5]	training's l2: 0.613814	valid_1's l2: 0.621409
[6]	training's l2: 0.597778	valid_1's l2: 0.606407
[7]	training's l2: 0.582869	valid_1's l2: 0.59354
[8]	training's l2: 0.569594	valid_1's l2: 0.581599
[9]	training's l2: 0.557218	valid_1's l2: 0.570703
[10]	training's l2: 0.545639	valid_1's l2: 0.560788
[11]	training's l2: 0.535327	valid_1's l2: 0.551816
[12]	training's l2: 0.525914	valid_1's l2: 0.543727
[13]	training's l2: 0.517277	valid_1's l2: 0.537535
[14]	training's l2: 0.50867	valid_1's l2: 0.531047
[15]	training's l2: 0.50094	valid_1's l2: 0.524616
[16]	training's l2: 0.49383	valid_1's l2: 0.518807
[17]	training's l2: 0.486865	valid_1's l2: 0.514245
[18]	training's l2: 0.480607	valid_1's l2: 0.509563
[19]	training's l2: 0.474661	valid_1's l2: 0.505227
[20]	training's l2: 0.46911	valid_1's l2: 0.501184
[21]	training's l2: 0.463691	valid_1's l2: 0.496971
[22]	training's l2: 0.458531	valid_1's l2: 0.49344
[23]	training's l2: 0.453858	valid_1's l2: 0.49044
[24]	training's l2: 0.449579	valid_1's l2: 0.487553
[25]	training's l2: 0.44534	valid_1's l2: 0.485168
[26]	training's l2: 0.441117	valid_1's l2: 0.48192
[27]	training's l2: 0.43697	valid_1's l2: 0.479527
[28]	training's l2: 0.433159	valid_1's l2: 0.477933
[29]	training's l2: 0.429667	valid_1's l2: 0.475934
[30]	training's l2: 0.426028	valid_1's l2: 0.473991
[31]	training's l2: 0.422538	valid_1's l2: 0.472359
[32]	training's l2: 0.419494	valid_1's l2: 0.471181
[33]	training's l2: 0.416597	valid_1's l2: 0.46991
[34]	training's l2: 0.413413	valid_1's l2: 0.468308
[35]	training's l2: 0.410326	valid_1's l2: 0.466722
[36]	training's l2: 0.407105	valid_1's l2: 0.465217
[37]	training's l2: 0.40431	valid_1's l2: 0.464082
[38]	training's l2: 0.401403	valid_1's l2: 0.462761
[39]	training's l2: 0.398879	valid_1's l2: 0.462115
[40]	training's l2: 0.396368	valid_1's l2: 0.460818
[41]	training's l2: 0.393873	valid_1's l2: 0.459759
[42]	training's l2: 0.391061	valid_1's l2: 0.458022
[43]	training's l2: 0.389021	valid_1's l2: 0.457224
[44]	training's l2: 0.386698	valid_1's l2: 0.456308
[45]	training's l2: 0.384218	valid_1's l2: 0.455359
[46]	training's l2: 0.382226	valid_1's l2: 0.454479
[47]	training's l2: 0.380256	valid_1's l2: 0.453885
[48]	training's l2: 0.378349	valid_1's l2: 0.453741
[49]	training's l2: 0.376122	valid_1's l2: 0.45282
[50]	training's l2: 0.373981	valid_1's l2: 0.452159
[51]	training's l2: 0.372163	valid_1's l2: 0.451348
[52]	training's l2: 0.370465	valid_1's l2: 0.451014
[53]	training's l2: 0.368478	valid_1's l2: 0.450374
[54]	training's l2: 0.366619	valid_1's l2: 0.449892
[55]	training's l2: 0.364277	valid_1's l2: 0.448476
[56]	training's l2: 0.362611	valid_1's l2: 0.448049
[57]	training's l2: 0.360117	valid_1's l2: 0.446969
[58]	training's l2: 0.358396	valid_1's l2: 0.446245
[59]	training's l2: 0.356615	valid_1's l2: 0.445545
[60]	training's l2: 0.354729	valid_1's l2: 0.444876
[61]	training's l2: 0.353252	valid_1's l2: 0.444517
[62]	training's l2: 0.351726	valid_1's l2: 0.444338
[63]	training's l2: 0.349581	valid_1's l2: 0.443331
[64]	training's l2: 0.347526	valid_1's l2: 0.442429
[65]	training's l2: 0.345803	valid_1's l2: 0.441865
[66]	training's l2: 0.343905	valid_1's l2: 0.441161
[67]	training's l2: 0.342454	valid_1's l2: 0.440933
[68]	training's l2: 0.340849	valid_1's l2: 0.440535
[69]	training's l2: 0.339543	valid_1's l2: 0.44032
[70]	training's l2: 0.337656	valid_1's l2: 0.439519
[71]	training's l2: 0.336331	valid_1's l2: 0.439345
[72]	training's l2: 0.33502	valid_1's l2: 0.439137
[73]	training's l2: 0.333665	valid_1's l2: 0.438777
[74]	training's l2: 0.332157	valid_1's l2: 0.438109
[75]	training's l2: 0.330844	valid_1's l2: 0.437618
[76]	training's l2: 0.329056	valid_1's l2: 0.436743
[77]	training's l2: 0.327413	valid_1's l2: 0.436181
[78]	training's l2: 0.32622	valid_1's l2: 0.436085
[79]	training's l2: 0.324944	valid_1's l2: 0.435705
[80]	training's l2: 0.323785	valid_1's l2: 0.435742
[81]	training's l2: 0.322601	valid_1's l2: 0.435574
[82]	training's l2: 0.321267	valid_1's l2: 0.435427
[83]	training's l2: 0.320084	valid_1's l2: 0.435271
[84]	training's l2: 0.318469	valid_1's l2: 0.434549
[85]	training's l2: 0.317056	valid_1's l2: 0.433995
[86]	training's l2: 0.315892	valid_1's l2: 0.43389
[87]	training's l2: 0.314761	valid_1's l2: 0.433859
[88]	training's l2: 0.313658	valid_1's l2: 0.433756
[89]	training's l2: 0.312256	valid_1's l2: 0.433113
[90]	training's l2: 0.311119	valid_1's l2: 0.433047
[91]	training's l2: 0.310021	valid_1's l2: 0.432978
[92]	training's l2: 0.308755	valid_1's l2: 0.432609
[93]	training's l2: 0.307713	valid_1's l2: 0.432452
[94]	training's l2: 0.306543	valid_1's l2: 0.432341
[95]	training's l2: 0.30538	valid_1's l2: 0.432053
[96]	training's l2: 0.304353	valid_1's l2: 0.432351
[97]	training's l2: 0.303111	valid_1's l2: 0.432388
[98]	training's l2: 0.302144	valid_1's l2: 0.432427
[99]	training's l2: 0.300946	valid_1's l2: 0.432407
[100]	training's l2: 0.299824	valid_1's l2: 0.432215
[101]	training's l2: 0.298818	valid_1's l2: 0.432175
[102]	training's l2: 0.297743	valid_1's l2: 0.431898
[103]	training's l2: 0.296653	valid_1's l2: 0.431968
[104]	training's l2: 0.295724	valid_1's l2: 0.432031
[105]	training's l2: 0.294698	valid_1's l2: 0.431805
[106]	training's l2: 0.293589	valid_1's l2: 0.431563
[107]	training's l2: 0.292688	valid_1's l2: 0.431422
[108]	training's l2: 0.2917	valid_1's l2: 0.431191
[109]	training's l2: 0.290667	valid_1's l2: 0.431156
[110]	training's l2: 0.289763	valid_1's l2: 0.430932
[111]	training's l2: 0.288845	valid_1's l2: 0.430823
[112]	training's l2: 0.287877	valid_1's l2: 0.430853
[113]	training's l2: 0.286882	valid_1's l2: 0.430838
[114]	training's l2: 0.285943	valid_1's l2: 0.430535
[115]	training's l2: 0.285075	valid_1's l2: 0.430497
[116]	training's l2: 0.284096	valid_1's l2: 0.430084
[117]	training's l2: 0.283244	valid_1's l2: 0.430093
[118]	training's l2: 0.282438	valid_1's l2: 0.430155
[119]	training's l2: 0.281359	valid_1's l2: 0.429652
[120]	training's l2: 0.280389	valid_1's l2: 0.429594
[121]	training's l2: 0.279402	valid_1's l2: 0.429526
[122]	training's l2: 0.278627	valid_1's l2: 0.429528
[123]	training's l2: 0.277837	valid_1's l2: 0.429347
[124]	training's l2: 0.277056	valid_1's l2: 0.4293
[125]	training's l2: 0.276266	valid_1's l2: 0.429433
[126]	training's l2: 0.275583	valid_1's l2: 0.429446
[127]	training's l2: 0.274697	valid_1's l2: 0.429485
[128]	training's l2: 0.273889	valid_1's l2: 0.429461
[129]	training's l2: 0.273094	valid_1's l2: 0.429547
[130]	training's l2: 0.272179	valid_1's l2: 0.42925
[131]	training's l2: 0.271383	valid_1's l2: 0.4292
[132]	training's l2: 0.27051	valid_1's l2: 0.428984
[133]	training's l2: 0.269755	valid_1's l2: 0.429085
[134]	training's l2: 0.268961	valid_1's l2: 0.42918
[135]	training's l2: 0.268237	valid_1's l2: 0.429131
[136]	training's l2: 0.267553	valid_1's l2: 0.429186
[137]	training's l2: 0.266759	valid_1's l2: 0.429219
[138]	training's l2: 0.265979	valid_1's l2: 0.429213
[139]	training's l2: 0.265162	valid_1's l2: 0.429249
[140]	training's l2: 0.264386	valid_1's l2: 0.429151
[141]	training's l2: 0.263517	valid_1's l2: 0.429044
[142]	training's l2: 0.262756	valid_1's l2: 0.429014
[143]	training's l2: 0.262058	valid_1's l2: 0.429033
[144]	training's l2: 0.261379	valid_1's l2: 0.428945
[145]	training's l2: 0.260695	valid_1's l2: 0.428875
[146]	training's l2: 0.259947	valid_1's l2: 0.428836
[147]	training's l2: 0.259206	valid_1's l2: 0.428739
[148]	training's l2: 0.258553	valid_1's l2: 0.428595
[149]	training's l2: 0.257803	valid_1's l2: 0.428538
[150]	training's l2: 0.256935	valid_1's l2: 0.428459
[151]	training's l2: 0.256361	valid_1's l2: 0.428413
[152]	training's l2: 0.255635	valid_1's l2: 0.428407
Did not meet early stopping. Best iteration is:
[152]	training's l2: 0.255635	valid_1's l2: 0.428407
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.203093 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.689063	valid_1's l2: 0.691662
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.660627	valid_1's l2: 0.664338
[3]	training's l2: 0.634815	valid_1's l2: 0.639976
[4]	training's l2: 0.611845	valid_1's l2: 0.618131
[5]	training's l2: 0.592162	valid_1's l2: 0.599271
[6]	training's l2: 0.573806	valid_1's l2: 0.582951
[7]	training's l2: 0.558233	valid_1's l2: 0.569732
[8]	training's l2: 0.544721	valid_1's l2: 0.557888
[9]	training's l2: 0.532225	valid_1's l2: 0.547321
[10]	training's l2: 0.520916	valid_1's l2: 0.538121
[11]	training's l2: 0.510619	valid_1's l2: 0.529378
[12]	training's l2: 0.501114	valid_1's l2: 0.522072
[13]	training's l2: 0.492739	valid_1's l2: 0.516115
[14]	training's l2: 0.484787	valid_1's l2: 0.510642
[15]	training's l2: 0.477611	valid_1's l2: 0.505425
[16]	training's l2: 0.470955	valid_1's l2: 0.500566
[17]	training's l2: 0.464891	valid_1's l2: 0.496296
[18]	training's l2: 0.459141	valid_1's l2: 0.492213
[19]	training's l2: 0.453673	valid_1's l2: 0.488823
[20]	training's l2: 0.448439	valid_1's l2: 0.485443
[21]	training's l2: 0.442963	valid_1's l2: 0.481848
[22]	training's l2: 0.438839	valid_1's l2: 0.479247
[23]	training's l2: 0.434459	valid_1's l2: 0.477009
[24]	training's l2: 0.430001	valid_1's l2: 0.474871
[25]	training's l2: 0.425274	valid_1's l2: 0.47202
[26]	training's l2: 0.421249	valid_1's l2: 0.469841
[27]	training's l2: 0.417641	valid_1's l2: 0.467939
[28]	training's l2: 0.414435	valid_1's l2: 0.466484
[29]	training's l2: 0.411314	valid_1's l2: 0.464917
[30]	training's l2: 0.407717	valid_1's l2: 0.463116
[31]	training's l2: 0.404224	valid_1's l2: 0.461609
[32]	training's l2: 0.401334	valid_1's l2: 0.460471
[33]	training's l2: 0.398242	valid_1's l2: 0.459307
[34]	training's l2: 0.395611	valid_1's l2: 0.458428
[35]	training's l2: 0.392491	valid_1's l2: 0.456758
[36]	training's l2: 0.389511	valid_1's l2: 0.455777
[37]	training's l2: 0.387016	valid_1's l2: 0.454882
[38]	training's l2: 0.384696	valid_1's l2: 0.454192
[39]	training's l2: 0.382308	valid_1's l2: 0.453239
[40]	training's l2: 0.379802	valid_1's l2: 0.452239
[41]	training's l2: 0.377355	valid_1's l2: 0.45121
[42]	training's l2: 0.375334	valid_1's l2: 0.450614
[43]	training's l2: 0.372009	valid_1's l2: 0.448936
[44]	training's l2: 0.369776	valid_1's l2: 0.448179
[45]	training's l2: 0.367764	valid_1's l2: 0.447808
[46]	training's l2: 0.365567	valid_1's l2: 0.447188
[47]	training's l2: 0.363552	valid_1's l2: 0.446356
[48]	training's l2: 0.361242	valid_1's l2: 0.445392
[49]	training's l2: 0.359235	valid_1's l2: 0.444902
[50]	training's l2: 0.357385	valid_1's l2: 0.444505
[51]	training's l2: 0.355207	valid_1's l2: 0.443459
[52]	training's l2: 0.352591	valid_1's l2: 0.441822
[53]	training's l2: 0.350352	valid_1's l2: 0.44078
[54]	training's l2: 0.348458	valid_1's l2: 0.440311
[55]	training's l2: 0.346857	valid_1's l2: 0.440231
[56]	training's l2: 0.345072	valid_1's l2: 0.439737
[57]	training's l2: 0.343053	valid_1's l2: 0.439068
[58]	training's l2: 0.341421	valid_1's l2: 0.438969
[59]	training's l2: 0.339865	valid_1's l2: 0.438663
[60]	training's l2: 0.337937	valid_1's l2: 0.437784
[61]	training's l2: 0.33639	valid_1's l2: 0.437563
[62]	training's l2: 0.334873	valid_1's l2: 0.437551
[63]	training's l2: 0.33318	valid_1's l2: 0.437084
[64]	training's l2: 0.331763	valid_1's l2: 0.437091
[65]	training's l2: 0.330243	valid_1's l2: 0.436592
[66]	training's l2: 0.328716	valid_1's l2: 0.436282
[67]	training's l2: 0.327174	valid_1's l2: 0.435854
[68]	training's l2: 0.325216	valid_1's l2: 0.435103
[69]	training's l2: 0.323838	valid_1's l2: 0.43517
[70]	training's l2: 0.322466	valid_1's l2: 0.434897
[71]	training's l2: 0.321005	valid_1's l2: 0.434854
[72]	training's l2: 0.319238	valid_1's l2: 0.434166
[73]	training's l2: 0.31795	valid_1's l2: 0.434094
[74]	training's l2: 0.316445	valid_1's l2: 0.433921
[75]	training's l2: 0.315214	valid_1's l2: 0.433844
[76]	training's l2: 0.313977	valid_1's l2: 0.433448
[77]	training's l2: 0.312708	valid_1's l2: 0.433376
[78]	training's l2: 0.311474	valid_1's l2: 0.433165
[79]	training's l2: 0.309883	valid_1's l2: 0.432591
[80]	training's l2: 0.30862	valid_1's l2: 0.432383
[81]	training's l2: 0.307441	valid_1's l2: 0.43225
[82]	training's l2: 0.306306	valid_1's l2: 0.432209
[83]	training's l2: 0.30499	valid_1's l2: 0.431939
[84]	training's l2: 0.303779	valid_1's l2: 0.431796
[85]	training's l2: 0.302613	valid_1's l2: 0.431824
[86]	training's l2: 0.301382	valid_1's l2: 0.431604
[87]	training's l2: 0.29999	valid_1's l2: 0.431313
[88]	training's l2: 0.298869	valid_1's l2: 0.431349
[89]	training's l2: 0.297771	valid_1's l2: 0.43138
[90]	training's l2: 0.296616	valid_1's l2: 0.431003
[91]	training's l2: 0.295475	valid_1's l2: 0.431001
[92]	training's l2: 0.294299	valid_1's l2: 0.430799
[93]	training's l2: 0.293118	valid_1's l2: 0.430591
[94]	training's l2: 0.29209	valid_1's l2: 0.430738
[95]	training's l2: 0.291123	valid_1's l2: 0.430552
[96]	training's l2: 0.29008	valid_1's l2: 0.430473
[97]	training's l2: 0.289069	valid_1's l2: 0.430435
[98]	training's l2: 0.288009	valid_1's l2: 0.430336
[99]	training's l2: 0.287062	valid_1's l2: 0.430501
[100]	training's l2: 0.285976	valid_1's l2: 0.430487
[101]	training's l2: 0.284955	valid_1's l2: 0.43062
[102]	training's l2: 0.283948	valid_1's l2: 0.430632
[103]	training's l2: 0.282999	valid_1's l2: 0.43061
[104]	training's l2: 0.282123	valid_1's l2: 0.430567
[105]	training's l2: 0.281059	valid_1's l2: 0.43063
[106]	training's l2: 0.280044	valid_1's l2: 0.43051
[107]	training's l2: 0.279069	valid_1's l2: 0.430696
[108]	training's l2: 0.278007	valid_1's l2: 0.430756
[109]	training's l2: 0.277058	valid_1's l2: 0.4307
[110]	training's l2: 0.276055	valid_1's l2: 0.430627
[111]	training's l2: 0.275182	valid_1's l2: 0.430705
[112]	training's l2: 0.27419	valid_1's l2: 0.430751
[113]	training's l2: 0.273218	valid_1's l2: 0.430835
[114]	training's l2: 0.272244	valid_1's l2: 0.430725
[115]	training's l2: 0.271312	valid_1's l2: 0.430752
[116]	training's l2: 0.270473	valid_1's l2: 0.430614
[117]	training's l2: 0.269622	valid_1's l2: 0.430867
[118]	training's l2: 0.268654	valid_1's l2: 0.430507
[119]	training's l2: 0.267846	valid_1's l2: 0.430519
[120]	training's l2: 0.266928	valid_1's l2: 0.430419
[121]	training's l2: 0.266078	valid_1's l2: 0.430444
[122]	training's l2: 0.265179	valid_1's l2: 0.430451
[123]	training's l2: 0.264281	valid_1's l2: 0.430459
[124]	training's l2: 0.263551	valid_1's l2: 0.430672
[125]	training's l2: 0.262675	valid_1's l2: 0.430819
[126]	training's l2: 0.261953	valid_1's l2: 0.43076
[127]	training's l2: 0.261125	valid_1's l2: 0.430761
[128]	training's l2: 0.260282	valid_1's l2: 0.43078
Early stopping, best iteration is:
[98]	training's l2: 0.288009	valid_1's l2: 0.430336
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.201479 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.682779	valid_1's l2: 0.685371
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.64913	valid_1's l2: 0.652798
[3]	training's l2: 0.621282	valid_1's l2: 0.626206
[4]	training's l2: 0.596701	valid_1's l2: 0.602803
[5]	training's l2: 0.575637	valid_1's l2: 0.583237
[6]	training's l2: 0.557752	valid_1's l2: 0.567133
[7]	training's l2: 0.541025	valid_1's l2: 0.55327
[8]	training's l2: 0.52724	valid_1's l2: 0.541147
[9]	training's l2: 0.514057	valid_1's l2: 0.530395
[10]	training's l2: 0.502303	valid_1's l2: 0.521253
[11]	training's l2: 0.492597	valid_1's l2: 0.513287
[12]	training's l2: 0.483066	valid_1's l2: 0.507291
[13]	training's l2: 0.474876	valid_1's l2: 0.501209
[14]	training's l2: 0.467081	valid_1's l2: 0.495201
[15]	training's l2: 0.460256	valid_1's l2: 0.491166
[16]	training's l2: 0.453578	valid_1's l2: 0.486546
[17]	training's l2: 0.447858	valid_1's l2: 0.482937
[18]	training's l2: 0.442393	valid_1's l2: 0.479565
[19]	training's l2: 0.436895	valid_1's l2: 0.476627
[20]	training's l2: 0.432175	valid_1's l2: 0.473733
[21]	training's l2: 0.427374	valid_1's l2: 0.470608
[22]	training's l2: 0.422651	valid_1's l2: 0.468261
[23]	training's l2: 0.418483	valid_1's l2: 0.466436
[24]	training's l2: 0.414344	valid_1's l2: 0.464671
[25]	training's l2: 0.410033	valid_1's l2: 0.46269
[26]	training's l2: 0.406693	valid_1's l2: 0.461479
[27]	training's l2: 0.403349	valid_1's l2: 0.460072
[28]	training's l2: 0.399459	valid_1's l2: 0.458139
[29]	training's l2: 0.395799	valid_1's l2: 0.456907
[30]	training's l2: 0.393074	valid_1's l2: 0.456033
[31]	training's l2: 0.390292	valid_1's l2: 0.455163
[32]	training's l2: 0.386483	valid_1's l2: 0.453311
[33]	training's l2: 0.383944	valid_1's l2: 0.452816
[34]	training's l2: 0.381092	valid_1's l2: 0.451745
[35]	training's l2: 0.377411	valid_1's l2: 0.44947
[36]	training's l2: 0.374637	valid_1's l2: 0.448793
[37]	training's l2: 0.372076	valid_1's l2: 0.44795
[38]	training's l2: 0.36987	valid_1's l2: 0.447755
[39]	training's l2: 0.367498	valid_1's l2: 0.447097
[40]	training's l2: 0.365034	valid_1's l2: 0.446544
[41]	training's l2: 0.36212	valid_1's l2: 0.445287
[42]	training's l2: 0.360051	valid_1's l2: 0.444946
[43]	training's l2: 0.357779	valid_1's l2: 0.443993
[44]	training's l2: 0.355856	valid_1's l2: 0.443661
[45]	training's l2: 0.353858	valid_1's l2: 0.442984
[46]	training's l2: 0.351012	valid_1's l2: 0.441058
[47]	training's l2: 0.348565	valid_1's l2: 0.439854
[48]	training's l2: 0.346716	valid_1's l2: 0.440059
[49]	training's l2: 0.344847	valid_1's l2: 0.439686
[50]	training's l2: 0.34299	valid_1's l2: 0.438809
[51]	training's l2: 0.340432	valid_1's l2: 0.43773
[52]	training's l2: 0.338653	valid_1's l2: 0.437016
[53]	training's l2: 0.337029	valid_1's l2: 0.436747
[54]	training's l2: 0.3354	valid_1's l2: 0.436127
[55]	training's l2: 0.333051	valid_1's l2: 0.434711
[56]	training's l2: 0.331012	valid_1's l2: 0.433776
[57]	training's l2: 0.329483	valid_1's l2: 0.433572
[58]	training's l2: 0.327949	valid_1's l2: 0.43345
[59]	training's l2: 0.326401	valid_1's l2: 0.433354
[60]	training's l2: 0.324723	valid_1's l2: 0.433026
[61]	training's l2: 0.323123	valid_1's l2: 0.432862
[62]	training's l2: 0.321438	valid_1's l2: 0.432506
[63]	training's l2: 0.320095	valid_1's l2: 0.432142
[64]	training's l2: 0.318718	valid_1's l2: 0.431966
[65]	training's l2: 0.316949	valid_1's l2: 0.431358
[66]	training's l2: 0.315589	valid_1's l2: 0.431314
[67]	training's l2: 0.314304	valid_1's l2: 0.431274
[68]	training's l2: 0.31272	valid_1's l2: 0.431045
[69]	training's l2: 0.311191	valid_1's l2: 0.430815
[70]	training's l2: 0.309553	valid_1's l2: 0.430449
[71]	training's l2: 0.308282	valid_1's l2: 0.430192
[72]	training's l2: 0.306852	valid_1's l2: 0.42969
[73]	training's l2: 0.305588	valid_1's l2: 0.429832
[74]	training's l2: 0.304296	valid_1's l2: 0.429676
[75]	training's l2: 0.303053	valid_1's l2: 0.429566
[76]	training's l2: 0.301748	valid_1's l2: 0.429593
[77]	training's l2: 0.300304	valid_1's l2: 0.429197
[78]	training's l2: 0.299109	valid_1's l2: 0.429331
[79]	training's l2: 0.297764	valid_1's l2: 0.428971
[80]	training's l2: 0.296467	valid_1's l2: 0.42868
[81]	training's l2: 0.295317	valid_1's l2: 0.428582
[82]	training's l2: 0.294186	valid_1's l2: 0.428766
[83]	training's l2: 0.292949	valid_1's l2: 0.42872
[84]	training's l2: 0.291687	valid_1's l2: 0.428638
[85]	training's l2: 0.290603	valid_1's l2: 0.428895
[86]	training's l2: 0.289283	valid_1's l2: 0.42871
[87]	training's l2: 0.288215	valid_1's l2: 0.428822
[88]	training's l2: 0.287115	valid_1's l2: 0.42885
[89]	training's l2: 0.286056	valid_1's l2: 0.428784
[90]	training's l2: 0.284817	valid_1's l2: 0.42859
[91]	training's l2: 0.283777	valid_1's l2: 0.428574
[92]	training's l2: 0.282818	valid_1's l2: 0.428963
[93]	training's l2: 0.281596	valid_1's l2: 0.428659
[94]	training's l2: 0.280563	valid_1's l2: 0.428743
[95]	training's l2: 0.279466	valid_1's l2: 0.428783
[96]	training's l2: 0.27854	valid_1's l2: 0.428769
[97]	training's l2: 0.277321	valid_1's l2: 0.428669
[98]	training's l2: 0.276204	valid_1's l2: 0.428695
[99]	training's l2: 0.274988	valid_1's l2: 0.428711
[100]	training's l2: 0.273898	valid_1's l2: 0.428625
[101]	training's l2: 0.272771	valid_1's l2: 0.428517
[102]	training's l2: 0.271713	valid_1's l2: 0.428429
[103]	training's l2: 0.270654	valid_1's l2: 0.428502
[104]	training's l2: 0.269844	valid_1's l2: 0.428512
[105]	training's l2: 0.268983	valid_1's l2: 0.428561
[106]	training's l2: 0.268226	valid_1's l2: 0.428629
[107]	training's l2: 0.267325	valid_1's l2: 0.428621
[108]	training's l2: 0.266331	valid_1's l2: 0.428864
[109]	training's l2: 0.265183	valid_1's l2: 0.428623
[110]	training's l2: 0.264442	valid_1's l2: 0.428771
[111]	training's l2: 0.263534	valid_1's l2: 0.428691
[112]	training's l2: 0.262695	valid_1's l2: 0.428593
[113]	training's l2: 0.261692	valid_1's l2: 0.428683
[114]	training's l2: 0.26073	valid_1's l2: 0.428564
[115]	training's l2: 0.259844	valid_1's l2: 0.428505
[116]	training's l2: 0.258943	valid_1's l2: 0.42844
[117]	training's l2: 0.258181	valid_1's l2: 0.42834
[118]	training's l2: 0.257183	valid_1's l2: 0.428032
[119]	training's l2: 0.256354	valid_1's l2: 0.427951
[120]	training's l2: 0.255472	valid_1's l2: 0.42801
[121]	training's l2: 0.254494	valid_1's l2: 0.427878
[122]	training's l2: 0.253648	valid_1's l2: 0.427971
[123]	training's l2: 0.25277	valid_1's l2: 0.428054
[124]	training's l2: 0.251759	valid_1's l2: 0.428316
[125]	training's l2: 0.250964	valid_1's l2: 0.428226
[126]	training's l2: 0.25014	valid_1's l2: 0.428147
[127]	training's l2: 0.249315	valid_1's l2: 0.428219
[128]	training's l2: 0.248514	valid_1's l2: 0.428311
[129]	training's l2: 0.247593	valid_1's l2: 0.428357
[130]	training's l2: 0.246703	valid_1's l2: 0.428411
[131]	training's l2: 0.245693	valid_1's l2: 0.428372
[132]	training's l2: 0.244755	valid_1's l2: 0.428249
[133]	training's l2: 0.243861	valid_1's l2: 0.428238
[134]	training's l2: 0.243187	valid_1's l2: 0.428319
[135]	training's l2: 0.242411	valid_1's l2: 0.42825
[136]	training's l2: 0.241578	valid_1's l2: 0.428104
[137]	training's l2: 0.240593	valid_1's l2: 0.428546
[138]	training's l2: 0.239574	valid_1's l2: 0.428542
[139]	training's l2: 0.23867	valid_1's l2: 0.428509
[140]	training's l2: 0.23773	valid_1's l2: 0.428399
[141]	training's l2: 0.236959	valid_1's l2: 0.428336
[142]	training's l2: 0.236229	valid_1's l2: 0.428149
[143]	training's l2: 0.235312	valid_1's l2: 0.428173
[144]	training's l2: 0.234458	valid_1's l2: 0.428006
[145]	training's l2: 0.233731	valid_1's l2: 0.428019
[146]	training's l2: 0.233071	valid_1's l2: 0.427872
[147]	training's l2: 0.232217	valid_1's l2: 0.427719
[148]	training's l2: 0.231344	valid_1's l2: 0.427693
[149]	training's l2: 0.230649	valid_1's l2: 0.427538
[150]	training's l2: 0.229892	valid_1's l2: 0.427564
Did not meet early stopping. Best iteration is:
[150]	training's l2: 0.229892	valid_1's l2: 0.427564
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198700 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.685468	valid_1's l2: 0.687523
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.654505	valid_1's l2: 0.656546
[3]	training's l2: 0.628453	valid_1's l2: 0.631952
[4]	training's l2: 0.605746	valid_1's l2: 0.610048
[5]	training's l2: 0.585533	valid_1's l2: 0.590688
[6]	training's l2: 0.568735	valid_1's l2: 0.574215
[7]	training's l2: 0.554036	valid_1's l2: 0.560337
[8]	training's l2: 0.540676	valid_1's l2: 0.548765
[9]	training's l2: 0.529166	valid_1's l2: 0.538559
[10]	training's l2: 0.519142	valid_1's l2: 0.52952
[11]	training's l2: 0.509893	valid_1's l2: 0.522146
[12]	training's l2: 0.501777	valid_1's l2: 0.515604
[13]	training's l2: 0.493733	valid_1's l2: 0.508854
[14]	training's l2: 0.487058	valid_1's l2: 0.503651
[15]	training's l2: 0.479555	valid_1's l2: 0.498404
[16]	training's l2: 0.474019	valid_1's l2: 0.494376
[17]	training's l2: 0.468947	valid_1's l2: 0.490815
[18]	training's l2: 0.463669	valid_1's l2: 0.487189
[19]	training's l2: 0.458074	valid_1's l2: 0.484205
[20]	training's l2: 0.45307	valid_1's l2: 0.480876
[21]	training's l2: 0.449241	valid_1's l2: 0.478116
[22]	training's l2: 0.443938	valid_1's l2: 0.474232
[23]	training's l2: 0.440302	valid_1's l2: 0.47214
[24]	training's l2: 0.436648	valid_1's l2: 0.469978
[25]	training's l2: 0.433075	valid_1's l2: 0.467669
[26]	training's l2: 0.429355	valid_1's l2: 0.465665
[27]	training's l2: 0.425827	valid_1's l2: 0.463689
[28]	training's l2: 0.423172	valid_1's l2: 0.462522
[29]	training's l2: 0.420765	valid_1's l2: 0.461358
[30]	training's l2: 0.418517	valid_1's l2: 0.460222
[31]	training's l2: 0.4165	valid_1's l2: 0.459422
[32]	training's l2: 0.413996	valid_1's l2: 0.458054
[33]	training's l2: 0.411675	valid_1's l2: 0.456836
[34]	training's l2: 0.408879	valid_1's l2: 0.45577
[35]	training's l2: 0.406922	valid_1's l2: 0.455225
[36]	training's l2: 0.404584	valid_1's l2: 0.454574
[37]	training's l2: 0.402326	valid_1's l2: 0.453642
[38]	training's l2: 0.400239	valid_1's l2: 0.453353
[39]	training's l2: 0.398156	valid_1's l2: 0.452133
[40]	training's l2: 0.395089	valid_1's l2: 0.449658
[41]	training's l2: 0.393454	valid_1's l2: 0.448561
[42]	training's l2: 0.391817	valid_1's l2: 0.448009
[43]	training's l2: 0.389884	valid_1's l2: 0.446925
[44]	training's l2: 0.388041	valid_1's l2: 0.446088
[45]	training's l2: 0.386489	valid_1's l2: 0.44575
[46]	training's l2: 0.384286	valid_1's l2: 0.444926
[47]	training's l2: 0.382755	valid_1's l2: 0.444355
[48]	training's l2: 0.381246	valid_1's l2: 0.443983
[49]	training's l2: 0.379331	valid_1's l2: 0.442831
[50]	training's l2: 0.378043	valid_1's l2: 0.44241
[51]	training's l2: 0.37608	valid_1's l2: 0.441383
[52]	training's l2: 0.374734	valid_1's l2: 0.440912
[53]	training's l2: 0.372825	valid_1's l2: 0.440032
[54]	training's l2: 0.37133	valid_1's l2: 0.439125
[55]	training's l2: 0.369133	valid_1's l2: 0.437916
[56]	training's l2: 0.36784	valid_1's l2: 0.437972
[57]	training's l2: 0.366479	valid_1's l2: 0.437958
[58]	training's l2: 0.365242	valid_1's l2: 0.437847
[59]	training's l2: 0.363697	valid_1's l2: 0.436723
[60]	training's l2: 0.362397	valid_1's l2: 0.435745
[61]	training's l2: 0.361303	valid_1's l2: 0.435786
[62]	training's l2: 0.36018	valid_1's l2: 0.435405
[63]	training's l2: 0.359055	valid_1's l2: 0.435079
[64]	training's l2: 0.357645	valid_1's l2: 0.434691
[65]	training's l2: 0.355962	valid_1's l2: 0.433645
[66]	training's l2: 0.354908	valid_1's l2: 0.433992
[67]	training's l2: 0.353639	valid_1's l2: 0.433559
[68]	training's l2: 0.352259	valid_1's l2: 0.433178
[69]	training's l2: 0.351221	valid_1's l2: 0.433083
[70]	training's l2: 0.349756	valid_1's l2: 0.432079
[71]	training's l2: 0.348764	valid_1's l2: 0.43194
[72]	training's l2: 0.347742	valid_1's l2: 0.431855
[73]	training's l2: 0.346674	valid_1's l2: 0.431457
[74]	training's l2: 0.345647	valid_1's l2: 0.431396
[75]	training's l2: 0.34461	valid_1's l2: 0.43123
[76]	training's l2: 0.34369	valid_1's l2: 0.43125
[77]	training's l2: 0.342716	valid_1's l2: 0.431157
[78]	training's l2: 0.341671	valid_1's l2: 0.430895
[79]	training's l2: 0.34068	valid_1's l2: 0.430794
[80]	training's l2: 0.339659	valid_1's l2: 0.430506
[81]	training's l2: 0.338822	valid_1's l2: 0.430452
[82]	training's l2: 0.337514	valid_1's l2: 0.429778
[83]	training's l2: 0.3366	valid_1's l2: 0.429696
[84]	training's l2: 0.335738	valid_1's l2: 0.429607
[85]	training's l2: 0.334918	valid_1's l2: 0.429545
[86]	training's l2: 0.333939	valid_1's l2: 0.429841
[87]	training's l2: 0.333078	valid_1's l2: 0.42979
[88]	training's l2: 0.331991	valid_1's l2: 0.429336
[89]	training's l2: 0.330978	valid_1's l2: 0.429342
[90]	training's l2: 0.33019	valid_1's l2: 0.429035
[91]	training's l2: 0.32945	valid_1's l2: 0.429014
[92]	training's l2: 0.328594	valid_1's l2: 0.429107
[93]	training's l2: 0.327708	valid_1's l2: 0.429203
[94]	training's l2: 0.326898	valid_1's l2: 0.429224
[95]	training's l2: 0.326081	valid_1's l2: 0.428977
[96]	training's l2: 0.325156	valid_1's l2: 0.428852
[97]	training's l2: 0.324404	valid_1's l2: 0.428939
[98]	training's l2: 0.323783	valid_1's l2: 0.428942
[99]	training's l2: 0.322985	valid_1's l2: 0.429048
[100]	training's l2: 0.32213	valid_1's l2: 0.428933
[101]	training's l2: 0.32129	valid_1's l2: 0.42886
[102]	training's l2: 0.320491	valid_1's l2: 0.428889
[103]	training's l2: 0.319715	valid_1's l2: 0.428919
Did not meet early stopping. Best iteration is:
[103]	training's l2: 0.319715	valid_1's l2: 0.428919
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216586 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.690328	valid_1's l2: 0.691488
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.663164	valid_1's l2: 0.662988
[3]	training's l2: 0.640007	valid_1's l2: 0.63944
[4]	training's l2: 0.620278	valid_1's l2: 0.619507
[5]	training's l2: 0.60303	valid_1's l2: 0.602328
[6]	training's l2: 0.58781	valid_1's l2: 0.586805
[7]	training's l2: 0.573367	valid_1's l2: 0.572905
[8]	training's l2: 0.560815	valid_1's l2: 0.560953
[9]	training's l2: 0.549821	valid_1's l2: 0.550945
[10]	training's l2: 0.539917	valid_1's l2: 0.542712
[11]	training's l2: 0.531431	valid_1's l2: 0.535243
[12]	training's l2: 0.523093	valid_1's l2: 0.528071
[13]	training's l2: 0.515581	valid_1's l2: 0.521583
[14]	training's l2: 0.509042	valid_1's l2: 0.516268
[15]	training's l2: 0.502516	valid_1's l2: 0.510951
[16]	training's l2: 0.496505	valid_1's l2: 0.506306
[17]	training's l2: 0.491368	valid_1's l2: 0.502765
[18]	training's l2: 0.485724	valid_1's l2: 0.498211
[19]	training's l2: 0.481384	valid_1's l2: 0.495082
[20]	training's l2: 0.476403	valid_1's l2: 0.491583
[21]	training's l2: 0.47083	valid_1's l2: 0.488047
[22]	training's l2: 0.467009	valid_1's l2: 0.4852
[23]	training's l2: 0.463317	valid_1's l2: 0.482757
[24]	training's l2: 0.459148	valid_1's l2: 0.479991
[25]	training's l2: 0.456284	valid_1's l2: 0.47809
[26]	training's l2: 0.453527	valid_1's l2: 0.475905
[27]	training's l2: 0.450634	valid_1's l2: 0.473802
[28]	training's l2: 0.447634	valid_1's l2: 0.472097
[29]	training's l2: 0.444346	valid_1's l2: 0.470104
[30]	training's l2: 0.442124	valid_1's l2: 0.468808
[31]	training's l2: 0.439049	valid_1's l2: 0.467278
[32]	training's l2: 0.437172	valid_1's l2: 0.466158
[33]	training's l2: 0.435205	valid_1's l2: 0.465143
[34]	training's l2: 0.432854	valid_1's l2: 0.463845
[35]	training's l2: 0.430923	valid_1's l2: 0.462956
[36]	training's l2: 0.42803	valid_1's l2: 0.461122
[37]	training's l2: 0.425951	valid_1's l2: 0.459785
[38]	training's l2: 0.424238	valid_1's l2: 0.458822
[39]	training's l2: 0.421683	valid_1's l2: 0.457158
[40]	training's l2: 0.420216	valid_1's l2: 0.456695
[41]	training's l2: 0.418653	valid_1's l2: 0.455875
[42]	training's l2: 0.416409	valid_1's l2: 0.454523
[43]	training's l2: 0.414711	valid_1's l2: 0.453613
[44]	training's l2: 0.413045	valid_1's l2: 0.452991
[45]	training's l2: 0.410803	valid_1's l2: 0.45152
[46]	training's l2: 0.408859	valid_1's l2: 0.450587
[47]	training's l2: 0.407571	valid_1's l2: 0.450152
[48]	training's l2: 0.406096	valid_1's l2: 0.4495
[49]	training's l2: 0.404843	valid_1's l2: 0.448424
[50]	training's l2: 0.403611	valid_1's l2: 0.447938
[51]	training's l2: 0.402028	valid_1's l2: 0.446778
[52]	training's l2: 0.400576	valid_1's l2: 0.446272
[53]	training's l2: 0.399339	valid_1's l2: 0.44601
[54]	training's l2: 0.397056	valid_1's l2: 0.444856
[55]	training's l2: 0.395966	valid_1's l2: 0.444628
[56]	training's l2: 0.394873	valid_1's l2: 0.444274
[57]	training's l2: 0.393456	valid_1's l2: 0.443443
[58]	training's l2: 0.392259	valid_1's l2: 0.442972
[59]	training's l2: 0.391174	valid_1's l2: 0.442614
[60]	training's l2: 0.389091	valid_1's l2: 0.441448
[61]	training's l2: 0.388128	valid_1's l2: 0.441086
[62]	training's l2: 0.387141	valid_1's l2: 0.44068
[63]	training's l2: 0.386247	valid_1's l2: 0.440647
[64]	training's l2: 0.385333	valid_1's l2: 0.440241
[65]	training's l2: 0.383745	valid_1's l2: 0.439165
[66]	training's l2: 0.382632	valid_1's l2: 0.438879
[67]	training's l2: 0.381631	valid_1's l2: 0.438614
[68]	training's l2: 0.380782	valid_1's l2: 0.438533
[69]	training's l2: 0.379393	valid_1's l2: 0.437865
[70]	training's l2: 0.378377	valid_1's l2: 0.437701
[71]	training's l2: 0.377416	valid_1's l2: 0.43695
[72]	training's l2: 0.376152	valid_1's l2: 0.436266
[73]	training's l2: 0.374819	valid_1's l2: 0.435262
[74]	training's l2: 0.373932	valid_1's l2: 0.434973
[75]	training's l2: 0.372721	valid_1's l2: 0.43449
[76]	training's l2: 0.371943	valid_1's l2: 0.434059
[77]	training's l2: 0.370832	valid_1's l2: 0.433136
[78]	training's l2: 0.370002	valid_1's l2: 0.43293
[79]	training's l2: 0.368842	valid_1's l2: 0.432523
[80]	training's l2: 0.367966	valid_1's l2: 0.432328
[81]	training's l2: 0.367199	valid_1's l2: 0.432158
[82]	training's l2: 0.366459	valid_1's l2: 0.432031
[83]	training's l2: 0.365444	valid_1's l2: 0.431541
[84]	training's l2: 0.364641	valid_1's l2: 0.431438
[85]	training's l2: 0.363721	valid_1's l2: 0.431332
[86]	training's l2: 0.362977	valid_1's l2: 0.431215
[87]	training's l2: 0.362297	valid_1's l2: 0.431206
[88]	training's l2: 0.36142	valid_1's l2: 0.430779
[89]	training's l2: 0.360739	valid_1's l2: 0.430562
[90]	training's l2: 0.36005	valid_1's l2: 0.430415
[91]	training's l2: 0.359259	valid_1's l2: 0.430373
[92]	training's l2: 0.358613	valid_1's l2: 0.430215
[93]	training's l2: 0.357811	valid_1's l2: 0.430279
[94]	training's l2: 0.357064	valid_1's l2: 0.430587
[95]	training's l2: 0.356434	valid_1's l2: 0.430597
[96]	training's l2: 0.35571	valid_1's l2: 0.430388
[97]	training's l2: 0.354752	valid_1's l2: 0.429766
[98]	training's l2: 0.354097	valid_1's l2: 0.429843
[99]	training's l2: 0.353409	valid_1's l2: 0.42963
[100]	training's l2: 0.352739	valid_1's l2: 0.429844
[101]	training's l2: 0.352038	valid_1's l2: 0.429747
[102]	training's l2: 0.351411	valid_1's l2: 0.429822
[103]	training's l2: 0.350734	valid_1's l2: 0.430006
[104]	training's l2: 0.350092	valid_1's l2: 0.429942
[105]	training's l2: 0.349502	valid_1's l2: 0.43
[106]	training's l2: 0.348852	valid_1's l2: 0.430054
[107]	training's l2: 0.34814	valid_1's l2: 0.429863
[108]	training's l2: 0.347544	valid_1's l2: 0.429773
[109]	training's l2: 0.3469	valid_1's l2: 0.429768
[110]	training's l2: 0.346307	valid_1's l2: 0.429857
[111]	training's l2: 0.345501	valid_1's l2: 0.429663
[112]	training's l2: 0.344941	valid_1's l2: 0.429638
[113]	training's l2: 0.344185	valid_1's l2: 0.429673
[114]	training's l2: 0.343536	valid_1's l2: 0.429679
[115]	training's l2: 0.34302	valid_1's l2: 0.429727
[116]	training's l2: 0.342462	valid_1's l2: 0.429813
[117]	training's l2: 0.341784	valid_1's l2: 0.429738
[118]	training's l2: 0.341091	valid_1's l2: 0.429671
[119]	training's l2: 0.340373	valid_1's l2: 0.42979
[120]	training's l2: 0.339723	valid_1's l2: 0.429992
[121]	training's l2: 0.339202	valid_1's l2: 0.429918
[122]	training's l2: 0.33848	valid_1's l2: 0.429806
[123]	training's l2: 0.337852	valid_1's l2: 0.429881
[124]	training's l2: 0.337357	valid_1's l2: 0.429749
[125]	training's l2: 0.336829	valid_1's l2: 0.429823
[126]	training's l2: 0.336279	valid_1's l2: 0.429749
[127]	training's l2: 0.335758	valid_1's l2: 0.429748
[128]	training's l2: 0.335118	valid_1's l2: 0.429816
[129]	training's l2: 0.334592	valid_1's l2: 0.429852
Early stopping, best iteration is:
[99]	training's l2: 0.353409	valid_1's l2: 0.42963
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208388 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.73632	valid_1's l2: 0.717534
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.706247	valid_1's l2: 0.692743
[3]	training's l2: 0.681314	valid_1's l2: 0.671601
[4]	training's l2: 0.658598	valid_1's l2: 0.653056
[5]	training's l2: 0.638124	valid_1's l2: 0.63651
[6]	training's l2: 0.620601	valid_1's l2: 0.622353
[7]	training's l2: 0.603987	valid_1's l2: 0.610028
[8]	training's l2: 0.590404	valid_1's l2: 0.600006
[9]	training's l2: 0.577157	valid_1's l2: 0.590425
[10]	training's l2: 0.566337	valid_1's l2: 0.582213
[11]	training's l2: 0.553777	valid_1's l2: 0.572152
[12]	training's l2: 0.543686	valid_1's l2: 0.565038
[13]	training's l2: 0.533132	valid_1's l2: 0.557016
[14]	training's l2: 0.525298	valid_1's l2: 0.551297
[15]	training's l2: 0.518203	valid_1's l2: 0.545909
[16]	training's l2: 0.510869	valid_1's l2: 0.541103
[17]	training's l2: 0.503732	valid_1's l2: 0.535215
[18]	training's l2: 0.495919	valid_1's l2: 0.529489
[19]	training's l2: 0.49042	valid_1's l2: 0.526392
[20]	training's l2: 0.484673	valid_1's l2: 0.522828
[21]	training's l2: 0.479175	valid_1's l2: 0.520088
[22]	training's l2: 0.473713	valid_1's l2: 0.5166
[23]	training's l2: 0.468491	valid_1's l2: 0.513669
[24]	training's l2: 0.464301	valid_1's l2: 0.511433
[25]	training's l2: 0.460083	valid_1's l2: 0.509373
[26]	training's l2: 0.455624	valid_1's l2: 0.506294
[27]	training's l2: 0.451894	valid_1's l2: 0.504774
[28]	training's l2: 0.447298	valid_1's l2: 0.501838
[29]	training's l2: 0.444283	valid_1's l2: 0.500712
[30]	training's l2: 0.441274	valid_1's l2: 0.499188
[31]	training's l2: 0.436032	valid_1's l2: 0.494841
[32]	training's l2: 0.433047	valid_1's l2: 0.493594
[33]	training's l2: 0.428908	valid_1's l2: 0.490311
[34]	training's l2: 0.426356	valid_1's l2: 0.489086
[35]	training's l2: 0.423804	valid_1's l2: 0.487833
[36]	training's l2: 0.419779	valid_1's l2: 0.484943
[37]	training's l2: 0.416855	valid_1's l2: 0.483484
[38]	training's l2: 0.41451	valid_1's l2: 0.482846
[39]	training's l2: 0.412219	valid_1's l2: 0.481802
[40]	training's l2: 0.410052	valid_1's l2: 0.481156
[41]	training's l2: 0.407377	valid_1's l2: 0.479502
[42]	training's l2: 0.405168	valid_1's l2: 0.478756
[43]	training's l2: 0.402308	valid_1's l2: 0.477495
[44]	training's l2: 0.399963	valid_1's l2: 0.476242
[45]	training's l2: 0.397949	valid_1's l2: 0.475392
[46]	training's l2: 0.395588	valid_1's l2: 0.473841
[47]	training's l2: 0.393813	valid_1's l2: 0.473313
[48]	training's l2: 0.391775	valid_1's l2: 0.47275
[49]	training's l2: 0.389806	valid_1's l2: 0.472293
[50]	training's l2: 0.388065	valid_1's l2: 0.471911
[51]	training's l2: 0.385489	valid_1's l2: 0.470826
[52]	training's l2: 0.383602	valid_1's l2: 0.469929
[53]	training's l2: 0.382031	valid_1's l2: 0.469713
[54]	training's l2: 0.380069	valid_1's l2: 0.469075
[55]	training's l2: 0.378275	valid_1's l2: 0.468324
[56]	training's l2: 0.376754	valid_1's l2: 0.468388
[57]	training's l2: 0.374186	valid_1's l2: 0.466788
[58]	training's l2: 0.372806	valid_1's l2: 0.466534
[59]	training's l2: 0.371171	valid_1's l2: 0.465965
[60]	training's l2: 0.369682	valid_1's l2: 0.465742
[61]	training's l2: 0.368044	valid_1's l2: 0.465169
[62]	training's l2: 0.366057	valid_1's l2: 0.464959
[63]	training's l2: 0.364605	valid_1's l2: 0.464574
[64]	training's l2: 0.362991	valid_1's l2: 0.464409
[65]	training's l2: 0.3611	valid_1's l2: 0.463995
[66]	training's l2: 0.35976	valid_1's l2: 0.463815
[67]	training's l2: 0.358407	valid_1's l2: 0.463575
[68]	training's l2: 0.35691	valid_1's l2: 0.463366
[69]	training's l2: 0.355534	valid_1's l2: 0.463162
[70]	training's l2: 0.354103	valid_1's l2: 0.462739
[71]	training's l2: 0.352841	valid_1's l2: 0.462722
[72]	training's l2: 0.351155	valid_1's l2: 0.46227
[73]	training's l2: 0.349399	valid_1's l2: 0.461434
[74]	training's l2: 0.34815	valid_1's l2: 0.461126
[75]	training's l2: 0.346321	valid_1's l2: 0.459996
[76]	training's l2: 0.345054	valid_1's l2: 0.459629
[77]	training's l2: 0.343785	valid_1's l2: 0.459265
[78]	training's l2: 0.342227	valid_1's l2: 0.458805
[79]	training's l2: 0.340804	valid_1's l2: 0.459144
[80]	training's l2: 0.339471	valid_1's l2: 0.458825
[81]	training's l2: 0.338299	valid_1's l2: 0.458856
[82]	training's l2: 0.337106	valid_1's l2: 0.459102
[83]	training's l2: 0.335747	valid_1's l2: 0.4585
[84]	training's l2: 0.334302	valid_1's l2: 0.458276
[85]	training's l2: 0.332607	valid_1's l2: 0.457347
[86]	training's l2: 0.331287	valid_1's l2: 0.457185
[87]	training's l2: 0.330235	valid_1's l2: 0.45731
[88]	training's l2: 0.329085	valid_1's l2: 0.457241
[89]	training's l2: 0.327943	valid_1's l2: 0.457137
[90]	training's l2: 0.326748	valid_1's l2: 0.45695
[91]	training's l2: 0.325735	valid_1's l2: 0.456934
[92]	training's l2: 0.324395	valid_1's l2: 0.456421
[93]	training's l2: 0.323372	valid_1's l2: 0.456329
[94]	training's l2: 0.322284	valid_1's l2: 0.456143
[95]	training's l2: 0.321198	valid_1's l2: 0.456207
[96]	training's l2: 0.320096	valid_1's l2: 0.456272
[97]	training's l2: 0.319042	valid_1's l2: 0.456321
[98]	training's l2: 0.317913	valid_1's l2: 0.455836
[99]	training's l2: 0.31679	valid_1's l2: 0.455484
[100]	training's l2: 0.315499	valid_1's l2: 0.45525
[101]	training's l2: 0.314426	valid_1's l2: 0.455419
[102]	training's l2: 0.313334	valid_1's l2: 0.455475
[103]	training's l2: 0.312214	valid_1's l2: 0.455093
[104]	training's l2: 0.311114	valid_1's l2: 0.455074
[105]	training's l2: 0.31009	valid_1's l2: 0.454908
[106]	training's l2: 0.308703	valid_1's l2: 0.454524
[107]	training's l2: 0.307742	valid_1's l2: 0.454494
[108]	training's l2: 0.306762	valid_1's l2: 0.454375
[109]	training's l2: 0.305806	valid_1's l2: 0.454264
[110]	training's l2: 0.30456	valid_1's l2: 0.453648
[111]	training's l2: 0.303649	valid_1's l2: 0.453485
[112]	training's l2: 0.302665	valid_1's l2: 0.453455
[113]	training's l2: 0.30182	valid_1's l2: 0.453653
[114]	training's l2: 0.300922	valid_1's l2: 0.453549
[115]	training's l2: 0.300043	valid_1's l2: 0.453264
[116]	training's l2: 0.299199	valid_1's l2: 0.453295
[117]	training's l2: 0.298372	valid_1's l2: 0.45344
[118]	training's l2: 0.297426	valid_1's l2: 0.45343
[119]	training's l2: 0.296567	valid_1's l2: 0.453388
[120]	training's l2: 0.295586	valid_1's l2: 0.453162
[121]	training's l2: 0.294695	valid_1's l2: 0.453068
[122]	training's l2: 0.29394	valid_1's l2: 0.453407
[123]	training's l2: 0.293188	valid_1's l2: 0.453342
[124]	training's l2: 0.292284	valid_1's l2: 0.452963
[125]	training's l2: 0.291446	valid_1's l2: 0.452906
[126]	training's l2: 0.290559	valid_1's l2: 0.452974
[127]	training's l2: 0.289639	valid_1's l2: 0.453031
[128]	training's l2: 0.288824	valid_1's l2: 0.453036
[129]	training's l2: 0.2877	valid_1's l2: 0.452668
[130]	training's l2: 0.286977	valid_1's l2: 0.452692
[131]	training's l2: 0.286117	valid_1's l2: 0.452842
[132]	training's l2: 0.285279	valid_1's l2: 0.452977
[133]	training's l2: 0.284335	valid_1's l2: 0.453197
[134]	training's l2: 0.283505	valid_1's l2: 0.453227
[135]	training's l2: 0.282775	valid_1's l2: 0.45318
[136]	training's l2: 0.281941	valid_1's l2: 0.453198
[137]	training's l2: 0.2811	valid_1's l2: 0.453255
[138]	training's l2: 0.28022	valid_1's l2: 0.453201
[139]	training's l2: 0.279325	valid_1's l2: 0.453139
[140]	training's l2: 0.278628	valid_1's l2: 0.453146
[141]	training's l2: 0.277791	valid_1's l2: 0.452977
[142]	training's l2: 0.277109	valid_1's l2: 0.452939
[143]	training's l2: 0.276353	valid_1's l2: 0.452822
[144]	training's l2: 0.275575	valid_1's l2: 0.452899
[145]	training's l2: 0.274755	valid_1's l2: 0.452991
[146]	training's l2: 0.274026	valid_1's l2: 0.452894
[147]	training's l2: 0.273192	valid_1's l2: 0.452981
[148]	training's l2: 0.272414	valid_1's l2: 0.452966
[149]	training's l2: 0.27162	valid_1's l2: 0.452837
[150]	training's l2: 0.270871	valid_1's l2: 0.452719
[151]	training's l2: 0.270127	valid_1's l2: 0.45276
[152]	training's l2: 0.269393	valid_1's l2: 0.452772
[153]	training's l2: 0.268633	valid_1's l2: 0.452679
[154]	training's l2: 0.268002	valid_1's l2: 0.452732
[155]	training's l2: 0.267155	valid_1's l2: 0.452773
[156]	training's l2: 0.266317	valid_1's l2: 0.453088
[157]	training's l2: 0.265442	valid_1's l2: 0.453205
[158]	training's l2: 0.264698	valid_1's l2: 0.453219
[159]	training's l2: 0.263941	valid_1's l2: 0.453383
Early stopping, best iteration is:
[129]	training's l2: 0.2877	valid_1's l2: 0.452668
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215706 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.74129	valid_1's l2: 0.722351
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.716355	valid_1's l2: 0.702067
[3]	training's l2: 0.694204	valid_1's l2: 0.683654
[4]	training's l2: 0.673538	valid_1's l2: 0.666702
[5]	training's l2: 0.655099	valid_1's l2: 0.651322
[6]	training's l2: 0.638449	valid_1's l2: 0.638642
[7]	training's l2: 0.622278	valid_1's l2: 0.626412
[8]	training's l2: 0.609036	valid_1's l2: 0.61652
[9]	training's l2: 0.596592	valid_1's l2: 0.607212
[10]	training's l2: 0.584245	valid_1's l2: 0.598241
[11]	training's l2: 0.573176	valid_1's l2: 0.590335
[12]	training's l2: 0.563569	valid_1's l2: 0.582998
[13]	training's l2: 0.554103	valid_1's l2: 0.576621
[14]	training's l2: 0.544753	valid_1's l2: 0.569841
[15]	training's l2: 0.53568	valid_1's l2: 0.562667
[16]	training's l2: 0.528277	valid_1's l2: 0.557836
[17]	training's l2: 0.521253	valid_1's l2: 0.552587
[18]	training's l2: 0.515296	valid_1's l2: 0.548703
[19]	training's l2: 0.508132	valid_1's l2: 0.543092
[20]	training's l2: 0.502063	valid_1's l2: 0.539436
[21]	training's l2: 0.495634	valid_1's l2: 0.534668
[22]	training's l2: 0.490421	valid_1's l2: 0.531515
[23]	training's l2: 0.485505	valid_1's l2: 0.529027
[24]	training's l2: 0.479801	valid_1's l2: 0.525378
[25]	training's l2: 0.474436	valid_1's l2: 0.521691
[26]	training's l2: 0.470275	valid_1's l2: 0.519265
[27]	training's l2: 0.46558	valid_1's l2: 0.51706
[28]	training's l2: 0.460967	valid_1's l2: 0.514581
[29]	training's l2: 0.456381	valid_1's l2: 0.511514
[30]	training's l2: 0.452772	valid_1's l2: 0.509129
[31]	training's l2: 0.448858	valid_1's l2: 0.507002
[32]	training's l2: 0.445198	valid_1's l2: 0.50473
[33]	training's l2: 0.442124	valid_1's l2: 0.503469
[34]	training's l2: 0.439371	valid_1's l2: 0.502632
[35]	training's l2: 0.436561	valid_1's l2: 0.501326
[36]	training's l2: 0.433729	valid_1's l2: 0.49984
[37]	training's l2: 0.430338	valid_1's l2: 0.498315
[38]	training's l2: 0.427917	valid_1's l2: 0.497396
[39]	training's l2: 0.423903	valid_1's l2: 0.494384
[40]	training's l2: 0.421264	valid_1's l2: 0.49307
[41]	training's l2: 0.418715	valid_1's l2: 0.492336
[42]	training's l2: 0.41563	valid_1's l2: 0.490416
[43]	training's l2: 0.413373	valid_1's l2: 0.489552
[44]	training's l2: 0.410818	valid_1's l2: 0.488281
[45]	training's l2: 0.407527	valid_1's l2: 0.486013
[46]	training's l2: 0.405336	valid_1's l2: 0.485685
[47]	training's l2: 0.40348	valid_1's l2: 0.485137
[48]	training's l2: 0.401585	valid_1's l2: 0.484359
[49]	training's l2: 0.399052	valid_1's l2: 0.483559
[50]	training's l2: 0.397317	valid_1's l2: 0.482897
[51]	training's l2: 0.39549	valid_1's l2: 0.482253
[52]	training's l2: 0.393847	valid_1's l2: 0.481841
[53]	training's l2: 0.391673	valid_1's l2: 0.480918
[54]	training's l2: 0.389734	valid_1's l2: 0.479937
[55]	training's l2: 0.387866	valid_1's l2: 0.479276
[56]	training's l2: 0.385369	valid_1's l2: 0.478111
[57]	training's l2: 0.383291	valid_1's l2: 0.476607
[58]	training's l2: 0.381739	valid_1's l2: 0.476293
[59]	training's l2: 0.379374	valid_1's l2: 0.475571
[60]	training's l2: 0.37771	valid_1's l2: 0.474836
[61]	training's l2: 0.376111	valid_1's l2: 0.474699
[62]	training's l2: 0.374545	valid_1's l2: 0.474334
[63]	training's l2: 0.372901	valid_1's l2: 0.473894
[64]	training's l2: 0.37113	valid_1's l2: 0.473205
[65]	training's l2: 0.369591	valid_1's l2: 0.472788
[66]	training's l2: 0.367972	valid_1's l2: 0.471875
[67]	training's l2: 0.36594	valid_1's l2: 0.47087
[68]	training's l2: 0.364528	valid_1's l2: 0.470546
[69]	training's l2: 0.363154	valid_1's l2: 0.470347
[70]	training's l2: 0.361787	valid_1's l2: 0.470049
[71]	training's l2: 0.36022	valid_1's l2: 0.470133
[72]	training's l2: 0.358382	valid_1's l2: 0.469455
[73]	training's l2: 0.357053	valid_1's l2: 0.469347
[74]	training's l2: 0.355737	valid_1's l2: 0.469086
[75]	training's l2: 0.354382	valid_1's l2: 0.468904
[76]	training's l2: 0.352894	valid_1's l2: 0.468637
[77]	training's l2: 0.351547	valid_1's l2: 0.468368
[78]	training's l2: 0.350005	valid_1's l2: 0.468012
[79]	training's l2: 0.348635	valid_1's l2: 0.467763
[80]	training's l2: 0.347457	valid_1's l2: 0.467699
[81]	training's l2: 0.346194	valid_1's l2: 0.467316
[82]	training's l2: 0.345004	valid_1's l2: 0.467221
[83]	training's l2: 0.343359	valid_1's l2: 0.466331
[84]	training's l2: 0.342222	valid_1's l2: 0.466376
[85]	training's l2: 0.340981	valid_1's l2: 0.466376
[86]	training's l2: 0.339222	valid_1's l2: 0.465306
[87]	training's l2: 0.337626	valid_1's l2: 0.464213
[88]	training's l2: 0.33648	valid_1's l2: 0.464103
[89]	training's l2: 0.335401	valid_1's l2: 0.463927
[90]	training's l2: 0.334283	valid_1's l2: 0.463757
[91]	training's l2: 0.333023	valid_1's l2: 0.463559
[92]	training's l2: 0.331914	valid_1's l2: 0.463526
[93]	training's l2: 0.330879	valid_1's l2: 0.463589
[94]	training's l2: 0.329834	valid_1's l2: 0.463555
[95]	training's l2: 0.328588	valid_1's l2: 0.463428
[96]	training's l2: 0.327326	valid_1's l2: 0.462838
[97]	training's l2: 0.326211	valid_1's l2: 0.462555
[98]	training's l2: 0.325031	valid_1's l2: 0.462281
[99]	training's l2: 0.323732	valid_1's l2: 0.462127
[100]	training's l2: 0.322642	valid_1's l2: 0.462128
[101]	training's l2: 0.321579	valid_1's l2: 0.461991
[102]	training's l2: 0.320507	valid_1's l2: 0.461726
[103]	training's l2: 0.319437	valid_1's l2: 0.46172
[104]	training's l2: 0.318304	valid_1's l2: 0.46137
[105]	training's l2: 0.316784	valid_1's l2: 0.460532
[106]	training's l2: 0.315553	valid_1's l2: 0.460755
[107]	training's l2: 0.314401	valid_1's l2: 0.460352
[108]	training's l2: 0.313105	valid_1's l2: 0.459962
[109]	training's l2: 0.312025	valid_1's l2: 0.459731
[110]	training's l2: 0.311002	valid_1's l2: 0.459948
[111]	training's l2: 0.310009	valid_1's l2: 0.459952
[112]	training's l2: 0.309026	valid_1's l2: 0.460114
[113]	training's l2: 0.308097	valid_1's l2: 0.459979
[114]	training's l2: 0.307124	valid_1's l2: 0.460303
[115]	training's l2: 0.306203	valid_1's l2: 0.460428
[116]	training's l2: 0.305172	valid_1's l2: 0.460292
[117]	training's l2: 0.304248	valid_1's l2: 0.460158
[118]	training's l2: 0.303031	valid_1's l2: 0.459665
[119]	training's l2: 0.302009	valid_1's l2: 0.459581
[120]	training's l2: 0.301071	valid_1's l2: 0.45943
[121]	training's l2: 0.300245	valid_1's l2: 0.459511
[122]	training's l2: 0.29928	valid_1's l2: 0.459359
[123]	training's l2: 0.298069	valid_1's l2: 0.45895
[124]	training's l2: 0.297	valid_1's l2: 0.458695
[125]	training's l2: 0.29609	valid_1's l2: 0.458608
[126]	training's l2: 0.295201	valid_1's l2: 0.458685
[127]	training's l2: 0.294274	valid_1's l2: 0.45873
[128]	training's l2: 0.293415	valid_1's l2: 0.45885
[129]	training's l2: 0.292603	valid_1's l2: 0.459148
[130]	training's l2: 0.291695	valid_1's l2: 0.459265
[131]	training's l2: 0.290835	valid_1's l2: 0.459196
[132]	training's l2: 0.28983	valid_1's l2: 0.459012
[133]	training's l2: 0.288945	valid_1's l2: 0.45898
[134]	training's l2: 0.288181	valid_1's l2: 0.458807
[135]	training's l2: 0.287317	valid_1's l2: 0.458902
[136]	training's l2: 0.286428	valid_1's l2: 0.458802
[137]	training's l2: 0.285647	valid_1's l2: 0.45883
[138]	training's l2: 0.284824	valid_1's l2: 0.458899
[139]	training's l2: 0.283882	valid_1's l2: 0.459011
[140]	training's l2: 0.283073	valid_1's l2: 0.459189
[141]	training's l2: 0.282283	valid_1's l2: 0.459153
[142]	training's l2: 0.281501	valid_1's l2: 0.459162
[143]	training's l2: 0.280627	valid_1's l2: 0.459147
[144]	training's l2: 0.279907	valid_1's l2: 0.459263
[145]	training's l2: 0.278864	valid_1's l2: 0.458986
[146]	training's l2: 0.278052	valid_1's l2: 0.458878
[147]	training's l2: 0.27712	valid_1's l2: 0.458644
[148]	training's l2: 0.276429	valid_1's l2: 0.458877
[149]	training's l2: 0.275663	valid_1's l2: 0.458691
[150]	training's l2: 0.274877	valid_1's l2: 0.458681
[151]	training's l2: 0.274002	valid_1's l2: 0.458609
[152]	training's l2: 0.273263	valid_1's l2: 0.4585
[153]	training's l2: 0.272623	valid_1's l2: 0.458483
[154]	training's l2: 0.271808	valid_1's l2: 0.458485
[155]	training's l2: 0.271118	valid_1's l2: 0.458515
[156]	training's l2: 0.270352	valid_1's l2: 0.458488
[157]	training's l2: 0.269734	valid_1's l2: 0.458548
[158]	training's l2: 0.268933	valid_1's l2: 0.458513
[159]	training's l2: 0.26816	valid_1's l2: 0.458594
[160]	training's l2: 0.267145	valid_1's l2: 0.45849
[161]	training's l2: 0.266253	valid_1's l2: 0.458225
[162]	training's l2: 0.265513	valid_1's l2: 0.458174
[163]	training's l2: 0.264639	valid_1's l2: 0.457871
[164]	training's l2: 0.263996	valid_1's l2: 0.457815
[165]	training's l2: 0.263315	valid_1's l2: 0.457609
[166]	training's l2: 0.262498	valid_1's l2: 0.457503
[167]	training's l2: 0.26189	valid_1's l2: 0.457593
[168]	training's l2: 0.261183	valid_1's l2: 0.457598
[169]	training's l2: 0.260465	valid_1's l2: 0.457693
[170]	training's l2: 0.259597	valid_1's l2: 0.45777
[171]	training's l2: 0.258968	valid_1's l2: 0.457702
[172]	training's l2: 0.258294	valid_1's l2: 0.457708
[173]	training's l2: 0.25765	valid_1's l2: 0.457709
[174]	training's l2: 0.256957	valid_1's l2: 0.457788
[175]	training's l2: 0.256338	valid_1's l2: 0.457851
[176]	training's l2: 0.255795	valid_1's l2: 0.457798
[177]	training's l2: 0.255134	valid_1's l2: 0.45781
[178]	training's l2: 0.254412	valid_1's l2: 0.458028
[179]	training's l2: 0.253748	valid_1's l2: 0.457943
Did not meet early stopping. Best iteration is:
[179]	training's l2: 0.253748	valid_1's l2: 0.457943
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.198979 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.744406	valid_1's l2: 0.725593
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.721631	valid_1's l2: 0.707437
[3]	training's l2: 0.701122	valid_1's l2: 0.690596
[4]	training's l2: 0.681877	valid_1's l2: 0.674643
[5]	training's l2: 0.664097	valid_1's l2: 0.660462
[6]	training's l2: 0.647609	valid_1's l2: 0.647532
[7]	training's l2: 0.632856	valid_1's l2: 0.636515
[8]	training's l2: 0.618563	valid_1's l2: 0.625509
[9]	training's l2: 0.606505	valid_1's l2: 0.616707
[10]	training's l2: 0.595075	valid_1's l2: 0.607903
[11]	training's l2: 0.584623	valid_1's l2: 0.599873
[12]	training's l2: 0.57398	valid_1's l2: 0.592417
[13]	training's l2: 0.564465	valid_1's l2: 0.584863
[14]	training's l2: 0.555862	valid_1's l2: 0.578888
[15]	training's l2: 0.547623	valid_1's l2: 0.572764
[16]	training's l2: 0.539338	valid_1's l2: 0.566895
[17]	training's l2: 0.53243	valid_1's l2: 0.562521
[18]	training's l2: 0.524224	valid_1's l2: 0.55615
[19]	training's l2: 0.517474	valid_1's l2: 0.551948
[20]	training's l2: 0.510893	valid_1's l2: 0.547557
[21]	training's l2: 0.504197	valid_1's l2: 0.542538
[22]	training's l2: 0.498728	valid_1's l2: 0.538739
[23]	training's l2: 0.492914	valid_1's l2: 0.534662
[24]	training's l2: 0.487535	valid_1's l2: 0.53156
[25]	training's l2: 0.481316	valid_1's l2: 0.526823
[26]	training's l2: 0.476016	valid_1's l2: 0.523722
[27]	training's l2: 0.471891	valid_1's l2: 0.521663
[28]	training's l2: 0.467783	valid_1's l2: 0.519415
[29]	training's l2: 0.463413	valid_1's l2: 0.517184
[30]	training's l2: 0.45922	valid_1's l2: 0.514785
[31]	training's l2: 0.455919	valid_1's l2: 0.513114
[32]	training's l2: 0.452743	valid_1's l2: 0.512015
[33]	training's l2: 0.449362	valid_1's l2: 0.510632
[34]	training's l2: 0.445759	valid_1's l2: 0.508771
[35]	training's l2: 0.442466	valid_1's l2: 0.507348
[36]	training's l2: 0.438994	valid_1's l2: 0.505575
[37]	training's l2: 0.436134	valid_1's l2: 0.504431
[38]	training's l2: 0.433303	valid_1's l2: 0.503173
[39]	training's l2: 0.430302	valid_1's l2: 0.501729
[40]	training's l2: 0.42801	valid_1's l2: 0.501203
[41]	training's l2: 0.425607	valid_1's l2: 0.499865
[42]	training's l2: 0.423316	valid_1's l2: 0.499067
[43]	training's l2: 0.420922	valid_1's l2: 0.49802
[44]	training's l2: 0.418215	valid_1's l2: 0.497176
[45]	training's l2: 0.415921	valid_1's l2: 0.496525
[46]	training's l2: 0.412939	valid_1's l2: 0.495155
[47]	training's l2: 0.410711	valid_1's l2: 0.493948
[48]	training's l2: 0.407335	valid_1's l2: 0.491677
[49]	training's l2: 0.404803	valid_1's l2: 0.490766
[50]	training's l2: 0.40153	valid_1's l2: 0.48868
[51]	training's l2: 0.399481	valid_1's l2: 0.487748
[52]	training's l2: 0.396833	valid_1's l2: 0.486769
[53]	training's l2: 0.394453	valid_1's l2: 0.485972
[54]	training's l2: 0.392523	valid_1's l2: 0.485332
[55]	training's l2: 0.389669	valid_1's l2: 0.483424
[56]	training's l2: 0.387927	valid_1's l2: 0.482736
[57]	training's l2: 0.386138	valid_1's l2: 0.482088
[58]	training's l2: 0.384225	valid_1's l2: 0.481576
[59]	training's l2: 0.382595	valid_1's l2: 0.481139
[60]	training's l2: 0.381003	valid_1's l2: 0.480582
[61]	training's l2: 0.378404	valid_1's l2: 0.479329
[62]	training's l2: 0.376216	valid_1's l2: 0.478711
[63]	training's l2: 0.374603	valid_1's l2: 0.478389
[64]	training's l2: 0.372998	valid_1's l2: 0.477602
[65]	training's l2: 0.37134	valid_1's l2: 0.476974
[66]	training's l2: 0.369108	valid_1's l2: 0.47604
[67]	training's l2: 0.367692	valid_1's l2: 0.475917
[68]	training's l2: 0.366152	valid_1's l2: 0.475531
[69]	training's l2: 0.364766	valid_1's l2: 0.475279
[70]	training's l2: 0.362776	valid_1's l2: 0.474492
[71]	training's l2: 0.361242	valid_1's l2: 0.473892
[72]	training's l2: 0.359874	valid_1's l2: 0.473563
[73]	training's l2: 0.35849	valid_1's l2: 0.473756
[74]	training's l2: 0.356685	valid_1's l2: 0.472915
[75]	training's l2: 0.355302	valid_1's l2: 0.472776
[76]	training's l2: 0.353905	valid_1's l2: 0.472447
[77]	training's l2: 0.35229	valid_1's l2: 0.471753
[78]	training's l2: 0.350869	valid_1's l2: 0.471193
[79]	training's l2: 0.349479	valid_1's l2: 0.470863
[80]	training's l2: 0.348184	valid_1's l2: 0.47065
[81]	training's l2: 0.346606	valid_1's l2: 0.470129
[82]	training's l2: 0.345442	valid_1's l2: 0.469782
[83]	training's l2: 0.343958	valid_1's l2: 0.469433
[84]	training's l2: 0.342768	valid_1's l2: 0.469394
[85]	training's l2: 0.341609	valid_1's l2: 0.469159
[86]	training's l2: 0.340247	valid_1's l2: 0.468838
[87]	training's l2: 0.338683	valid_1's l2: 0.468201
[88]	training's l2: 0.337531	valid_1's l2: 0.46808
[89]	training's l2: 0.336336	valid_1's l2: 0.467967
[90]	training's l2: 0.335147	valid_1's l2: 0.46782
[91]	training's l2: 0.333414	valid_1's l2: 0.466667
[92]	training's l2: 0.332189	valid_1's l2: 0.466389
[93]	training's l2: 0.330987	valid_1's l2: 0.466079
[94]	training's l2: 0.329852	valid_1's l2: 0.465918
[95]	training's l2: 0.328626	valid_1's l2: 0.46551
[96]	training's l2: 0.32733	valid_1's l2: 0.465138
[97]	training's l2: 0.326115	valid_1's l2: 0.464889
[98]	training's l2: 0.324898	valid_1's l2: 0.464502
[99]	training's l2: 0.32336	valid_1's l2: 0.463637
[100]	training's l2: 0.322166	valid_1's l2: 0.463431
[101]	training's l2: 0.320959	valid_1's l2: 0.463266
[102]	training's l2: 0.319875	valid_1's l2: 0.463363
[103]	training's l2: 0.318788	valid_1's l2: 0.463205
[104]	training's l2: 0.317613	valid_1's l2: 0.463007
[105]	training's l2: 0.316482	valid_1's l2: 0.462847
[106]	training's l2: 0.315305	valid_1's l2: 0.462579
[107]	training's l2: 0.3142	valid_1's l2: 0.462396
[108]	training's l2: 0.313082	valid_1's l2: 0.462403
[109]	training's l2: 0.311827	valid_1's l2: 0.461859
[110]	training's l2: 0.310588	valid_1's l2: 0.46156
[111]	training's l2: 0.309566	valid_1's l2: 0.461421
[112]	training's l2: 0.308446	valid_1's l2: 0.461045
[113]	training's l2: 0.307401	valid_1's l2: 0.460723
[114]	training's l2: 0.306423	valid_1's l2: 0.460461
[115]	training's l2: 0.305387	valid_1's l2: 0.460476
[116]	training's l2: 0.304094	valid_1's l2: 0.459793
[117]	training's l2: 0.303144	valid_1's l2: 0.459825
[118]	training's l2: 0.302203	valid_1's l2: 0.45969
[119]	training's l2: 0.300999	valid_1's l2: 0.459197
[120]	training's l2: 0.30006	valid_1's l2: 0.459217
[121]	training's l2: 0.299144	valid_1's l2: 0.459232
[122]	training's l2: 0.298132	valid_1's l2: 0.458959
[123]	training's l2: 0.297109	valid_1's l2: 0.458821
[124]	training's l2: 0.296136	valid_1's l2: 0.45869
[125]	training's l2: 0.295132	valid_1's l2: 0.458655
[126]	training's l2: 0.294122	valid_1's l2: 0.458411
[127]	training's l2: 0.293261	valid_1's l2: 0.458367
[128]	training's l2: 0.292395	valid_1's l2: 0.458242
[129]	training's l2: 0.291463	valid_1's l2: 0.457908
[130]	training's l2: 0.29043	valid_1's l2: 0.457706
[131]	training's l2: 0.289378	valid_1's l2: 0.457509
[132]	training's l2: 0.288468	valid_1's l2: 0.457833
[133]	training's l2: 0.287618	valid_1's l2: 0.457756
[134]	training's l2: 0.286671	valid_1's l2: 0.457792
[135]	training's l2: 0.285778	valid_1's l2: 0.457678
[136]	training's l2: 0.28494	valid_1's l2: 0.457625
[137]	training's l2: 0.284082	valid_1's l2: 0.457426
[138]	training's l2: 0.282991	valid_1's l2: 0.457025
[139]	training's l2: 0.282035	valid_1's l2: 0.456954
[140]	training's l2: 0.28118	valid_1's l2: 0.457058
[141]	training's l2: 0.280229	valid_1's l2: 0.456988
[142]	training's l2: 0.279369	valid_1's l2: 0.45705
[143]	training's l2: 0.278442	valid_1's l2: 0.45695
[144]	training's l2: 0.277397	valid_1's l2: 0.456637
[145]	training's l2: 0.276486	valid_1's l2: 0.456628
[146]	training's l2: 0.275711	valid_1's l2: 0.456612
[147]	training's l2: 0.274905	valid_1's l2: 0.456629
[148]	training's l2: 0.274162	valid_1's l2: 0.456561
[149]	training's l2: 0.273422	valid_1's l2: 0.45638
[150]	training's l2: 0.272598	valid_1's l2: 0.456354
[151]	training's l2: 0.271764	valid_1's l2: 0.456342
[152]	training's l2: 0.271025	valid_1's l2: 0.456132
[153]	training's l2: 0.270235	valid_1's l2: 0.455968
[154]	training's l2: 0.269318	valid_1's l2: 0.455928
[155]	training's l2: 0.268613	valid_1's l2: 0.455887
[156]	training's l2: 0.267887	valid_1's l2: 0.455784
[157]	training's l2: 0.267093	valid_1's l2: 0.4558
[158]	training's l2: 0.26634	valid_1's l2: 0.455718
[159]	training's l2: 0.265598	valid_1's l2: 0.455832
[160]	training's l2: 0.264846	valid_1's l2: 0.455933
[161]	training's l2: 0.264068	valid_1's l2: 0.456045
[162]	training's l2: 0.263303	valid_1's l2: 0.455967
[163]	training's l2: 0.262557	valid_1's l2: 0.456149
[164]	training's l2: 0.26181	valid_1's l2: 0.456007
[165]	training's l2: 0.260936	valid_1's l2: 0.455813
[166]	training's l2: 0.260285	valid_1's l2: 0.455659
[167]	training's l2: 0.259683	valid_1's l2: 0.455641
[168]	training's l2: 0.258973	valid_1's l2: 0.455744
[169]	training's l2: 0.258361	valid_1's l2: 0.455856
[170]	training's l2: 0.257792	valid_1's l2: 0.455789
[171]	training's l2: 0.257126	valid_1's l2: 0.455755
[172]	training's l2: 0.256398	valid_1's l2: 0.455616
[173]	training's l2: 0.255713	valid_1's l2: 0.45553
[174]	training's l2: 0.255089	valid_1's l2: 0.45542
[175]	training's l2: 0.254337	valid_1's l2: 0.455294
[176]	training's l2: 0.253651	valid_1's l2: 0.455324
[177]	training's l2: 0.253017	valid_1's l2: 0.455417
[178]	training's l2: 0.252341	valid_1's l2: 0.455506
[179]	training's l2: 0.251641	valid_1's l2: 0.455287
[180]	training's l2: 0.250929	valid_1's l2: 0.455272
[181]	training's l2: 0.250192	valid_1's l2: 0.45528
[182]	training's l2: 0.2495	valid_1's l2: 0.455176
[183]	training's l2: 0.248698	valid_1's l2: 0.455397
Did not meet early stopping. Best iteration is:
[183]	training's l2: 0.248698	valid_1's l2: 0.455397
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.187941 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.746666	valid_1's l2: 0.726145
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.725818	valid_1's l2: 0.708095
[3]	training's l2: 0.706082	valid_1's l2: 0.69169
[4]	training's l2: 0.689138	valid_1's l2: 0.67758
[5]	training's l2: 0.673126	valid_1's l2: 0.664408
[6]	training's l2: 0.658591	valid_1's l2: 0.652396
[7]	training's l2: 0.644833	valid_1's l2: 0.641002
[8]	training's l2: 0.632338	valid_1's l2: 0.630878
[9]	training's l2: 0.620705	valid_1's l2: 0.621819
[10]	training's l2: 0.610473	valid_1's l2: 0.613213
[11]	training's l2: 0.601403	valid_1's l2: 0.606537
[12]	training's l2: 0.592623	valid_1's l2: 0.600201
[13]	training's l2: 0.584677	valid_1's l2: 0.593877
[14]	training's l2: 0.576667	valid_1's l2: 0.58839
[15]	training's l2: 0.567972	valid_1's l2: 0.581036
[16]	training's l2: 0.561345	valid_1's l2: 0.576285
[17]	training's l2: 0.555111	valid_1's l2: 0.572282
[18]	training's l2: 0.54752	valid_1's l2: 0.56597
[19]	training's l2: 0.542077	valid_1's l2: 0.562453
[20]	training's l2: 0.536784	valid_1's l2: 0.558393
[21]	training's l2: 0.530798	valid_1's l2: 0.554669
[22]	training's l2: 0.5259	valid_1's l2: 0.550964
[23]	training's l2: 0.519423	valid_1's l2: 0.546217
[24]	training's l2: 0.514274	valid_1's l2: 0.542256
[25]	training's l2: 0.509899	valid_1's l2: 0.538884
[26]	training's l2: 0.506052	valid_1's l2: 0.536425
[27]	training's l2: 0.500793	valid_1's l2: 0.532332
[28]	training's l2: 0.497242	valid_1's l2: 0.530106
[29]	training's l2: 0.493006	valid_1's l2: 0.527672
[30]	training's l2: 0.489882	valid_1's l2: 0.52571
[31]	training's l2: 0.485393	valid_1's l2: 0.522911
[32]	training's l2: 0.482241	valid_1's l2: 0.521025
[33]	training's l2: 0.478412	valid_1's l2: 0.518222
[34]	training's l2: 0.474916	valid_1's l2: 0.515796
[35]	training's l2: 0.472431	valid_1's l2: 0.514183
[36]	training's l2: 0.469679	valid_1's l2: 0.512282
[37]	training's l2: 0.466994	valid_1's l2: 0.510924
[38]	training's l2: 0.463565	valid_1's l2: 0.508479
[39]	training's l2: 0.461207	valid_1's l2: 0.507152
[40]	training's l2: 0.459153	valid_1's l2: 0.506028
[41]	training's l2: 0.456479	valid_1's l2: 0.504323
[42]	training's l2: 0.454459	valid_1's l2: 0.50347
[43]	training's l2: 0.451501	valid_1's l2: 0.501987
[44]	training's l2: 0.448974	valid_1's l2: 0.500332
[45]	training's l2: 0.446807	valid_1's l2: 0.499344
[46]	training's l2: 0.444285	valid_1's l2: 0.497883
[47]	training's l2: 0.442378	valid_1's l2: 0.496973
[48]	training's l2: 0.440256	valid_1's l2: 0.495876
[49]	training's l2: 0.438681	valid_1's l2: 0.495295
[50]	training's l2: 0.43673	valid_1's l2: 0.494333
[51]	training's l2: 0.434933	valid_1's l2: 0.493623
[52]	training's l2: 0.433113	valid_1's l2: 0.49255
[53]	training's l2: 0.430858	valid_1's l2: 0.491072
[54]	training's l2: 0.428988	valid_1's l2: 0.489718
[55]	training's l2: 0.427572	valid_1's l2: 0.489241
[56]	training's l2: 0.425264	valid_1's l2: 0.487757
[57]	training's l2: 0.423893	valid_1's l2: 0.487388
[58]	training's l2: 0.422307	valid_1's l2: 0.486952
[59]	training's l2: 0.4208	valid_1's l2: 0.486433
[60]	training's l2: 0.419002	valid_1's l2: 0.485086
[61]	training's l2: 0.417693	valid_1's l2: 0.484662
[62]	training's l2: 0.415933	valid_1's l2: 0.483577
[63]	training's l2: 0.414194	valid_1's l2: 0.482707
[64]	training's l2: 0.412198	valid_1's l2: 0.481617
[65]	training's l2: 0.410755	valid_1's l2: 0.480788
[66]	training's l2: 0.409361	valid_1's l2: 0.480371
[67]	training's l2: 0.407804	valid_1's l2: 0.479292
[68]	training's l2: 0.406589	valid_1's l2: 0.478808
[69]	training's l2: 0.404955	valid_1's l2: 0.477928
[70]	training's l2: 0.403731	valid_1's l2: 0.477649
[71]	training's l2: 0.401856	valid_1's l2: 0.476763
[72]	training's l2: 0.400652	valid_1's l2: 0.476446
[73]	training's l2: 0.399597	valid_1's l2: 0.476245
[74]	training's l2: 0.398388	valid_1's l2: 0.475741
[75]	training's l2: 0.396677	valid_1's l2: 0.47491
[76]	training's l2: 0.395536	valid_1's l2: 0.474481
[77]	training's l2: 0.394548	valid_1's l2: 0.474267
[78]	training's l2: 0.39354	valid_1's l2: 0.473818
[79]	training's l2: 0.392076	valid_1's l2: 0.473351
[80]	training's l2: 0.39071	valid_1's l2: 0.472542
[81]	training's l2: 0.389586	valid_1's l2: 0.472467
[82]	training's l2: 0.38834	valid_1's l2: 0.472172
[83]	training's l2: 0.387342	valid_1's l2: 0.47195
[84]	training's l2: 0.38633	valid_1's l2: 0.471448
[85]	training's l2: 0.385208	valid_1's l2: 0.470993
[86]	training's l2: 0.3843	valid_1's l2: 0.470883
[87]	training's l2: 0.382829	valid_1's l2: 0.470191
[88]	training's l2: 0.381835	valid_1's l2: 0.469789
[89]	training's l2: 0.38046	valid_1's l2: 0.46895
[90]	training's l2: 0.379426	valid_1's l2: 0.46854
[91]	training's l2: 0.378062	valid_1's l2: 0.46764
[92]	training's l2: 0.377117	valid_1's l2: 0.467489
[93]	training's l2: 0.376147	valid_1's l2: 0.467241
[94]	training's l2: 0.375171	valid_1's l2: 0.466797
[95]	training's l2: 0.373889	valid_1's l2: 0.466234
[96]	training's l2: 0.372966	valid_1's l2: 0.466047
[97]	training's l2: 0.372165	valid_1's l2: 0.465843
[98]	training's l2: 0.370964	valid_1's l2: 0.465135
[99]	training's l2: 0.369743	valid_1's l2: 0.46444
[100]	training's l2: 0.368885	valid_1's l2: 0.46439
[101]	training's l2: 0.368057	valid_1's l2: 0.464198
[102]	training's l2: 0.367092	valid_1's l2: 0.464086
[103]	training's l2: 0.365707	valid_1's l2: 0.463428
[104]	training's l2: 0.364961	valid_1's l2: 0.46329
[105]	training's l2: 0.364162	valid_1's l2: 0.463168
[106]	training's l2: 0.363313	valid_1's l2: 0.462978
[107]	training's l2: 0.36241	valid_1's l2: 0.462554
[108]	training's l2: 0.361455	valid_1's l2: 0.462317
[109]	training's l2: 0.360663	valid_1's l2: 0.462259
[110]	training's l2: 0.359499	valid_1's l2: 0.461624
[111]	training's l2: 0.358473	valid_1's l2: 0.461235
[112]	training's l2: 0.357272	valid_1's l2: 0.460884
[113]	training's l2: 0.356549	valid_1's l2: 0.460866
[114]	training's l2: 0.355728	valid_1's l2: 0.460768
[115]	training's l2: 0.354923	valid_1's l2: 0.460441
[116]	training's l2: 0.354052	valid_1's l2: 0.460039
[117]	training's l2: 0.353315	valid_1's l2: 0.459825
[118]	training's l2: 0.352396	valid_1's l2: 0.459581
[119]	training's l2: 0.351608	valid_1's l2: 0.459474
[120]	training's l2: 0.350699	valid_1's l2: 0.459032
[121]	training's l2: 0.349899	valid_1's l2: 0.458763
[122]	training's l2: 0.349053	valid_1's l2: 0.458536
[123]	training's l2: 0.348385	valid_1's l2: 0.458436
[124]	training's l2: 0.347683	valid_1's l2: 0.458302
[125]	training's l2: 0.346986	valid_1's l2: 0.458081
[126]	training's l2: 0.346166	valid_1's l2: 0.457822
[127]	training's l2: 0.345383	valid_1's l2: 0.457912
[128]	training's l2: 0.344691	valid_1's l2: 0.457795
[129]	training's l2: 0.343629	valid_1's l2: 0.457145
[130]	training's l2: 0.342926	valid_1's l2: 0.45714
[131]	training's l2: 0.342234	valid_1's l2: 0.457026
[132]	training's l2: 0.341511	valid_1's l2: 0.457088
[133]	training's l2: 0.340814	valid_1's l2: 0.456835
[134]	training's l2: 0.340136	valid_1's l2: 0.456771
[135]	training's l2: 0.339481	valid_1's l2: 0.45677
[136]	training's l2: 0.338691	valid_1's l2: 0.45673
[137]	training's l2: 0.337997	valid_1's l2: 0.456771
[138]	training's l2: 0.337225	valid_1's l2: 0.456851
[139]	training's l2: 0.336576	valid_1's l2: 0.456914
[140]	training's l2: 0.335895	valid_1's l2: 0.457004
[141]	training's l2: 0.335157	valid_1's l2: 0.45671
[142]	training's l2: 0.334411	valid_1's l2: 0.456693
[143]	training's l2: 0.33336	valid_1's l2: 0.455912
[144]	training's l2: 0.332526	valid_1's l2: 0.45562
[145]	training's l2: 0.33178	valid_1's l2: 0.455503
[146]	training's l2: 0.331101	valid_1's l2: 0.455667
[147]	training's l2: 0.33037	valid_1's l2: 0.45568
[148]	training's l2: 0.329677	valid_1's l2: 0.45576
[149]	training's l2: 0.329003	valid_1's l2: 0.455646
[150]	training's l2: 0.328314	valid_1's l2: 0.455563
[151]	training's l2: 0.327643	valid_1's l2: 0.45551
[152]	training's l2: 0.327053	valid_1's l2: 0.455671
[153]	training's l2: 0.326428	valid_1's l2: 0.455687
[154]	training's l2: 0.325775	valid_1's l2: 0.455605
[155]	training's l2: 0.324986	valid_1's l2: 0.455482
[156]	training's l2: 0.324405	valid_1's l2: 0.455483
[157]	training's l2: 0.323718	valid_1's l2: 0.455347
[158]	training's l2: 0.322857	valid_1's l2: 0.455189
[159]	training's l2: 0.321994	valid_1's l2: 0.454733
[160]	training's l2: 0.321431	valid_1's l2: 0.454826
[161]	training's l2: 0.320878	valid_1's l2: 0.454883
[162]	training's l2: 0.320184	valid_1's l2: 0.454723
[163]	training's l2: 0.319585	valid_1's l2: 0.454792
[164]	training's l2: 0.318941	valid_1's l2: 0.454749
[165]	training's l2: 0.318376	valid_1's l2: 0.454631
[166]	training's l2: 0.317814	valid_1's l2: 0.454692
[167]	training's l2: 0.31723	valid_1's l2: 0.454671
[168]	training's l2: 0.316677	valid_1's l2: 0.454715
[169]	training's l2: 0.316114	valid_1's l2: 0.454779
[170]	training's l2: 0.31548	valid_1's l2: 0.454659
[171]	training's l2: 0.314827	valid_1's l2: 0.454666
[172]	training's l2: 0.314276	valid_1's l2: 0.454517
[173]	training's l2: 0.313482	valid_1's l2: 0.454101
[174]	training's l2: 0.312898	valid_1's l2: 0.45394
[175]	training's l2: 0.312348	valid_1's l2: 0.454131
[176]	training's l2: 0.311832	valid_1's l2: 0.454106
[177]	training's l2: 0.311286	valid_1's l2: 0.453942
[178]	training's l2: 0.310447	valid_1's l2: 0.453764
[179]	training's l2: 0.309957	valid_1's l2: 0.453764
[180]	training's l2: 0.309364	valid_1's l2: 0.453509
[181]	training's l2: 0.308756	valid_1's l2: 0.453586
[182]	training's l2: 0.308108	valid_1's l2: 0.453476
[183]	training's l2: 0.307579	valid_1's l2: 0.453612
[184]	training's l2: 0.307054	valid_1's l2: 0.453655
[185]	training's l2: 0.306538	valid_1's l2: 0.453704
[186]	training's l2: 0.305963	valid_1's l2: 0.453724
[187]	training's l2: 0.305441	valid_1's l2: 0.453719
[188]	training's l2: 0.304829	valid_1's l2: 0.453505
[189]	training's l2: 0.304175	valid_1's l2: 0.453589
[190]	training's l2: 0.303745	valid_1's l2: 0.453527
[191]	training's l2: 0.303278	valid_1's l2: 0.453599
[192]	training's l2: 0.302724	valid_1's l2: 0.453763
[193]	training's l2: 0.30207	valid_1's l2: 0.45342
[194]	training's l2: 0.301452	valid_1's l2: 0.453081
[195]	training's l2: 0.300954	valid_1's l2: 0.452927
[196]	training's l2: 0.300464	valid_1's l2: 0.452874
[197]	training's l2: 0.299899	valid_1's l2: 0.452985
[198]	training's l2: 0.299385	valid_1's l2: 0.452936
[199]	training's l2: 0.298885	valid_1's l2: 0.45293
[200]	training's l2: 0.298251	valid_1's l2: 0.452916
Did not meet early stopping. Best iteration is:
[200]	training's l2: 0.298251	valid_1's l2: 0.452916
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185256 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.742171	valid_1's l2: 0.722287
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.716833	valid_1's l2: 0.700982
[3]	training's l2: 0.695338	valid_1's l2: 0.682321
[4]	training's l2: 0.67572	valid_1's l2: 0.666217
[5]	training's l2: 0.657295	valid_1's l2: 0.65059
[6]	training's l2: 0.640695	valid_1's l2: 0.636942
[7]	training's l2: 0.626106	valid_1's l2: 0.625295
[8]	training's l2: 0.614164	valid_1's l2: 0.615648
[9]	training's l2: 0.602896	valid_1's l2: 0.606449
[10]	training's l2: 0.592002	valid_1's l2: 0.598356
[11]	training's l2: 0.581858	valid_1's l2: 0.59073
[12]	training's l2: 0.573142	valid_1's l2: 0.584735
[13]	training's l2: 0.565161	valid_1's l2: 0.578372
[14]	training's l2: 0.557499	valid_1's l2: 0.572372
[15]	training's l2: 0.550782	valid_1's l2: 0.567088
[16]	training's l2: 0.541745	valid_1's l2: 0.560055
[17]	training's l2: 0.533003	valid_1's l2: 0.554083
[18]	training's l2: 0.526872	valid_1's l2: 0.549648
[19]	training's l2: 0.521167	valid_1's l2: 0.545204
[20]	training's l2: 0.515547	valid_1's l2: 0.541546
[21]	training's l2: 0.509687	valid_1's l2: 0.536666
[22]	training's l2: 0.504381	valid_1's l2: 0.533388
[23]	training's l2: 0.500265	valid_1's l2: 0.53071
[24]	training's l2: 0.495363	valid_1's l2: 0.527186
[25]	training's l2: 0.490374	valid_1's l2: 0.523624
[26]	training's l2: 0.485922	valid_1's l2: 0.520498
[27]	training's l2: 0.482217	valid_1's l2: 0.518372
[28]	training's l2: 0.479033	valid_1's l2: 0.516825
[29]	training's l2: 0.474926	valid_1's l2: 0.514641
[30]	training's l2: 0.471775	valid_1's l2: 0.51291
[31]	training's l2: 0.46879	valid_1's l2: 0.511147
[32]	training's l2: 0.46612	valid_1's l2: 0.509666
[33]	training's l2: 0.463202	valid_1's l2: 0.508516
[34]	training's l2: 0.460764	valid_1's l2: 0.506999
[35]	training's l2: 0.458673	valid_1's l2: 0.506319
[36]	training's l2: 0.455147	valid_1's l2: 0.5042
[37]	training's l2: 0.453056	valid_1's l2: 0.503252
[38]	training's l2: 0.44867	valid_1's l2: 0.499658
[39]	training's l2: 0.446443	valid_1's l2: 0.49863
[40]	training's l2: 0.444246	valid_1's l2: 0.497582
[41]	training's l2: 0.442222	valid_1's l2: 0.496454
[42]	training's l2: 0.440175	valid_1's l2: 0.495411
[43]	training's l2: 0.438091	valid_1's l2: 0.494578
[44]	training's l2: 0.436137	valid_1's l2: 0.493596
[45]	training's l2: 0.433941	valid_1's l2: 0.492338
[46]	training's l2: 0.432208	valid_1's l2: 0.491639
[47]	training's l2: 0.428685	valid_1's l2: 0.488926
[48]	training's l2: 0.427037	valid_1's l2: 0.488408
[49]	training's l2: 0.424928	valid_1's l2: 0.48701
[50]	training's l2: 0.421972	valid_1's l2: 0.484931
[51]	training's l2: 0.419481	valid_1's l2: 0.482799
[52]	training's l2: 0.417457	valid_1's l2: 0.481629
[53]	training's l2: 0.415888	valid_1's l2: 0.480944
[54]	training's l2: 0.414173	valid_1's l2: 0.480076
[55]	training's l2: 0.411926	valid_1's l2: 0.479347
[56]	training's l2: 0.410229	valid_1's l2: 0.478199
[57]	training's l2: 0.408737	valid_1's l2: 0.477988
[58]	training's l2: 0.407359	valid_1's l2: 0.477859
[59]	training's l2: 0.405805	valid_1's l2: 0.476988
[60]	training's l2: 0.404333	valid_1's l2: 0.476425
[61]	training's l2: 0.403131	valid_1's l2: 0.476177
[62]	training's l2: 0.401902	valid_1's l2: 0.475729
[63]	training's l2: 0.400055	valid_1's l2: 0.474909
[64]	training's l2: 0.398545	valid_1's l2: 0.474384
[65]	training's l2: 0.397422	valid_1's l2: 0.474238
[66]	training's l2: 0.396256	valid_1's l2: 0.473616
[67]	training's l2: 0.395019	valid_1's l2: 0.473224
[68]	training's l2: 0.393386	valid_1's l2: 0.472555
[69]	training's l2: 0.392314	valid_1's l2: 0.472328
[70]	training's l2: 0.391125	valid_1's l2: 0.471997
[71]	training's l2: 0.389892	valid_1's l2: 0.471684
[72]	training's l2: 0.388135	valid_1's l2: 0.470867
[73]	training's l2: 0.386966	valid_1's l2: 0.470423
[74]	training's l2: 0.385798	valid_1's l2: 0.470064
[75]	training's l2: 0.384697	valid_1's l2: 0.469871
[76]	training's l2: 0.383224	valid_1's l2: 0.46924
[77]	training's l2: 0.382104	valid_1's l2: 0.468825
[78]	training's l2: 0.380119	valid_1's l2: 0.46753
[79]	training's l2: 0.379008	valid_1's l2: 0.467044
[80]	training's l2: 0.377998	valid_1's l2: 0.466869
[81]	training's l2: 0.376889	valid_1's l2: 0.466546
[82]	training's l2: 0.375946	valid_1's l2: 0.466569
[83]	training's l2: 0.374965	valid_1's l2: 0.466406
[84]	training's l2: 0.374027	valid_1's l2: 0.466233
[85]	training's l2: 0.372769	valid_1's l2: 0.465753
[86]	training's l2: 0.371341	valid_1's l2: 0.465119
[87]	training's l2: 0.37039	valid_1's l2: 0.465224
[88]	training's l2: 0.369001	valid_1's l2: 0.464605
[89]	training's l2: 0.36797	valid_1's l2: 0.464209
[90]	training's l2: 0.366896	valid_1's l2: 0.463822
[91]	training's l2: 0.366001	valid_1's l2: 0.4635
[92]	training's l2: 0.364909	valid_1's l2: 0.463135
[93]	training's l2: 0.363772	valid_1's l2: 0.462881
[94]	training's l2: 0.362856	valid_1's l2: 0.462567
[95]	training's l2: 0.361703	valid_1's l2: 0.46229
[96]	training's l2: 0.360838	valid_1's l2: 0.462121
[97]	training's l2: 0.359876	valid_1's l2: 0.462084
[98]	training's l2: 0.359047	valid_1's l2: 0.461998
[99]	training's l2: 0.358171	valid_1's l2: 0.461846
[100]	training's l2: 0.357343	valid_1's l2: 0.461735
[101]	training's l2: 0.356428	valid_1's l2: 0.461615
[102]	training's l2: 0.355507	valid_1's l2: 0.461288
[103]	training's l2: 0.354397	valid_1's l2: 0.461112
[104]	training's l2: 0.353084	valid_1's l2: 0.460493
[105]	training's l2: 0.352308	valid_1's l2: 0.460466
[106]	training's l2: 0.3515	valid_1's l2: 0.460307
[107]	training's l2: 0.350524	valid_1's l2: 0.459849
[108]	training's l2: 0.349367	valid_1's l2: 0.459538
[109]	training's l2: 0.348421	valid_1's l2: 0.459384
[110]	training's l2: 0.347634	valid_1's l2: 0.459189
[111]	training's l2: 0.346776	valid_1's l2: 0.459216
[112]	training's l2: 0.346029	valid_1's l2: 0.459149
[113]	training's l2: 0.345239	valid_1's l2: 0.459055
[114]	training's l2: 0.344433	valid_1's l2: 0.458995
[115]	training's l2: 0.343662	valid_1's l2: 0.458969
[116]	training's l2: 0.342767	valid_1's l2: 0.458856
[117]	training's l2: 0.342031	valid_1's l2: 0.458862
[118]	training's l2: 0.340962	valid_1's l2: 0.458525
[119]	training's l2: 0.340182	valid_1's l2: 0.458573
[120]	training's l2: 0.339322	valid_1's l2: 0.458594
[121]	training's l2: 0.338557	valid_1's l2: 0.45866
[122]	training's l2: 0.337789	valid_1's l2: 0.458753
[123]	training's l2: 0.336997	valid_1's l2: 0.458553
[124]	training's l2: 0.336327	valid_1's l2: 0.458671
[125]	training's l2: 0.335616	valid_1's l2: 0.458708
[126]	training's l2: 0.334914	valid_1's l2: 0.458639
[127]	training's l2: 0.334107	valid_1's l2: 0.458612
[128]	training's l2: 0.333195	valid_1's l2: 0.458529
[129]	training's l2: 0.332409	valid_1's l2: 0.458313
[130]	training's l2: 0.331774	valid_1's l2: 0.4583
[131]	training's l2: 0.331094	valid_1's l2: 0.458353
[132]	training's l2: 0.330214	valid_1's l2: 0.458052
[133]	training's l2: 0.329503	valid_1's l2: 0.457994
[134]	training's l2: 0.328448	valid_1's l2: 0.457569
[135]	training's l2: 0.327761	valid_1's l2: 0.45758
[136]	training's l2: 0.327076	valid_1's l2: 0.457354
[137]	training's l2: 0.326423	valid_1's l2: 0.457231
[138]	training's l2: 0.325724	valid_1's l2: 0.457309
[139]	training's l2: 0.325081	valid_1's l2: 0.457185
[140]	training's l2: 0.324379	valid_1's l2: 0.457186
[141]	training's l2: 0.323772	valid_1's l2: 0.457266
[142]	training's l2: 0.323135	valid_1's l2: 0.457197
[143]	training's l2: 0.322471	valid_1's l2: 0.457134
[144]	training's l2: 0.3218	valid_1's l2: 0.45702
[145]	training's l2: 0.321119	valid_1's l2: 0.456968
[146]	training's l2: 0.320373	valid_1's l2: 0.456841
[147]	training's l2: 0.319687	valid_1's l2: 0.456686
[148]	training's l2: 0.319067	valid_1's l2: 0.456755
[149]	training's l2: 0.318311	valid_1's l2: 0.456672
[150]	training's l2: 0.317723	valid_1's l2: 0.456675
[151]	training's l2: 0.317106	valid_1's l2: 0.456664
[152]	training's l2: 0.316527	valid_1's l2: 0.456651
[153]	training's l2: 0.315813	valid_1's l2: 0.456637
[154]	training's l2: 0.315222	valid_1's l2: 0.456508
[155]	training's l2: 0.3146	valid_1's l2: 0.456478
[156]	training's l2: 0.314076	valid_1's l2: 0.456461
[157]	training's l2: 0.313241	valid_1's l2: 0.456492
[158]	training's l2: 0.312579	valid_1's l2: 0.456446
[159]	training's l2: 0.311957	valid_1's l2: 0.456595
[160]	training's l2: 0.311395	valid_1's l2: 0.456597
[161]	training's l2: 0.31081	valid_1's l2: 0.456506
[162]	training's l2: 0.310239	valid_1's l2: 0.456379
[163]	training's l2: 0.309647	valid_1's l2: 0.456483
[164]	training's l2: 0.309094	valid_1's l2: 0.45636
[165]	training's l2: 0.308515	valid_1's l2: 0.45633
[166]	training's l2: 0.307932	valid_1's l2: 0.456573
[167]	training's l2: 0.307334	valid_1's l2: 0.456504
[168]	training's l2: 0.306687	valid_1's l2: 0.456628
[169]	training's l2: 0.306119	valid_1's l2: 0.456763
[170]	training's l2: 0.305463	valid_1's l2: 0.456736
[171]	training's l2: 0.304912	valid_1's l2: 0.456769
[172]	training's l2: 0.304287	valid_1's l2: 0.456428
[173]	training's l2: 0.30374	valid_1's l2: 0.456485
[174]	training's l2: 0.3029	valid_1's l2: 0.456179
[175]	training's l2: 0.302288	valid_1's l2: 0.456135
[176]	training's l2: 0.30177	valid_1's l2: 0.456179
[177]	training's l2: 0.301087	valid_1's l2: 0.456154
[178]	training's l2: 0.300613	valid_1's l2: 0.456234
[179]	training's l2: 0.300049	valid_1's l2: 0.456176
[180]	training's l2: 0.2994	valid_1's l2: 0.455932
[181]	training's l2: 0.298863	valid_1's l2: 0.455949
[182]	training's l2: 0.298354	valid_1's l2: 0.455989
[183]	training's l2: 0.297907	valid_1's l2: 0.45599
[184]	training's l2: 0.297289	valid_1's l2: 0.455938
Did not meet early stopping. Best iteration is:
[184]	training's l2: 0.297289	valid_1's l2: 0.455938
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188321 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.733947	valid_1's l2: 0.716055
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.702764	valid_1's l2: 0.69015
[3]	training's l2: 0.676105	valid_1's l2: 0.667874
[4]	training's l2: 0.652423	valid_1's l2: 0.648807
[5]	training's l2: 0.631108	valid_1's l2: 0.6325
[6]	training's l2: 0.61301	valid_1's l2: 0.618394
[7]	training's l2: 0.595916	valid_1's l2: 0.60514
[8]	training's l2: 0.581976	valid_1's l2: 0.594607
[9]	training's l2: 0.568458	valid_1's l2: 0.584271
[10]	training's l2: 0.556178	valid_1's l2: 0.575844
[11]	training's l2: 0.545554	valid_1's l2: 0.568063
[12]	training's l2: 0.535153	valid_1's l2: 0.560361
[13]	training's l2: 0.524132	valid_1's l2: 0.55235
[14]	training's l2: 0.515642	valid_1's l2: 0.546913
[15]	training's l2: 0.506088	valid_1's l2: 0.539847
[16]	training's l2: 0.498107	valid_1's l2: 0.535199
[17]	training's l2: 0.489075	valid_1's l2: 0.52836
[18]	training's l2: 0.482911	valid_1's l2: 0.525158
[19]	training's l2: 0.476635	valid_1's l2: 0.521143
[20]	training's l2: 0.470393	valid_1's l2: 0.517672
[21]	training's l2: 0.4657	valid_1's l2: 0.515558
[22]	training's l2: 0.461226	valid_1's l2: 0.512759
[23]	training's l2: 0.456607	valid_1's l2: 0.510542
[24]	training's l2: 0.452336	valid_1's l2: 0.508565
[25]	training's l2: 0.448107	valid_1's l2: 0.506856
[26]	training's l2: 0.44324	valid_1's l2: 0.503265
[27]	training's l2: 0.439229	valid_1's l2: 0.501572
[28]	training's l2: 0.434622	valid_1's l2: 0.499151
[29]	training's l2: 0.431248	valid_1's l2: 0.497744
[30]	training's l2: 0.428134	valid_1's l2: 0.496694
[31]	training's l2: 0.425037	valid_1's l2: 0.495338
[32]	training's l2: 0.420399	valid_1's l2: 0.492454
[33]	training's l2: 0.416602	valid_1's l2: 0.490623
[34]	training's l2: 0.41368	valid_1's l2: 0.489231
[35]	training's l2: 0.409171	valid_1's l2: 0.486475
[36]	training's l2: 0.406135	valid_1's l2: 0.485041
[37]	training's l2: 0.403547	valid_1's l2: 0.483904
[38]	training's l2: 0.400434	valid_1's l2: 0.482337
[39]	training's l2: 0.397993	valid_1's l2: 0.48138
[40]	training's l2: 0.394293	valid_1's l2: 0.479039
[41]	training's l2: 0.391101	valid_1's l2: 0.478033
[42]	training's l2: 0.388905	valid_1's l2: 0.477407
[43]	training's l2: 0.386466	valid_1's l2: 0.477223
[44]	training's l2: 0.38416	valid_1's l2: 0.476505
[45]	training's l2: 0.382039	valid_1's l2: 0.475973
[46]	training's l2: 0.378978	valid_1's l2: 0.474858
[47]	training's l2: 0.376911	valid_1's l2: 0.474488
[48]	training's l2: 0.374871	valid_1's l2: 0.474573
[49]	training's l2: 0.372878	valid_1's l2: 0.473958
[50]	training's l2: 0.371019	valid_1's l2: 0.473729
[51]	training's l2: 0.368364	valid_1's l2: 0.472208
[52]	training's l2: 0.366481	valid_1's l2: 0.472009
[53]	training's l2: 0.364596	valid_1's l2: 0.471672
[54]	training's l2: 0.361997	valid_1's l2: 0.47032
[55]	training's l2: 0.360301	valid_1's l2: 0.469974
[56]	training's l2: 0.358479	valid_1's l2: 0.470016
[57]	training's l2: 0.356615	valid_1's l2: 0.469955
[58]	training's l2: 0.354806	valid_1's l2: 0.469179
[59]	training's l2: 0.352451	valid_1's l2: 0.468534
[60]	training's l2: 0.350799	valid_1's l2: 0.468373
[61]	training's l2: 0.349218	valid_1's l2: 0.468111
[62]	training's l2: 0.347661	valid_1's l2: 0.468099
[63]	training's l2: 0.345987	valid_1's l2: 0.467692
[64]	training's l2: 0.344318	valid_1's l2: 0.467335
[65]	training's l2: 0.342122	valid_1's l2: 0.466774
[66]	training's l2: 0.340634	valid_1's l2: 0.466466
[67]	training's l2: 0.339123	valid_1's l2: 0.466191
[68]	training's l2: 0.337687	valid_1's l2: 0.466085
[69]	training's l2: 0.33614	valid_1's l2: 0.465982
[70]	training's l2: 0.334536	valid_1's l2: 0.465568
[71]	training's l2: 0.332818	valid_1's l2: 0.465202
[72]	training's l2: 0.331339	valid_1's l2: 0.465123
[73]	training's l2: 0.32992	valid_1's l2: 0.464681
[74]	training's l2: 0.328309	valid_1's l2: 0.464112
[75]	training's l2: 0.326826	valid_1's l2: 0.463771
[76]	training's l2: 0.325376	valid_1's l2: 0.463233
[77]	training's l2: 0.323793	valid_1's l2: 0.462794
[78]	training's l2: 0.322538	valid_1's l2: 0.462713
[79]	training's l2: 0.321154	valid_1's l2: 0.462373
[80]	training's l2: 0.319308	valid_1's l2: 0.461903
[81]	training's l2: 0.317968	valid_1's l2: 0.46157
[82]	training's l2: 0.316518	valid_1's l2: 0.461718
[83]	training's l2: 0.314892	valid_1's l2: 0.461265
[84]	training's l2: 0.313658	valid_1's l2: 0.461201
[85]	training's l2: 0.312351	valid_1's l2: 0.460988
[86]	training's l2: 0.31084	valid_1's l2: 0.460743
[87]	training's l2: 0.309674	valid_1's l2: 0.461061
[88]	training's l2: 0.308339	valid_1's l2: 0.461009
[89]	training's l2: 0.30707	valid_1's l2: 0.460862
[90]	training's l2: 0.30537	valid_1's l2: 0.460713
[91]	training's l2: 0.304176	valid_1's l2: 0.460906
[92]	training's l2: 0.302939	valid_1's l2: 0.460982
[93]	training's l2: 0.30151	valid_1's l2: 0.460892
[94]	training's l2: 0.300383	valid_1's l2: 0.460911
[95]	training's l2: 0.299131	valid_1's l2: 0.460886
[96]	training's l2: 0.298052	valid_1's l2: 0.460933
[97]	training's l2: 0.296908	valid_1's l2: 0.460869
[98]	training's l2: 0.29573	valid_1's l2: 0.461124
[99]	training's l2: 0.294651	valid_1's l2: 0.461264
[100]	training's l2: 0.293447	valid_1's l2: 0.461194
[101]	training's l2: 0.29194	valid_1's l2: 0.460709
[102]	training's l2: 0.290879	valid_1's l2: 0.460877
[103]	training's l2: 0.289667	valid_1's l2: 0.460833
[104]	training's l2: 0.288531	valid_1's l2: 0.460852
[105]	training's l2: 0.287205	valid_1's l2: 0.460628
[106]	training's l2: 0.286051	valid_1's l2: 0.460536
[107]	training's l2: 0.284715	valid_1's l2: 0.460115
[108]	training's l2: 0.283593	valid_1's l2: 0.460139
[109]	training's l2: 0.282601	valid_1's l2: 0.460085
[110]	training's l2: 0.281633	valid_1's l2: 0.460364
[111]	training's l2: 0.280512	valid_1's l2: 0.46049
[112]	training's l2: 0.279583	valid_1's l2: 0.460479
[113]	training's l2: 0.278686	valid_1's l2: 0.460641
[114]	training's l2: 0.277577	valid_1's l2: 0.460639
[115]	training's l2: 0.276449	valid_1's l2: 0.460655
[116]	training's l2: 0.275527	valid_1's l2: 0.460842
[117]	training's l2: 0.274391	valid_1's l2: 0.4609
[118]	training's l2: 0.273394	valid_1's l2: 0.460874
[119]	training's l2: 0.272433	valid_1's l2: 0.460978
[120]	training's l2: 0.271645	valid_1's l2: 0.461051
[121]	training's l2: 0.270654	valid_1's l2: 0.460938
[122]	training's l2: 0.2694	valid_1's l2: 0.46046
[123]	training's l2: 0.268261	valid_1's l2: 0.460132
[124]	training's l2: 0.26728	valid_1's l2: 0.460172
[125]	training's l2: 0.266365	valid_1's l2: 0.460077
[126]	training's l2: 0.265167	valid_1's l2: 0.459586
[127]	training's l2: 0.264274	valid_1's l2: 0.459403
[128]	training's l2: 0.263369	valid_1's l2: 0.459259
[129]	training's l2: 0.262606	valid_1's l2: 0.459282
[130]	training's l2: 0.261707	valid_1's l2: 0.459397
[131]	training's l2: 0.260806	valid_1's l2: 0.45933
[132]	training's l2: 0.259908	valid_1's l2: 0.459392
[133]	training's l2: 0.259015	valid_1's l2: 0.459367
[134]	training's l2: 0.258162	valid_1's l2: 0.459296
[135]	training's l2: 0.257431	valid_1's l2: 0.45934
[136]	training's l2: 0.256603	valid_1's l2: 0.459286
[137]	training's l2: 0.25573	valid_1's l2: 0.459403
[138]	training's l2: 0.254828	valid_1's l2: 0.459287
[139]	training's l2: 0.25402	valid_1's l2: 0.459119
[140]	training's l2: 0.253027	valid_1's l2: 0.45874
[141]	training's l2: 0.252091	valid_1's l2: 0.45852
[142]	training's l2: 0.251219	valid_1's l2: 0.458337
[143]	training's l2: 0.250407	valid_1's l2: 0.458263
[144]	training's l2: 0.249514	valid_1's l2: 0.458242
[145]	training's l2: 0.248623	valid_1's l2: 0.458118
[146]	training's l2: 0.247599	valid_1's l2: 0.457753
[147]	training's l2: 0.246797	valid_1's l2: 0.45768
[148]	training's l2: 0.24596	valid_1's l2: 0.457641
[149]	training's l2: 0.245122	valid_1's l2: 0.457833
[150]	training's l2: 0.244351	valid_1's l2: 0.457803
[151]	training's l2: 0.243594	valid_1's l2: 0.457765
[152]	training's l2: 0.242846	valid_1's l2: 0.457544
[153]	training's l2: 0.242062	valid_1's l2: 0.457482
[154]	training's l2: 0.241324	valid_1's l2: 0.457464
[155]	training's l2: 0.240552	valid_1's l2: 0.457479
[156]	training's l2: 0.239804	valid_1's l2: 0.457398
[157]	training's l2: 0.239061	valid_1's l2: 0.457371
[158]	training's l2: 0.238273	valid_1's l2: 0.457365
[159]	training's l2: 0.237505	valid_1's l2: 0.45725
[160]	training's l2: 0.236775	valid_1's l2: 0.457083
[161]	training's l2: 0.236123	valid_1's l2: 0.457129
[162]	training's l2: 0.235401	valid_1's l2: 0.45692
[163]	training's l2: 0.234759	valid_1's l2: 0.456763
[164]	training's l2: 0.233908	valid_1's l2: 0.45671
[165]	training's l2: 0.233271	valid_1's l2: 0.456668
[166]	training's l2: 0.2326	valid_1's l2: 0.456573
[167]	training's l2: 0.231851	valid_1's l2: 0.456431
[168]	training's l2: 0.231251	valid_1's l2: 0.456468
[169]	training's l2: 0.230559	valid_1's l2: 0.456553
[170]	training's l2: 0.229962	valid_1's l2: 0.456574
[171]	training's l2: 0.229307	valid_1's l2: 0.456536
[172]	training's l2: 0.22853	valid_1's l2: 0.456424
[173]	training's l2: 0.227738	valid_1's l2: 0.4565
[174]	training's l2: 0.226832	valid_1's l2: 0.456429
[175]	training's l2: 0.226113	valid_1's l2: 0.456557
[176]	training's l2: 0.225368	valid_1's l2: 0.456594
[177]	training's l2: 0.224632	valid_1's l2: 0.456514
[178]	training's l2: 0.224002	valid_1's l2: 0.456388
[179]	training's l2: 0.223332	valid_1's l2: 0.456256
[180]	training's l2: 0.222509	valid_1's l2: 0.456334
[181]	training's l2: 0.221855	valid_1's l2: 0.456325
[182]	training's l2: 0.221087	valid_1's l2: 0.456293
[183]	training's l2: 0.220538	valid_1's l2: 0.456471
[184]	training's l2: 0.219796	valid_1's l2: 0.456339
[185]	training's l2: 0.219205	valid_1's l2: 0.456149
[186]	training's l2: 0.218559	valid_1's l2: 0.45592
[187]	training's l2: 0.217844	valid_1's l2: 0.456002
[188]	training's l2: 0.217015	valid_1's l2: 0.456009
[189]	training's l2: 0.216404	valid_1's l2: 0.456058
[190]	training's l2: 0.215765	valid_1's l2: 0.456017
[191]	training's l2: 0.215181	valid_1's l2: 0.456057
[192]	training's l2: 0.214338	valid_1's l2: 0.456353
[193]	training's l2: 0.213735	valid_1's l2: 0.456505
[194]	training's l2: 0.213154	valid_1's l2: 0.456694
[195]	training's l2: 0.212513	valid_1's l2: 0.456681
[196]	training's l2: 0.211719	valid_1's l2: 0.456739
[197]	training's l2: 0.211047	valid_1's l2: 0.456572
[198]	training's l2: 0.210482	valid_1's l2: 0.456567
[199]	training's l2: 0.209857	valid_1's l2: 0.456561
[200]	training's l2: 0.20911	valid_1's l2: 0.45651
Did not meet early stopping. Best iteration is:
[200]	training's l2: 0.20911	valid_1's l2: 0.45651
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177915 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.741891	valid_1's l2: 0.722964
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.717405	valid_1's l2: 0.702325
[3]	training's l2: 0.694719	valid_1's l2: 0.684136
[4]	training's l2: 0.674386	valid_1's l2: 0.667487
[5]	training's l2: 0.656249	valid_1's l2: 0.652862
[6]	training's l2: 0.639877	valid_1's l2: 0.640098
[7]	training's l2: 0.62567	valid_1's l2: 0.629082
[8]	training's l2: 0.611303	valid_1's l2: 0.617937
[9]	training's l2: 0.599577	valid_1's l2: 0.608943
[10]	training's l2: 0.587747	valid_1's l2: 0.599915
[11]	training's l2: 0.577657	valid_1's l2: 0.592246
[12]	training's l2: 0.568551	valid_1's l2: 0.585677
[13]	training's l2: 0.559375	valid_1's l2: 0.578635
[14]	training's l2: 0.551243	valid_1's l2: 0.572513
[15]	training's l2: 0.541799	valid_1's l2: 0.56525
[16]	training's l2: 0.535162	valid_1's l2: 0.560728
[17]	training's l2: 0.52669	valid_1's l2: 0.554584
[18]	training's l2: 0.520405	valid_1's l2: 0.5496
[19]	training's l2: 0.514715	valid_1's l2: 0.545917
[20]	training's l2: 0.508561	valid_1's l2: 0.541949
[21]	training's l2: 0.501232	valid_1's l2: 0.536291
[22]	training's l2: 0.496074	valid_1's l2: 0.533215
[23]	training's l2: 0.490211	valid_1's l2: 0.529227
[24]	training's l2: 0.484339	valid_1's l2: 0.52557
[25]	training's l2: 0.480168	valid_1's l2: 0.523153
[26]	training's l2: 0.475422	valid_1's l2: 0.520069
[27]	training's l2: 0.471346	valid_1's l2: 0.517642
[28]	training's l2: 0.467574	valid_1's l2: 0.515243
[29]	training's l2: 0.464383	valid_1's l2: 0.513848
[30]	training's l2: 0.460414	valid_1's l2: 0.511429
[31]	training's l2: 0.456642	valid_1's l2: 0.509505
[32]	training's l2: 0.453576	valid_1's l2: 0.508255
[33]	training's l2: 0.450346	valid_1's l2: 0.506536
[34]	training's l2: 0.447259	valid_1's l2: 0.50532
[35]	training's l2: 0.444613	valid_1's l2: 0.503989
[36]	training's l2: 0.44168	valid_1's l2: 0.502933
[37]	training's l2: 0.438387	valid_1's l2: 0.500901
[38]	training's l2: 0.43529	valid_1's l2: 0.498636
[39]	training's l2: 0.431944	valid_1's l2: 0.496316
[40]	training's l2: 0.43002	valid_1's l2: 0.495808
[41]	training's l2: 0.427511	valid_1's l2: 0.49478
[42]	training's l2: 0.423811	valid_1's l2: 0.492179
[43]	training's l2: 0.421705	valid_1's l2: 0.491451
[44]	training's l2: 0.418661	valid_1's l2: 0.489389
[45]	training's l2: 0.416567	valid_1's l2: 0.488314
[46]	training's l2: 0.414073	valid_1's l2: 0.487023
[47]	training's l2: 0.412252	valid_1's l2: 0.486436
[48]	training's l2: 0.410462	valid_1's l2: 0.485997
[49]	training's l2: 0.407883	valid_1's l2: 0.484621
[50]	training's l2: 0.405941	valid_1's l2: 0.483825
[51]	training's l2: 0.403856	valid_1's l2: 0.483033
[52]	training's l2: 0.402031	valid_1's l2: 0.482458
[53]	training's l2: 0.399864	valid_1's l2: 0.48115
[54]	training's l2: 0.397624	valid_1's l2: 0.480173
[55]	training's l2: 0.39607	valid_1's l2: 0.479916
[56]	training's l2: 0.393522	valid_1's l2: 0.478522
[57]	training's l2: 0.39126	valid_1's l2: 0.477282
[58]	training's l2: 0.389565	valid_1's l2: 0.4763
[59]	training's l2: 0.387726	valid_1's l2: 0.475553
[60]	training's l2: 0.386202	valid_1's l2: 0.475118
[61]	training's l2: 0.384005	valid_1's l2: 0.474023
[62]	training's l2: 0.382516	valid_1's l2: 0.473696
[63]	training's l2: 0.381051	valid_1's l2: 0.473331
[64]	training's l2: 0.379137	valid_1's l2: 0.471947
[65]	training's l2: 0.377516	valid_1's l2: 0.471458
[66]	training's l2: 0.376143	valid_1's l2: 0.471447
[67]	training's l2: 0.374172	valid_1's l2: 0.47058
[68]	training's l2: 0.372882	valid_1's l2: 0.470442
[69]	training's l2: 0.3716	valid_1's l2: 0.47031
[70]	training's l2: 0.370142	valid_1's l2: 0.469431
[71]	training's l2: 0.368988	valid_1's l2: 0.469264
[72]	training's l2: 0.367625	valid_1's l2: 0.468925
[73]	training's l2: 0.365836	valid_1's l2: 0.468237
[74]	training's l2: 0.364586	valid_1's l2: 0.468138
[75]	training's l2: 0.36337	valid_1's l2: 0.468027
[76]	training's l2: 0.362173	valid_1's l2: 0.46796
[77]	training's l2: 0.360849	valid_1's l2: 0.467583
[78]	training's l2: 0.35959	valid_1's l2: 0.467366
[79]	training's l2: 0.358011	valid_1's l2: 0.466764
[80]	training's l2: 0.356818	valid_1's l2: 0.466761
[81]	training's l2: 0.355595	valid_1's l2: 0.466435
[82]	training's l2: 0.354492	valid_1's l2: 0.466281
[83]	training's l2: 0.35336	valid_1's l2: 0.465788
[84]	training's l2: 0.352096	valid_1's l2: 0.465593
[85]	training's l2: 0.350275	valid_1's l2: 0.464388
[86]	training's l2: 0.348916	valid_1's l2: 0.463862
[87]	training's l2: 0.347799	valid_1's l2: 0.463654
[88]	training's l2: 0.346658	valid_1's l2: 0.463358
[89]	training's l2: 0.345449	valid_1's l2: 0.463085
[90]	training's l2: 0.344333	valid_1's l2: 0.463018
[91]	training's l2: 0.343334	valid_1's l2: 0.463036
[92]	training's l2: 0.34202	valid_1's l2: 0.462645
[93]	training's l2: 0.340696	valid_1's l2: 0.462323
[94]	training's l2: 0.339505	valid_1's l2: 0.462202
[95]	training's l2: 0.338471	valid_1's l2: 0.462345
[96]	training's l2: 0.337435	valid_1's l2: 0.462396
[97]	training's l2: 0.336381	valid_1's l2: 0.461897
[98]	training's l2: 0.335292	valid_1's l2: 0.461632
[99]	training's l2: 0.33428	valid_1's l2: 0.461195
[100]	training's l2: 0.333196	valid_1's l2: 0.460998
[101]	training's l2: 0.332161	valid_1's l2: 0.460804
[102]	training's l2: 0.331132	valid_1's l2: 0.460811
[103]	training's l2: 0.330183	valid_1's l2: 0.460791
[104]	training's l2: 0.329275	valid_1's l2: 0.460835
[105]	training's l2: 0.328293	valid_1's l2: 0.460803
[106]	training's l2: 0.327082	valid_1's l2: 0.460604
[107]	training's l2: 0.326093	valid_1's l2: 0.460474
[108]	training's l2: 0.324936	valid_1's l2: 0.460206
[109]	training's l2: 0.323981	valid_1's l2: 0.460174
[110]	training's l2: 0.323069	valid_1's l2: 0.459764
[111]	training's l2: 0.322057	valid_1's l2: 0.459334
[112]	training's l2: 0.320854	valid_1's l2: 0.458831
[113]	training's l2: 0.319969	valid_1's l2: 0.458751
[114]	training's l2: 0.318992	valid_1's l2: 0.458519
[115]	training's l2: 0.31803	valid_1's l2: 0.45857
[116]	training's l2: 0.317215	valid_1's l2: 0.458544
[117]	training's l2: 0.316252	valid_1's l2: 0.458179
[118]	training's l2: 0.31542	valid_1's l2: 0.458115
[119]	training's l2: 0.314585	valid_1's l2: 0.457903
[120]	training's l2: 0.313671	valid_1's l2: 0.457829
[121]	training's l2: 0.312708	valid_1's l2: 0.457922
[122]	training's l2: 0.31174	valid_1's l2: 0.457753
[123]	training's l2: 0.310883	valid_1's l2: 0.457875
[124]	training's l2: 0.310077	valid_1's l2: 0.457874
[125]	training's l2: 0.3093	valid_1's l2: 0.457817
[126]	training's l2: 0.308438	valid_1's l2: 0.457825
[127]	training's l2: 0.307552	valid_1's l2: 0.457704
[128]	training's l2: 0.306678	valid_1's l2: 0.457481
[129]	training's l2: 0.305667	valid_1's l2: 0.457647
[130]	training's l2: 0.304638	valid_1's l2: 0.457566
[131]	training's l2: 0.303498	valid_1's l2: 0.456987
[132]	training's l2: 0.302597	valid_1's l2: 0.456619
[133]	training's l2: 0.301496	valid_1's l2: 0.456045
[134]	training's l2: 0.300606	valid_1's l2: 0.455974
[135]	training's l2: 0.299854	valid_1's l2: 0.455947
[136]	training's l2: 0.299132	valid_1's l2: 0.455911
[137]	training's l2: 0.298313	valid_1's l2: 0.455889
[138]	training's l2: 0.297559	valid_1's l2: 0.455894
[139]	training's l2: 0.296793	valid_1's l2: 0.456193
[140]	training's l2: 0.295924	valid_1's l2: 0.455994
[141]	training's l2: 0.295211	valid_1's l2: 0.455936
[142]	training's l2: 0.29445	valid_1's l2: 0.455904
[143]	training's l2: 0.293751	valid_1's l2: 0.455939
[144]	training's l2: 0.292992	valid_1's l2: 0.455963
[145]	training's l2: 0.292252	valid_1's l2: 0.455766
[146]	training's l2: 0.291521	valid_1's l2: 0.455863
[147]	training's l2: 0.290838	valid_1's l2: 0.455845
[148]	training's l2: 0.290127	valid_1's l2: 0.455979
[149]	training's l2: 0.289361	valid_1's l2: 0.455674
[150]	training's l2: 0.288315	valid_1's l2: 0.45523
[151]	training's l2: 0.287593	valid_1's l2: 0.455307
[152]	training's l2: 0.286988	valid_1's l2: 0.45541
[153]	training's l2: 0.286252	valid_1's l2: 0.455187
[154]	training's l2: 0.285491	valid_1's l2: 0.455284
[155]	training's l2: 0.284852	valid_1's l2: 0.455183
[156]	training's l2: 0.284147	valid_1's l2: 0.455159
[157]	training's l2: 0.283501	valid_1's l2: 0.455206
[158]	training's l2: 0.282798	valid_1's l2: 0.455191
[159]	training's l2: 0.282175	valid_1's l2: 0.45508
[160]	training's l2: 0.281337	valid_1's l2: 0.454826
[161]	training's l2: 0.28069	valid_1's l2: 0.454757
[162]	training's l2: 0.280022	valid_1's l2: 0.454974
[163]	training's l2: 0.27936	valid_1's l2: 0.454971
[164]	training's l2: 0.278622	valid_1's l2: 0.454974
[165]	training's l2: 0.278045	valid_1's l2: 0.454906
[166]	training's l2: 0.277422	valid_1's l2: 0.454952
[167]	training's l2: 0.276739	valid_1's l2: 0.454971
[168]	training's l2: 0.276067	valid_1's l2: 0.454956
[169]	training's l2: 0.275484	valid_1's l2: 0.455011
[170]	training's l2: 0.274884	valid_1's l2: 0.454985
[171]	training's l2: 0.274076	valid_1's l2: 0.4549
[172]	training's l2: 0.273323	valid_1's l2: 0.455029
[173]	training's l2: 0.272646	valid_1's l2: 0.454988
[174]	training's l2: 0.272034	valid_1's l2: 0.454977
[175]	training's l2: 0.271315	valid_1's l2: 0.454875
[176]	training's l2: 0.270657	valid_1's l2: 0.454866
[177]	training's l2: 0.270045	valid_1's l2: 0.454711
[178]	training's l2: 0.269496	valid_1's l2: 0.454681
[179]	training's l2: 0.268869	valid_1's l2: 0.454698
Did not meet early stopping. Best iteration is:
[179]	training's l2: 0.268869	valid_1's l2: 0.454698
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189188 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.734311	valid_1's l2: 0.716314
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.703337	valid_1's l2: 0.690593
[3]	training's l2: 0.67706	valid_1's l2: 0.668293
[4]	training's l2: 0.653366	valid_1's l2: 0.649472
[5]	training's l2: 0.632318	valid_1's l2: 0.633435
[6]	training's l2: 0.614511	valid_1's l2: 0.620206
[7]	training's l2: 0.597462	valid_1's l2: 0.607503
[8]	training's l2: 0.583377	valid_1's l2: 0.596921
[9]	training's l2: 0.569997	valid_1's l2: 0.586028
[10]	training's l2: 0.55787	valid_1's l2: 0.578165
[11]	training's l2: 0.546552	valid_1's l2: 0.570234
[12]	training's l2: 0.53719	valid_1's l2: 0.563403
[13]	training's l2: 0.526621	valid_1's l2: 0.555623
[14]	training's l2: 0.518522	valid_1's l2: 0.550039
[15]	training's l2: 0.511354	valid_1's l2: 0.545003
[16]	training's l2: 0.503264	valid_1's l2: 0.539512
[17]	training's l2: 0.495694	valid_1's l2: 0.534628
[18]	training's l2: 0.487566	valid_1's l2: 0.528987
[19]	training's l2: 0.47989	valid_1's l2: 0.523963
[20]	training's l2: 0.475039	valid_1's l2: 0.521896
[21]	training's l2: 0.469269	valid_1's l2: 0.517605
[22]	training's l2: 0.464184	valid_1's l2: 0.514925
[23]	training's l2: 0.460003	valid_1's l2: 0.512492
[24]	training's l2: 0.455369	valid_1's l2: 0.510073
[25]	training's l2: 0.451305	valid_1's l2: 0.508116
[26]	training's l2: 0.447742	valid_1's l2: 0.506325
[27]	training's l2: 0.444188	valid_1's l2: 0.504925
[28]	training's l2: 0.440481	valid_1's l2: 0.502637
[29]	training's l2: 0.436068	valid_1's l2: 0.500565
[30]	training's l2: 0.433151	valid_1's l2: 0.4995
[31]	training's l2: 0.427812	valid_1's l2: 0.49554
[32]	training's l2: 0.42473	valid_1's l2: 0.49405
[33]	training's l2: 0.420932	valid_1's l2: 0.491813
[34]	training's l2: 0.417824	valid_1's l2: 0.490758
[35]	training's l2: 0.413535	valid_1's l2: 0.487886
[36]	training's l2: 0.410555	valid_1's l2: 0.486792
[37]	training's l2: 0.407555	valid_1's l2: 0.485771
[38]	training's l2: 0.404665	valid_1's l2: 0.484389
[39]	training's l2: 0.402471	valid_1's l2: 0.483668
[40]	training's l2: 0.399641	valid_1's l2: 0.482694
[41]	training's l2: 0.397476	valid_1's l2: 0.482017
[42]	training's l2: 0.394941	valid_1's l2: 0.480858
[43]	training's l2: 0.392482	valid_1's l2: 0.479971
[44]	training's l2: 0.390042	valid_1's l2: 0.479331
[45]	training's l2: 0.387654	valid_1's l2: 0.47896
[46]	training's l2: 0.385328	valid_1's l2: 0.478359
[47]	training's l2: 0.383171	valid_1's l2: 0.4774
[48]	training's l2: 0.380095	valid_1's l2: 0.475472
[49]	training's l2: 0.378069	valid_1's l2: 0.475332
[50]	training's l2: 0.376211	valid_1's l2: 0.474844
[51]	training's l2: 0.373916	valid_1's l2: 0.474093
[52]	training's l2: 0.371954	valid_1's l2: 0.473586
[53]	training's l2: 0.370258	valid_1's l2: 0.47337
[54]	training's l2: 0.367691	valid_1's l2: 0.472433
[55]	training's l2: 0.366021	valid_1's l2: 0.472008
[56]	training's l2: 0.364421	valid_1's l2: 0.47224
[57]	training's l2: 0.362424	valid_1's l2: 0.47177
[58]	training's l2: 0.360949	valid_1's l2: 0.471524
[59]	training's l2: 0.359363	valid_1's l2: 0.471109
[60]	training's l2: 0.357307	valid_1's l2: 0.470341
[61]	training's l2: 0.355689	valid_1's l2: 0.470063
[62]	training's l2: 0.354157	valid_1's l2: 0.469721
[63]	training's l2: 0.351532	valid_1's l2: 0.468164
[64]	training's l2: 0.349791	valid_1's l2: 0.467442
[65]	training's l2: 0.348293	valid_1's l2: 0.467277
[66]	training's l2: 0.346865	valid_1's l2: 0.46714
[67]	training's l2: 0.345272	valid_1's l2: 0.466577
[68]	training's l2: 0.343457	valid_1's l2: 0.466208
[69]	training's l2: 0.341844	valid_1's l2: 0.46591
[70]	training's l2: 0.340104	valid_1's l2: 0.46551
[71]	training's l2: 0.338624	valid_1's l2: 0.465257
[72]	training's l2: 0.337249	valid_1's l2: 0.465278
[73]	training's l2: 0.335688	valid_1's l2: 0.464703
[74]	training's l2: 0.334266	valid_1's l2: 0.464871
[75]	training's l2: 0.333054	valid_1's l2: 0.464646
[76]	training's l2: 0.33164	valid_1's l2: 0.464247
[77]	training's l2: 0.330383	valid_1's l2: 0.464174
[78]	training's l2: 0.329022	valid_1's l2: 0.463911
[79]	training's l2: 0.327703	valid_1's l2: 0.463619
[80]	training's l2: 0.326026	valid_1's l2: 0.462891
[81]	training's l2: 0.324617	valid_1's l2: 0.46235
[82]	training's l2: 0.323217	valid_1's l2: 0.462136
[83]	training's l2: 0.321999	valid_1's l2: 0.462107
[84]	training's l2: 0.320755	valid_1's l2: 0.462237
[85]	training's l2: 0.319519	valid_1's l2: 0.462247
[86]	training's l2: 0.318089	valid_1's l2: 0.462143
[87]	training's l2: 0.316822	valid_1's l2: 0.461895
[88]	training's l2: 0.315106	valid_1's l2: 0.461052
[89]	training's l2: 0.31379	valid_1's l2: 0.460827
[90]	training's l2: 0.312307	valid_1's l2: 0.460422
[91]	training's l2: 0.311058	valid_1's l2: 0.460227
[92]	training's l2: 0.309726	valid_1's l2: 0.460043
[93]	training's l2: 0.308544	valid_1's l2: 0.460126
[94]	training's l2: 0.30732	valid_1's l2: 0.460185
[95]	training's l2: 0.306226	valid_1's l2: 0.460251
[96]	training's l2: 0.305131	valid_1's l2: 0.460265
[97]	training's l2: 0.303982	valid_1's l2: 0.460152
[98]	training's l2: 0.302861	valid_1's l2: 0.459989
[99]	training's l2: 0.301356	valid_1's l2: 0.459846
[100]	training's l2: 0.300254	valid_1's l2: 0.459985
[101]	training's l2: 0.29916	valid_1's l2: 0.459941
[102]	training's l2: 0.298117	valid_1's l2: 0.459871
[103]	training's l2: 0.297059	valid_1's l2: 0.459907
[104]	training's l2: 0.296016	valid_1's l2: 0.459992
[105]	training's l2: 0.294969	valid_1's l2: 0.459904
[106]	training's l2: 0.293822	valid_1's l2: 0.459572
[107]	training's l2: 0.29262	valid_1's l2: 0.459416
[108]	training's l2: 0.291531	valid_1's l2: 0.459233
[109]	training's l2: 0.29048	valid_1's l2: 0.459202
[110]	training's l2: 0.289466	valid_1's l2: 0.459215
[111]	training's l2: 0.2884	valid_1's l2: 0.459418
[112]	training's l2: 0.287409	valid_1's l2: 0.459339
[113]	training's l2: 0.286483	valid_1's l2: 0.459326
[114]	training's l2: 0.285451	valid_1's l2: 0.459234
[115]	training's l2: 0.284323	valid_1's l2: 0.459159
[116]	training's l2: 0.283192	valid_1's l2: 0.459019
[117]	training's l2: 0.282242	valid_1's l2: 0.45918
[118]	training's l2: 0.281366	valid_1's l2: 0.459233
[119]	training's l2: 0.28017	valid_1's l2: 0.458872
[120]	training's l2: 0.279258	valid_1's l2: 0.458738
[121]	training's l2: 0.278387	valid_1's l2: 0.459067
[122]	training's l2: 0.277323	valid_1's l2: 0.458991
[123]	training's l2: 0.275892	valid_1's l2: 0.457835
[124]	training's l2: 0.274999	valid_1's l2: 0.457948
[125]	training's l2: 0.273963	valid_1's l2: 0.458034
[126]	training's l2: 0.272986	valid_1's l2: 0.458014
[127]	training's l2: 0.27216	valid_1's l2: 0.458
[128]	training's l2: 0.271143	valid_1's l2: 0.45823
[129]	training's l2: 0.27026	valid_1's l2: 0.458205
[130]	training's l2: 0.269344	valid_1's l2: 0.458213
[131]	training's l2: 0.2683	valid_1's l2: 0.458292
[132]	training's l2: 0.267248	valid_1's l2: 0.457936
[133]	training's l2: 0.266185	valid_1's l2: 0.457813
[134]	training's l2: 0.265392	valid_1's l2: 0.457982
[135]	training's l2: 0.264599	valid_1's l2: 0.457921
[136]	training's l2: 0.263656	valid_1's l2: 0.457781
[137]	training's l2: 0.262569	valid_1's l2: 0.457378
[138]	training's l2: 0.261638	valid_1's l2: 0.457399
[139]	training's l2: 0.260821	valid_1's l2: 0.457448
[140]	training's l2: 0.260058	valid_1's l2: 0.457473
[141]	training's l2: 0.259273	valid_1's l2: 0.457383
[142]	training's l2: 0.258321	valid_1's l2: 0.457448
[143]	training's l2: 0.2576	valid_1's l2: 0.457333
[144]	training's l2: 0.25689	valid_1's l2: 0.457332
[145]	training's l2: 0.256177	valid_1's l2: 0.457311
[146]	training's l2: 0.255533	valid_1's l2: 0.457235
[147]	training's l2: 0.254861	valid_1's l2: 0.457234
[148]	training's l2: 0.254181	valid_1's l2: 0.457198
[149]	training's l2: 0.253213	valid_1's l2: 0.457005
[150]	training's l2: 0.252446	valid_1's l2: 0.45674
[151]	training's l2: 0.251609	valid_1's l2: 0.456493
[152]	training's l2: 0.2508	valid_1's l2: 0.456402
[153]	training's l2: 0.249995	valid_1's l2: 0.456388
[154]	training's l2: 0.249223	valid_1's l2: 0.456423
[155]	training's l2: 0.24844	valid_1's l2: 0.456703
[156]	training's l2: 0.247625	valid_1's l2: 0.456799
[157]	training's l2: 0.246933	valid_1's l2: 0.456706
[158]	training's l2: 0.246202	valid_1's l2: 0.456732
[159]	training's l2: 0.245334	valid_1's l2: 0.456561
[160]	training's l2: 0.244692	valid_1's l2: 0.456662
[161]	training's l2: 0.244043	valid_1's l2: 0.456814
[162]	training's l2: 0.243341	valid_1's l2: 0.457407
[163]	training's l2: 0.242631	valid_1's l2: 0.457418
[164]	training's l2: 0.241841	valid_1's l2: 0.457489
[165]	training's l2: 0.24114	valid_1's l2: 0.457769
[166]	training's l2: 0.240325	valid_1's l2: 0.457807
[167]	training's l2: 0.239631	valid_1's l2: 0.457665
[168]	training's l2: 0.23891	valid_1's l2: 0.457785
[169]	training's l2: 0.238316	valid_1's l2: 0.457779
[170]	training's l2: 0.237608	valid_1's l2: 0.457732
[171]	training's l2: 0.237008	valid_1's l2: 0.457887
[172]	training's l2: 0.236282	valid_1's l2: 0.457987
[173]	training's l2: 0.235598	valid_1's l2: 0.45788
[174]	training's l2: 0.234981	valid_1's l2: 0.457857
[175]	training's l2: 0.234224	valid_1's l2: 0.457863
[176]	training's l2: 0.233612	valid_1's l2: 0.457699
Did not meet early stopping. Best iteration is:
[176]	training's l2: 0.233612	valid_1's l2: 0.457699
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.193451 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.740675	valid_1's l2: 0.721985
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.715063	valid_1's l2: 0.701039
[3]	training's l2: 0.692296	valid_1's l2: 0.682309
[4]	training's l2: 0.671277	valid_1's l2: 0.665141
[5]	training's l2: 0.651916	valid_1's l2: 0.64978
[6]	training's l2: 0.634329	valid_1's l2: 0.636113
[7]	training's l2: 0.617577	valid_1's l2: 0.623712
[8]	training's l2: 0.602203	valid_1's l2: 0.612111
[9]	training's l2: 0.588019	valid_1's l2: 0.601542
[10]	training's l2: 0.57648	valid_1's l2: 0.593327
[11]	training's l2: 0.565617	valid_1's l2: 0.584957
[12]	training's l2: 0.554952	valid_1's l2: 0.57733
[13]	training's l2: 0.545907	valid_1's l2: 0.570873
[14]	training's l2: 0.536673	valid_1's l2: 0.564613
[15]	training's l2: 0.528258	valid_1's l2: 0.558423
[16]	training's l2: 0.518927	valid_1's l2: 0.55195
[17]	training's l2: 0.511392	valid_1's l2: 0.546841
[18]	training's l2: 0.505341	valid_1's l2: 0.543266
[19]	training's l2: 0.497926	valid_1's l2: 0.53728
[20]	training's l2: 0.491974	valid_1's l2: 0.533624
[21]	training's l2: 0.485919	valid_1's l2: 0.530082
[22]	training's l2: 0.480228	valid_1's l2: 0.526594
[23]	training's l2: 0.474409	valid_1's l2: 0.523709
[24]	training's l2: 0.469412	valid_1's l2: 0.520744
[25]	training's l2: 0.463239	valid_1's l2: 0.516507
[26]	training's l2: 0.458114	valid_1's l2: 0.51317
[27]	training's l2: 0.453978	valid_1's l2: 0.51097
[28]	training's l2: 0.450132	valid_1's l2: 0.509324
[29]	training's l2: 0.446203	valid_1's l2: 0.507054
[30]	training's l2: 0.442481	valid_1's l2: 0.50542
[31]	training's l2: 0.438852	valid_1's l2: 0.503922
[32]	training's l2: 0.43566	valid_1's l2: 0.502619
[33]	training's l2: 0.432221	valid_1's l2: 0.501405
[34]	training's l2: 0.428337	valid_1's l2: 0.49903
[35]	training's l2: 0.425338	valid_1's l2: 0.497743
[36]	training's l2: 0.421871	valid_1's l2: 0.495805
[37]	training's l2: 0.419088	valid_1's l2: 0.494427
[38]	training's l2: 0.416092	valid_1's l2: 0.493137
[39]	training's l2: 0.412646	valid_1's l2: 0.491439
[40]	training's l2: 0.410088	valid_1's l2: 0.490652
[41]	training's l2: 0.407611	valid_1's l2: 0.489454
[42]	training's l2: 0.404569	valid_1's l2: 0.488371
[43]	training's l2: 0.401153	valid_1's l2: 0.486258
[44]	training's l2: 0.398894	valid_1's l2: 0.485501
[45]	training's l2: 0.396652	valid_1's l2: 0.485194
[46]	training's l2: 0.394153	valid_1's l2: 0.484136
[47]	training's l2: 0.391776	valid_1's l2: 0.483412
[48]	training's l2: 0.389571	valid_1's l2: 0.482971
[49]	training's l2: 0.387127	valid_1's l2: 0.482093
[50]	training's l2: 0.384319	valid_1's l2: 0.480802
[51]	training's l2: 0.38191	valid_1's l2: 0.480072
[52]	training's l2: 0.37948	valid_1's l2: 0.47891
[53]	training's l2: 0.377473	valid_1's l2: 0.478802
[54]	training's l2: 0.375314	valid_1's l2: 0.477587
[55]	training's l2: 0.372618	valid_1's l2: 0.476368
[56]	training's l2: 0.370639	valid_1's l2: 0.47553
[57]	training's l2: 0.368467	valid_1's l2: 0.474992
[58]	training's l2: 0.366609	valid_1's l2: 0.474602
[59]	training's l2: 0.364668	valid_1's l2: 0.473781
[60]	training's l2: 0.363076	valid_1's l2: 0.473485
[61]	training's l2: 0.360921	valid_1's l2: 0.47254
[62]	training's l2: 0.359279	valid_1's l2: 0.472468
[63]	training's l2: 0.357667	valid_1's l2: 0.4722
[64]	training's l2: 0.355716	valid_1's l2: 0.471531
[65]	training's l2: 0.354051	valid_1's l2: 0.471134
[66]	training's l2: 0.352378	valid_1's l2: 0.470702
[67]	training's l2: 0.35083	valid_1's l2: 0.470537
[68]	training's l2: 0.348989	valid_1's l2: 0.4701
[69]	training's l2: 0.347191	valid_1's l2: 0.469381
[70]	training's l2: 0.345528	valid_1's l2: 0.468966
[71]	training's l2: 0.343969	valid_1's l2: 0.468323
[72]	training's l2: 0.342482	valid_1's l2: 0.468248
[73]	training's l2: 0.341084	valid_1's l2: 0.467905
[74]	training's l2: 0.339376	valid_1's l2: 0.467143
[75]	training's l2: 0.337447	valid_1's l2: 0.466234
[76]	training's l2: 0.336008	valid_1's l2: 0.465926
[77]	training's l2: 0.334567	valid_1's l2: 0.465604
[78]	training's l2: 0.332779	valid_1's l2: 0.464884
[79]	training's l2: 0.331322	valid_1's l2: 0.464592
[80]	training's l2: 0.330042	valid_1's l2: 0.464452
[81]	training's l2: 0.328786	valid_1's l2: 0.464338
[82]	training's l2: 0.32738	valid_1's l2: 0.463808
[83]	training's l2: 0.325981	valid_1's l2: 0.463759
[84]	training's l2: 0.324671	valid_1's l2: 0.463989
[85]	training's l2: 0.323358	valid_1's l2: 0.463885
[86]	training's l2: 0.321992	valid_1's l2: 0.463563
[87]	training's l2: 0.320135	valid_1's l2: 0.462946
[88]	training's l2: 0.31874	valid_1's l2: 0.462697
[89]	training's l2: 0.317422	valid_1's l2: 0.462873
[90]	training's l2: 0.316006	valid_1's l2: 0.462568
[91]	training's l2: 0.314803	valid_1's l2: 0.462402
[92]	training's l2: 0.313443	valid_1's l2: 0.462201
[93]	training's l2: 0.312137	valid_1's l2: 0.462043
[94]	training's l2: 0.310968	valid_1's l2: 0.461972
[95]	training's l2: 0.309673	valid_1's l2: 0.461949
[96]	training's l2: 0.308442	valid_1's l2: 0.461384
[97]	training's l2: 0.306818	valid_1's l2: 0.460936
[98]	training's l2: 0.30547	valid_1's l2: 0.46072
[99]	training's l2: 0.304149	valid_1's l2: 0.460764
[100]	training's l2: 0.302974	valid_1's l2: 0.460665
[101]	training's l2: 0.301741	valid_1's l2: 0.460426
[102]	training's l2: 0.300652	valid_1's l2: 0.460294
[103]	training's l2: 0.299496	valid_1's l2: 0.460198
[104]	training's l2: 0.298319	valid_1's l2: 0.460003
[105]	training's l2: 0.297187	valid_1's l2: 0.459939
[106]	training's l2: 0.296092	valid_1's l2: 0.459807
[107]	training's l2: 0.294578	valid_1's l2: 0.459336
[108]	training's l2: 0.293525	valid_1's l2: 0.459503
[109]	training's l2: 0.292455	valid_1's l2: 0.459379
[110]	training's l2: 0.291394	valid_1's l2: 0.459329
[111]	training's l2: 0.290024	valid_1's l2: 0.458907
[112]	training's l2: 0.288953	valid_1's l2: 0.459097
[113]	training's l2: 0.287961	valid_1's l2: 0.459114
[114]	training's l2: 0.286807	valid_1's l2: 0.459066
[115]	training's l2: 0.285758	valid_1's l2: 0.458716
[116]	training's l2: 0.284792	valid_1's l2: 0.458818
[117]	training's l2: 0.283774	valid_1's l2: 0.45884
[118]	training's l2: 0.282383	valid_1's l2: 0.457801
[119]	training's l2: 0.281384	valid_1's l2: 0.457793
[120]	training's l2: 0.280356	valid_1's l2: 0.457676
[121]	training's l2: 0.27941	valid_1's l2: 0.457753
[122]	training's l2: 0.278219	valid_1's l2: 0.457477
[123]	training's l2: 0.277226	valid_1's l2: 0.457485
[124]	training's l2: 0.276178	valid_1's l2: 0.457102
[125]	training's l2: 0.275159	valid_1's l2: 0.457065
[126]	training's l2: 0.274248	valid_1's l2: 0.4574
[127]	training's l2: 0.273323	valid_1's l2: 0.457343
[128]	training's l2: 0.272379	valid_1's l2: 0.456951
[129]	training's l2: 0.271522	valid_1's l2: 0.456877
[130]	training's l2: 0.270289	valid_1's l2: 0.456514
[131]	training's l2: 0.269376	valid_1's l2: 0.456438
[132]	training's l2: 0.268339	valid_1's l2: 0.456425
[133]	training's l2: 0.267353	valid_1's l2: 0.456643
[134]	training's l2: 0.266188	valid_1's l2: 0.455733
[135]	training's l2: 0.265263	valid_1's l2: 0.455674
[136]	training's l2: 0.264408	valid_1's l2: 0.455706
[137]	training's l2: 0.263348	valid_1's l2: 0.455752
[138]	training's l2: 0.262382	valid_1's l2: 0.455643
[139]	training's l2: 0.261311	valid_1's l2: 0.455077
[140]	training's l2: 0.26043	valid_1's l2: 0.455144
[141]	training's l2: 0.259431	valid_1's l2: 0.454925
[142]	training's l2: 0.258458	valid_1's l2: 0.454945
[143]	training's l2: 0.257558	valid_1's l2: 0.454804
[144]	training's l2: 0.256786	valid_1's l2: 0.4548
[145]	training's l2: 0.255913	valid_1's l2: 0.454705
[146]	training's l2: 0.255134	valid_1's l2: 0.454676
[147]	training's l2: 0.2543	valid_1's l2: 0.454754
[148]	training's l2: 0.25348	valid_1's l2: 0.454711
[149]	training's l2: 0.252394	valid_1's l2: 0.454422
[150]	training's l2: 0.251543	valid_1's l2: 0.454578
[151]	training's l2: 0.250757	valid_1's l2: 0.454615
[152]	training's l2: 0.249932	valid_1's l2: 0.454418
[153]	training's l2: 0.249179	valid_1's l2: 0.454376
[154]	training's l2: 0.248358	valid_1's l2: 0.454368
[155]	training's l2: 0.247603	valid_1's l2: 0.454392
[156]	training's l2: 0.246896	valid_1's l2: 0.454375
[157]	training's l2: 0.246265	valid_1's l2: 0.454412
[158]	training's l2: 0.245481	valid_1's l2: 0.454456
[159]	training's l2: 0.244737	valid_1's l2: 0.454668
[160]	training's l2: 0.243964	valid_1's l2: 0.454761
[161]	training's l2: 0.243169	valid_1's l2: 0.454721
Did not meet early stopping. Best iteration is:
[161]	training's l2: 0.243169	valid_1's l2: 0.454721
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189326 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139405
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.730218	valid_1's l2: 0.711861
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.695504	valid_1's l2: 0.683076
[3]	training's l2: 0.667562	valid_1's l2: 0.659796
[4]	training's l2: 0.64196	valid_1's l2: 0.638594
[5]	training's l2: 0.619917	valid_1's l2: 0.621163
[6]	training's l2: 0.602124	valid_1's l2: 0.60723
[7]	training's l2: 0.584476	valid_1's l2: 0.592622
[8]	training's l2: 0.570517	valid_1's l2: 0.581821
[9]	training's l2: 0.558785	valid_1's l2: 0.572268
[10]	training's l2: 0.547765	valid_1's l2: 0.563962
[11]	training's l2: 0.537847	valid_1's l2: 0.557084
[12]	training's l2: 0.52653	valid_1's l2: 0.548795
[13]	training's l2: 0.517356	valid_1's l2: 0.542961
[14]	training's l2: 0.509347	valid_1's l2: 0.537189
[15]	training's l2: 0.501558	valid_1's l2: 0.532455
[16]	training's l2: 0.492581	valid_1's l2: 0.525491
[17]	training's l2: 0.486871	valid_1's l2: 0.523467
[18]	training's l2: 0.480867	valid_1's l2: 0.519804
[19]	training's l2: 0.474783	valid_1's l2: 0.516317
[20]	training's l2: 0.46955	valid_1's l2: 0.513536
[21]	training's l2: 0.463497	valid_1's l2: 0.509505
[22]	training's l2: 0.459363	valid_1's l2: 0.507493
[23]	training's l2: 0.455633	valid_1's l2: 0.50579
[24]	training's l2: 0.450996	valid_1's l2: 0.502773
[25]	training's l2: 0.446947	valid_1's l2: 0.500895
[26]	training's l2: 0.440785	valid_1's l2: 0.496245
[27]	training's l2: 0.437848	valid_1's l2: 0.494906
[28]	training's l2: 0.43341	valid_1's l2: 0.492424
[29]	training's l2: 0.429819	valid_1's l2: 0.490737
[30]	training's l2: 0.426857	valid_1's l2: 0.489584
[31]	training's l2: 0.422466	valid_1's l2: 0.486705
[32]	training's l2: 0.419458	valid_1's l2: 0.485428
[33]	training's l2: 0.416672	valid_1's l2: 0.484506
[34]	training's l2: 0.414379	valid_1's l2: 0.483883
[35]	training's l2: 0.410537	valid_1's l2: 0.482004
[36]	training's l2: 0.408044	valid_1's l2: 0.481203
[37]	training's l2: 0.405589	valid_1's l2: 0.48036
[38]	training's l2: 0.403546	valid_1's l2: 0.47985
[39]	training's l2: 0.400369	valid_1's l2: 0.478543
[40]	training's l2: 0.398098	valid_1's l2: 0.477423
[41]	training's l2: 0.396205	valid_1's l2: 0.477146
[42]	training's l2: 0.393386	valid_1's l2: 0.475805
[43]	training's l2: 0.391617	valid_1's l2: 0.475374
[44]	training's l2: 0.389685	valid_1's l2: 0.474843
[45]	training's l2: 0.387869	valid_1's l2: 0.474809
[46]	training's l2: 0.385312	valid_1's l2: 0.473878
[47]	training's l2: 0.383654	valid_1's l2: 0.473566
[48]	training's l2: 0.381503	valid_1's l2: 0.473216
[49]	training's l2: 0.379698	valid_1's l2: 0.473192
[50]	training's l2: 0.377767	valid_1's l2: 0.472555
[51]	training's l2: 0.375731	valid_1's l2: 0.471973
[52]	training's l2: 0.374159	valid_1's l2: 0.472006
[53]	training's l2: 0.37257	valid_1's l2: 0.471826
[54]	training's l2: 0.370933	valid_1's l2: 0.47139
[55]	training's l2: 0.369181	valid_1's l2: 0.471039
[56]	training's l2: 0.367428	valid_1's l2: 0.470477
[57]	training's l2: 0.365849	valid_1's l2: 0.470094
[58]	training's l2: 0.364142	valid_1's l2: 0.470189
[59]	training's l2: 0.362557	valid_1's l2: 0.469822
[60]	training's l2: 0.360377	valid_1's l2: 0.468953
[61]	training's l2: 0.358739	valid_1's l2: 0.468524
[62]	training's l2: 0.357248	valid_1's l2: 0.468374
[63]	training's l2: 0.355277	valid_1's l2: 0.46768
[64]	training's l2: 0.353368	valid_1's l2: 0.467012
[65]	training's l2: 0.351987	valid_1's l2: 0.466431
[66]	training's l2: 0.350437	valid_1's l2: 0.466328
[67]	training's l2: 0.348655	valid_1's l2: 0.465509
[68]	training's l2: 0.347295	valid_1's l2: 0.465479
[69]	training's l2: 0.345903	valid_1's l2: 0.465458
[70]	training's l2: 0.344598	valid_1's l2: 0.465568
[71]	training's l2: 0.343061	valid_1's l2: 0.465221
[72]	training's l2: 0.34153	valid_1's l2: 0.464864
[73]	training's l2: 0.34016	valid_1's l2: 0.464109
[74]	training's l2: 0.33883	valid_1's l2: 0.463859
[75]	training's l2: 0.337397	valid_1's l2: 0.463163
[76]	training's l2: 0.335912	valid_1's l2: 0.462947
[77]	training's l2: 0.334532	valid_1's l2: 0.462445
[78]	training's l2: 0.33317	valid_1's l2: 0.462109
[79]	training's l2: 0.332002	valid_1's l2: 0.462125
[80]	training's l2: 0.330576	valid_1's l2: 0.46161
[81]	training's l2: 0.329309	valid_1's l2: 0.461458
[82]	training's l2: 0.328155	valid_1's l2: 0.461487
[83]	training's l2: 0.327126	valid_1's l2: 0.461335
[84]	training's l2: 0.325899	valid_1's l2: 0.461258
[85]	training's l2: 0.324396	valid_1's l2: 0.461063
[86]	training's l2: 0.323173	valid_1's l2: 0.461146
[87]	training's l2: 0.322052	valid_1's l2: 0.460881
[88]	training's l2: 0.321004	valid_1's l2: 0.460823
[89]	training's l2: 0.319726	valid_1's l2: 0.460593
[90]	training's l2: 0.318538	valid_1's l2: 0.46055
[91]	training's l2: 0.317414	valid_1's l2: 0.460571
[92]	training's l2: 0.31635	valid_1's l2: 0.460588
[93]	training's l2: 0.315246	valid_1's l2: 0.460522
[94]	training's l2: 0.314191	valid_1's l2: 0.460675
[95]	training's l2: 0.312994	valid_1's l2: 0.46031
[96]	training's l2: 0.311812	valid_1's l2: 0.460352
[97]	training's l2: 0.31071	valid_1's l2: 0.460142
[98]	training's l2: 0.308834	valid_1's l2: 0.459191
[99]	training's l2: 0.307791	valid_1's l2: 0.459263
[100]	training's l2: 0.306735	valid_1's l2: 0.45917
[101]	training's l2: 0.305676	valid_1's l2: 0.459186
[102]	training's l2: 0.304773	valid_1's l2: 0.459443
[103]	training's l2: 0.303706	valid_1's l2: 0.459398
[104]	training's l2: 0.30268	valid_1's l2: 0.459072
[105]	training's l2: 0.301241	valid_1's l2: 0.458301
[106]	training's l2: 0.300164	valid_1's l2: 0.458207
[107]	training's l2: 0.299013	valid_1's l2: 0.45814
[108]	training's l2: 0.298039	valid_1's l2: 0.458133
[109]	training's l2: 0.297079	valid_1's l2: 0.458326
[110]	training's l2: 0.296106	valid_1's l2: 0.458396
[111]	training's l2: 0.29522	valid_1's l2: 0.45818
[112]	training's l2: 0.294106	valid_1's l2: 0.457902
[113]	training's l2: 0.293204	valid_1's l2: 0.457948
[114]	training's l2: 0.292247	valid_1's l2: 0.458017
[115]	training's l2: 0.291462	valid_1's l2: 0.458052
[116]	training's l2: 0.290245	valid_1's l2: 0.457716
[117]	training's l2: 0.289321	valid_1's l2: 0.457692
[118]	training's l2: 0.288339	valid_1's l2: 0.457727
[119]	training's l2: 0.287348	valid_1's l2: 0.457686
[120]	training's l2: 0.286513	valid_1's l2: 0.457622
[121]	training's l2: 0.285723	valid_1's l2: 0.457754
[122]	training's l2: 0.284897	valid_1's l2: 0.457858
[123]	training's l2: 0.284171	valid_1's l2: 0.457918
[124]	training's l2: 0.283274	valid_1's l2: 0.45792
[125]	training's l2: 0.282432	valid_1's l2: 0.457982
[126]	training's l2: 0.281547	valid_1's l2: 0.45787
[127]	training's l2: 0.280729	valid_1's l2: 0.457977
[128]	training's l2: 0.279908	valid_1's l2: 0.458179
[129]	training's l2: 0.279054	valid_1's l2: 0.458185
[130]	training's l2: 0.278096	valid_1's l2: 0.458027
[131]	training's l2: 0.277271	valid_1's l2: 0.45801
[132]	training's l2: 0.276476	valid_1's l2: 0.457939
[133]	training's l2: 0.275523	valid_1's l2: 0.457804
[134]	training's l2: 0.274581	valid_1's l2: 0.457722
[135]	training's l2: 0.273786	valid_1's l2: 0.45772
[136]	training's l2: 0.272965	valid_1's l2: 0.457653
[137]	training's l2: 0.272068	valid_1's l2: 0.457638
[138]	training's l2: 0.2713	valid_1's l2: 0.457686
[139]	training's l2: 0.27054	valid_1's l2: 0.457691
[140]	training's l2: 0.269693	valid_1's l2: 0.457888
[141]	training's l2: 0.268694	valid_1's l2: 0.457644
[142]	training's l2: 0.267929	valid_1's l2: 0.457603
[143]	training's l2: 0.267082	valid_1's l2: 0.457604
[144]	training's l2: 0.266432	valid_1's l2: 0.457989
[145]	training's l2: 0.265378	valid_1's l2: 0.457822
[146]	training's l2: 0.264555	valid_1's l2: 0.457851
[147]	training's l2: 0.263943	valid_1's l2: 0.457794
[148]	training's l2: 0.263177	valid_1's l2: 0.457905
[149]	training's l2: 0.262383	valid_1's l2: 0.457879
[150]	training's l2: 0.261597	valid_1's l2: 0.457911
[151]	training's l2: 0.260769	valid_1's l2: 0.457993
[152]	training's l2: 0.260077	valid_1's l2: 0.457966
[153]	training's l2: 0.258975	valid_1's l2: 0.458036
[154]	training's l2: 0.25817	valid_1's l2: 0.458081
[155]	training's l2: 0.257479	valid_1's l2: 0.457901
[156]	training's l2: 0.256803	valid_1's l2: 0.457878
[157]	training's l2: 0.256007	valid_1's l2: 0.457865
[158]	training's l2: 0.255206	valid_1's l2: 0.457931
[159]	training's l2: 0.254572	valid_1's l2: 0.458251
[160]	training's l2: 0.25394	valid_1's l2: 0.458371
[161]	training's l2: 0.253167	valid_1's l2: 0.458435
[162]	training's l2: 0.252418	valid_1's l2: 0.458359
Did not meet early stopping. Best iteration is:
[162]	training's l2: 0.252418	valid_1's l2: 0.458359
pace score: 1.3978694008449635
pace_regression score: 0.01612353133257057
before_pace_regression score: 0.026711849257639794
after_pace_regression score: 0.04369082653567118
pace_conv score: 0.054586097770674975
first_up3 score: 0.6404269831661307
last_up3 score: 0.6698252071180004
