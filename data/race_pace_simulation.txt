Looking in indexes: https://pypi.org/simple, http://100.95.241.19
Requirement already satisfied: SekitobaLibrary in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (1.2.114)
Requirement already satisfied: requests in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (2.28.1)
Requirement already satisfied: pandas in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (1.5.2)
Requirement already satisfied: lightgbm in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (3.3.3)
Requirement already satisfied: numpy in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (1.24.1)
Requirement already satisfied: matplotlib in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (3.6.2)
Requirement already satisfied: tqdm in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (4.64.1)
Requirement already satisfied: statistics in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (1.0.3.5)
Requirement already satisfied: boto3 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (1.26.42)
Requirement already satisfied: torch in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (1.13.1)
Requirement already satisfied: mpi4py in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (3.1.4)
Requirement already satisfied: trueskill in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (0.4.5)
Requirement already satisfied: bs4 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (0.0.1)
Requirement already satisfied: jpholiday in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from SekitobaLibrary) (0.1.8)
Requirement already satisfied: botocore<1.30.0,>=1.29.42 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from boto3->SekitobaLibrary) (1.29.42)
Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from boto3->SekitobaLibrary) (1.0.1)
Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from boto3->SekitobaLibrary) (0.6.0)
Requirement already satisfied: beautifulsoup4 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from bs4->SekitobaLibrary) (4.11.1)
Requirement already satisfied: wheel in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from lightgbm->SekitobaLibrary) (0.38.4)
Requirement already satisfied: scipy in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from lightgbm->SekitobaLibrary) (1.9.3)
Requirement already satisfied: scikit-learn!=0.22.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from lightgbm->SekitobaLibrary) (1.2.0)
Requirement already satisfied: contourpy>=1.0.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (1.0.6)
Requirement already satisfied: cycler>=0.10 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (4.38.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (1.4.4)
Requirement already satisfied: packaging>=20.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (22.0)
Requirement already satisfied: pillow>=6.2.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (9.3.0)
Requirement already satisfied: pyparsing>=2.2.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (3.0.9)
Requirement already satisfied: python-dateutil>=2.7 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from matplotlib->SekitobaLibrary) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from pandas->SekitobaLibrary) (2022.7)
Requirement already satisfied: charset-normalizer<3,>=2 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from requests->SekitobaLibrary) (2.1.1)
Requirement already satisfied: idna<4,>=2.5 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from requests->SekitobaLibrary) (3.4)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from requests->SekitobaLibrary) (1.26.13)
Requirement already satisfied: certifi>=2017.4.17 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from requests->SekitobaLibrary) (2022.12.7)
Requirement already satisfied: docutils>=0.3 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from statistics->SekitobaLibrary) (0.19)
Requirement already satisfied: typing-extensions in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from torch->SekitobaLibrary) (4.4.0)
Requirement already satisfied: six in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from trueskill->SekitobaLibrary) (1.16.0)
Requirement already satisfied: joblib>=1.1.1 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm->SekitobaLibrary) (1.2.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from scikit-learn!=0.22.0->lightgbm->SekitobaLibrary) (3.1.0)
Requirement already satisfied: soupsieve>1.2 in /Users/kansei/.pyenv/versions/3.10.1/lib/python3.10/site-packages (from beautifulsoup4->bs4->SekitobaLibrary) (2.3.2.post1)
wrap_data.pickle download finish Gilgamesh
race_cource_info.pickle download finish Gilgamesh
race_pace_analyze_data.pickle download finish Gilgamesh
start rank:1
start rank:3
start rank:2
start rank:5
start rank:4
























1-instance.pickle download finish Gilgamesh
2-instance.pickle download finish Gilgamesh
3-instance.pickle download finish Gilgamesh
4-instance.pickle download finish Gilgamesh
5-instance.pickle download finish Gilgamesh
pace_learn_data.pickle download finish Gilgamesh
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.216113 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59015	valid_1's l2: 2.64271
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.5453	valid_1's l2: 2.59798
[3]	training's l2: 2.50237	valid_1's l2: 2.55719
[4]	training's l2: 2.46481	valid_1's l2: 2.52062
[5]	training's l2: 2.42822	valid_1's l2: 2.48693
[6]	training's l2: 2.39618	valid_1's l2: 2.45673
[7]	training's l2: 2.3651	valid_1's l2: 2.428
[8]	training's l2: 2.33752	valid_1's l2: 2.40152
[9]	training's l2: 2.31006	valid_1's l2: 2.3768
[10]	training's l2: 2.28498	valid_1's l2: 2.35538
[11]	training's l2: 2.26284	valid_1's l2: 2.33616
[12]	training's l2: 2.24097	valid_1's l2: 2.31761
[13]	training's l2: 2.22055	valid_1's l2: 2.3008
[14]	training's l2: 2.2015	valid_1's l2: 2.28646
[15]	training's l2: 2.18335	valid_1's l2: 2.27256
[16]	training's l2: 2.16601	valid_1's l2: 2.25984
[17]	training's l2: 2.149	valid_1's l2: 2.24659
[18]	training's l2: 2.13383	valid_1's l2: 2.23589
[19]	training's l2: 2.11974	valid_1's l2: 2.2253
[20]	training's l2: 2.10599	valid_1's l2: 2.21466
[21]	training's l2: 2.09314	valid_1's l2: 2.20608
[22]	training's l2: 2.07919	valid_1's l2: 2.19648
[23]	training's l2: 2.06711	valid_1's l2: 2.18781
[24]	training's l2: 2.05508	valid_1's l2: 2.17969
[25]	training's l2: 2.04427	valid_1's l2: 2.17331
[26]	training's l2: 2.03347	valid_1's l2: 2.16653
[27]	training's l2: 2.02327	valid_1's l2: 2.1619
[28]	training's l2: 2.01295	valid_1's l2: 2.15587
[29]	training's l2: 2.00352	valid_1's l2: 2.15091
[30]	training's l2: 1.99474	valid_1's l2: 2.14642
[31]	training's l2: 1.98577	valid_1's l2: 2.1411
[32]	training's l2: 1.97703	valid_1's l2: 2.13753
[33]	training's l2: 1.96895	valid_1's l2: 2.13371
[34]	training's l2: 1.96043	valid_1's l2: 2.12993
[35]	training's l2: 1.95148	valid_1's l2: 2.12455
[36]	training's l2: 1.94367	valid_1's l2: 2.12053
[37]	training's l2: 1.93579	valid_1's l2: 2.1177
[38]	training's l2: 1.92829	valid_1's l2: 2.11415
[39]	training's l2: 1.9209	valid_1's l2: 2.11203
[40]	training's l2: 1.91355	valid_1's l2: 2.1085
[41]	training's l2: 1.90642	valid_1's l2: 2.10544
[42]	training's l2: 1.89963	valid_1's l2: 2.10405
[43]	training's l2: 1.89252	valid_1's l2: 2.10193
[44]	training's l2: 1.88544	valid_1's l2: 2.09945
[45]	training's l2: 1.87833	valid_1's l2: 2.09664
[46]	training's l2: 1.87177	valid_1's l2: 2.09504
[47]	training's l2: 1.86474	valid_1's l2: 2.09206
[48]	training's l2: 1.85794	valid_1's l2: 2.08981
[49]	training's l2: 1.85233	valid_1's l2: 2.08923
[50]	training's l2: 1.84579	valid_1's l2: 2.08685
[51]	training's l2: 1.83979	valid_1's l2: 2.08434
[52]	training's l2: 1.83376	valid_1's l2: 2.08179
[53]	training's l2: 1.82793	valid_1's l2: 2.07924
[54]	training's l2: 1.82225	valid_1's l2: 2.07695
[55]	training's l2: 1.81714	valid_1's l2: 2.07616
[56]	training's l2: 1.81049	valid_1's l2: 2.07433
[57]	training's l2: 1.8043	valid_1's l2: 2.07214
[58]	training's l2: 1.79728	valid_1's l2: 2.07004
[59]	training's l2: 1.79199	valid_1's l2: 2.06895
[60]	training's l2: 1.78667	valid_1's l2: 2.06788
[61]	training's l2: 1.78011	valid_1's l2: 2.06614
[62]	training's l2: 1.77534	valid_1's l2: 2.06522
[63]	training's l2: 1.76947	valid_1's l2: 2.06529
[64]	training's l2: 1.76449	valid_1's l2: 2.06457
[65]	training's l2: 1.7585	valid_1's l2: 2.06207
[66]	training's l2: 1.75294	valid_1's l2: 2.06082
[67]	training's l2: 1.74766	valid_1's l2: 2.05877
[68]	training's l2: 1.74278	valid_1's l2: 2.05841
[69]	training's l2: 1.73668	valid_1's l2: 2.05697
[70]	training's l2: 1.73161	valid_1's l2: 2.05553
[71]	training's l2: 1.72609	valid_1's l2: 2.05462
[72]	training's l2: 1.72156	valid_1's l2: 2.05485
[73]	training's l2: 1.71656	valid_1's l2: 2.05315
[74]	training's l2: 1.71202	valid_1's l2: 2.05133
[75]	training's l2: 1.70747	valid_1's l2: 2.05051
[76]	training's l2: 1.70248	valid_1's l2: 2.05012
[77]	training's l2: 1.69816	valid_1's l2: 2.04905
[78]	training's l2: 1.69347	valid_1's l2: 2.04854
[79]	training's l2: 1.68941	valid_1's l2: 2.04875
[80]	training's l2: 1.68394	valid_1's l2: 2.04734
[81]	training's l2: 1.67978	valid_1's l2: 2.0464
[82]	training's l2: 1.67578	valid_1's l2: 2.04526
[83]	training's l2: 1.6718	valid_1's l2: 2.04613
[84]	training's l2: 1.66802	valid_1's l2: 2.04633
[85]	training's l2: 1.66302	valid_1's l2: 2.04594
[86]	training's l2: 1.65904	valid_1's l2: 2.04527
[87]	training's l2: 1.65519	valid_1's l2: 2.04497
[88]	training's l2: 1.65109	valid_1's l2: 2.04343
[89]	training's l2: 1.64696	valid_1's l2: 2.04304
[90]	training's l2: 1.64317	valid_1's l2: 2.04269
[91]	training's l2: 1.63888	valid_1's l2: 2.04231
[92]	training's l2: 1.63518	valid_1's l2: 2.04279
[93]	training's l2: 1.63125	valid_1's l2: 2.04259
[94]	training's l2: 1.62768	valid_1's l2: 2.0422
[95]	training's l2: 1.62352	valid_1's l2: 2.04264
[96]	training's l2: 1.61915	valid_1's l2: 2.04183
[97]	training's l2: 1.61549	valid_1's l2: 2.04244
[98]	training's l2: 1.612	valid_1's l2: 2.04232
[99]	training's l2: 1.60855	valid_1's l2: 2.0427
[100]	training's l2: 1.60487	valid_1's l2: 2.04207
[101]	training's l2: 1.60117	valid_1's l2: 2.04162
[102]	training's l2: 1.59775	valid_1's l2: 2.04154
[103]	training's l2: 1.59443	valid_1's l2: 2.04187
[104]	training's l2: 1.58991	valid_1's l2: 2.03949
[105]	training's l2: 1.58646	valid_1's l2: 2.03911
[106]	training's l2: 1.58306	valid_1's l2: 2.0385
[107]	training's l2: 1.57956	valid_1's l2: 2.03834
[108]	training's l2: 1.57587	valid_1's l2: 2.03774
[109]	training's l2: 1.57204	valid_1's l2: 2.03688
[110]	training's l2: 1.56888	valid_1's l2: 2.03654
[111]	training's l2: 1.56573	valid_1's l2: 2.03678
[112]	training's l2: 1.56255	valid_1's l2: 2.03656
[113]	training's l2: 1.5594	valid_1's l2: 2.03643
[114]	training's l2: 1.5561	valid_1's l2: 2.03673
[115]	training's l2: 1.55276	valid_1's l2: 2.03626
[116]	training's l2: 1.5497	valid_1's l2: 2.03672
[117]	training's l2: 1.54645	valid_1's l2: 2.03641
[118]	training's l2: 1.54325	valid_1's l2: 2.03671
[119]	training's l2: 1.53975	valid_1's l2: 2.03645
[120]	training's l2: 1.53676	valid_1's l2: 2.03754
[121]	training's l2: 1.53354	valid_1's l2: 2.03838
[122]	training's l2: 1.53028	valid_1's l2: 2.03887
[123]	training's l2: 1.52707	valid_1's l2: 2.03933
[124]	training's l2: 1.52401	valid_1's l2: 2.03883
[125]	training's l2: 1.52108	valid_1's l2: 2.03911
[126]	training's l2: 1.51836	valid_1's l2: 2.03866
[127]	training's l2: 1.51532	valid_1's l2: 2.03926
[128]	training's l2: 1.51253	valid_1's l2: 2.03908
[129]	training's l2: 1.50963	valid_1's l2: 2.03829
[130]	training's l2: 1.50695	valid_1's l2: 2.03827
[131]	training's l2: 1.50382	valid_1's l2: 2.03764
[132]	training's l2: 1.50068	valid_1's l2: 2.03757
[133]	training's l2: 1.49808	valid_1's l2: 2.03772
[134]	training's l2: 1.49538	valid_1's l2: 2.03871
[135]	training's l2: 1.49207	valid_1's l2: 2.03778
[136]	training's l2: 1.48943	valid_1's l2: 2.03762
[137]	training's l2: 1.48657	valid_1's l2: 2.03656
[138]	training's l2: 1.4832	valid_1's l2: 2.03599
[139]	training's l2: 1.47992	valid_1's l2: 2.03554
[140]	training's l2: 1.47687	valid_1's l2: 2.03569
[141]	training's l2: 1.47387	valid_1's l2: 2.03575
[142]	training's l2: 1.47128	valid_1's l2: 2.0358
[143]	training's l2: 1.4686	valid_1's l2: 2.03469
[144]	training's l2: 1.46612	valid_1's l2: 2.03542
[145]	training's l2: 1.46373	valid_1's l2: 2.03607
[146]	training's l2: 1.46074	valid_1's l2: 2.03503
[147]	training's l2: 1.45765	valid_1's l2: 2.03539
[148]	training's l2: 1.45506	valid_1's l2: 2.03479
[149]	training's l2: 1.45205	valid_1's l2: 2.03419
[150]	training's l2: 1.44907	valid_1's l2: 2.03445
[151]	training's l2: 1.44632	valid_1's l2: 2.03462
[152]	training's l2: 1.44384	valid_1's l2: 2.03487
[153]	training's l2: 1.44122	valid_1's l2: 2.03469
[154]	training's l2: 1.4384	valid_1's l2: 2.03407
[155]	training's l2: 1.43589	valid_1's l2: 2.03469
[156]	training's l2: 1.4335	valid_1's l2: 2.03483
[157]	training's l2: 1.43117	valid_1's l2: 2.03546
[158]	training's l2: 1.42712	valid_1's l2: 2.03506
[159]	training's l2: 1.42458	valid_1's l2: 2.03505
[160]	training's l2: 1.42134	valid_1's l2: 2.03463
[161]	training's l2: 1.41898	valid_1's l2: 2.03491
[162]	training's l2: 1.41642	valid_1's l2: 2.0351
[163]	training's l2: 1.41394	valid_1's l2: 2.03552
[164]	training's l2: 1.4104	valid_1's l2: 2.0339
[165]	training's l2: 1.40796	valid_1's l2: 2.03443
[166]	training's l2: 1.40545	valid_1's l2: 2.03425
[167]	training's l2: 1.40264	valid_1's l2: 2.03438
[168]	training's l2: 1.40006	valid_1's l2: 2.0347
[169]	training's l2: 1.39746	valid_1's l2: 2.03491
[170]	training's l2: 1.39421	valid_1's l2: 2.03376
[171]	training's l2: 1.39138	valid_1's l2: 2.03279
[172]	training's l2: 1.38863	valid_1's l2: 2.03305
[173]	training's l2: 1.38598	valid_1's l2: 2.03372
[174]	training's l2: 1.38318	valid_1's l2: 2.03389
[175]	training's l2: 1.38094	valid_1's l2: 2.03389
[176]	training's l2: 1.37847	valid_1's l2: 2.03369
[177]	training's l2: 1.37583	valid_1's l2: 2.03365
[178]	training's l2: 1.37346	valid_1's l2: 2.03354
[179]	training's l2: 1.37096	valid_1's l2: 2.03358
[180]	training's l2: 1.36846	valid_1's l2: 2.03285
[181]	training's l2: 1.36613	valid_1's l2: 2.03289
[182]	training's l2: 1.3639	valid_1's l2: 2.03318
[183]	training's l2: 1.36159	valid_1's l2: 2.03349
[184]	training's l2: 1.359	valid_1's l2: 2.03319
[185]	training's l2: 1.35685	valid_1's l2: 2.03358
[186]	training's l2: 1.35483	valid_1's l2: 2.03364
[187]	training's l2: 1.35237	valid_1's l2: 2.03377
[188]	training's l2: 1.35052	valid_1's l2: 2.03353
[189]	training's l2: 1.34819	valid_1's l2: 2.03352
[190]	training's l2: 1.3458	valid_1's l2: 2.03327
[191]	training's l2: 1.34375	valid_1's l2: 2.03328
[192]	training's l2: 1.34141	valid_1's l2: 2.03394
[193]	training's l2: 1.33918	valid_1's l2: 2.03389
[194]	training's l2: 1.33668	valid_1's l2: 2.03352
[195]	training's l2: 1.33457	valid_1's l2: 2.03347
[196]	training's l2: 1.33235	valid_1's l2: 2.03363
[197]	training's l2: 1.33032	valid_1's l2: 2.0335
Did not meet early stopping. Best iteration is:
[197]	training's l2: 1.33032	valid_1's l2: 2.0335
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.169192 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59471	valid_1's l2: 2.64888
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.55367	valid_1's l2: 2.60922
[3]	training's l2: 2.51406	valid_1's l2: 2.57124
[4]	training's l2: 2.47851	valid_1's l2: 2.53812
[5]	training's l2: 2.44423	valid_1's l2: 2.50662
[6]	training's l2: 2.41344	valid_1's l2: 2.47784
[7]	training's l2: 2.38301	valid_1's l2: 2.45029
[8]	training's l2: 2.35557	valid_1's l2: 2.42553
[9]	training's l2: 2.32893	valid_1's l2: 2.40198
[10]	training's l2: 2.30334	valid_1's l2: 2.37946
[11]	training's l2: 2.28138	valid_1's l2: 2.35861
[12]	training's l2: 2.259	valid_1's l2: 2.34033
[13]	training's l2: 2.23865	valid_1's l2: 2.32277
[14]	training's l2: 2.2191	valid_1's l2: 2.30776
[15]	training's l2: 2.2001	valid_1's l2: 2.29158
[16]	training's l2: 2.18216	valid_1's l2: 2.27808
[17]	training's l2: 2.16525	valid_1's l2: 2.26564
[18]	training's l2: 2.14995	valid_1's l2: 2.25422
[19]	training's l2: 2.13486	valid_1's l2: 2.2439
[20]	training's l2: 2.11996	valid_1's l2: 2.23336
[21]	training's l2: 2.10606	valid_1's l2: 2.22412
[22]	training's l2: 2.09322	valid_1's l2: 2.21537
[23]	training's l2: 2.0801	valid_1's l2: 2.20603
[24]	training's l2: 2.06791	valid_1's l2: 2.19771
[25]	training's l2: 2.05587	valid_1's l2: 2.19011
[26]	training's l2: 2.04479	valid_1's l2: 2.18332
[27]	training's l2: 2.03421	valid_1's l2: 2.17727
[28]	training's l2: 2.02333	valid_1's l2: 2.17046
[29]	training's l2: 2.01315	valid_1's l2: 2.1641
[30]	training's l2: 2.00373	valid_1's l2: 2.15833
[31]	training's l2: 1.99462	valid_1's l2: 2.15428
[32]	training's l2: 1.98493	valid_1's l2: 2.14966
[33]	training's l2: 1.9761	valid_1's l2: 2.1447
[34]	training's l2: 1.96792	valid_1's l2: 2.14171
[35]	training's l2: 1.95909	valid_1's l2: 2.13681
[36]	training's l2: 1.95006	valid_1's l2: 2.13229
[37]	training's l2: 1.94238	valid_1's l2: 2.12902
[38]	training's l2: 1.93444	valid_1's l2: 2.12709
[39]	training's l2: 1.9264	valid_1's l2: 2.12326
[40]	training's l2: 1.91909	valid_1's l2: 2.1205
[41]	training's l2: 1.9116	valid_1's l2: 2.11741
[42]	training's l2: 1.90413	valid_1's l2: 2.11493
[43]	training's l2: 1.89722	valid_1's l2: 2.1126
[44]	training's l2: 1.89009	valid_1's l2: 2.10936
[45]	training's l2: 1.8822	valid_1's l2: 2.10596
[46]	training's l2: 1.87511	valid_1's l2: 2.1039
[47]	training's l2: 1.86858	valid_1's l2: 2.1017
[48]	training's l2: 1.86208	valid_1's l2: 2.10001
[49]	training's l2: 1.85483	valid_1's l2: 2.09683
[50]	training's l2: 1.84894	valid_1's l2: 2.0953
[51]	training's l2: 1.84204	valid_1's l2: 2.09336
[52]	training's l2: 1.83537	valid_1's l2: 2.09113
[53]	training's l2: 1.82881	valid_1's l2: 2.08888
[54]	training's l2: 1.82253	valid_1's l2: 2.08638
[55]	training's l2: 1.81703	valid_1's l2: 2.08557
[56]	training's l2: 1.81178	valid_1's l2: 2.08517
[57]	training's l2: 1.80535	valid_1's l2: 2.08257
[58]	training's l2: 1.79947	valid_1's l2: 2.08059
[59]	training's l2: 1.79374	valid_1's l2: 2.07934
[60]	training's l2: 1.78819	valid_1's l2: 2.07827
[61]	training's l2: 1.78266	valid_1's l2: 2.07667
[62]	training's l2: 1.77647	valid_1's l2: 2.07389
[63]	training's l2: 1.7717	valid_1's l2: 2.07238
[64]	training's l2: 1.76498	valid_1's l2: 2.07021
[65]	training's l2: 1.76003	valid_1's l2: 2.06895
[66]	training's l2: 1.75352	valid_1's l2: 2.06611
[67]	training's l2: 1.74846	valid_1's l2: 2.06511
[68]	training's l2: 1.74352	valid_1's l2: 2.064
[69]	training's l2: 1.73841	valid_1's l2: 2.06317
[70]	training's l2: 1.73372	valid_1's l2: 2.06341
[71]	training's l2: 1.72774	valid_1's l2: 2.06139
[72]	training's l2: 1.72309	valid_1's l2: 2.06091
[73]	training's l2: 1.71864	valid_1's l2: 2.06009
[74]	training's l2: 1.71289	valid_1's l2: 2.05883
[75]	training's l2: 1.70823	valid_1's l2: 2.05852
[76]	training's l2: 1.70255	valid_1's l2: 2.05691
[77]	training's l2: 1.69816	valid_1's l2: 2.05637
[78]	training's l2: 1.693	valid_1's l2: 2.05605
[79]	training's l2: 1.68883	valid_1's l2: 2.05574
[80]	training's l2: 1.6831	valid_1's l2: 2.05462
[81]	training's l2: 1.67776	valid_1's l2: 2.05468
[82]	training's l2: 1.67321	valid_1's l2: 2.05392
[83]	training's l2: 1.66854	valid_1's l2: 2.05336
[84]	training's l2: 1.66403	valid_1's l2: 2.05228
[85]	training's l2: 1.6593	valid_1's l2: 2.05117
[86]	training's l2: 1.65497	valid_1's l2: 2.05085
[87]	training's l2: 1.64979	valid_1's l2: 2.05002
[88]	training's l2: 1.64572	valid_1's l2: 2.04954
[89]	training's l2: 1.64157	valid_1's l2: 2.04847
[90]	training's l2: 1.63716	valid_1's l2: 2.04828
[91]	training's l2: 1.63212	valid_1's l2: 2.04761
[92]	training's l2: 1.62839	valid_1's l2: 2.04731
[93]	training's l2: 1.62416	valid_1's l2: 2.04667
[94]	training's l2: 1.62007	valid_1's l2: 2.04592
[95]	training's l2: 1.61557	valid_1's l2: 2.0448
[96]	training's l2: 1.61155	valid_1's l2: 2.0446
[97]	training's l2: 1.6079	valid_1's l2: 2.04447
[98]	training's l2: 1.60414	valid_1's l2: 2.04436
[99]	training's l2: 1.59931	valid_1's l2: 2.04182
[100]	training's l2: 1.59561	valid_1's l2: 2.04142
[101]	training's l2: 1.59167	valid_1's l2: 2.04117
[102]	training's l2: 1.58785	valid_1's l2: 2.0399
[103]	training's l2: 1.58365	valid_1's l2: 2.03791
[104]	training's l2: 1.58025	valid_1's l2: 2.03863
[105]	training's l2: 1.57645	valid_1's l2: 2.0382
[106]	training's l2: 1.57298	valid_1's l2: 2.0385
[107]	training's l2: 1.56902	valid_1's l2: 2.03822
[108]	training's l2: 1.56528	valid_1's l2: 2.03866
[109]	training's l2: 1.56086	valid_1's l2: 2.03717
[110]	training's l2: 1.55697	valid_1's l2: 2.03659
[111]	training's l2: 1.55376	valid_1's l2: 2.03563
[112]	training's l2: 1.55032	valid_1's l2: 2.03567
[113]	training's l2: 1.54659	valid_1's l2: 2.03587
[114]	training's l2: 1.54343	valid_1's l2: 2.03599
[115]	training's l2: 1.53996	valid_1's l2: 2.03613
[116]	training's l2: 1.53669	valid_1's l2: 2.03613
[117]	training's l2: 1.53304	valid_1's l2: 2.0364
[118]	training's l2: 1.5299	valid_1's l2: 2.03663
[119]	training's l2: 1.52662	valid_1's l2: 2.03696
[120]	training's l2: 1.52356	valid_1's l2: 2.03686
[121]	training's l2: 1.52017	valid_1's l2: 2.03573
[122]	training's l2: 1.51672	valid_1's l2: 2.03533
[123]	training's l2: 1.51309	valid_1's l2: 2.03474
[124]	training's l2: 1.50965	valid_1's l2: 2.03469
[125]	training's l2: 1.50654	valid_1's l2: 2.03499
[126]	training's l2: 1.50285	valid_1's l2: 2.03484
[127]	training's l2: 1.49983	valid_1's l2: 2.0342
[128]	training's l2: 1.49646	valid_1's l2: 2.03503
[129]	training's l2: 1.49295	valid_1's l2: 2.03453
[130]	training's l2: 1.48975	valid_1's l2: 2.03532
[131]	training's l2: 1.48673	valid_1's l2: 2.0349
[132]	training's l2: 1.48346	valid_1's l2: 2.03433
[133]	training's l2: 1.48074	valid_1's l2: 2.03438
[134]	training's l2: 1.47787	valid_1's l2: 2.03479
[135]	training's l2: 1.47466	valid_1's l2: 2.03446
[136]	training's l2: 1.47149	valid_1's l2: 2.03459
[137]	training's l2: 1.46837	valid_1's l2: 2.03422
[138]	training's l2: 1.46559	valid_1's l2: 2.03414
[139]	training's l2: 1.46259	valid_1's l2: 2.03443
[140]	training's l2: 1.45956	valid_1's l2: 2.03409
[141]	training's l2: 1.45632	valid_1's l2: 2.03382
[142]	training's l2: 1.45339	valid_1's l2: 2.03357
[143]	training's l2: 1.45001	valid_1's l2: 2.03352
[144]	training's l2: 1.4471	valid_1's l2: 2.03369
[145]	training's l2: 1.44433	valid_1's l2: 2.03396
[146]	training's l2: 1.44147	valid_1's l2: 2.03356
[147]	training's l2: 1.43803	valid_1's l2: 2.03293
[148]	training's l2: 1.43512	valid_1's l2: 2.03325
[149]	training's l2: 1.4319	valid_1's l2: 2.03304
[150]	training's l2: 1.42882	valid_1's l2: 2.03276
[151]	training's l2: 1.42526	valid_1's l2: 2.03234
[152]	training's l2: 1.42219	valid_1's l2: 2.03276
[153]	training's l2: 1.41935	valid_1's l2: 2.03258
[154]	training's l2: 1.41671	valid_1's l2: 2.03242
[155]	training's l2: 1.41357	valid_1's l2: 2.03139
[156]	training's l2: 1.41092	valid_1's l2: 2.03158
[157]	training's l2: 1.40847	valid_1's l2: 2.03178
[158]	training's l2: 1.40577	valid_1's l2: 2.03171
[159]	training's l2: 1.40274	valid_1's l2: 2.03163
[160]	training's l2: 1.39986	valid_1's l2: 2.03177
[161]	training's l2: 1.39689	valid_1's l2: 2.03145
[162]	training's l2: 1.39429	valid_1's l2: 2.03202
[163]	training's l2: 1.39129	valid_1's l2: 2.03246
[164]	training's l2: 1.38889	valid_1's l2: 2.03255
[165]	training's l2: 1.38598	valid_1's l2: 2.03214
[166]	training's l2: 1.38331	valid_1's l2: 2.03236
[167]	training's l2: 1.38053	valid_1's l2: 2.03212
[168]	training's l2: 1.37816	valid_1's l2: 2.03198
[169]	training's l2: 1.37562	valid_1's l2: 2.03173
[170]	training's l2: 1.3731	valid_1's l2: 2.03277
[171]	training's l2: 1.37028	valid_1's l2: 2.03247
[172]	training's l2: 1.36751	valid_1's l2: 2.03245
[173]	training's l2: 1.36491	valid_1's l2: 2.03216
[174]	training's l2: 1.36257	valid_1's l2: 2.03246
[175]	training's l2: 1.3601	valid_1's l2: 2.03252
[176]	training's l2: 1.35781	valid_1's l2: 2.03298
[177]	training's l2: 1.35462	valid_1's l2: 2.03319
[178]	training's l2: 1.35201	valid_1's l2: 2.03266
[179]	training's l2: 1.34957	valid_1's l2: 2.03332
[180]	training's l2: 1.34725	valid_1's l2: 2.03341
[181]	training's l2: 1.34472	valid_1's l2: 2.03341
[182]	training's l2: 1.34234	valid_1's l2: 2.03349
[183]	training's l2: 1.33948	valid_1's l2: 2.03265
[184]	training's l2: 1.33678	valid_1's l2: 2.03294
[185]	training's l2: 1.3343	valid_1's l2: 2.03275
Early stopping, best iteration is:
[155]	training's l2: 1.41357	valid_1's l2: 2.03139
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172333 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59559	valid_1's l2: 2.64983
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.55512	valid_1's l2: 2.61101
[3]	training's l2: 2.51612	valid_1's l2: 2.57433
[4]	training's l2: 2.4813	valid_1's l2: 2.54108
[5]	training's l2: 2.44747	valid_1's l2: 2.50999
[6]	training's l2: 2.41683	valid_1's l2: 2.48128
[7]	training's l2: 2.38668	valid_1's l2: 2.4541
[8]	training's l2: 2.35938	valid_1's l2: 2.43011
[9]	training's l2: 2.33265	valid_1's l2: 2.4065
[10]	training's l2: 2.30903	valid_1's l2: 2.38458
[11]	training's l2: 2.28512	valid_1's l2: 2.36441
[12]	training's l2: 2.2625	valid_1's l2: 2.3454
[13]	training's l2: 2.24285	valid_1's l2: 2.32922
[14]	training's l2: 2.22343	valid_1's l2: 2.31261
[15]	training's l2: 2.20447	valid_1's l2: 2.29796
[16]	training's l2: 2.18784	valid_1's l2: 2.28444
[17]	training's l2: 2.17087	valid_1's l2: 2.27179
[18]	training's l2: 2.15443	valid_1's l2: 2.26049
[19]	training's l2: 2.1392	valid_1's l2: 2.25021
[20]	training's l2: 2.12367	valid_1's l2: 2.23961
[21]	training's l2: 2.10914	valid_1's l2: 2.22903
[22]	training's l2: 2.09575	valid_1's l2: 2.2201
[23]	training's l2: 2.0817	valid_1's l2: 2.21013
[24]	training's l2: 2.0695	valid_1's l2: 2.20228
[25]	training's l2: 2.05799	valid_1's l2: 2.19581
[26]	training's l2: 2.0465	valid_1's l2: 2.1891
[27]	training's l2: 2.03543	valid_1's l2: 2.18054
[28]	training's l2: 2.02453	valid_1's l2: 2.17532
[29]	training's l2: 2.01448	valid_1's l2: 2.16888
[30]	training's l2: 2.00441	valid_1's l2: 2.16276
[31]	training's l2: 1.99443	valid_1's l2: 2.15758
[32]	training's l2: 1.9846	valid_1's l2: 2.15196
[33]	training's l2: 1.97536	valid_1's l2: 2.14701
[34]	training's l2: 1.9664	valid_1's l2: 2.14232
[35]	training's l2: 1.958	valid_1's l2: 2.13887
[36]	training's l2: 1.94892	valid_1's l2: 2.13408
[37]	training's l2: 1.94076	valid_1's l2: 2.12929
[38]	training's l2: 1.93314	valid_1's l2: 2.12703
[39]	training's l2: 1.92566	valid_1's l2: 2.12421
[40]	training's l2: 1.91768	valid_1's l2: 2.12173
[41]	training's l2: 1.91016	valid_1's l2: 2.11955
[42]	training's l2: 1.90259	valid_1's l2: 2.11655
[43]	training's l2: 1.89539	valid_1's l2: 2.11267
[44]	training's l2: 1.88833	valid_1's l2: 2.11008
[45]	training's l2: 1.88055	valid_1's l2: 2.10747
[46]	training's l2: 1.87368	valid_1's l2: 2.10541
[47]	training's l2: 1.86683	valid_1's l2: 2.10385
[48]	training's l2: 1.86022	valid_1's l2: 2.10184
[49]	training's l2: 1.85263	valid_1's l2: 2.09896
[50]	training's l2: 1.84579	valid_1's l2: 2.09761
[51]	training's l2: 1.83866	valid_1's l2: 2.09432
[52]	training's l2: 1.83218	valid_1's l2: 2.09227
[53]	training's l2: 1.82607	valid_1's l2: 2.09105
[54]	training's l2: 1.82001	valid_1's l2: 2.08975
[55]	training's l2: 1.81354	valid_1's l2: 2.08622
[56]	training's l2: 1.80741	valid_1's l2: 2.08379
[57]	training's l2: 1.80174	valid_1's l2: 2.08331
[58]	training's l2: 1.7956	valid_1's l2: 2.08078
[59]	training's l2: 1.78927	valid_1's l2: 2.07916
[60]	training's l2: 1.78357	valid_1's l2: 2.07705
[61]	training's l2: 1.77836	valid_1's l2: 2.07627
[62]	training's l2: 1.77141	valid_1's l2: 2.07389
[63]	training's l2: 1.76564	valid_1's l2: 2.07237
[64]	training's l2: 1.76047	valid_1's l2: 2.07138
[65]	training's l2: 1.75529	valid_1's l2: 2.0697
[66]	training's l2: 1.75014	valid_1's l2: 2.06856
[67]	training's l2: 1.74535	valid_1's l2: 2.06775
[68]	training's l2: 1.73904	valid_1's l2: 2.06554
[69]	training's l2: 1.73364	valid_1's l2: 2.06482
[70]	training's l2: 1.72828	valid_1's l2: 2.06357
[71]	training's l2: 1.72201	valid_1's l2: 2.06139
[72]	training's l2: 1.71704	valid_1's l2: 2.06057
[73]	training's l2: 1.71254	valid_1's l2: 2.06001
[74]	training's l2: 1.70724	valid_1's l2: 2.05881
[75]	training's l2: 1.70211	valid_1's l2: 2.05707
[76]	training's l2: 1.69613	valid_1's l2: 2.05352
[77]	training's l2: 1.69051	valid_1's l2: 2.05165
[78]	training's l2: 1.68595	valid_1's l2: 2.05067
[79]	training's l2: 1.68167	valid_1's l2: 2.04957
[80]	training's l2: 1.67673	valid_1's l2: 2.04845
[81]	training's l2: 1.67123	valid_1's l2: 2.04698
[82]	training's l2: 1.66588	valid_1's l2: 2.04643
[83]	training's l2: 1.66119	valid_1's l2: 2.0455
[84]	training's l2: 1.65635	valid_1's l2: 2.04404
[85]	training's l2: 1.6512	valid_1's l2: 2.04363
[86]	training's l2: 1.64705	valid_1's l2: 2.04351
[87]	training's l2: 1.64295	valid_1's l2: 2.04291
[88]	training's l2: 1.6388	valid_1's l2: 2.04164
[89]	training's l2: 1.63417	valid_1's l2: 2.04112
[90]	training's l2: 1.62992	valid_1's l2: 2.04058
[91]	training's l2: 1.62602	valid_1's l2: 2.04008
[92]	training's l2: 1.62199	valid_1's l2: 2.03977
[93]	training's l2: 1.61784	valid_1's l2: 2.03936
[94]	training's l2: 1.61323	valid_1's l2: 2.03685
[95]	training's l2: 1.60936	valid_1's l2: 2.03753
[96]	training's l2: 1.6042	valid_1's l2: 2.03572
[97]	training's l2: 1.60038	valid_1's l2: 2.03536
[98]	training's l2: 1.59645	valid_1's l2: 2.03622
[99]	training's l2: 1.59271	valid_1's l2: 2.03609
[100]	training's l2: 1.58871	valid_1's l2: 2.03626
[101]	training's l2: 1.58499	valid_1's l2: 2.03565
[102]	training's l2: 1.58101	valid_1's l2: 2.03509
[103]	training's l2: 1.57735	valid_1's l2: 2.03497
[104]	training's l2: 1.57326	valid_1's l2: 2.03457
[105]	training's l2: 1.56952	valid_1's l2: 2.03429
[106]	training's l2: 1.5658	valid_1's l2: 2.03407
[107]	training's l2: 1.562	valid_1's l2: 2.03391
[108]	training's l2: 1.55732	valid_1's l2: 2.0315
[109]	training's l2: 1.55396	valid_1's l2: 2.03167
[110]	training's l2: 1.55053	valid_1's l2: 2.0313
[111]	training's l2: 1.54686	valid_1's l2: 2.03119
[112]	training's l2: 1.54361	valid_1's l2: 2.03134
[113]	training's l2: 1.53986	valid_1's l2: 2.03146
[114]	training's l2: 1.53602	valid_1's l2: 2.03076
[115]	training's l2: 1.53273	valid_1's l2: 2.0307
[116]	training's l2: 1.52948	valid_1's l2: 2.03068
[117]	training's l2: 1.52589	valid_1's l2: 2.0315
[118]	training's l2: 1.52243	valid_1's l2: 2.03236
[119]	training's l2: 1.51924	valid_1's l2: 2.03194
[120]	training's l2: 1.51605	valid_1's l2: 2.03179
[121]	training's l2: 1.51263	valid_1's l2: 2.0315
[122]	training's l2: 1.50883	valid_1's l2: 2.03147
[123]	training's l2: 1.50548	valid_1's l2: 2.03155
[124]	training's l2: 1.50188	valid_1's l2: 2.03128
[125]	training's l2: 1.49853	valid_1's l2: 2.03124
[126]	training's l2: 1.49536	valid_1's l2: 2.03129
[127]	training's l2: 1.49212	valid_1's l2: 2.03168
[128]	training's l2: 1.48881	valid_1's l2: 2.03133
[129]	training's l2: 1.48568	valid_1's l2: 2.03127
[130]	training's l2: 1.48251	valid_1's l2: 2.03099
[131]	training's l2: 1.47903	valid_1's l2: 2.03106
[132]	training's l2: 1.47608	valid_1's l2: 2.03063
[133]	training's l2: 1.47323	valid_1's l2: 2.03107
[134]	training's l2: 1.4696	valid_1's l2: 2.03103
[135]	training's l2: 1.46638	valid_1's l2: 2.03002
[136]	training's l2: 1.46311	valid_1's l2: 2.02987
[137]	training's l2: 1.46024	valid_1's l2: 2.02981
[138]	training's l2: 1.45734	valid_1's l2: 2.03019
[139]	training's l2: 1.4541	valid_1's l2: 2.0298
[140]	training's l2: 1.45129	valid_1's l2: 2.02914
[141]	training's l2: 1.44815	valid_1's l2: 2.02886
[142]	training's l2: 1.44479	valid_1's l2: 2.02841
[143]	training's l2: 1.44156	valid_1's l2: 2.02739
[144]	training's l2: 1.43807	valid_1's l2: 2.02687
[145]	training's l2: 1.43519	valid_1's l2: 2.02732
[146]	training's l2: 1.4323	valid_1's l2: 2.02833
[147]	training's l2: 1.42925	valid_1's l2: 2.02845
[148]	training's l2: 1.4264	valid_1's l2: 2.02902
[149]	training's l2: 1.42374	valid_1's l2: 2.02943
[150]	training's l2: 1.42033	valid_1's l2: 2.02938
[151]	training's l2: 1.41781	valid_1's l2: 2.0297
[152]	training's l2: 1.4152	valid_1's l2: 2.03017
[153]	training's l2: 1.41208	valid_1's l2: 2.03008
[154]	training's l2: 1.40842	valid_1's l2: 2.02948
[155]	training's l2: 1.40548	valid_1's l2: 2.02946
[156]	training's l2: 1.40273	valid_1's l2: 2.02884
[157]	training's l2: 1.40004	valid_1's l2: 2.02918
[158]	training's l2: 1.3971	valid_1's l2: 2.02864
[159]	training's l2: 1.39388	valid_1's l2: 2.02834
[160]	training's l2: 1.39108	valid_1's l2: 2.02854
[161]	training's l2: 1.38853	valid_1's l2: 2.02885
[162]	training's l2: 1.3854	valid_1's l2: 2.02904
[163]	training's l2: 1.38269	valid_1's l2: 2.02894
[164]	training's l2: 1.38	valid_1's l2: 2.02974
[165]	training's l2: 1.37729	valid_1's l2: 2.03018
[166]	training's l2: 1.37459	valid_1's l2: 2.03106
[167]	training's l2: 1.37217	valid_1's l2: 2.0311
[168]	training's l2: 1.36961	valid_1's l2: 2.03101
[169]	training's l2: 1.36696	valid_1's l2: 2.03094
[170]	training's l2: 1.36434	valid_1's l2: 2.031
[171]	training's l2: 1.36077	valid_1's l2: 2.03007
[172]	training's l2: 1.35827	valid_1's l2: 2.03031
Did not meet early stopping. Best iteration is:
[172]	training's l2: 1.35827	valid_1's l2: 2.03031
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176429 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.58619	valid_1's l2: 2.63277
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.53794	valid_1's l2: 2.58191
[3]	training's l2: 2.49578	valid_1's l2: 2.53356
[4]	training's l2: 2.45625	valid_1's l2: 2.49419
[5]	training's l2: 2.42231	valid_1's l2: 2.45838
[6]	training's l2: 2.39082	valid_1's l2: 2.42661
[7]	training's l2: 2.36274	valid_1's l2: 2.39955
[8]	training's l2: 2.33618	valid_1's l2: 2.37334
[9]	training's l2: 2.31225	valid_1's l2: 2.35085
[10]	training's l2: 2.29009	valid_1's l2: 2.32899
[11]	training's l2: 2.27009	valid_1's l2: 2.30951
[12]	training's l2: 2.25152	valid_1's l2: 2.29336
[13]	training's l2: 2.23369	valid_1's l2: 2.2762
[14]	training's l2: 2.21815	valid_1's l2: 2.26256
[15]	training's l2: 2.20326	valid_1's l2: 2.25038
[16]	training's l2: 2.18826	valid_1's l2: 2.23799
[17]	training's l2: 2.17502	valid_1's l2: 2.22672
[18]	training's l2: 2.16276	valid_1's l2: 2.21568
[19]	training's l2: 2.15147	valid_1's l2: 2.20711
[20]	training's l2: 2.14051	valid_1's l2: 2.19727
[21]	training's l2: 2.12942	valid_1's l2: 2.18843
[22]	training's l2: 2.11984	valid_1's l2: 2.18057
[23]	training's l2: 2.11005	valid_1's l2: 2.17388
[24]	training's l2: 2.10074	valid_1's l2: 2.16765
[25]	training's l2: 2.09264	valid_1's l2: 2.16165
[26]	training's l2: 2.08454	valid_1's l2: 2.15603
[27]	training's l2: 2.07726	valid_1's l2: 2.15118
[28]	training's l2: 2.06891	valid_1's l2: 2.14586
[29]	training's l2: 2.0614	valid_1's l2: 2.14088
[30]	training's l2: 2.05417	valid_1's l2: 2.13544
[31]	training's l2: 2.04564	valid_1's l2: 2.12924
[32]	training's l2: 2.03942	valid_1's l2: 2.12629
[33]	training's l2: 2.0333	valid_1's l2: 2.12273
[34]	training's l2: 2.02624	valid_1's l2: 2.11826
[35]	training's l2: 2.02009	valid_1's l2: 2.1146
[36]	training's l2: 2.01342	valid_1's l2: 2.11138
[37]	training's l2: 2.00677	valid_1's l2: 2.10651
[38]	training's l2: 2.00113	valid_1's l2: 2.10279
[39]	training's l2: 1.99471	valid_1's l2: 2.10006
[40]	training's l2: 1.989	valid_1's l2: 2.09715
[41]	training's l2: 1.98288	valid_1's l2: 2.0932
[42]	training's l2: 1.97817	valid_1's l2: 2.09131
[43]	training's l2: 1.97336	valid_1's l2: 2.0887
[44]	training's l2: 1.96808	valid_1's l2: 2.08635
[45]	training's l2: 1.96336	valid_1's l2: 2.08417
[46]	training's l2: 1.95817	valid_1's l2: 2.0834
[47]	training's l2: 1.95249	valid_1's l2: 2.08029
[48]	training's l2: 1.94752	valid_1's l2: 2.07846
[49]	training's l2: 1.94083	valid_1's l2: 2.0758
[50]	training's l2: 1.93669	valid_1's l2: 2.07465
[51]	training's l2: 1.9328	valid_1's l2: 2.07485
[52]	training's l2: 1.92817	valid_1's l2: 2.0716
[53]	training's l2: 1.92406	valid_1's l2: 2.06956
[54]	training's l2: 1.91971	valid_1's l2: 2.06876
[55]	training's l2: 1.91567	valid_1's l2: 2.0679
[56]	training's l2: 1.91159	valid_1's l2: 2.06534
[57]	training's l2: 1.90643	valid_1's l2: 2.06322
[58]	training's l2: 1.90222	valid_1's l2: 2.06288
[59]	training's l2: 1.89856	valid_1's l2: 2.06235
[60]	training's l2: 1.89499	valid_1's l2: 2.06098
[61]	training's l2: 1.89049	valid_1's l2: 2.05911
[62]	training's l2: 1.88514	valid_1's l2: 2.05661
[63]	training's l2: 1.88125	valid_1's l2: 2.05502
[64]	training's l2: 1.87707	valid_1's l2: 2.05403
[65]	training's l2: 1.87357	valid_1's l2: 2.05247
[66]	training's l2: 1.87053	valid_1's l2: 2.0526
[67]	training's l2: 1.86708	valid_1's l2: 2.05092
[68]	training's l2: 1.86343	valid_1's l2: 2.04985
[69]	training's l2: 1.85994	valid_1's l2: 2.04973
[70]	training's l2: 1.85646	valid_1's l2: 2.04957
[71]	training's l2: 1.85335	valid_1's l2: 2.04927
[72]	training's l2: 1.84987	valid_1's l2: 2.04791
[73]	training's l2: 1.84495	valid_1's l2: 2.04468
[74]	training's l2: 1.8415	valid_1's l2: 2.04478
[75]	training's l2: 1.83889	valid_1's l2: 2.04425
[76]	training's l2: 1.83467	valid_1's l2: 2.04312
[77]	training's l2: 1.83164	valid_1's l2: 2.04249
[78]	training's l2: 1.82887	valid_1's l2: 2.04202
[79]	training's l2: 1.82547	valid_1's l2: 2.04253
[80]	training's l2: 1.82184	valid_1's l2: 2.0437
[81]	training's l2: 1.81895	valid_1's l2: 2.04399
[82]	training's l2: 1.81567	valid_1's l2: 2.04251
[83]	training's l2: 1.81272	valid_1's l2: 2.04248
[84]	training's l2: 1.80988	valid_1's l2: 2.04205
[85]	training's l2: 1.80703	valid_1's l2: 2.04156
[86]	training's l2: 1.80428	valid_1's l2: 2.04109
[87]	training's l2: 1.80164	valid_1's l2: 2.04182
[88]	training's l2: 1.79878	valid_1's l2: 2.04132
[89]	training's l2: 1.79599	valid_1's l2: 2.04116
[90]	training's l2: 1.79372	valid_1's l2: 2.0412
[91]	training's l2: 1.79111	valid_1's l2: 2.0404
[92]	training's l2: 1.78853	valid_1's l2: 2.04013
[93]	training's l2: 1.78537	valid_1's l2: 2.03819
[94]	training's l2: 1.78237	valid_1's l2: 2.03926
[95]	training's l2: 1.77984	valid_1's l2: 2.03966
[96]	training's l2: 1.77771	valid_1's l2: 2.03953
[97]	training's l2: 1.77542	valid_1's l2: 2.03968
[98]	training's l2: 1.77289	valid_1's l2: 2.03965
[99]	training's l2: 1.76991	valid_1's l2: 2.03875
[100]	training's l2: 1.76623	valid_1's l2: 2.03746
[101]	training's l2: 1.76385	valid_1's l2: 2.03753
[102]	training's l2: 1.76119	valid_1's l2: 2.03705
[103]	training's l2: 1.75875	valid_1's l2: 2.03629
[104]	training's l2: 1.75639	valid_1's l2: 2.03611
[105]	training's l2: 1.75425	valid_1's l2: 2.03653
[106]	training's l2: 1.75179	valid_1's l2: 2.03683
[107]	training's l2: 1.74864	valid_1's l2: 2.03641
[108]	training's l2: 1.74646	valid_1's l2: 2.03576
[109]	training's l2: 1.74405	valid_1's l2: 2.03571
[110]	training's l2: 1.7411	valid_1's l2: 2.03553
[111]	training's l2: 1.73915	valid_1's l2: 2.03597
[112]	training's l2: 1.73689	valid_1's l2: 2.03603
[113]	training's l2: 1.73488	valid_1's l2: 2.03616
[114]	training's l2: 1.7321	valid_1's l2: 2.03479
[115]	training's l2: 1.72855	valid_1's l2: 2.03317
[116]	training's l2: 1.72649	valid_1's l2: 2.03286
[117]	training's l2: 1.72426	valid_1's l2: 2.03175
[118]	training's l2: 1.72178	valid_1's l2: 2.03135
[119]	training's l2: 1.71975	valid_1's l2: 2.03112
[120]	training's l2: 1.71755	valid_1's l2: 2.03093
[121]	training's l2: 1.71582	valid_1's l2: 2.0306
[122]	training's l2: 1.71362	valid_1's l2: 2.03034
[123]	training's l2: 1.71184	valid_1's l2: 2.03056
[124]	training's l2: 1.70963	valid_1's l2: 2.0306
[125]	training's l2: 1.70754	valid_1's l2: 2.03052
[126]	training's l2: 1.70493	valid_1's l2: 2.03018
[127]	training's l2: 1.70265	valid_1's l2: 2.02894
[128]	training's l2: 1.70031	valid_1's l2: 2.02913
[129]	training's l2: 1.69777	valid_1's l2: 2.02869
[130]	training's l2: 1.69573	valid_1's l2: 2.02874
[131]	training's l2: 1.69351	valid_1's l2: 2.02836
[132]	training's l2: 1.69132	valid_1's l2: 2.02801
[133]	training's l2: 1.68934	valid_1's l2: 2.02799
[134]	training's l2: 1.68744	valid_1's l2: 2.02811
[135]	training's l2: 1.6856	valid_1's l2: 2.02839
[136]	training's l2: 1.6835	valid_1's l2: 2.02836
[137]	training's l2: 1.68141	valid_1's l2: 2.02755
[138]	training's l2: 1.67896	valid_1's l2: 2.02768
[139]	training's l2: 1.67722	valid_1's l2: 2.02685
[140]	training's l2: 1.67543	valid_1's l2: 2.02702
[141]	training's l2: 1.67382	valid_1's l2: 2.02774
[142]	training's l2: 1.67222	valid_1's l2: 2.02794
[143]	training's l2: 1.67041	valid_1's l2: 2.02785
[144]	training's l2: 1.66813	valid_1's l2: 2.02751
[145]	training's l2: 1.6664	valid_1's l2: 2.02729
[146]	training's l2: 1.66388	valid_1's l2: 2.02714
[147]	training's l2: 1.66153	valid_1's l2: 2.02693
[148]	training's l2: 1.65959	valid_1's l2: 2.02616
[149]	training's l2: 1.65739	valid_1's l2: 2.02587
[150]	training's l2: 1.65534	valid_1's l2: 2.02582
[151]	training's l2: 1.65354	valid_1's l2: 2.02613
[152]	training's l2: 1.65148	valid_1's l2: 2.02518
[153]	training's l2: 1.64955	valid_1's l2: 2.0249
[154]	training's l2: 1.64782	valid_1's l2: 2.02495
[155]	training's l2: 1.64622	valid_1's l2: 2.02483
[156]	training's l2: 1.64422	valid_1's l2: 2.02473
[157]	training's l2: 1.64218	valid_1's l2: 2.02522
[158]	training's l2: 1.6406	valid_1's l2: 2.02498
[159]	training's l2: 1.63739	valid_1's l2: 2.02435
[160]	training's l2: 1.63549	valid_1's l2: 2.02445
[161]	training's l2: 1.63374	valid_1's l2: 2.02447
[162]	training's l2: 1.63144	valid_1's l2: 2.02386
[163]	training's l2: 1.62941	valid_1's l2: 2.02342
[164]	training's l2: 1.62718	valid_1's l2: 2.02338
[165]	training's l2: 1.62554	valid_1's l2: 2.02319
[166]	training's l2: 1.62361	valid_1's l2: 2.02317
[167]	training's l2: 1.62212	valid_1's l2: 2.02317
Did not meet early stopping. Best iteration is:
[167]	training's l2: 1.62212	valid_1's l2: 2.02317
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184018 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.56273	valid_1's l2: 2.61592
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.49384	valid_1's l2: 2.55073
[3]	training's l2: 2.43592	valid_1's l2: 2.49372
[4]	training's l2: 2.38291	valid_1's l2: 2.44432
[5]	training's l2: 2.33699	valid_1's l2: 2.40157
[6]	training's l2: 2.29417	valid_1's l2: 2.36291
[7]	training's l2: 2.2575	valid_1's l2: 2.33052
[8]	training's l2: 2.22443	valid_1's l2: 2.30282
[9]	training's l2: 2.19335	valid_1's l2: 2.27615
[10]	training's l2: 2.16637	valid_1's l2: 2.25549
[11]	training's l2: 2.14046	valid_1's l2: 2.23536
[12]	training's l2: 2.11537	valid_1's l2: 2.21766
[13]	training's l2: 2.09406	valid_1's l2: 2.2041
[14]	training's l2: 2.07432	valid_1's l2: 2.19008
[15]	training's l2: 2.05425	valid_1's l2: 2.17822
[16]	training's l2: 2.037	valid_1's l2: 2.16621
[17]	training's l2: 2.02043	valid_1's l2: 2.15626
[18]	training's l2: 2.00509	valid_1's l2: 2.14904
[19]	training's l2: 1.98994	valid_1's l2: 2.14224
[20]	training's l2: 1.97601	valid_1's l2: 2.13491
[21]	training's l2: 1.96159	valid_1's l2: 2.12827
[22]	training's l2: 1.94847	valid_1's l2: 2.12192
[23]	training's l2: 1.936	valid_1's l2: 2.1164
[24]	training's l2: 1.92407	valid_1's l2: 2.11137
[25]	training's l2: 1.91242	valid_1's l2: 2.1073
[26]	training's l2: 1.90023	valid_1's l2: 2.10167
[27]	training's l2: 1.89036	valid_1's l2: 2.09811
[28]	training's l2: 1.87967	valid_1's l2: 2.09569
[29]	training's l2: 1.86791	valid_1's l2: 2.08991
[30]	training's l2: 1.85713	valid_1's l2: 2.08547
[31]	training's l2: 1.84802	valid_1's l2: 2.08422
[32]	training's l2: 1.83752	valid_1's l2: 2.07955
[33]	training's l2: 1.82784	valid_1's l2: 2.07666
[34]	training's l2: 1.81856	valid_1's l2: 2.07339
[35]	training's l2: 1.80963	valid_1's l2: 2.07122
[36]	training's l2: 1.79811	valid_1's l2: 2.06805
[37]	training's l2: 1.78968	valid_1's l2: 2.06682
[38]	training's l2: 1.78008	valid_1's l2: 2.06487
[39]	training's l2: 1.77161	valid_1's l2: 2.06271
[40]	training's l2: 1.7642	valid_1's l2: 2.06159
[41]	training's l2: 1.75497	valid_1's l2: 2.06227
[42]	training's l2: 1.74605	valid_1's l2: 2.05901
[43]	training's l2: 1.73758	valid_1's l2: 2.05685
[44]	training's l2: 1.7293	valid_1's l2: 2.05535
[45]	training's l2: 1.71973	valid_1's l2: 2.05295
[46]	training's l2: 1.71193	valid_1's l2: 2.04929
[47]	training's l2: 1.70366	valid_1's l2: 2.04855
[48]	training's l2: 1.69699	valid_1's l2: 2.04874
[49]	training's l2: 1.69054	valid_1's l2: 2.04849
[50]	training's l2: 1.68155	valid_1's l2: 2.04373
[51]	training's l2: 1.6752	valid_1's l2: 2.04215
[52]	training's l2: 1.66876	valid_1's l2: 2.04294
[53]	training's l2: 1.66221	valid_1's l2: 2.04217
[54]	training's l2: 1.65628	valid_1's l2: 2.04171
[55]	training's l2: 1.64982	valid_1's l2: 2.03951
[56]	training's l2: 1.64165	valid_1's l2: 2.03589
[57]	training's l2: 1.63489	valid_1's l2: 2.03572
[58]	training's l2: 1.62911	valid_1's l2: 2.03625
[59]	training's l2: 1.62301	valid_1's l2: 2.03778
[60]	training's l2: 1.6177	valid_1's l2: 2.03698
[61]	training's l2: 1.61107	valid_1's l2: 2.03618
[62]	training's l2: 1.60451	valid_1's l2: 2.03612
[63]	training's l2: 1.5987	valid_1's l2: 2.0354
[64]	training's l2: 1.59294	valid_1's l2: 2.03605
[65]	training's l2: 1.58734	valid_1's l2: 2.03484
[66]	training's l2: 1.58173	valid_1's l2: 2.03359
[67]	training's l2: 1.57639	valid_1's l2: 2.03521
[68]	training's l2: 1.5703	valid_1's l2: 2.03353
[69]	training's l2: 1.565	valid_1's l2: 2.03352
[70]	training's l2: 1.55986	valid_1's l2: 2.03295
[71]	training's l2: 1.55489	valid_1's l2: 2.03323
[72]	training's l2: 1.5493	valid_1's l2: 2.03223
[73]	training's l2: 1.54437	valid_1's l2: 2.03201
[74]	training's l2: 1.53938	valid_1's l2: 2.032
[75]	training's l2: 1.53342	valid_1's l2: 2.03142
[76]	training's l2: 1.52868	valid_1's l2: 2.0311
[77]	training's l2: 1.52345	valid_1's l2: 2.03189
[78]	training's l2: 1.51859	valid_1's l2: 2.03184
[79]	training's l2: 1.51396	valid_1's l2: 2.03114
[80]	training's l2: 1.50909	valid_1's l2: 2.03089
[81]	training's l2: 1.50201	valid_1's l2: 2.03053
[82]	training's l2: 1.4972	valid_1's l2: 2.03259
[83]	training's l2: 1.49199	valid_1's l2: 2.03299
[84]	training's l2: 1.48729	valid_1's l2: 2.03239
[85]	training's l2: 1.48275	valid_1's l2: 2.03246
[86]	training's l2: 1.47647	valid_1's l2: 2.03058
[87]	training's l2: 1.47232	valid_1's l2: 2.03062
[88]	training's l2: 1.46845	valid_1's l2: 2.0323
[89]	training's l2: 1.46472	valid_1's l2: 2.03321
[90]	training's l2: 1.46022	valid_1's l2: 2.03339
[91]	training's l2: 1.45598	valid_1's l2: 2.03341
[92]	training's l2: 1.45102	valid_1's l2: 2.034
[93]	training's l2: 1.44682	valid_1's l2: 2.03418
[94]	training's l2: 1.44201	valid_1's l2: 2.03386
[95]	training's l2: 1.43786	valid_1's l2: 2.03469
[96]	training's l2: 1.43322	valid_1's l2: 2.03482
[97]	training's l2: 1.42882	valid_1's l2: 2.03438
[98]	training's l2: 1.42458	valid_1's l2: 2.03458
[99]	training's l2: 1.42016	valid_1's l2: 2.03453
[100]	training's l2: 1.41582	valid_1's l2: 2.03398
[101]	training's l2: 1.41188	valid_1's l2: 2.03401
[102]	training's l2: 1.40806	valid_1's l2: 2.03428
[103]	training's l2: 1.40391	valid_1's l2: 2.03594
[104]	training's l2: 1.39925	valid_1's l2: 2.03553
[105]	training's l2: 1.39541	valid_1's l2: 2.03524
[106]	training's l2: 1.39083	valid_1's l2: 2.03499
[107]	training's l2: 1.38602	valid_1's l2: 2.03476
[108]	training's l2: 1.38222	valid_1's l2: 2.03538
[109]	training's l2: 1.37861	valid_1's l2: 2.03618
[110]	training's l2: 1.37462	valid_1's l2: 2.03638
[111]	training's l2: 1.37116	valid_1's l2: 2.03597
Early stopping, best iteration is:
[81]	training's l2: 1.50201	valid_1's l2: 2.03053
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178639 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59692	valid_1's l2: 2.64749
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.55701	valid_1's l2: 2.60724
[3]	training's l2: 2.52085	valid_1's l2: 2.56905
[4]	training's l2: 2.48798	valid_1's l2: 2.53457
[5]	training's l2: 2.45595	valid_1's l2: 2.50351
[6]	training's l2: 2.42793	valid_1's l2: 2.47421
[7]	training's l2: 2.40061	valid_1's l2: 2.44711
[8]	training's l2: 2.37634	valid_1's l2: 2.42286
[9]	training's l2: 2.35279	valid_1's l2: 2.40078
[10]	training's l2: 2.33001	valid_1's l2: 2.37828
[11]	training's l2: 2.31045	valid_1's l2: 2.35935
[12]	training's l2: 2.29115	valid_1's l2: 2.34032
[13]	training's l2: 2.2732	valid_1's l2: 2.32393
[14]	training's l2: 2.25584	valid_1's l2: 2.30897
[15]	training's l2: 2.23975	valid_1's l2: 2.29527
[16]	training's l2: 2.22386	valid_1's l2: 2.2819
[17]	training's l2: 2.20955	valid_1's l2: 2.27056
[18]	training's l2: 2.19603	valid_1's l2: 2.25997
[19]	training's l2: 2.18257	valid_1's l2: 2.24834
[20]	training's l2: 2.17	valid_1's l2: 2.23844
[21]	training's l2: 2.15855	valid_1's l2: 2.23008
[22]	training's l2: 2.14772	valid_1's l2: 2.22273
[23]	training's l2: 2.137	valid_1's l2: 2.21456
[24]	training's l2: 2.12641	valid_1's l2: 2.20648
[25]	training's l2: 2.11585	valid_1's l2: 2.19878
[26]	training's l2: 2.10635	valid_1's l2: 2.19106
[27]	training's l2: 2.09725	valid_1's l2: 2.18448
[28]	training's l2: 2.08893	valid_1's l2: 2.17792
[29]	training's l2: 2.08018	valid_1's l2: 2.1713
[30]	training's l2: 2.07169	valid_1's l2: 2.16629
[31]	training's l2: 2.06384	valid_1's l2: 2.16155
[32]	training's l2: 2.05616	valid_1's l2: 2.15674
[33]	training's l2: 2.04832	valid_1's l2: 2.15163
[34]	training's l2: 2.04076	valid_1's l2: 2.14635
[35]	training's l2: 2.03392	valid_1's l2: 2.14236
[36]	training's l2: 2.0269	valid_1's l2: 2.13839
[37]	training's l2: 2.0201	valid_1's l2: 2.13336
[38]	training's l2: 2.01387	valid_1's l2: 2.13018
[39]	training's l2: 2.00787	valid_1's l2: 2.12795
[40]	training's l2: 2.0018	valid_1's l2: 2.12504
[41]	training's l2: 1.99526	valid_1's l2: 2.12175
[42]	training's l2: 1.98946	valid_1's l2: 2.11898
[43]	training's l2: 1.98354	valid_1's l2: 2.11633
[44]	training's l2: 1.97798	valid_1's l2: 2.11394
[45]	training's l2: 1.97199	valid_1's l2: 2.11105
[46]	training's l2: 1.9667	valid_1's l2: 2.10802
[47]	training's l2: 1.96048	valid_1's l2: 2.10527
[48]	training's l2: 1.95489	valid_1's l2: 2.10222
[49]	training's l2: 1.94901	valid_1's l2: 2.09872
[50]	training's l2: 1.94419	valid_1's l2: 2.09925
[51]	training's l2: 1.93918	valid_1's l2: 2.09703
[52]	training's l2: 1.93346	valid_1's l2: 2.09425
[53]	training's l2: 1.929	valid_1's l2: 2.09297
[54]	training's l2: 1.92352	valid_1's l2: 2.08988
[55]	training's l2: 1.91925	valid_1's l2: 2.08836
[56]	training's l2: 1.91478	valid_1's l2: 2.0865
[57]	training's l2: 1.91047	valid_1's l2: 2.08464
[58]	training's l2: 1.90498	valid_1's l2: 2.08176
[59]	training's l2: 1.90067	valid_1's l2: 2.08033
[60]	training's l2: 1.89641	valid_1's l2: 2.07886
[61]	training's l2: 1.89162	valid_1's l2: 2.07591
[62]	training's l2: 1.88766	valid_1's l2: 2.07478
[63]	training's l2: 1.88356	valid_1's l2: 2.0743
[64]	training's l2: 1.87789	valid_1's l2: 2.07167
[65]	training's l2: 1.87376	valid_1's l2: 2.06947
[66]	training's l2: 1.86918	valid_1's l2: 2.06732
[67]	training's l2: 1.86373	valid_1's l2: 2.06497
[68]	training's l2: 1.85999	valid_1's l2: 2.06463
[69]	training's l2: 1.85526	valid_1's l2: 2.06364
[70]	training's l2: 1.85056	valid_1's l2: 2.06269
[71]	training's l2: 1.84697	valid_1's l2: 2.06245
[72]	training's l2: 1.84288	valid_1's l2: 2.06122
[73]	training's l2: 1.83945	valid_1's l2: 2.06023
[74]	training's l2: 1.83423	valid_1's l2: 2.05752
[75]	training's l2: 1.83022	valid_1's l2: 2.05698
[76]	training's l2: 1.82707	valid_1's l2: 2.05594
[77]	training's l2: 1.82369	valid_1's l2: 2.05513
[78]	training's l2: 1.81967	valid_1's l2: 2.05431
[79]	training's l2: 1.8159	valid_1's l2: 2.05289
[80]	training's l2: 1.8127	valid_1's l2: 2.05279
[81]	training's l2: 1.80908	valid_1's l2: 2.05115
[82]	training's l2: 1.8046	valid_1's l2: 2.04989
[83]	training's l2: 1.80079	valid_1's l2: 2.04975
[84]	training's l2: 1.79756	valid_1's l2: 2.04925
[85]	training's l2: 1.79396	valid_1's l2: 2.04751
[86]	training's l2: 1.7901	valid_1's l2: 2.04658
[87]	training's l2: 1.78697	valid_1's l2: 2.04617
[88]	training's l2: 1.78238	valid_1's l2: 2.04414
[89]	training's l2: 1.77888	valid_1's l2: 2.04286
[90]	training's l2: 1.77589	valid_1's l2: 2.04268
[91]	training's l2: 1.77283	valid_1's l2: 2.04228
[92]	training's l2: 1.76958	valid_1's l2: 2.0416
[93]	training's l2: 1.76672	valid_1's l2: 2.04096
[94]	training's l2: 1.76322	valid_1's l2: 2.03909
[95]	training's l2: 1.76014	valid_1's l2: 2.03992
[96]	training's l2: 1.75698	valid_1's l2: 2.0402
[97]	training's l2: 1.75395	valid_1's l2: 2.03969
[98]	training's l2: 1.75007	valid_1's l2: 2.03871
[99]	training's l2: 1.74684	valid_1's l2: 2.03809
[100]	training's l2: 1.74377	valid_1's l2: 2.03731
[101]	training's l2: 1.74073	valid_1's l2: 2.03721
[102]	training's l2: 1.73784	valid_1's l2: 2.03704
[103]	training's l2: 1.73544	valid_1's l2: 2.03665
[104]	training's l2: 1.73286	valid_1's l2: 2.0368
[105]	training's l2: 1.72913	valid_1's l2: 2.0344
[106]	training's l2: 1.72617	valid_1's l2: 2.03383
[107]	training's l2: 1.72358	valid_1's l2: 2.03382
[108]	training's l2: 1.72072	valid_1's l2: 2.03334
[109]	training's l2: 1.71789	valid_1's l2: 2.03324
[110]	training's l2: 1.71528	valid_1's l2: 2.0324
[111]	training's l2: 1.71225	valid_1's l2: 2.03225
[112]	training's l2: 1.70968	valid_1's l2: 2.03223
[113]	training's l2: 1.70696	valid_1's l2: 2.0323
[114]	training's l2: 1.70402	valid_1's l2: 2.03244
[115]	training's l2: 1.70136	valid_1's l2: 2.03214
[116]	training's l2: 1.69825	valid_1's l2: 2.03195
[117]	training's l2: 1.69549	valid_1's l2: 2.03191
[118]	training's l2: 1.69308	valid_1's l2: 2.03129
[119]	training's l2: 1.69058	valid_1's l2: 2.03105
[120]	training's l2: 1.68813	valid_1's l2: 2.03078
[121]	training's l2: 1.68505	valid_1's l2: 2.03166
[122]	training's l2: 1.68243	valid_1's l2: 2.03135
[123]	training's l2: 1.67964	valid_1's l2: 2.03106
[124]	training's l2: 1.67712	valid_1's l2: 2.03121
[125]	training's l2: 1.67455	valid_1's l2: 2.03057
[126]	training's l2: 1.67195	valid_1's l2: 2.03075
[127]	training's l2: 1.66946	valid_1's l2: 2.03086
[128]	training's l2: 1.66725	valid_1's l2: 2.03086
[129]	training's l2: 1.66453	valid_1's l2: 2.0311
[130]	training's l2: 1.66158	valid_1's l2: 2.03109
[131]	training's l2: 1.65936	valid_1's l2: 2.03108
[132]	training's l2: 1.65676	valid_1's l2: 2.03024
[133]	training's l2: 1.65415	valid_1's l2: 2.03174
[134]	training's l2: 1.65193	valid_1's l2: 2.03201
[135]	training's l2: 1.64953	valid_1's l2: 2.03206
[136]	training's l2: 1.64664	valid_1's l2: 2.03194
[137]	training's l2: 1.64446	valid_1's l2: 2.03196
[138]	training's l2: 1.64217	valid_1's l2: 2.03197
[139]	training's l2: 1.63932	valid_1's l2: 2.03129
[140]	training's l2: 1.63689	valid_1's l2: 2.03157
[141]	training's l2: 1.63474	valid_1's l2: 2.0311
[142]	training's l2: 1.6322	valid_1's l2: 2.0311
[143]	training's l2: 1.63025	valid_1's l2: 2.03106
[144]	training's l2: 1.62779	valid_1's l2: 2.03087
[145]	training's l2: 1.62558	valid_1's l2: 2.03092
[146]	training's l2: 1.62358	valid_1's l2: 2.03083
[147]	training's l2: 1.6215	valid_1's l2: 2.03104
[148]	training's l2: 1.61925	valid_1's l2: 2.03082
[149]	training's l2: 1.61707	valid_1's l2: 2.03064
[150]	training's l2: 1.61447	valid_1's l2: 2.03073
[151]	training's l2: 1.61226	valid_1's l2: 2.03033
[152]	training's l2: 1.60981	valid_1's l2: 2.03052
[153]	training's l2: 1.60776	valid_1's l2: 2.03102
[154]	training's l2: 1.60556	valid_1's l2: 2.03023
[155]	training's l2: 1.60351	valid_1's l2: 2.03018
[156]	training's l2: 1.60161	valid_1's l2: 2.0306
[157]	training's l2: 1.59972	valid_1's l2: 2.03074
[158]	training's l2: 1.59786	valid_1's l2: 2.03118
[159]	training's l2: 1.59456	valid_1's l2: 2.03058
[160]	training's l2: 1.5915	valid_1's l2: 2.02937
[161]	training's l2: 1.58886	valid_1's l2: 2.02933
[162]	training's l2: 1.58702	valid_1's l2: 2.02965
[163]	training's l2: 1.58517	valid_1's l2: 2.0306
[164]	training's l2: 1.58332	valid_1's l2: 2.03045
[165]	training's l2: 1.58136	valid_1's l2: 2.0306
[166]	training's l2: 1.57933	valid_1's l2: 2.03061
[167]	training's l2: 1.57635	valid_1's l2: 2.02966
[168]	training's l2: 1.57432	valid_1's l2: 2.02956
[169]	training's l2: 1.57257	valid_1's l2: 2.02999
[170]	training's l2: 1.57067	valid_1's l2: 2.03032
[171]	training's l2: 1.56853	valid_1's l2: 2.03034
[172]	training's l2: 1.56662	valid_1's l2: 2.03042
[173]	training's l2: 1.56495	valid_1's l2: 2.03064
[174]	training's l2: 1.56292	valid_1's l2: 2.03069
[175]	training's l2: 1.56124	valid_1's l2: 2.03017
[176]	training's l2: 1.55904	valid_1's l2: 2.02995
[177]	training's l2: 1.55721	valid_1's l2: 2.02999
[178]	training's l2: 1.55508	valid_1's l2: 2.02973
[179]	training's l2: 1.55321	valid_1's l2: 2.02977
[180]	training's l2: 1.55139	valid_1's l2: 2.02956
[181]	training's l2: 1.54968	valid_1's l2: 2.02933
[182]	training's l2: 1.54751	valid_1's l2: 2.02901
[183]	training's l2: 1.54572	valid_1's l2: 2.02894
[184]	training's l2: 1.5436	valid_1's l2: 2.02899
[185]	training's l2: 1.54139	valid_1's l2: 2.02885
[186]	training's l2: 1.53974	valid_1's l2: 2.02929
[187]	training's l2: 1.5378	valid_1's l2: 2.02976
[188]	training's l2: 1.53584	valid_1's l2: 2.02981
[189]	training's l2: 1.53393	valid_1's l2: 2.02963
[190]	training's l2: 1.53174	valid_1's l2: 2.02886
Did not meet early stopping. Best iteration is:
[190]	training's l2: 1.53174	valid_1's l2: 2.02886
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177488 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.57311	valid_1's l2: 2.62095
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.51422	valid_1's l2: 2.56168
[3]	training's l2: 2.46397	valid_1's l2: 2.50747
[4]	training's l2: 2.41814	valid_1's l2: 2.46213
[5]	training's l2: 2.37948	valid_1's l2: 2.42202
[6]	training's l2: 2.34549	valid_1's l2: 2.3858
[7]	training's l2: 2.31225	valid_1's l2: 2.3533
[8]	training's l2: 2.28415	valid_1's l2: 2.32794
[9]	training's l2: 2.25817	valid_1's l2: 2.30467
[10]	training's l2: 2.23449	valid_1's l2: 2.28327
[11]	training's l2: 2.21315	valid_1's l2: 2.26518
[12]	training's l2: 2.19348	valid_1's l2: 2.24936
[13]	training's l2: 2.17576	valid_1's l2: 2.23589
[14]	training's l2: 2.15829	valid_1's l2: 2.22024
[15]	training's l2: 2.14346	valid_1's l2: 2.20922
[16]	training's l2: 2.12829	valid_1's l2: 2.19853
[17]	training's l2: 2.11525	valid_1's l2: 2.18583
[18]	training's l2: 2.10289	valid_1's l2: 2.17725
[19]	training's l2: 2.09026	valid_1's l2: 2.16795
[20]	training's l2: 2.07858	valid_1's l2: 2.16059
[21]	training's l2: 2.06688	valid_1's l2: 2.15198
[22]	training's l2: 2.05644	valid_1's l2: 2.14555
[23]	training's l2: 2.04639	valid_1's l2: 2.13907
[24]	training's l2: 2.03609	valid_1's l2: 2.13416
[25]	training's l2: 2.02713	valid_1's l2: 2.13065
[26]	training's l2: 2.01839	valid_1's l2: 2.12698
[27]	training's l2: 2.00983	valid_1's l2: 2.12267
[28]	training's l2: 2.00049	valid_1's l2: 2.11915
[29]	training's l2: 1.99169	valid_1's l2: 2.11496
[30]	training's l2: 1.98377	valid_1's l2: 2.11045
[31]	training's l2: 1.97563	valid_1's l2: 2.10542
[32]	training's l2: 1.96662	valid_1's l2: 2.1011
[33]	training's l2: 1.96035	valid_1's l2: 2.09904
[34]	training's l2: 1.9521	valid_1's l2: 2.09477
[35]	training's l2: 1.94558	valid_1's l2: 2.09223
[36]	training's l2: 1.938	valid_1's l2: 2.0881
[37]	training's l2: 1.93205	valid_1's l2: 2.087
[38]	training's l2: 1.92607	valid_1's l2: 2.08547
[39]	training's l2: 1.91873	valid_1's l2: 2.08268
[40]	training's l2: 1.91337	valid_1's l2: 2.07995
[41]	training's l2: 1.90489	valid_1's l2: 2.07527
[42]	training's l2: 1.89946	valid_1's l2: 2.07234
[43]	training's l2: 1.89294	valid_1's l2: 2.06896
[44]	training's l2: 1.88708	valid_1's l2: 2.06621
[45]	training's l2: 1.88022	valid_1's l2: 2.065
[46]	training's l2: 1.87451	valid_1's l2: 2.06485
[47]	training's l2: 1.86964	valid_1's l2: 2.06354
[48]	training's l2: 1.8644	valid_1's l2: 2.06086
[49]	training's l2: 1.85732	valid_1's l2: 2.05811
[50]	training's l2: 1.85228	valid_1's l2: 2.05818
[51]	training's l2: 1.84783	valid_1's l2: 2.05814
[52]	training's l2: 1.84284	valid_1's l2: 2.05817
[53]	training's l2: 1.83802	valid_1's l2: 2.05678
[54]	training's l2: 1.83195	valid_1's l2: 2.05506
[55]	training's l2: 1.82713	valid_1's l2: 2.05403
[56]	training's l2: 1.82243	valid_1's l2: 2.05336
[57]	training's l2: 1.81837	valid_1's l2: 2.05406
[58]	training's l2: 1.81384	valid_1's l2: 2.053
[59]	training's l2: 1.80945	valid_1's l2: 2.05134
[60]	training's l2: 1.80479	valid_1's l2: 2.05146
[61]	training's l2: 1.80087	valid_1's l2: 2.05169
[62]	training's l2: 1.79661	valid_1's l2: 2.05029
[63]	training's l2: 1.79288	valid_1's l2: 2.0509
[64]	training's l2: 1.78843	valid_1's l2: 2.05145
[65]	training's l2: 1.78228	valid_1's l2: 2.04834
[66]	training's l2: 1.77664	valid_1's l2: 2.04711
[67]	training's l2: 1.77249	valid_1's l2: 2.04759
[68]	training's l2: 1.76852	valid_1's l2: 2.04725
[69]	training's l2: 1.76389	valid_1's l2: 2.04661
[70]	training's l2: 1.75981	valid_1's l2: 2.04765
[71]	training's l2: 1.75621	valid_1's l2: 2.04817
[72]	training's l2: 1.7502	valid_1's l2: 2.04566
[73]	training's l2: 1.74696	valid_1's l2: 2.04512
[74]	training's l2: 1.74364	valid_1's l2: 2.04605
[75]	training's l2: 1.73945	valid_1's l2: 2.04431
[76]	training's l2: 1.73564	valid_1's l2: 2.04488
[77]	training's l2: 1.73254	valid_1's l2: 2.04519
[78]	training's l2: 1.72926	valid_1's l2: 2.04447
[79]	training's l2: 1.72509	valid_1's l2: 2.04377
[80]	training's l2: 1.72181	valid_1's l2: 2.04417
[81]	training's l2: 1.7183	valid_1's l2: 2.04413
[82]	training's l2: 1.71475	valid_1's l2: 2.04272
[83]	training's l2: 1.71134	valid_1's l2: 2.04283
[84]	training's l2: 1.7057	valid_1's l2: 2.04176
[85]	training's l2: 1.70284	valid_1's l2: 2.04146
[86]	training's l2: 1.69921	valid_1's l2: 2.04067
[87]	training's l2: 1.696	valid_1's l2: 2.04012
[88]	training's l2: 1.69194	valid_1's l2: 2.04172
[89]	training's l2: 1.68905	valid_1's l2: 2.0418
[90]	training's l2: 1.68541	valid_1's l2: 2.04239
[91]	training's l2: 1.68154	valid_1's l2: 2.04196
[92]	training's l2: 1.67835	valid_1's l2: 2.04175
[93]	training's l2: 1.67452	valid_1's l2: 2.04114
[94]	training's l2: 1.67141	valid_1's l2: 2.04073
[95]	training's l2: 1.66786	valid_1's l2: 2.03964
[96]	training's l2: 1.66475	valid_1's l2: 2.03937
[97]	training's l2: 1.66173	valid_1's l2: 2.03975
[98]	training's l2: 1.65852	valid_1's l2: 2.04022
[99]	training's l2: 1.65578	valid_1's l2: 2.04087
[100]	training's l2: 1.65269	valid_1's l2: 2.04017
[101]	training's l2: 1.64957	valid_1's l2: 2.03922
[102]	training's l2: 1.64699	valid_1's l2: 2.03942
[103]	training's l2: 1.64447	valid_1's l2: 2.03792
[104]	training's l2: 1.64118	valid_1's l2: 2.03769
[105]	training's l2: 1.63827	valid_1's l2: 2.03764
[106]	training's l2: 1.6356	valid_1's l2: 2.03711
[107]	training's l2: 1.63274	valid_1's l2: 2.03719
[108]	training's l2: 1.62904	valid_1's l2: 2.03936
[109]	training's l2: 1.62646	valid_1's l2: 2.04066
[110]	training's l2: 1.62367	valid_1's l2: 2.04022
[111]	training's l2: 1.62086	valid_1's l2: 2.03988
[112]	training's l2: 1.61831	valid_1's l2: 2.03964
[113]	training's l2: 1.61479	valid_1's l2: 2.03894
[114]	training's l2: 1.61219	valid_1's l2: 2.03919
[115]	training's l2: 1.60981	valid_1's l2: 2.03875
[116]	training's l2: 1.60708	valid_1's l2: 2.03911
[117]	training's l2: 1.60405	valid_1's l2: 2.03787
[118]	training's l2: 1.60071	valid_1's l2: 2.03958
[119]	training's l2: 1.59758	valid_1's l2: 2.04015
[120]	training's l2: 1.59534	valid_1's l2: 2.04106
[121]	training's l2: 1.59304	valid_1's l2: 2.04133
[122]	training's l2: 1.58983	valid_1's l2: 2.04134
[123]	training's l2: 1.58741	valid_1's l2: 2.04147
[124]	training's l2: 1.58478	valid_1's l2: 2.04067
[125]	training's l2: 1.58245	valid_1's l2: 2.04071
[126]	training's l2: 1.57976	valid_1's l2: 2.03999
[127]	training's l2: 1.57623	valid_1's l2: 2.0403
[128]	training's l2: 1.57406	valid_1's l2: 2.04035
[129]	training's l2: 1.57119	valid_1's l2: 2.03944
[130]	training's l2: 1.5686	valid_1's l2: 2.04002
[131]	training's l2: 1.56586	valid_1's l2: 2.04035
[132]	training's l2: 1.56368	valid_1's l2: 2.04024
[133]	training's l2: 1.56128	valid_1's l2: 2.04014
[134]	training's l2: 1.55842	valid_1's l2: 2.0395
[135]	training's l2: 1.55619	valid_1's l2: 2.03924
[136]	training's l2: 1.55349	valid_1's l2: 2.03923
Early stopping, best iteration is:
[106]	training's l2: 1.6356	valid_1's l2: 2.03711
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.184612 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59629	valid_1's l2: 2.6514
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.55616	valid_1's l2: 2.61467
[3]	training's l2: 2.5173	valid_1's l2: 2.5802
[4]	training's l2: 2.48196	valid_1's l2: 2.54895
[5]	training's l2: 2.44779	valid_1's l2: 2.5187
[6]	training's l2: 2.4168	valid_1's l2: 2.49151
[7]	training's l2: 2.38615	valid_1's l2: 2.46506
[8]	training's l2: 2.35844	valid_1's l2: 2.4415
[9]	training's l2: 2.33266	valid_1's l2: 2.41892
[10]	training's l2: 2.3066	valid_1's l2: 2.39674
[11]	training's l2: 2.28318	valid_1's l2: 2.37746
[12]	training's l2: 2.2595	valid_1's l2: 2.3589
[13]	training's l2: 2.23763	valid_1's l2: 2.34188
[14]	training's l2: 2.21804	valid_1's l2: 2.32644
[15]	training's l2: 2.19775	valid_1's l2: 2.31087
[16]	training's l2: 2.17876	valid_1's l2: 2.29774
[17]	training's l2: 2.161	valid_1's l2: 2.28637
[18]	training's l2: 2.1443	valid_1's l2: 2.27559
[19]	training's l2: 2.12686	valid_1's l2: 2.26341
[20]	training's l2: 2.11033	valid_1's l2: 2.25295
[21]	training's l2: 2.09501	valid_1's l2: 2.24306
[22]	training's l2: 2.08022	valid_1's l2: 2.23349
[23]	training's l2: 2.06564	valid_1's l2: 2.2251
[24]	training's l2: 2.0519	valid_1's l2: 2.21675
[25]	training's l2: 2.03782	valid_1's l2: 2.20865
[26]	training's l2: 2.02553	valid_1's l2: 2.20198
[27]	training's l2: 2.01283	valid_1's l2: 2.19453
[28]	training's l2: 2.00117	valid_1's l2: 2.18755
[29]	training's l2: 1.98984	valid_1's l2: 2.18184
[30]	training's l2: 1.97891	valid_1's l2: 2.17622
[31]	training's l2: 1.96823	valid_1's l2: 2.1698
[32]	training's l2: 1.95719	valid_1's l2: 2.16447
[33]	training's l2: 1.94675	valid_1's l2: 2.15988
[34]	training's l2: 1.93671	valid_1's l2: 2.15556
[35]	training's l2: 1.92677	valid_1's l2: 2.15023
[36]	training's l2: 1.9176	valid_1's l2: 2.14745
[37]	training's l2: 1.90735	valid_1's l2: 2.14332
[38]	training's l2: 1.89824	valid_1's l2: 2.13975
[39]	training's l2: 1.88917	valid_1's l2: 2.13562
[40]	training's l2: 1.88056	valid_1's l2: 2.13093
[41]	training's l2: 1.87235	valid_1's l2: 2.12836
[42]	training's l2: 1.86369	valid_1's l2: 2.12443
[43]	training's l2: 1.85547	valid_1's l2: 2.12045
[44]	training's l2: 1.84771	valid_1's l2: 2.11732
[45]	training's l2: 1.84009	valid_1's l2: 2.1144
[46]	training's l2: 1.83306	valid_1's l2: 2.11279
[47]	training's l2: 1.82544	valid_1's l2: 2.11027
[48]	training's l2: 1.8176	valid_1's l2: 2.107
[49]	training's l2: 1.81024	valid_1's l2: 2.10524
[50]	training's l2: 1.80306	valid_1's l2: 2.10317
[51]	training's l2: 1.79549	valid_1's l2: 2.10068
[52]	training's l2: 1.78807	valid_1's l2: 2.09851
[53]	training's l2: 1.78143	valid_1's l2: 2.09642
[54]	training's l2: 1.77479	valid_1's l2: 2.09441
[55]	training's l2: 1.76867	valid_1's l2: 2.09361
[56]	training's l2: 1.76166	valid_1's l2: 2.09072
[57]	training's l2: 1.75533	valid_1's l2: 2.08902
[58]	training's l2: 1.7483	valid_1's l2: 2.0867
[59]	training's l2: 1.74191	valid_1's l2: 2.08548
[60]	training's l2: 1.73517	valid_1's l2: 2.08267
[61]	training's l2: 1.72905	valid_1's l2: 2.08073
[62]	training's l2: 1.72251	valid_1's l2: 2.07955
[63]	training's l2: 1.71612	valid_1's l2: 2.07774
[64]	training's l2: 1.70993	valid_1's l2: 2.07561
[65]	training's l2: 1.70452	valid_1's l2: 2.07458
[66]	training's l2: 1.69893	valid_1's l2: 2.07373
[67]	training's l2: 1.6929	valid_1's l2: 2.07203
[68]	training's l2: 1.68696	valid_1's l2: 2.07109
[69]	training's l2: 1.68138	valid_1's l2: 2.06988
[70]	training's l2: 1.67557	valid_1's l2: 2.06907
[71]	training's l2: 1.67021	valid_1's l2: 2.06778
[72]	training's l2: 1.66378	valid_1's l2: 2.06559
[73]	training's l2: 1.65833	valid_1's l2: 2.06507
[74]	training's l2: 1.6522	valid_1's l2: 2.06289
[75]	training's l2: 1.64693	valid_1's l2: 2.06238
[76]	training's l2: 1.64179	valid_1's l2: 2.06193
[77]	training's l2: 1.63695	valid_1's l2: 2.06099
[78]	training's l2: 1.63101	valid_1's l2: 2.06027
[79]	training's l2: 1.62495	valid_1's l2: 2.05805
[80]	training's l2: 1.61952	valid_1's l2: 2.05682
[81]	training's l2: 1.61486	valid_1's l2: 2.05702
[82]	training's l2: 1.6096	valid_1's l2: 2.05618
[83]	training's l2: 1.60361	valid_1's l2: 2.05394
[84]	training's l2: 1.59872	valid_1's l2: 2.05339
[85]	training's l2: 1.59368	valid_1's l2: 2.05362
[86]	training's l2: 1.58935	valid_1's l2: 2.05394
[87]	training's l2: 1.58442	valid_1's l2: 2.05309
[88]	training's l2: 1.5795	valid_1's l2: 2.05219
[89]	training's l2: 1.57516	valid_1's l2: 2.0515
[90]	training's l2: 1.57046	valid_1's l2: 2.05085
[91]	training's l2: 1.5651	valid_1's l2: 2.04984
[92]	training's l2: 1.55964	valid_1's l2: 2.04896
[93]	training's l2: 1.5552	valid_1's l2: 2.04805
[94]	training's l2: 1.55086	valid_1's l2: 2.04666
[95]	training's l2: 1.54686	valid_1's l2: 2.04669
[96]	training's l2: 1.54189	valid_1's l2: 2.04588
[97]	training's l2: 1.53785	valid_1's l2: 2.0459
[98]	training's l2: 1.53265	valid_1's l2: 2.04612
[99]	training's l2: 1.52784	valid_1's l2: 2.04497
[100]	training's l2: 1.52366	valid_1's l2: 2.04425
[101]	training's l2: 1.51907	valid_1's l2: 2.04455
[102]	training's l2: 1.51436	valid_1's l2: 2.04385
[103]	training's l2: 1.50982	valid_1's l2: 2.04337
[104]	training's l2: 1.50568	valid_1's l2: 2.04362
[105]	training's l2: 1.50121	valid_1's l2: 2.04341
[106]	training's l2: 1.49724	valid_1's l2: 2.04279
[107]	training's l2: 1.493	valid_1's l2: 2.04234
[108]	training's l2: 1.48892	valid_1's l2: 2.04143
[109]	training's l2: 1.4847	valid_1's l2: 2.041
[110]	training's l2: 1.48085	valid_1's l2: 2.04059
[111]	training's l2: 1.47715	valid_1's l2: 2.04014
[112]	training's l2: 1.47289	valid_1's l2: 2.04015
[113]	training's l2: 1.46873	valid_1's l2: 2.03924
[114]	training's l2: 1.46401	valid_1's l2: 2.03737
[115]	training's l2: 1.46011	valid_1's l2: 2.03716
[116]	training's l2: 1.45634	valid_1's l2: 2.03688
[117]	training's l2: 1.4524	valid_1's l2: 2.03706
[118]	training's l2: 1.44872	valid_1's l2: 2.03682
[119]	training's l2: 1.44508	valid_1's l2: 2.03663
[120]	training's l2: 1.44122	valid_1's l2: 2.03677
[121]	training's l2: 1.43747	valid_1's l2: 2.03682
[122]	training's l2: 1.43309	valid_1's l2: 2.03535
[123]	training's l2: 1.42896	valid_1's l2: 2.03513
[124]	training's l2: 1.42544	valid_1's l2: 2.03454
[125]	training's l2: 1.42203	valid_1's l2: 2.03471
[126]	training's l2: 1.41826	valid_1's l2: 2.0345
[127]	training's l2: 1.41426	valid_1's l2: 2.03421
[128]	training's l2: 1.41079	valid_1's l2: 2.03383
[129]	training's l2: 1.40727	valid_1's l2: 2.03332
[130]	training's l2: 1.40376	valid_1's l2: 2.03318
[131]	training's l2: 1.40023	valid_1's l2: 2.03334
[132]	training's l2: 1.39653	valid_1's l2: 2.03352
[133]	training's l2: 1.39307	valid_1's l2: 2.03384
[134]	training's l2: 1.38948	valid_1's l2: 2.03388
[135]	training's l2: 1.38609	valid_1's l2: 2.03371
[136]	training's l2: 1.38299	valid_1's l2: 2.03397
[137]	training's l2: 1.37949	valid_1's l2: 2.03394
[138]	training's l2: 1.37569	valid_1's l2: 2.03454
[139]	training's l2: 1.37241	valid_1's l2: 2.03461
[140]	training's l2: 1.36919	valid_1's l2: 2.03473
[141]	training's l2: 1.36616	valid_1's l2: 2.03496
[142]	training's l2: 1.36296	valid_1's l2: 2.03451
[143]	training's l2: 1.35986	valid_1's l2: 2.03492
[144]	training's l2: 1.35639	valid_1's l2: 2.03478
[145]	training's l2: 1.3533	valid_1's l2: 2.03506
[146]	training's l2: 1.34968	valid_1's l2: 2.03398
[147]	training's l2: 1.34683	valid_1's l2: 2.03343
[148]	training's l2: 1.34359	valid_1's l2: 2.03396
[149]	training's l2: 1.34049	valid_1's l2: 2.03371
[150]	training's l2: 1.33737	valid_1's l2: 2.03377
[151]	training's l2: 1.33441	valid_1's l2: 2.03385
[152]	training's l2: 1.33033	valid_1's l2: 2.03262
[153]	training's l2: 1.32716	valid_1's l2: 2.03254
[154]	training's l2: 1.32383	valid_1's l2: 2.03208
[155]	training's l2: 1.32074	valid_1's l2: 2.03196
[156]	training's l2: 1.31789	valid_1's l2: 2.03228
[157]	training's l2: 1.31491	valid_1's l2: 2.03231
[158]	training's l2: 1.31212	valid_1's l2: 2.03301
[159]	training's l2: 1.30883	valid_1's l2: 2.03375
[160]	training's l2: 1.30546	valid_1's l2: 2.03413
[161]	training's l2: 1.30267	valid_1's l2: 2.03409
[162]	training's l2: 1.29991	valid_1's l2: 2.03346
[163]	training's l2: 1.29714	valid_1's l2: 2.03376
[164]	training's l2: 1.29356	valid_1's l2: 2.03255
[165]	training's l2: 1.29024	valid_1's l2: 2.03277
[166]	training's l2: 1.28713	valid_1's l2: 2.03278
[167]	training's l2: 1.2846	valid_1's l2: 2.03313
[168]	training's l2: 1.28123	valid_1's l2: 2.03282
[169]	training's l2: 1.27837	valid_1's l2: 2.0322
[170]	training's l2: 1.27554	valid_1's l2: 2.03242
[171]	training's l2: 1.27269	valid_1's l2: 2.03247
[172]	training's l2: 1.27007	valid_1's l2: 2.03217
[173]	training's l2: 1.26716	valid_1's l2: 2.03211
[174]	training's l2: 1.2641	valid_1's l2: 2.03165
[175]	training's l2: 1.26094	valid_1's l2: 2.03153
[176]	training's l2: 1.25804	valid_1's l2: 2.03078
Did not meet early stopping. Best iteration is:
[176]	training's l2: 1.25804	valid_1's l2: 2.03078
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179981 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.59476	valid_1's l2: 2.6489
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.55351	valid_1's l2: 2.60969
[3]	training's l2: 2.51385	valid_1's l2: 2.57234
[4]	training's l2: 2.47817	valid_1's l2: 2.53928
[5]	training's l2: 2.44381	valid_1's l2: 2.50784
[6]	training's l2: 2.41297	valid_1's l2: 2.47936
[7]	training's l2: 2.3823	valid_1's l2: 2.45265
[8]	training's l2: 2.35487	valid_1's l2: 2.42799
[9]	training's l2: 2.32763	valid_1's l2: 2.40355
[10]	training's l2: 2.30366	valid_1's l2: 2.38212
[11]	training's l2: 2.27941	valid_1's l2: 2.36195
[12]	training's l2: 2.2572	valid_1's l2: 2.34337
[13]	training's l2: 2.23577	valid_1's l2: 2.32648
[14]	training's l2: 2.21696	valid_1's l2: 2.31046
[15]	training's l2: 2.19755	valid_1's l2: 2.29446
[16]	training's l2: 2.18003	valid_1's l2: 2.28124
[17]	training's l2: 2.16287	valid_1's l2: 2.2681
[18]	training's l2: 2.14666	valid_1's l2: 2.25663
[19]	training's l2: 2.13062	valid_1's l2: 2.24562
[20]	training's l2: 2.11602	valid_1's l2: 2.2363
[21]	training's l2: 2.1017	valid_1's l2: 2.22645
[22]	training's l2: 2.08802	valid_1's l2: 2.21731
[23]	training's l2: 2.07406	valid_1's l2: 2.20706
[24]	training's l2: 2.0612	valid_1's l2: 2.19859
[25]	training's l2: 2.04921	valid_1's l2: 2.19141
[26]	training's l2: 2.03771	valid_1's l2: 2.18433
[27]	training's l2: 2.02576	valid_1's l2: 2.17746
[28]	training's l2: 2.01506	valid_1's l2: 2.16985
[29]	training's l2: 2.00469	valid_1's l2: 2.16466
[30]	training's l2: 1.99469	valid_1's l2: 2.16066
[31]	training's l2: 1.98478	valid_1's l2: 2.15528
[32]	training's l2: 1.97469	valid_1's l2: 2.14993
[33]	training's l2: 1.96545	valid_1's l2: 2.1455
[34]	training's l2: 1.95657	valid_1's l2: 2.14232
[35]	training's l2: 1.94799	valid_1's l2: 2.13793
[36]	training's l2: 1.94007	valid_1's l2: 2.135
[37]	training's l2: 1.93057	valid_1's l2: 2.13076
[38]	training's l2: 1.92213	valid_1's l2: 2.12611
[39]	training's l2: 1.9142	valid_1's l2: 2.12311
[40]	training's l2: 1.90673	valid_1's l2: 2.1199
[41]	training's l2: 1.89847	valid_1's l2: 2.1166
[42]	training's l2: 1.89017	valid_1's l2: 2.11285
[43]	training's l2: 1.88272	valid_1's l2: 2.11076
[44]	training's l2: 1.87506	valid_1's l2: 2.10737
[45]	training's l2: 1.86862	valid_1's l2: 2.1059
[46]	training's l2: 1.86137	valid_1's l2: 2.10434
[47]	training's l2: 1.85443	valid_1's l2: 2.1018
[48]	training's l2: 1.84768	valid_1's l2: 2.10075
[49]	training's l2: 1.84095	valid_1's l2: 2.09901
[50]	training's l2: 1.83462	valid_1's l2: 2.0972
[51]	training's l2: 1.82763	valid_1's l2: 2.09428
[52]	training's l2: 1.82076	valid_1's l2: 2.09327
[53]	training's l2: 1.8139	valid_1's l2: 2.09182
[54]	training's l2: 1.80809	valid_1's l2: 2.09026
[55]	training's l2: 1.80124	valid_1's l2: 2.0887
[56]	training's l2: 1.79471	valid_1's l2: 2.08673
[57]	training's l2: 1.78849	valid_1's l2: 2.08527
[58]	training's l2: 1.78185	valid_1's l2: 2.08311
[59]	training's l2: 1.77572	valid_1's l2: 2.08105
[60]	training's l2: 1.76967	valid_1's l2: 2.0801
[61]	training's l2: 1.76443	valid_1's l2: 2.07963
[62]	training's l2: 1.75896	valid_1's l2: 2.07816
[63]	training's l2: 1.75267	valid_1's l2: 2.07529
[64]	training's l2: 1.74663	valid_1's l2: 2.07349
[65]	training's l2: 1.74117	valid_1's l2: 2.07304
[66]	training's l2: 1.73601	valid_1's l2: 2.07212
[67]	training's l2: 1.73007	valid_1's l2: 2.07039
[68]	training's l2: 1.72494	valid_1's l2: 2.06902
[69]	training's l2: 1.71885	valid_1's l2: 2.06789
[70]	training's l2: 1.71409	valid_1's l2: 2.06696
[71]	training's l2: 1.70753	valid_1's l2: 2.06544
[72]	training's l2: 1.70247	valid_1's l2: 2.06534
[73]	training's l2: 1.69698	valid_1's l2: 2.06405
[74]	training's l2: 1.69239	valid_1's l2: 2.06344
[75]	training's l2: 1.68725	valid_1's l2: 2.06231
[76]	training's l2: 1.68271	valid_1's l2: 2.06155
[77]	training's l2: 1.67769	valid_1's l2: 2.06046
[78]	training's l2: 1.6734	valid_1's l2: 2.06112
[79]	training's l2: 1.66821	valid_1's l2: 2.06115
[80]	training's l2: 1.66312	valid_1's l2: 2.06139
[81]	training's l2: 1.65807	valid_1's l2: 2.0604
[82]	training's l2: 1.65397	valid_1's l2: 2.0603
[83]	training's l2: 1.64885	valid_1's l2: 2.05881
[84]	training's l2: 1.64387	valid_1's l2: 2.059
[85]	training's l2: 1.63918	valid_1's l2: 2.05884
[86]	training's l2: 1.63499	valid_1's l2: 2.05803
[87]	training's l2: 1.63089	valid_1's l2: 2.05801
[88]	training's l2: 1.62656	valid_1's l2: 2.05738
[89]	training's l2: 1.62225	valid_1's l2: 2.05684
[90]	training's l2: 1.61681	valid_1's l2: 2.05513
[91]	training's l2: 1.61247	valid_1's l2: 2.05478
[92]	training's l2: 1.60814	valid_1's l2: 2.05411
[93]	training's l2: 1.60323	valid_1's l2: 2.05384
[94]	training's l2: 1.59905	valid_1's l2: 2.05324
[95]	training's l2: 1.59517	valid_1's l2: 2.05317
[96]	training's l2: 1.59106	valid_1's l2: 2.0528
[97]	training's l2: 1.58658	valid_1's l2: 2.05063
[98]	training's l2: 1.58259	valid_1's l2: 2.05017
[99]	training's l2: 1.57846	valid_1's l2: 2.05022
[100]	training's l2: 1.57449	valid_1's l2: 2.04971
[101]	training's l2: 1.57071	valid_1's l2: 2.04997
[102]	training's l2: 1.567	valid_1's l2: 2.04994
[103]	training's l2: 1.56201	valid_1's l2: 2.04986
[104]	training's l2: 1.55853	valid_1's l2: 2.04902
[105]	training's l2: 1.55486	valid_1's l2: 2.04879
[106]	training's l2: 1.55025	valid_1's l2: 2.04726
[107]	training's l2: 1.5465	valid_1's l2: 2.04739
[108]	training's l2: 1.54245	valid_1's l2: 2.0469
[109]	training's l2: 1.53886	valid_1's l2: 2.04647
[110]	training's l2: 1.53492	valid_1's l2: 2.0459
[111]	training's l2: 1.53133	valid_1's l2: 2.04635
[112]	training's l2: 1.52817	valid_1's l2: 2.04554
[113]	training's l2: 1.52407	valid_1's l2: 2.04623
[114]	training's l2: 1.5206	valid_1's l2: 2.04579
[115]	training's l2: 1.51718	valid_1's l2: 2.0452
[116]	training's l2: 1.5137	valid_1's l2: 2.0448
[117]	training's l2: 1.51065	valid_1's l2: 2.04505
[118]	training's l2: 1.50716	valid_1's l2: 2.04544
[119]	training's l2: 1.50395	valid_1's l2: 2.0456
[120]	training's l2: 1.50014	valid_1's l2: 2.04596
[121]	training's l2: 1.49689	valid_1's l2: 2.04598
[122]	training's l2: 1.49346	valid_1's l2: 2.04542
[123]	training's l2: 1.48993	valid_1's l2: 2.04623
[124]	training's l2: 1.4862	valid_1's l2: 2.04574
[125]	training's l2: 1.48261	valid_1's l2: 2.04541
[126]	training's l2: 1.47907	valid_1's l2: 2.04516
[127]	training's l2: 1.47577	valid_1's l2: 2.04488
[128]	training's l2: 1.47253	valid_1's l2: 2.0447
[129]	training's l2: 1.46924	valid_1's l2: 2.04493
[130]	training's l2: 1.46574	valid_1's l2: 2.04499
[131]	training's l2: 1.46255	valid_1's l2: 2.04544
[132]	training's l2: 1.45942	valid_1's l2: 2.04587
[133]	training's l2: 1.45634	valid_1's l2: 2.04562
[134]	training's l2: 1.45285	valid_1's l2: 2.04488
[135]	training's l2: 1.44946	valid_1's l2: 2.04442
[136]	training's l2: 1.44654	valid_1's l2: 2.04471
[137]	training's l2: 1.44336	valid_1's l2: 2.04474
[138]	training's l2: 1.44038	valid_1's l2: 2.04486
[139]	training's l2: 1.43749	valid_1's l2: 2.04462
[140]	training's l2: 1.43439	valid_1's l2: 2.04503
Did not meet early stopping. Best iteration is:
[140]	training's l2: 1.43439	valid_1's l2: 2.04503
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179631 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.015052
[1]	training's l2: 2.587	valid_1's l2: 2.63945
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 2.53807	valid_1's l2: 2.59259
[3]	training's l2: 2.49459	valid_1's l2: 2.54933
[4]	training's l2: 2.45548	valid_1's l2: 2.50985
[5]	training's l2: 2.41747	valid_1's l2: 2.47516
[6]	training's l2: 2.38435	valid_1's l2: 2.44419
[7]	training's l2: 2.35226	valid_1's l2: 2.41546
[8]	training's l2: 2.32203	valid_1's l2: 2.38713
[9]	training's l2: 2.29587	valid_1's l2: 2.36212
[10]	training's l2: 2.27115	valid_1's l2: 2.34039
[11]	training's l2: 2.24758	valid_1's l2: 2.32139
[12]	training's l2: 2.22678	valid_1's l2: 2.30383
[13]	training's l2: 2.20582	valid_1's l2: 2.28613
[14]	training's l2: 2.18679	valid_1's l2: 2.27192
[15]	training's l2: 2.16832	valid_1's l2: 2.25806
[16]	training's l2: 2.15129	valid_1's l2: 2.24592
[17]	training's l2: 2.13584	valid_1's l2: 2.23416
[18]	training's l2: 2.12019	valid_1's l2: 2.22317
[19]	training's l2: 2.10475	valid_1's l2: 2.21126
[20]	training's l2: 2.09081	valid_1's l2: 2.2019
[21]	training's l2: 2.07781	valid_1's l2: 2.1929
[22]	training's l2: 2.06549	valid_1's l2: 2.18535
[23]	training's l2: 2.05321	valid_1's l2: 2.17774
[24]	training's l2: 2.0413	valid_1's l2: 2.17084
[25]	training's l2: 2.03047	valid_1's l2: 2.16469
[26]	training's l2: 2.02014	valid_1's l2: 2.15814
[27]	training's l2: 2.00902	valid_1's l2: 2.15164
[28]	training's l2: 1.99976	valid_1's l2: 2.14675
[29]	training's l2: 1.98978	valid_1's l2: 2.14096
[30]	training's l2: 1.98057	valid_1's l2: 2.1375
[31]	training's l2: 1.97178	valid_1's l2: 2.1332
[32]	training's l2: 1.96264	valid_1's l2: 2.12841
[33]	training's l2: 1.95445	valid_1's l2: 2.12363
[34]	training's l2: 1.9462	valid_1's l2: 2.12009
[35]	training's l2: 1.93807	valid_1's l2: 2.11784
[36]	training's l2: 1.93057	valid_1's l2: 2.11496
[37]	training's l2: 1.92205	valid_1's l2: 2.11111
[38]	training's l2: 1.91449	valid_1's l2: 2.10864
[39]	training's l2: 1.90711	valid_1's l2: 2.10574
[40]	training's l2: 1.89955	valid_1's l2: 2.10328
[41]	training's l2: 1.89246	valid_1's l2: 2.10097
[42]	training's l2: 1.88539	valid_1's l2: 2.09843
[43]	training's l2: 1.87816	valid_1's l2: 2.09747
[44]	training's l2: 1.87097	valid_1's l2: 2.09381
[45]	training's l2: 1.86247	valid_1's l2: 2.0899
[46]	training's l2: 1.8561	valid_1's l2: 2.08733
[47]	training's l2: 1.84948	valid_1's l2: 2.08482
[48]	training's l2: 1.84265	valid_1's l2: 2.0829
[49]	training's l2: 1.83593	valid_1's l2: 2.08077
[50]	training's l2: 1.82804	valid_1's l2: 2.07788
[51]	training's l2: 1.82197	valid_1's l2: 2.07618
[52]	training's l2: 1.81584	valid_1's l2: 2.07513
[53]	training's l2: 1.81015	valid_1's l2: 2.07403
[54]	training's l2: 1.80459	valid_1's l2: 2.07341
[55]	training's l2: 1.79825	valid_1's l2: 2.07088
[56]	training's l2: 1.79311	valid_1's l2: 2.07011
[57]	training's l2: 1.7877	valid_1's l2: 2.06878
[58]	training's l2: 1.78263	valid_1's l2: 2.06798
[59]	training's l2: 1.77675	valid_1's l2: 2.06603
[60]	training's l2: 1.7708	valid_1's l2: 2.06437
[61]	training's l2: 1.76385	valid_1's l2: 2.06108
[62]	training's l2: 1.75905	valid_1's l2: 2.05967
[63]	training's l2: 1.75368	valid_1's l2: 2.05865
[64]	training's l2: 1.74826	valid_1's l2: 2.05688
[65]	training's l2: 1.7418	valid_1's l2: 2.05539
[66]	training's l2: 1.73631	valid_1's l2: 2.05378
[67]	training's l2: 1.73055	valid_1's l2: 2.05323
[68]	training's l2: 1.72602	valid_1's l2: 2.05278
[69]	training's l2: 1.72164	valid_1's l2: 2.05225
[70]	training's l2: 1.71569	valid_1's l2: 2.05098
[71]	training's l2: 1.71084	valid_1's l2: 2.04968
[72]	training's l2: 1.70577	valid_1's l2: 2.04801
[73]	training's l2: 1.70012	valid_1's l2: 2.04592
[74]	training's l2: 1.69547	valid_1's l2: 2.04478
[75]	training's l2: 1.69064	valid_1's l2: 2.04543
[76]	training's l2: 1.68661	valid_1's l2: 2.04509
[77]	training's l2: 1.68228	valid_1's l2: 2.04524
[78]	training's l2: 1.67809	valid_1's l2: 2.04508
[79]	training's l2: 1.67393	valid_1's l2: 2.04543
[80]	training's l2: 1.66945	valid_1's l2: 2.04538
[81]	training's l2: 1.66559	valid_1's l2: 2.04487
[82]	training's l2: 1.66054	valid_1's l2: 2.04353
[83]	training's l2: 1.65634	valid_1's l2: 2.04327
[84]	training's l2: 1.65194	valid_1's l2: 2.04192
[85]	training's l2: 1.64665	valid_1's l2: 2.04042
[86]	training's l2: 1.6426	valid_1's l2: 2.0407
[87]	training's l2: 1.63873	valid_1's l2: 2.03953
[88]	training's l2: 1.6348	valid_1's l2: 2.03954
[89]	training's l2: 1.63062	valid_1's l2: 2.03914
[90]	training's l2: 1.62589	valid_1's l2: 2.03806
[91]	training's l2: 1.62173	valid_1's l2: 2.03818
[92]	training's l2: 1.61721	valid_1's l2: 2.03687
[93]	training's l2: 1.6128	valid_1's l2: 2.03602
[94]	training's l2: 1.60896	valid_1's l2: 2.03585
[95]	training's l2: 1.60536	valid_1's l2: 2.036
[96]	training's l2: 1.60159	valid_1's l2: 2.03653
[97]	training's l2: 1.59834	valid_1's l2: 2.0364
[98]	training's l2: 1.59475	valid_1's l2: 2.03664
[99]	training's l2: 1.59094	valid_1's l2: 2.03629
[100]	training's l2: 1.58723	valid_1's l2: 2.03572
[101]	training's l2: 1.58337	valid_1's l2: 2.03526
[102]	training's l2: 1.57933	valid_1's l2: 2.03485
[103]	training's l2: 1.57567	valid_1's l2: 2.0347
[104]	training's l2: 1.57228	valid_1's l2: 2.03448
[105]	training's l2: 1.56878	valid_1's l2: 2.03405
[106]	training's l2: 1.56514	valid_1's l2: 2.03327
[107]	training's l2: 1.56187	valid_1's l2: 2.03411
[108]	training's l2: 1.55858	valid_1's l2: 2.034
[109]	training's l2: 1.55547	valid_1's l2: 2.03433
[110]	training's l2: 1.55209	valid_1's l2: 2.03451
[111]	training's l2: 1.54802	valid_1's l2: 2.03399
[112]	training's l2: 1.54451	valid_1's l2: 2.03397
[113]	training's l2: 1.54118	valid_1's l2: 2.03317
[114]	training's l2: 1.53805	valid_1's l2: 2.03421
[115]	training's l2: 1.53502	valid_1's l2: 2.03385
[116]	training's l2: 1.53184	valid_1's l2: 2.03464
[117]	training's l2: 1.52838	valid_1's l2: 2.03424
[118]	training's l2: 1.52352	valid_1's l2: 2.03344
[119]	training's l2: 1.52035	valid_1's l2: 2.03333
[120]	training's l2: 1.51724	valid_1's l2: 2.03416
[121]	training's l2: 1.51398	valid_1's l2: 2.03339
[122]	training's l2: 1.51112	valid_1's l2: 2.03312
[123]	training's l2: 1.50794	valid_1's l2: 2.03423
[124]	training's l2: 1.50459	valid_1's l2: 2.03451
[125]	training's l2: 1.50173	valid_1's l2: 2.03501
[126]	training's l2: 1.49857	valid_1's l2: 2.03458
[127]	training's l2: 1.49576	valid_1's l2: 2.03429
[128]	training's l2: 1.49253	valid_1's l2: 2.03396
[129]	training's l2: 1.48968	valid_1's l2: 2.03418
[130]	training's l2: 1.48622	valid_1's l2: 2.03404
[131]	training's l2: 1.48312	valid_1's l2: 2.03477
[132]	training's l2: 1.47971	valid_1's l2: 2.03522
[133]	training's l2: 1.47649	valid_1's l2: 2.03533
[134]	training's l2: 1.4739	valid_1's l2: 2.03506
[135]	training's l2: 1.47041	valid_1's l2: 2.0341
[136]	training's l2: 1.46788	valid_1's l2: 2.03436
[137]	training's l2: 1.46502	valid_1's l2: 2.03441
[138]	training's l2: 1.46243	valid_1's l2: 2.03464
[139]	training's l2: 1.45932	valid_1's l2: 2.03449
[140]	training's l2: 1.45626	valid_1's l2: 2.03498
[141]	training's l2: 1.45348	valid_1's l2: 2.03428
[142]	training's l2: 1.45052	valid_1's l2: 2.03496
[143]	training's l2: 1.4476	valid_1's l2: 2.03591
[144]	training's l2: 1.44466	valid_1's l2: 2.0359
[145]	training's l2: 1.44137	valid_1's l2: 2.03532
[146]	training's l2: 1.43871	valid_1's l2: 2.03566
[147]	training's l2: 1.43631	valid_1's l2: 2.03645
[148]	training's l2: 1.43258	valid_1's l2: 2.03519
[149]	training's l2: 1.43019	valid_1's l2: 2.03569
[150]	training's l2: 1.42755	valid_1's l2: 2.03558
[151]	training's l2: 1.42441	valid_1's l2: 2.03531
[152]	training's l2: 1.42129	valid_1's l2: 2.03522
Early stopping, best iteration is:
[122]	training's l2: 1.51112	valid_1's l2: 2.03312
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185136 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000387836	valid_1's l2: 0.000377526
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000378768	valid_1's l2: 0.000369154
[3]	training's l2: 0.00037058	valid_1's l2: 0.000361704
[4]	training's l2: 0.00036322	valid_1's l2: 0.000354858
[5]	training's l2: 0.000356395	valid_1's l2: 0.000348789
[6]	training's l2: 0.000350284	valid_1's l2: 0.000343169
[7]	training's l2: 0.000344651	valid_1's l2: 0.000337939
[8]	training's l2: 0.000339426	valid_1's l2: 0.000333462
[9]	training's l2: 0.000334709	valid_1's l2: 0.000329231
[10]	training's l2: 0.000330039	valid_1's l2: 0.000325059
[11]	training's l2: 0.000326076	valid_1's l2: 0.000321749
[12]	training's l2: 0.000322259	valid_1's l2: 0.000318673
[13]	training's l2: 0.00031867	valid_1's l2: 0.00031556
[14]	training's l2: 0.000315297	valid_1's l2: 0.000312588
[15]	training's l2: 0.00031216	valid_1's l2: 0.000309839
[16]	training's l2: 0.000309201	valid_1's l2: 0.000307645
[17]	training's l2: 0.000306389	valid_1's l2: 0.000305349
[18]	training's l2: 0.00030384	valid_1's l2: 0.000303486
[19]	training's l2: 0.000301498	valid_1's l2: 0.000301823
[20]	training's l2: 0.000299198	valid_1's l2: 0.000300054
[21]	training's l2: 0.000297064	valid_1's l2: 0.000298452
[22]	training's l2: 0.000295113	valid_1's l2: 0.000296886
[23]	training's l2: 0.000293173	valid_1's l2: 0.00029556
[24]	training's l2: 0.000291352	valid_1's l2: 0.000294358
[25]	training's l2: 0.000289624	valid_1's l2: 0.000293246
[26]	training's l2: 0.000288044	valid_1's l2: 0.000292103
[27]	training's l2: 0.000286531	valid_1's l2: 0.000291076
[28]	training's l2: 0.000284951	valid_1's l2: 0.00028996
[29]	training's l2: 0.000283436	valid_1's l2: 0.000288956
[30]	training's l2: 0.000282147	valid_1's l2: 0.000288058
[31]	training's l2: 0.000280877	valid_1's l2: 0.000287392
[32]	training's l2: 0.000279574	valid_1's l2: 0.000286521
[33]	training's l2: 0.000278447	valid_1's l2: 0.000285814
[34]	training's l2: 0.000277167	valid_1's l2: 0.00028509
[35]	training's l2: 0.000275874	valid_1's l2: 0.000284409
[36]	training's l2: 0.000274816	valid_1's l2: 0.000283758
[37]	training's l2: 0.000273609	valid_1's l2: 0.000282935
[38]	training's l2: 0.000272516	valid_1's l2: 0.000282252
[39]	training's l2: 0.000271644	valid_1's l2: 0.000281725
[40]	training's l2: 0.000270624	valid_1's l2: 0.000281143
[41]	training's l2: 0.00026953	valid_1's l2: 0.000280638
[42]	training's l2: 0.000268541	valid_1's l2: 0.000280223
[43]	training's l2: 0.000267459	valid_1's l2: 0.000279558
[44]	training's l2: 0.000266474	valid_1's l2: 0.00027893
[45]	training's l2: 0.000265657	valid_1's l2: 0.000278573
[46]	training's l2: 0.0002647	valid_1's l2: 0.000278082
[47]	training's l2: 0.000263582	valid_1's l2: 0.000277458
[48]	training's l2: 0.000262677	valid_1's l2: 0.000276998
[49]	training's l2: 0.000261914	valid_1's l2: 0.000276592
[50]	training's l2: 0.000261153	valid_1's l2: 0.000276314
[51]	training's l2: 0.000260462	valid_1's l2: 0.00027601
[52]	training's l2: 0.000259522	valid_1's l2: 0.000275601
[53]	training's l2: 0.000258832	valid_1's l2: 0.000275271
[54]	training's l2: 0.000258096	valid_1's l2: 0.000274967
[55]	training's l2: 0.000257493	valid_1's l2: 0.000274757
[56]	training's l2: 0.000256586	valid_1's l2: 0.000274391
[57]	training's l2: 0.000255694	valid_1's l2: 0.000273978
[58]	training's l2: 0.000255101	valid_1's l2: 0.000273833
[59]	training's l2: 0.000254281	valid_1's l2: 0.000273399
[60]	training's l2: 0.000253698	valid_1's l2: 0.000273255
[61]	training's l2: 0.000253035	valid_1's l2: 0.000273062
[62]	training's l2: 0.00025227	valid_1's l2: 0.000272731
[63]	training's l2: 0.00025167	valid_1's l2: 0.000272575
[64]	training's l2: 0.000251082	valid_1's l2: 0.000272433
[65]	training's l2: 0.000250536	valid_1's l2: 0.000272311
[66]	training's l2: 0.00024982	valid_1's l2: 0.000271972
[67]	training's l2: 0.000249088	valid_1's l2: 0.000271785
[68]	training's l2: 0.000248511	valid_1's l2: 0.000271748
[69]	training's l2: 0.000247961	valid_1's l2: 0.000271589
[70]	training's l2: 0.000247398	valid_1's l2: 0.00027142
[71]	training's l2: 0.000246814	valid_1's l2: 0.000271247
[72]	training's l2: 0.00024621	valid_1's l2: 0.000271008
[73]	training's l2: 0.000245659	valid_1's l2: 0.000270916
[74]	training's l2: 0.000245034	valid_1's l2: 0.000270702
[75]	training's l2: 0.000244413	valid_1's l2: 0.00027044
[76]	training's l2: 0.000243837	valid_1's l2: 0.000270281
[77]	training's l2: 0.000243307	valid_1's l2: 0.000270086
[78]	training's l2: 0.000242725	valid_1's l2: 0.000269942
[79]	training's l2: 0.000242274	valid_1's l2: 0.000269874
[80]	training's l2: 0.000241822	valid_1's l2: 0.000269816
[81]	training's l2: 0.00024114	valid_1's l2: 0.000269612
[82]	training's l2: 0.000240547	valid_1's l2: 0.000269399
[83]	training's l2: 0.000240056	valid_1's l2: 0.00026926
[84]	training's l2: 0.000239563	valid_1's l2: 0.000269121
[85]	training's l2: 0.000238934	valid_1's l2: 0.000268791
[86]	training's l2: 0.000238381	valid_1's l2: 0.000268645
[87]	training's l2: 0.000237925	valid_1's l2: 0.00026862
[88]	training's l2: 0.000237511	valid_1's l2: 0.000268652
[89]	training's l2: 0.000237045	valid_1's l2: 0.000268626
[90]	training's l2: 0.000236592	valid_1's l2: 0.000268591
[91]	training's l2: 0.000236049	valid_1's l2: 0.000268392
[92]	training's l2: 0.00023566	valid_1's l2: 0.000268353
[93]	training's l2: 0.000235181	valid_1's l2: 0.000268306
[94]	training's l2: 0.000234737	valid_1's l2: 0.0002683
[95]	training's l2: 0.000234252	valid_1's l2: 0.000268185
[96]	training's l2: 0.000233798	valid_1's l2: 0.000268071
[97]	training's l2: 0.00023337	valid_1's l2: 0.000268035
[98]	training's l2: 0.000232864	valid_1's l2: 0.000267901
[99]	training's l2: 0.000232358	valid_1's l2: 0.000267806
[100]	training's l2: 0.000231964	valid_1's l2: 0.0002678
[101]	training's l2: 0.000231577	valid_1's l2: 0.000267811
[102]	training's l2: 0.0002311	valid_1's l2: 0.000267764
[103]	training's l2: 0.000230615	valid_1's l2: 0.000267684
[104]	training's l2: 0.000230201	valid_1's l2: 0.000267601
[105]	training's l2: 0.000229784	valid_1's l2: 0.000267543
[106]	training's l2: 0.000229313	valid_1's l2: 0.000267425
[107]	training's l2: 0.000228941	valid_1's l2: 0.000267391
[108]	training's l2: 0.000228391	valid_1's l2: 0.000267164
[109]	training's l2: 0.000228027	valid_1's l2: 0.000267039
[110]	training's l2: 0.000227651	valid_1's l2: 0.000267006
[111]	training's l2: 0.000227224	valid_1's l2: 0.000266963
[112]	training's l2: 0.000226665	valid_1's l2: 0.00026692
[113]	training's l2: 0.000226291	valid_1's l2: 0.000266908
[114]	training's l2: 0.000225843	valid_1's l2: 0.000266698
[115]	training's l2: 0.000225506	valid_1's l2: 0.000266706
[116]	training's l2: 0.00022514	valid_1's l2: 0.000266657
[117]	training's l2: 0.000224763	valid_1's l2: 0.00026666
[118]	training's l2: 0.000224249	valid_1's l2: 0.000266437
[119]	training's l2: 0.000223878	valid_1's l2: 0.000266442
[120]	training's l2: 0.000223468	valid_1's l2: 0.000266448
[121]	training's l2: 0.000223118	valid_1's l2: 0.000266457
[122]	training's l2: 0.000222779	valid_1's l2: 0.000266388
[123]	training's l2: 0.000222319	valid_1's l2: 0.000266269
[124]	training's l2: 0.000221871	valid_1's l2: 0.000266197
[125]	training's l2: 0.000221514	valid_1's l2: 0.000266197
[126]	training's l2: 0.000221157	valid_1's l2: 0.000266137
[127]	training's l2: 0.000220819	valid_1's l2: 0.000266116
[128]	training's l2: 0.000220451	valid_1's l2: 0.000266124
[129]	training's l2: 0.000220062	valid_1's l2: 0.000266077
[130]	training's l2: 0.000219738	valid_1's l2: 0.000266041
[131]	training's l2: 0.000219335	valid_1's l2: 0.000266018
[132]	training's l2: 0.000218948	valid_1's l2: 0.000265945
[133]	training's l2: 0.00021859	valid_1's l2: 0.000265968
[134]	training's l2: 0.000218234	valid_1's l2: 0.000265942
[135]	training's l2: 0.000217919	valid_1's l2: 0.000265952
[136]	training's l2: 0.000217567	valid_1's l2: 0.000265998
[137]	training's l2: 0.000217207	valid_1's l2: 0.00026603
[138]	training's l2: 0.000216871	valid_1's l2: 0.000266075
[139]	training's l2: 0.000216555	valid_1's l2: 0.000266103
[140]	training's l2: 0.000216188	valid_1's l2: 0.000266087
[141]	training's l2: 0.000215883	valid_1's l2: 0.000266044
[142]	training's l2: 0.000215544	valid_1's l2: 0.000266042
[143]	training's l2: 0.00021522	valid_1's l2: 0.000265997
[144]	training's l2: 0.000214848	valid_1's l2: 0.000265919
[145]	training's l2: 0.00021453	valid_1's l2: 0.000265991
[146]	training's l2: 0.000214209	valid_1's l2: 0.000265929
[147]	training's l2: 0.000213717	valid_1's l2: 0.000265783
[148]	training's l2: 0.000213407	valid_1's l2: 0.000265793
[149]	training's l2: 0.00021307	valid_1's l2: 0.00026578
[150]	training's l2: 0.000212771	valid_1's l2: 0.000265783
[151]	training's l2: 0.000212417	valid_1's l2: 0.000265748
[152]	training's l2: 0.000212116	valid_1's l2: 0.000265716
[153]	training's l2: 0.000211795	valid_1's l2: 0.000265774
[154]	training's l2: 0.000211384	valid_1's l2: 0.000265702
[155]	training's l2: 0.000211068	valid_1's l2: 0.000265731
[156]	training's l2: 0.000210763	valid_1's l2: 0.000265713
[157]	training's l2: 0.000210437	valid_1's l2: 0.000265663
[158]	training's l2: 0.000210147	valid_1's l2: 0.000265627
[159]	training's l2: 0.000209773	valid_1's l2: 0.000265512
[160]	training's l2: 0.00020946	valid_1's l2: 0.000265443
[161]	training's l2: 0.000209122	valid_1's l2: 0.000265312
[162]	training's l2: 0.000208809	valid_1's l2: 0.000265304
[163]	training's l2: 0.000208515	valid_1's l2: 0.000265353
[164]	training's l2: 0.000208238	valid_1's l2: 0.000265333
[165]	training's l2: 0.000207941	valid_1's l2: 0.000265371
[166]	training's l2: 0.00020761	valid_1's l2: 0.000265337
[167]	training's l2: 0.000207288	valid_1's l2: 0.000265404
[168]	training's l2: 0.000207007	valid_1's l2: 0.000265486
[169]	training's l2: 0.000206743	valid_1's l2: 0.000265464
[170]	training's l2: 0.000206462	valid_1's l2: 0.000265494
[171]	training's l2: 0.00020617	valid_1's l2: 0.000265455
[172]	training's l2: 0.000205906	valid_1's l2: 0.000265453
[173]	training's l2: 0.000205621	valid_1's l2: 0.000265424
[174]	training's l2: 0.000205327	valid_1's l2: 0.000265442
[175]	training's l2: 0.000205039	valid_1's l2: 0.000265421
[176]	training's l2: 0.000204772	valid_1's l2: 0.000265454
[177]	training's l2: 0.000204489	valid_1's l2: 0.000265547
[178]	training's l2: 0.000204213	valid_1's l2: 0.00026554
Did not meet early stopping. Best iteration is:
[178]	training's l2: 0.000204213	valid_1's l2: 0.00026554
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171190 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000387345	valid_1's l2: 0.000377307
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000377783	valid_1's l2: 0.000368783
[3]	training's l2: 0.000369296	valid_1's l2: 0.000361193
[4]	training's l2: 0.000361443	valid_1's l2: 0.000354063
[5]	training's l2: 0.000354099	valid_1's l2: 0.000347702
[6]	training's l2: 0.000347551	valid_1's l2: 0.000342176
[7]	training's l2: 0.000341531	valid_1's l2: 0.000336846
[8]	training's l2: 0.000335817	valid_1's l2: 0.000332224
[9]	training's l2: 0.000330775	valid_1's l2: 0.000327994
[10]	training's l2: 0.000325778	valid_1's l2: 0.000323574
[11]	training's l2: 0.000321457	valid_1's l2: 0.00031994
[12]	training's l2: 0.000317179	valid_1's l2: 0.00031646
[13]	training's l2: 0.000313441	valid_1's l2: 0.000313551
[14]	training's l2: 0.000309889	valid_1's l2: 0.000310856
[15]	training's l2: 0.000306462	valid_1's l2: 0.000308253
[16]	training's l2: 0.000303289	valid_1's l2: 0.000305682
[17]	training's l2: 0.000300428	valid_1's l2: 0.000303408
[18]	training's l2: 0.000297592	valid_1's l2: 0.000301097
[19]	training's l2: 0.000294844	valid_1's l2: 0.000299219
[20]	training's l2: 0.000292463	valid_1's l2: 0.000297364
[21]	training's l2: 0.00029011	valid_1's l2: 0.000295802
[22]	training's l2: 0.000287894	valid_1's l2: 0.000294396
[23]	training's l2: 0.000285822	valid_1's l2: 0.000292979
[24]	training's l2: 0.000283778	valid_1's l2: 0.000291603
[25]	training's l2: 0.000281969	valid_1's l2: 0.000290671
[26]	training's l2: 0.00028003	valid_1's l2: 0.000289447
[27]	training's l2: 0.00027827	valid_1's l2: 0.000288306
[28]	training's l2: 0.000276666	valid_1's l2: 0.000287377
[29]	training's l2: 0.00027496	valid_1's l2: 0.000286332
[30]	training's l2: 0.000273411	valid_1's l2: 0.000285437
[31]	training's l2: 0.000271971	valid_1's l2: 0.000284681
[32]	training's l2: 0.000270642	valid_1's l2: 0.000283978
[33]	training's l2: 0.000269285	valid_1's l2: 0.000283358
[34]	training's l2: 0.00026793	valid_1's l2: 0.000282552
[35]	training's l2: 0.000266689	valid_1's l2: 0.000281884
[36]	training's l2: 0.000265361	valid_1's l2: 0.000281096
[37]	training's l2: 0.00026406	valid_1's l2: 0.000280523
[38]	training's l2: 0.00026302	valid_1's l2: 0.000279977
[39]	training's l2: 0.000261806	valid_1's l2: 0.00027944
[40]	training's l2: 0.000260708	valid_1's l2: 0.000278865
[41]	training's l2: 0.000259581	valid_1's l2: 0.000278329
[42]	training's l2: 0.000258555	valid_1's l2: 0.000277894
[43]	training's l2: 0.000257229	valid_1's l2: 0.000277163
[44]	training's l2: 0.000256185	valid_1's l2: 0.000276637
[45]	training's l2: 0.00025515	valid_1's l2: 0.00027621
[46]	training's l2: 0.000254028	valid_1's l2: 0.000275793
[47]	training's l2: 0.000253002	valid_1's l2: 0.000275344
[48]	training's l2: 0.000251864	valid_1's l2: 0.000274815
[49]	training's l2: 0.000250982	valid_1's l2: 0.000274589
[50]	training's l2: 0.000250046	valid_1's l2: 0.00027424
[51]	training's l2: 0.00024915	valid_1's l2: 0.000273935
[52]	training's l2: 0.000248355	valid_1's l2: 0.000273777
[53]	training's l2: 0.000247292	valid_1's l2: 0.000273326
[54]	training's l2: 0.000246452	valid_1's l2: 0.000272989
[55]	training's l2: 0.000245369	valid_1's l2: 0.000272545
[56]	training's l2: 0.000244648	valid_1's l2: 0.000272308
[57]	training's l2: 0.000243871	valid_1's l2: 0.000272137
[58]	training's l2: 0.000243183	valid_1's l2: 0.00027201
[59]	training's l2: 0.000242248	valid_1's l2: 0.000271729
[60]	training's l2: 0.000241386	valid_1's l2: 0.000271539
[61]	training's l2: 0.000240511	valid_1's l2: 0.000271238
[62]	training's l2: 0.000239776	valid_1's l2: 0.000271038
[63]	training's l2: 0.000239098	valid_1's l2: 0.000270948
[64]	training's l2: 0.000238291	valid_1's l2: 0.000270674
[65]	training's l2: 0.000237471	valid_1's l2: 0.000270373
[66]	training's l2: 0.000236799	valid_1's l2: 0.000270157
[67]	training's l2: 0.000236122	valid_1's l2: 0.000269998
[68]	training's l2: 0.000235382	valid_1's l2: 0.000269977
[69]	training's l2: 0.000234654	valid_1's l2: 0.000269749
[70]	training's l2: 0.000233913	valid_1's l2: 0.00026952
[71]	training's l2: 0.000233263	valid_1's l2: 0.000269399
[72]	training's l2: 0.000232597	valid_1's l2: 0.000269353
[73]	training's l2: 0.000231761	valid_1's l2: 0.000269104
[74]	training's l2: 0.000231056	valid_1's l2: 0.000268957
[75]	training's l2: 0.000230321	valid_1's l2: 0.000268796
[76]	training's l2: 0.000229771	valid_1's l2: 0.000268805
[77]	training's l2: 0.000229141	valid_1's l2: 0.000268703
[78]	training's l2: 0.000228525	valid_1's l2: 0.00026859
[79]	training's l2: 0.000227939	valid_1's l2: 0.0002685
[80]	training's l2: 0.000227379	valid_1's l2: 0.000268402
[81]	training's l2: 0.000226602	valid_1's l2: 0.00026825
[82]	training's l2: 0.000225919	valid_1's l2: 0.000268111
[83]	training's l2: 0.000225303	valid_1's l2: 0.000268018
[84]	training's l2: 0.000224748	valid_1's l2: 0.000268109
[85]	training's l2: 0.00022416	valid_1's l2: 0.000268019
[86]	training's l2: 0.000223555	valid_1's l2: 0.000268012
[87]	training's l2: 0.000222906	valid_1's l2: 0.000267926
[88]	training's l2: 0.000222322	valid_1's l2: 0.000267717
[89]	training's l2: 0.000221765	valid_1's l2: 0.000267629
[90]	training's l2: 0.000221125	valid_1's l2: 0.000267438
[91]	training's l2: 0.000220578	valid_1's l2: 0.000267472
[92]	training's l2: 0.000219976	valid_1's l2: 0.000267396
[93]	training's l2: 0.000219419	valid_1's l2: 0.000267324
[94]	training's l2: 0.000218779	valid_1's l2: 0.000267146
[95]	training's l2: 0.000218246	valid_1's l2: 0.000267015
[96]	training's l2: 0.0002177	valid_1's l2: 0.000266989
[97]	training's l2: 0.000217151	valid_1's l2: 0.000267008
[98]	training's l2: 0.00021653	valid_1's l2: 0.000266979
[99]	training's l2: 0.000216034	valid_1's l2: 0.000267007
[100]	training's l2: 0.000215529	valid_1's l2: 0.000267034
[101]	training's l2: 0.000214967	valid_1's l2: 0.000266922
[102]	training's l2: 0.000214448	valid_1's l2: 0.000266816
[103]	training's l2: 0.000213945	valid_1's l2: 0.000266774
[104]	training's l2: 0.00021331	valid_1's l2: 0.000266679
[105]	training's l2: 0.000212715	valid_1's l2: 0.00026649
[106]	training's l2: 0.000212189	valid_1's l2: 0.000266486
[107]	training's l2: 0.000211577	valid_1's l2: 0.000266382
[108]	training's l2: 0.000211079	valid_1's l2: 0.00026648
[109]	training's l2: 0.000210577	valid_1's l2: 0.000266572
[110]	training's l2: 0.000210024	valid_1's l2: 0.000266372
[111]	training's l2: 0.000209554	valid_1's l2: 0.000266361
[112]	training's l2: 0.00020907	valid_1's l2: 0.000266382
[113]	training's l2: 0.000208533	valid_1's l2: 0.000266247
[114]	training's l2: 0.000208023	valid_1's l2: 0.000266248
[115]	training's l2: 0.000207488	valid_1's l2: 0.000266128
[116]	training's l2: 0.000206953	valid_1's l2: 0.000266037
[117]	training's l2: 0.000206437	valid_1's l2: 0.000265926
[118]	training's l2: 0.000205977	valid_1's l2: 0.000265905
[119]	training's l2: 0.00020548	valid_1's l2: 0.000265849
[120]	training's l2: 0.000205042	valid_1's l2: 0.000265851
[121]	training's l2: 0.000204571	valid_1's l2: 0.000265836
[122]	training's l2: 0.000204129	valid_1's l2: 0.000265873
[123]	training's l2: 0.000203646	valid_1's l2: 0.00026571
[124]	training's l2: 0.00020318	valid_1's l2: 0.000265798
[125]	training's l2: 0.000202668	valid_1's l2: 0.000265679
[126]	training's l2: 0.000202247	valid_1's l2: 0.000265669
[127]	training's l2: 0.000201816	valid_1's l2: 0.000265653
[128]	training's l2: 0.000201353	valid_1's l2: 0.000265653
[129]	training's l2: 0.00020092	valid_1's l2: 0.000265715
[130]	training's l2: 0.000200458	valid_1's l2: 0.000265715
[131]	training's l2: 0.000199991	valid_1's l2: 0.000265638
[132]	training's l2: 0.000199549	valid_1's l2: 0.000265721
[133]	training's l2: 0.000199078	valid_1's l2: 0.000265471
[134]	training's l2: 0.000198672	valid_1's l2: 0.000265492
[135]	training's l2: 0.00019826	valid_1's l2: 0.000265676
[136]	training's l2: 0.000197874	valid_1's l2: 0.000265607
[137]	training's l2: 0.00019743	valid_1's l2: 0.000265569
[138]	training's l2: 0.00019693	valid_1's l2: 0.00026551
[139]	training's l2: 0.000196508	valid_1's l2: 0.000265483
[140]	training's l2: 0.000196113	valid_1's l2: 0.000265429
[141]	training's l2: 0.000195673	valid_1's l2: 0.000265439
[142]	training's l2: 0.000195263	valid_1's l2: 0.000265364
[143]	training's l2: 0.000194796	valid_1's l2: 0.000265296
[144]	training's l2: 0.000194373	valid_1's l2: 0.000265324
[145]	training's l2: 0.000193991	valid_1's l2: 0.000265299
[146]	training's l2: 0.000193598	valid_1's l2: 0.000265375
[147]	training's l2: 0.000193246	valid_1's l2: 0.000265455
[148]	training's l2: 0.000192871	valid_1's l2: 0.000265423
[149]	training's l2: 0.000192492	valid_1's l2: 0.000265434
[150]	training's l2: 0.000192051	valid_1's l2: 0.000265474
[151]	training's l2: 0.000191651	valid_1's l2: 0.000265504
[152]	training's l2: 0.000191301	valid_1's l2: 0.00026546
Did not meet early stopping. Best iteration is:
[152]	training's l2: 0.000191301	valid_1's l2: 0.00026546
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.173580 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000383756	valid_1's l2: 0.000373673
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000371584	valid_1's l2: 0.000362242
[3]	training's l2: 0.000361182	valid_1's l2: 0.000352988
[4]	training's l2: 0.000351893	valid_1's l2: 0.000344858
[5]	training's l2: 0.000343868	valid_1's l2: 0.000337305
[6]	training's l2: 0.000336983	valid_1's l2: 0.000331512
[7]	training's l2: 0.000330299	valid_1's l2: 0.000325624
[8]	training's l2: 0.000324585	valid_1's l2: 0.000320623
[9]	training's l2: 0.000319453	valid_1's l2: 0.000316228
[10]	training's l2: 0.000314787	valid_1's l2: 0.000312199
[11]	training's l2: 0.000310534	valid_1's l2: 0.00030857
[12]	training's l2: 0.000306982	valid_1's l2: 0.000305692
[13]	training's l2: 0.00030358	valid_1's l2: 0.000303304
[14]	training's l2: 0.000300378	valid_1's l2: 0.000300734
[15]	training's l2: 0.000297526	valid_1's l2: 0.000298534
[16]	training's l2: 0.000294888	valid_1's l2: 0.000296712
[17]	training's l2: 0.000292197	valid_1's l2: 0.000294646
[18]	training's l2: 0.000290045	valid_1's l2: 0.000293469
[19]	training's l2: 0.00028791	valid_1's l2: 0.000292012
[20]	training's l2: 0.000285753	valid_1's l2: 0.00029072
[21]	training's l2: 0.000283939	valid_1's l2: 0.000289534
[22]	training's l2: 0.000282285	valid_1's l2: 0.000288293
[23]	training's l2: 0.000280582	valid_1's l2: 0.000287271
[24]	training's l2: 0.000278609	valid_1's l2: 0.000285967
[25]	training's l2: 0.000277223	valid_1's l2: 0.0002851
[26]	training's l2: 0.000275886	valid_1's l2: 0.000284301
[27]	training's l2: 0.000274317	valid_1's l2: 0.000283435
[28]	training's l2: 0.000273077	valid_1's l2: 0.000282745
[29]	training's l2: 0.000271759	valid_1's l2: 0.00028196
[30]	training's l2: 0.000270103	valid_1's l2: 0.000280784
[31]	training's l2: 0.000268728	valid_1's l2: 0.000280025
[32]	training's l2: 0.000267389	valid_1's l2: 0.00027941
[33]	training's l2: 0.00026611	valid_1's l2: 0.000278633
[34]	training's l2: 0.000265212	valid_1's l2: 0.000278245
[35]	training's l2: 0.000263978	valid_1's l2: 0.000277654
[36]	training's l2: 0.000262925	valid_1's l2: 0.000277097
[37]	training's l2: 0.000261566	valid_1's l2: 0.000276403
[38]	training's l2: 0.000260682	valid_1's l2: 0.000276
[39]	training's l2: 0.00025973	valid_1's l2: 0.000275742
[40]	training's l2: 0.000258896	valid_1's l2: 0.000275412
[41]	training's l2: 0.000258118	valid_1's l2: 0.000275206
[42]	training's l2: 0.00025731	valid_1's l2: 0.00027496
[43]	training's l2: 0.00025604	valid_1's l2: 0.000274206
[44]	training's l2: 0.00025505	valid_1's l2: 0.000273827
[45]	training's l2: 0.000254276	valid_1's l2: 0.000273518
[46]	training's l2: 0.000253506	valid_1's l2: 0.000273278
[47]	training's l2: 0.000252657	valid_1's l2: 0.000273156
[48]	training's l2: 0.00025172	valid_1's l2: 0.000272805
[49]	training's l2: 0.000250885	valid_1's l2: 0.000272552
[50]	training's l2: 0.000250196	valid_1's l2: 0.000272359
[51]	training's l2: 0.000249451	valid_1's l2: 0.000272234
[52]	training's l2: 0.000248785	valid_1's l2: 0.000272075
[53]	training's l2: 0.00024779	valid_1's l2: 0.000271651
[54]	training's l2: 0.00024702	valid_1's l2: 0.000271296
[55]	training's l2: 0.000246303	valid_1's l2: 0.000271235
[56]	training's l2: 0.000245748	valid_1's l2: 0.000271126
[57]	training's l2: 0.000245061	valid_1's l2: 0.000270913
[58]	training's l2: 0.000244259	valid_1's l2: 0.000270631
[59]	training's l2: 0.000243659	valid_1's l2: 0.000270572
[60]	training's l2: 0.000243023	valid_1's l2: 0.000270536
[61]	training's l2: 0.000242271	valid_1's l2: 0.000270294
[62]	training's l2: 0.000241614	valid_1's l2: 0.000270147
[63]	training's l2: 0.000241047	valid_1's l2: 0.000270088
[64]	training's l2: 0.000240467	valid_1's l2: 0.000270042
[65]	training's l2: 0.000239913	valid_1's l2: 0.000269914
[66]	training's l2: 0.000239238	valid_1's l2: 0.000269749
[67]	training's l2: 0.000238555	valid_1's l2: 0.000269684
[68]	training's l2: 0.000237856	valid_1's l2: 0.000269592
[69]	training's l2: 0.000237288	valid_1's l2: 0.000269562
[70]	training's l2: 0.000236633	valid_1's l2: 0.000269334
[71]	training's l2: 0.000236092	valid_1's l2: 0.000269278
[72]	training's l2: 0.000235552	valid_1's l2: 0.000269217
[73]	training's l2: 0.000235045	valid_1's l2: 0.000269148
[74]	training's l2: 0.000234361	valid_1's l2: 0.000268975
[75]	training's l2: 0.000233769	valid_1's l2: 0.000268895
[76]	training's l2: 0.00023328	valid_1's l2: 0.000268742
[77]	training's l2: 0.000232757	valid_1's l2: 0.000268711
[78]	training's l2: 0.000232284	valid_1's l2: 0.000268652
[79]	training's l2: 0.000231638	valid_1's l2: 0.000268508
[80]	training's l2: 0.000231073	valid_1's l2: 0.000268353
[81]	training's l2: 0.000230435	valid_1's l2: 0.000268278
[82]	training's l2: 0.000229836	valid_1's l2: 0.000268161
[83]	training's l2: 0.000229334	valid_1's l2: 0.00026811
[84]	training's l2: 0.0002287	valid_1's l2: 0.000267822
[85]	training's l2: 0.000228234	valid_1's l2: 0.000267814
[86]	training's l2: 0.000227553	valid_1's l2: 0.000267575
[87]	training's l2: 0.000226952	valid_1's l2: 0.000267357
[88]	training's l2: 0.000226463	valid_1's l2: 0.000267244
[89]	training's l2: 0.000225982	valid_1's l2: 0.00026713
[90]	training's l2: 0.000225536	valid_1's l2: 0.000267005
[91]	training's l2: 0.000225085	valid_1's l2: 0.000267005
[92]	training's l2: 0.000224577	valid_1's l2: 0.000266953
[93]	training's l2: 0.000224128	valid_1's l2: 0.000266857
[94]	training's l2: 0.000223675	valid_1's l2: 0.000266898
[95]	training's l2: 0.000223268	valid_1's l2: 0.000266857
[96]	training's l2: 0.000222729	valid_1's l2: 0.000266741
[97]	training's l2: 0.000222295	valid_1's l2: 0.00026672
[98]	training's l2: 0.000221774	valid_1's l2: 0.00026661
[99]	training's l2: 0.000221354	valid_1's l2: 0.000266528
[100]	training's l2: 0.000220782	valid_1's l2: 0.000266402
[101]	training's l2: 0.000220368	valid_1's l2: 0.000266426
[102]	training's l2: 0.000219966	valid_1's l2: 0.000266398
[103]	training's l2: 0.000219561	valid_1's l2: 0.000266355
[104]	training's l2: 0.000219074	valid_1's l2: 0.000266225
[105]	training's l2: 0.000218605	valid_1's l2: 0.000266065
[106]	training's l2: 0.000218173	valid_1's l2: 0.000266001
[107]	training's l2: 0.000217786	valid_1's l2: 0.000265972
[108]	training's l2: 0.000217328	valid_1's l2: 0.00026599
[109]	training's l2: 0.00021697	valid_1's l2: 0.000265875
[110]	training's l2: 0.000216512	valid_1's l2: 0.000265878
[111]	training's l2: 0.000216132	valid_1's l2: 0.000266
[112]	training's l2: 0.000215634	valid_1's l2: 0.000265874
[113]	training's l2: 0.00021524	valid_1's l2: 0.000265902
[114]	training's l2: 0.000214858	valid_1's l2: 0.000265929
[115]	training's l2: 0.0002144	valid_1's l2: 0.000265892
[116]	training's l2: 0.000214012	valid_1's l2: 0.000265871
[117]	training's l2: 0.000213659	valid_1's l2: 0.000265898
[118]	training's l2: 0.000213216	valid_1's l2: 0.000265807
[119]	training's l2: 0.000212773	valid_1's l2: 0.000265729
[120]	training's l2: 0.000212345	valid_1's l2: 0.000265589
[121]	training's l2: 0.000211929	valid_1's l2: 0.00026554
[122]	training's l2: 0.000211551	valid_1's l2: 0.000265544
[123]	training's l2: 0.00021119	valid_1's l2: 0.000265466
[124]	training's l2: 0.000210812	valid_1's l2: 0.000265545
[125]	training's l2: 0.000210339	valid_1's l2: 0.000265499
[126]	training's l2: 0.000209898	valid_1's l2: 0.000265377
[127]	training's l2: 0.000209552	valid_1's l2: 0.000265345
[128]	training's l2: 0.00020915	valid_1's l2: 0.000265307
[129]	training's l2: 0.000208746	valid_1's l2: 0.00026529
[130]	training's l2: 0.000208407	valid_1's l2: 0.000265365
[131]	training's l2: 0.000208026	valid_1's l2: 0.000265408
[132]	training's l2: 0.000207647	valid_1's l2: 0.000265361
[133]	training's l2: 0.000207288	valid_1's l2: 0.000265353
[134]	training's l2: 0.000206902	valid_1's l2: 0.000265366
[135]	training's l2: 0.000206493	valid_1's l2: 0.000265296
[136]	training's l2: 0.000206095	valid_1's l2: 0.000265216
[137]	training's l2: 0.000205757	valid_1's l2: 0.000265261
[138]	training's l2: 0.000205398	valid_1's l2: 0.000265356
[139]	training's l2: 0.000205017	valid_1's l2: 0.000265407
[140]	training's l2: 0.000204679	valid_1's l2: 0.00026547
[141]	training's l2: 0.00020435	valid_1's l2: 0.000265483
[142]	training's l2: 0.000203986	valid_1's l2: 0.000265649
[143]	training's l2: 0.000203646	valid_1's l2: 0.000265618
[144]	training's l2: 0.000203308	valid_1's l2: 0.000265658
[145]	training's l2: 0.000202929	valid_1's l2: 0.000265614
[146]	training's l2: 0.000202595	valid_1's l2: 0.000265688
[147]	training's l2: 0.000202257	valid_1's l2: 0.000265686
[148]	training's l2: 0.000201908	valid_1's l2: 0.000265671
[149]	training's l2: 0.000201566	valid_1's l2: 0.000265721
[150]	training's l2: 0.000201255	valid_1's l2: 0.000265719
[151]	training's l2: 0.000200933	valid_1's l2: 0.000265746
[152]	training's l2: 0.000200604	valid_1's l2: 0.000265798
[153]	training's l2: 0.000200268	valid_1's l2: 0.000265816
[154]	training's l2: 0.000199889	valid_1's l2: 0.000265756
[155]	training's l2: 0.00019958	valid_1's l2: 0.00026578
[156]	training's l2: 0.000199208	valid_1's l2: 0.000265672
[157]	training's l2: 0.00019886	valid_1's l2: 0.000265559
[158]	training's l2: 0.000198382	valid_1's l2: 0.000265459
[159]	training's l2: 0.00019805	valid_1's l2: 0.000265484
[160]	training's l2: 0.000197716	valid_1's l2: 0.000265533
[161]	training's l2: 0.000197377	valid_1's l2: 0.000265548
[162]	training's l2: 0.000196985	valid_1's l2: 0.000265522
[163]	training's l2: 0.000196641	valid_1's l2: 0.000265515
[164]	training's l2: 0.000196296	valid_1's l2: 0.000265596
[165]	training's l2: 0.000195979	valid_1's l2: 0.000265677
[166]	training's l2: 0.000195676	valid_1's l2: 0.000265698
Early stopping, best iteration is:
[136]	training's l2: 0.000206095	valid_1's l2: 0.000265216
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.173828 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000390049	valid_1's l2: 0.000379847
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000382734	valid_1's l2: 0.000373502
[3]	training's l2: 0.00037595	valid_1's l2: 0.000367558
[4]	training's l2: 0.000369606	valid_1's l2: 0.00036202
[5]	training's l2: 0.000363772	valid_1's l2: 0.000356805
[6]	training's l2: 0.000358388	valid_1's l2: 0.00035238
[7]	training's l2: 0.00035319	valid_1's l2: 0.000347944
[8]	training's l2: 0.000348377	valid_1's l2: 0.000343727
[9]	training's l2: 0.000343731	valid_1's l2: 0.000339979
[10]	training's l2: 0.000339477	valid_1's l2: 0.000336177
[11]	training's l2: 0.000335485	valid_1's l2: 0.000332641
[12]	training's l2: 0.000331721	valid_1's l2: 0.000329536
[13]	training's l2: 0.000327977	valid_1's l2: 0.000326316
[14]	training's l2: 0.000324634	valid_1's l2: 0.000323597
[15]	training's l2: 0.000321295	valid_1's l2: 0.000320843
[16]	training's l2: 0.000318279	valid_1's l2: 0.000318427
[17]	training's l2: 0.000315317	valid_1's l2: 0.000316014
[18]	training's l2: 0.000312644	valid_1's l2: 0.000313919
[19]	training's l2: 0.000310005	valid_1's l2: 0.000311799
[20]	training's l2: 0.00030753	valid_1's l2: 0.000309739
[21]	training's l2: 0.000305112	valid_1's l2: 0.000307794
[22]	training's l2: 0.000302949	valid_1's l2: 0.000306103
[23]	training's l2: 0.000300733	valid_1's l2: 0.000304259
[24]	training's l2: 0.000298631	valid_1's l2: 0.000302789
[25]	training's l2: 0.000296727	valid_1's l2: 0.000301408
[26]	training's l2: 0.000294814	valid_1's l2: 0.000300135
[27]	training's l2: 0.000292852	valid_1's l2: 0.000298785
[28]	training's l2: 0.000291214	valid_1's l2: 0.000297702
[29]	training's l2: 0.000289543	valid_1's l2: 0.000296557
[30]	training's l2: 0.000287927	valid_1's l2: 0.000295496
[31]	training's l2: 0.000286347	valid_1's l2: 0.000294401
[32]	training's l2: 0.000284903	valid_1's l2: 0.000293602
[33]	training's l2: 0.000283466	valid_1's l2: 0.0002927
[34]	training's l2: 0.000282094	valid_1's l2: 0.000291842
[35]	training's l2: 0.000280805	valid_1's l2: 0.00029108
[36]	training's l2: 0.000279476	valid_1's l2: 0.000290253
[37]	training's l2: 0.000278166	valid_1's l2: 0.000289613
[38]	training's l2: 0.000276942	valid_1's l2: 0.000288912
[39]	training's l2: 0.0002758	valid_1's l2: 0.000288215
[40]	training's l2: 0.000274621	valid_1's l2: 0.000287477
[41]	training's l2: 0.000273526	valid_1's l2: 0.00028686
[42]	training's l2: 0.000272515	valid_1's l2: 0.000286262
[43]	training's l2: 0.000271526	valid_1's l2: 0.000285707
[44]	training's l2: 0.000270447	valid_1's l2: 0.000285093
[45]	training's l2: 0.000269387	valid_1's l2: 0.000284505
[46]	training's l2: 0.000268427	valid_1's l2: 0.000283975
[47]	training's l2: 0.000267529	valid_1's l2: 0.00028349
[48]	training's l2: 0.000266521	valid_1's l2: 0.00028296
[49]	training's l2: 0.000265501	valid_1's l2: 0.000282309
[50]	training's l2: 0.00026455	valid_1's l2: 0.000281801
[51]	training's l2: 0.000263772	valid_1's l2: 0.000281407
[52]	training's l2: 0.00026283	valid_1's l2: 0.00028092
[53]	training's l2: 0.00026187	valid_1's l2: 0.000280502
[54]	training's l2: 0.000261177	valid_1's l2: 0.000280197
[55]	training's l2: 0.000260342	valid_1's l2: 0.000279771
[56]	training's l2: 0.000259517	valid_1's l2: 0.00027942
[57]	training's l2: 0.000258787	valid_1's l2: 0.000279166
[58]	training's l2: 0.000257889	valid_1's l2: 0.000278688
[59]	training's l2: 0.000257112	valid_1's l2: 0.000278457
[60]	training's l2: 0.000256302	valid_1's l2: 0.000278069
[61]	training's l2: 0.000255626	valid_1's l2: 0.00027781
[62]	training's l2: 0.000254802	valid_1's l2: 0.000277422
[63]	training's l2: 0.000254056	valid_1's l2: 0.000277185
[64]	training's l2: 0.000253255	valid_1's l2: 0.000276948
[65]	training's l2: 0.000252484	valid_1's l2: 0.000276607
[66]	training's l2: 0.000251811	valid_1's l2: 0.000276359
[67]	training's l2: 0.000251201	valid_1's l2: 0.000276176
[68]	training's l2: 0.000250489	valid_1's l2: 0.000275921
[69]	training's l2: 0.000249745	valid_1's l2: 0.000275592
[70]	training's l2: 0.000248867	valid_1's l2: 0.000275142
[71]	training's l2: 0.00024829	valid_1's l2: 0.000274925
[72]	training's l2: 0.000247584	valid_1's l2: 0.0002747
[73]	training's l2: 0.000246973	valid_1's l2: 0.000274514
[74]	training's l2: 0.000246181	valid_1's l2: 0.000274217
[75]	training's l2: 0.000245581	valid_1's l2: 0.000274052
[76]	training's l2: 0.000245	valid_1's l2: 0.000273966
[77]	training's l2: 0.000244432	valid_1's l2: 0.00027386
[78]	training's l2: 0.000243744	valid_1's l2: 0.000273546
[79]	training's l2: 0.000242975	valid_1's l2: 0.000273212
[80]	training's l2: 0.000242432	valid_1's l2: 0.00027309
[81]	training's l2: 0.000241925	valid_1's l2: 0.000273009
[82]	training's l2: 0.000241349	valid_1's l2: 0.000272822
[83]	training's l2: 0.000240697	valid_1's l2: 0.000272553
[84]	training's l2: 0.000240074	valid_1's l2: 0.000272286
[85]	training's l2: 0.000239569	valid_1's l2: 0.000272194
[86]	training's l2: 0.000239074	valid_1's l2: 0.000272134
[87]	training's l2: 0.000238511	valid_1's l2: 0.000271922
[88]	training's l2: 0.000237983	valid_1's l2: 0.000271797
[89]	training's l2: 0.000237488	valid_1's l2: 0.000271687
[90]	training's l2: 0.000236863	valid_1's l2: 0.000271441
[91]	training's l2: 0.000236336	valid_1's l2: 0.000271357
[92]	training's l2: 0.0002359	valid_1's l2: 0.000271298
[93]	training's l2: 0.000235327	valid_1's l2: 0.000271103
[94]	training's l2: 0.000234762	valid_1's l2: 0.000270965
[95]	training's l2: 0.000234208	valid_1's l2: 0.000270862
[96]	training's l2: 0.000233696	valid_1's l2: 0.000270803
[97]	training's l2: 0.00023323	valid_1's l2: 0.000270682
[98]	training's l2: 0.000232705	valid_1's l2: 0.000270635
[99]	training's l2: 0.000232111	valid_1's l2: 0.000270365
[100]	training's l2: 0.000231693	valid_1's l2: 0.000270346
[101]	training's l2: 0.000231209	valid_1's l2: 0.000270233
[102]	training's l2: 0.000230626	valid_1's l2: 0.000270122
[103]	training's l2: 0.000230142	valid_1's l2: 0.000270042
[104]	training's l2: 0.000229563	valid_1's l2: 0.000269876
[105]	training's l2: 0.000229028	valid_1's l2: 0.000269694
[106]	training's l2: 0.000228584	valid_1's l2: 0.000269617
[107]	training's l2: 0.00022816	valid_1's l2: 0.000269592
[108]	training's l2: 0.000227696	valid_1's l2: 0.000269447
[109]	training's l2: 0.000227261	valid_1's l2: 0.000269381
[110]	training's l2: 0.000226731	valid_1's l2: 0.000269182
[111]	training's l2: 0.00022623	valid_1's l2: 0.00026913
[112]	training's l2: 0.000225809	valid_1's l2: 0.000269001
[113]	training's l2: 0.000225434	valid_1's l2: 0.000268932
[114]	training's l2: 0.000225016	valid_1's l2: 0.000268853
[115]	training's l2: 0.000224503	valid_1's l2: 0.000268708
[116]	training's l2: 0.00022401	valid_1's l2: 0.000268613
[117]	training's l2: 0.000223622	valid_1's l2: 0.000268609
[118]	training's l2: 0.000223206	valid_1's l2: 0.000268551
[119]	training's l2: 0.000222805	valid_1's l2: 0.000268432
[120]	training's l2: 0.000222407	valid_1's l2: 0.000268401
[121]	training's l2: 0.000221945	valid_1's l2: 0.000268348
[122]	training's l2: 0.000221532	valid_1's l2: 0.000268405
[123]	training's l2: 0.00022114	valid_1's l2: 0.000268449
[124]	training's l2: 0.000220666	valid_1's l2: 0.000268332
[125]	training's l2: 0.000220224	valid_1's l2: 0.000268213
[126]	training's l2: 0.00021981	valid_1's l2: 0.0002682
[127]	training's l2: 0.000219405	valid_1's l2: 0.000268117
[128]	training's l2: 0.000218989	valid_1's l2: 0.000268015
[129]	training's l2: 0.000218617	valid_1's l2: 0.000267945
[130]	training's l2: 0.000218202	valid_1's l2: 0.00026785
[131]	training's l2: 0.000217702	valid_1's l2: 0.000267725
[132]	training's l2: 0.000217335	valid_1's l2: 0.0002677
[133]	training's l2: 0.000216964	valid_1's l2: 0.000267642
[134]	training's l2: 0.000216631	valid_1's l2: 0.000267597
[135]	training's l2: 0.000216209	valid_1's l2: 0.000267503
[136]	training's l2: 0.000215838	valid_1's l2: 0.000267451
[137]	training's l2: 0.000215426	valid_1's l2: 0.000267516
[138]	training's l2: 0.000215025	valid_1's l2: 0.000267473
[139]	training's l2: 0.000214657	valid_1's l2: 0.000267494
[140]	training's l2: 0.000214285	valid_1's l2: 0.000267379
[141]	training's l2: 0.000213901	valid_1's l2: 0.000267378
[142]	training's l2: 0.000213526	valid_1's l2: 0.000267326
[143]	training's l2: 0.000213097	valid_1's l2: 0.000267274
[144]	training's l2: 0.000212706	valid_1's l2: 0.000267235
[145]	training's l2: 0.000212312	valid_1's l2: 0.000267236
[146]	training's l2: 0.000211957	valid_1's l2: 0.000267265
[147]	training's l2: 0.000211554	valid_1's l2: 0.000267168
[148]	training's l2: 0.000211157	valid_1's l2: 0.000267153
[149]	training's l2: 0.00021079	valid_1's l2: 0.000267106
[150]	training's l2: 0.000210379	valid_1's l2: 0.00026697
[151]	training's l2: 0.000210022	valid_1's l2: 0.000266948
[152]	training's l2: 0.000209681	valid_1's l2: 0.000266909
[153]	training's l2: 0.000209352	valid_1's l2: 0.000266935
[154]	training's l2: 0.000208986	valid_1's l2: 0.000266975
[155]	training's l2: 0.000208638	valid_1's l2: 0.000267006
[156]	training's l2: 0.000208256	valid_1's l2: 0.000266994
[157]	training's l2: 0.000207841	valid_1's l2: 0.000267007
[158]	training's l2: 0.000207472	valid_1's l2: 0.000266995
[159]	training's l2: 0.000207148	valid_1's l2: 0.000267049
[160]	training's l2: 0.00020676	valid_1's l2: 0.000267003
[161]	training's l2: 0.000206435	valid_1's l2: 0.00026696
[162]	training's l2: 0.000206085	valid_1's l2: 0.000266873
[163]	training's l2: 0.000205753	valid_1's l2: 0.000266845
[164]	training's l2: 0.00020541	valid_1's l2: 0.000266815
[165]	training's l2: 0.000205063	valid_1's l2: 0.000266813
[166]	training's l2: 0.000204667	valid_1's l2: 0.000266772
[167]	training's l2: 0.000204329	valid_1's l2: 0.000266785
[168]	training's l2: 0.000204023	valid_1's l2: 0.000266736
[169]	training's l2: 0.000203704	valid_1's l2: 0.000266693
[170]	training's l2: 0.000203357	valid_1's l2: 0.000266639
[171]	training's l2: 0.000203002	valid_1's l2: 0.000266546
[172]	training's l2: 0.000202684	valid_1's l2: 0.000266567
[173]	training's l2: 0.000202334	valid_1's l2: 0.000266519
[174]	training's l2: 0.000202007	valid_1's l2: 0.000266493
[175]	training's l2: 0.000201678	valid_1's l2: 0.000266514
[176]	training's l2: 0.000201357	valid_1's l2: 0.000266517
[177]	training's l2: 0.000201023	valid_1's l2: 0.000266487
[178]	training's l2: 0.000200726	valid_1's l2: 0.000266519
[179]	training's l2: 0.000200421	valid_1's l2: 0.000266505
[180]	training's l2: 0.000200111	valid_1's l2: 0.000266493
[181]	training's l2: 0.00019976	valid_1's l2: 0.000266396
[182]	training's l2: 0.000199445	valid_1's l2: 0.000266341
[183]	training's l2: 0.000199123	valid_1's l2: 0.000266256
[184]	training's l2: 0.00019884	valid_1's l2: 0.000266285
Did not meet early stopping. Best iteration is:
[184]	training's l2: 0.00019884	valid_1's l2: 0.000266285
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170190 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000389951	valid_1's l2: 0.000379665
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000382581	valid_1's l2: 0.000373104
[3]	training's l2: 0.000375758	valid_1's l2: 0.000367151
[4]	training's l2: 0.000369436	valid_1's l2: 0.000361391
[5]	training's l2: 0.000363556	valid_1's l2: 0.000355867
[6]	training's l2: 0.000358155	valid_1's l2: 0.000350982
[7]	training's l2: 0.000352827	valid_1's l2: 0.000346313
[8]	training's l2: 0.000348045	valid_1's l2: 0.00034217
[9]	training's l2: 0.000343545	valid_1's l2: 0.000338183
[10]	training's l2: 0.000339171	valid_1's l2: 0.00033469
[11]	training's l2: 0.000335062	valid_1's l2: 0.000331109
[12]	training's l2: 0.000331063	valid_1's l2: 0.000327508
[13]	training's l2: 0.000327323	valid_1's l2: 0.000324492
[14]	training's l2: 0.000324007	valid_1's l2: 0.000321766
[15]	training's l2: 0.00032078	valid_1's l2: 0.000319138
[16]	training's l2: 0.00031767	valid_1's l2: 0.000316533
[17]	training's l2: 0.000314869	valid_1's l2: 0.000314312
[18]	training's l2: 0.000312087	valid_1's l2: 0.000311919
[19]	training's l2: 0.000309507	valid_1's l2: 0.000309684
[20]	training's l2: 0.000306986	valid_1's l2: 0.000307758
[21]	training's l2: 0.000304729	valid_1's l2: 0.000305889
[22]	training's l2: 0.000302501	valid_1's l2: 0.000304192
[23]	training's l2: 0.000300322	valid_1's l2: 0.00030245
[24]	training's l2: 0.000298358	valid_1's l2: 0.000300924
[25]	training's l2: 0.00029643	valid_1's l2: 0.000299615
[26]	training's l2: 0.000294496	valid_1's l2: 0.000298288
[27]	training's l2: 0.000292727	valid_1's l2: 0.000296982
[28]	training's l2: 0.000291067	valid_1's l2: 0.000295794
[29]	training's l2: 0.000289454	valid_1's l2: 0.000294858
[30]	training's l2: 0.000287838	valid_1's l2: 0.000293843
[31]	training's l2: 0.000286307	valid_1's l2: 0.000292897
[32]	training's l2: 0.000284904	valid_1's l2: 0.000292003
[33]	training's l2: 0.000283389	valid_1's l2: 0.000291058
[34]	training's l2: 0.000282015	valid_1's l2: 0.000290123
[35]	training's l2: 0.000280607	valid_1's l2: 0.000289135
[36]	training's l2: 0.000279358	valid_1's l2: 0.000288286
[37]	training's l2: 0.000278162	valid_1's l2: 0.000287603
[38]	training's l2: 0.000276947	valid_1's l2: 0.000286806
[39]	training's l2: 0.000275789	valid_1's l2: 0.000286092
[40]	training's l2: 0.000274644	valid_1's l2: 0.000285451
[41]	training's l2: 0.000273475	valid_1's l2: 0.000284708
[42]	training's l2: 0.000272461	valid_1's l2: 0.000284071
[43]	training's l2: 0.000271456	valid_1's l2: 0.000283529
[44]	training's l2: 0.000270517	valid_1's l2: 0.000283072
[45]	training's l2: 0.000269579	valid_1's l2: 0.000282572
[46]	training's l2: 0.000268709	valid_1's l2: 0.000282077
[47]	training's l2: 0.000267747	valid_1's l2: 0.000281594
[48]	training's l2: 0.000266811	valid_1's l2: 0.000281113
[49]	training's l2: 0.000265955	valid_1's l2: 0.000280612
[50]	training's l2: 0.000264866	valid_1's l2: 0.000279858
[51]	training's l2: 0.000264085	valid_1's l2: 0.00027953
[52]	training's l2: 0.000263089	valid_1's l2: 0.000278997
[53]	training's l2: 0.000262271	valid_1's l2: 0.000278627
[54]	training's l2: 0.000261436	valid_1's l2: 0.000278226
[55]	training's l2: 0.000260603	valid_1's l2: 0.000277842
[56]	training's l2: 0.000259673	valid_1's l2: 0.000277347
[57]	training's l2: 0.00025876	valid_1's l2: 0.000276922
[58]	training's l2: 0.000257772	valid_1's l2: 0.000276443
[59]	training's l2: 0.00025695	valid_1's l2: 0.000276139
[60]	training's l2: 0.000256155	valid_1's l2: 0.000275835
[61]	training's l2: 0.000255354	valid_1's l2: 0.000275437
[62]	training's l2: 0.000254653	valid_1's l2: 0.000275203
[63]	training's l2: 0.000253945	valid_1's l2: 0.000274949
[64]	training's l2: 0.000253248	valid_1's l2: 0.000274697
[65]	training's l2: 0.000252549	valid_1's l2: 0.000274387
[66]	training's l2: 0.000251916	valid_1's l2: 0.000274164
[67]	training's l2: 0.000251204	valid_1's l2: 0.000273908
[68]	training's l2: 0.000250554	valid_1's l2: 0.00027367
[69]	training's l2: 0.000249869	valid_1's l2: 0.000273463
[70]	training's l2: 0.000249196	valid_1's l2: 0.000273193
[71]	training's l2: 0.000248448	valid_1's l2: 0.000272882
[72]	training's l2: 0.000247828	valid_1's l2: 0.000272749
[73]	training's l2: 0.000247207	valid_1's l2: 0.000272544
[74]	training's l2: 0.000246606	valid_1's l2: 0.000272414
[75]	training's l2: 0.000245862	valid_1's l2: 0.000272043
[76]	training's l2: 0.000245073	valid_1's l2: 0.000271674
[77]	training's l2: 0.000244455	valid_1's l2: 0.000271497
[78]	training's l2: 0.000243931	valid_1's l2: 0.000271311
[79]	training's l2: 0.000243392	valid_1's l2: 0.000271214
[80]	training's l2: 0.000242869	valid_1's l2: 0.000271115
[81]	training's l2: 0.0002422	valid_1's l2: 0.000270906
[82]	training's l2: 0.000241513	valid_1's l2: 0.000270619
[83]	training's l2: 0.000240979	valid_1's l2: 0.000270481
[84]	training's l2: 0.000240481	valid_1's l2: 0.000270365
[85]	training's l2: 0.000239792	valid_1's l2: 0.000270137
[86]	training's l2: 0.000239255	valid_1's l2: 0.000269993
[87]	training's l2: 0.000238742	valid_1's l2: 0.000269949
[88]	training's l2: 0.000238175	valid_1's l2: 0.000269753
[89]	training's l2: 0.000237565	valid_1's l2: 0.000269615
[90]	training's l2: 0.000236938	valid_1's l2: 0.000269512
[91]	training's l2: 0.000236468	valid_1's l2: 0.000269412
[92]	training's l2: 0.000235983	valid_1's l2: 0.000269383
[93]	training's l2: 0.000235377	valid_1's l2: 0.000269131
[94]	training's l2: 0.000234882	valid_1's l2: 0.000269
[95]	training's l2: 0.000234395	valid_1's l2: 0.000268931
[96]	training's l2: 0.000233879	valid_1's l2: 0.000268717
[97]	training's l2: 0.000233237	valid_1's l2: 0.000268514
[98]	training's l2: 0.000232636	valid_1's l2: 0.000268291
[99]	training's l2: 0.000232018	valid_1's l2: 0.000268054
[100]	training's l2: 0.000231515	valid_1's l2: 0.00026804
[101]	training's l2: 0.000231075	valid_1's l2: 0.000268092
[102]	training's l2: 0.000230552	valid_1's l2: 0.000267932
[103]	training's l2: 0.000230047	valid_1's l2: 0.000267796
[104]	training's l2: 0.000229448	valid_1's l2: 0.000267606
[105]	training's l2: 0.000229021	valid_1's l2: 0.000267604
[106]	training's l2: 0.000228489	valid_1's l2: 0.000267537
[107]	training's l2: 0.00022796	valid_1's l2: 0.000267487
[108]	training's l2: 0.000227523	valid_1's l2: 0.000267466
[109]	training's l2: 0.000227087	valid_1's l2: 0.000267452
[110]	training's l2: 0.000226667	valid_1's l2: 0.000267397
[111]	training's l2: 0.000226202	valid_1's l2: 0.000267349
[112]	training's l2: 0.000225758	valid_1's l2: 0.000267274
[113]	training's l2: 0.000225314	valid_1's l2: 0.000267236
[114]	training's l2: 0.000224905	valid_1's l2: 0.000267215
[115]	training's l2: 0.00022438	valid_1's l2: 0.000267133
[116]	training's l2: 0.000223976	valid_1's l2: 0.000267087
[117]	training's l2: 0.000223569	valid_1's l2: 0.000267086
[118]	training's l2: 0.000223169	valid_1's l2: 0.000267001
[119]	training's l2: 0.000222642	valid_1's l2: 0.000266972
[120]	training's l2: 0.000222229	valid_1's l2: 0.000266915
[121]	training's l2: 0.00022179	valid_1's l2: 0.000266899
[122]	training's l2: 0.000221355	valid_1's l2: 0.000266764
[123]	training's l2: 0.000220934	valid_1's l2: 0.000266711
[124]	training's l2: 0.000220486	valid_1's l2: 0.000266612
[125]	training's l2: 0.000220018	valid_1's l2: 0.000266591
[126]	training's l2: 0.000219587	valid_1's l2: 0.000266526
[127]	training's l2: 0.00021919	valid_1's l2: 0.000266458
[128]	training's l2: 0.000218783	valid_1's l2: 0.00026641
[129]	training's l2: 0.000218414	valid_1's l2: 0.00026644
[130]	training's l2: 0.000217973	valid_1's l2: 0.000266374
[131]	training's l2: 0.000217561	valid_1's l2: 0.000266379
[132]	training's l2: 0.000217147	valid_1's l2: 0.000266359
[133]	training's l2: 0.000216635	valid_1's l2: 0.000266259
[134]	training's l2: 0.000216209	valid_1's l2: 0.000266252
[135]	training's l2: 0.000215804	valid_1's l2: 0.000266224
[136]	training's l2: 0.000215465	valid_1's l2: 0.000266186
[137]	training's l2: 0.000215036	valid_1's l2: 0.00026606
[138]	training's l2: 0.000214671	valid_1's l2: 0.000266104
[139]	training's l2: 0.000214284	valid_1's l2: 0.00026611
[140]	training's l2: 0.000213849	valid_1's l2: 0.000266017
[141]	training's l2: 0.000213484	valid_1's l2: 0.000266013
[142]	training's l2: 0.000213069	valid_1's l2: 0.000266035
[143]	training's l2: 0.000212716	valid_1's l2: 0.000266042
[144]	training's l2: 0.000212325	valid_1's l2: 0.000266073
[145]	training's l2: 0.000212004	valid_1's l2: 0.00026606
[146]	training's l2: 0.000211614	valid_1's l2: 0.000265975
[147]	training's l2: 0.000211258	valid_1's l2: 0.000266027
[148]	training's l2: 0.000210895	valid_1's l2: 0.000266055
[149]	training's l2: 0.00021048	valid_1's l2: 0.000266017
[150]	training's l2: 0.000210151	valid_1's l2: 0.000265999
[151]	training's l2: 0.000209784	valid_1's l2: 0.000265955
[152]	training's l2: 0.000209382	valid_1's l2: 0.00026587
[153]	training's l2: 0.000209022	valid_1's l2: 0.000265869
[154]	training's l2: 0.000208682	valid_1's l2: 0.000265813
[155]	training's l2: 0.000208349	valid_1's l2: 0.000265865
[156]	training's l2: 0.000207979	valid_1's l2: 0.000265867
[157]	training's l2: 0.000207668	valid_1's l2: 0.000265896
[158]	training's l2: 0.000207308	valid_1's l2: 0.000265873
[159]	training's l2: 0.000206939	valid_1's l2: 0.000265872
[160]	training's l2: 0.000206591	valid_1's l2: 0.000265897
[161]	training's l2: 0.000206163	valid_1's l2: 0.000265848
[162]	training's l2: 0.000205844	valid_1's l2: 0.000265821
[163]	training's l2: 0.000205506	valid_1's l2: 0.000265839
[164]	training's l2: 0.000205168	valid_1's l2: 0.000265883
[165]	training's l2: 0.000204821	valid_1's l2: 0.000265846
[166]	training's l2: 0.000204511	valid_1's l2: 0.000265881
[167]	training's l2: 0.000204143	valid_1's l2: 0.000265812
[168]	training's l2: 0.000203797	valid_1's l2: 0.00026581
[169]	training's l2: 0.000203387	valid_1's l2: 0.000265766
[170]	training's l2: 0.000203048	valid_1's l2: 0.000265724
[171]	training's l2: 0.000202704	valid_1's l2: 0.000265703
[172]	training's l2: 0.000202382	valid_1's l2: 0.000265686
[173]	training's l2: 0.000202049	valid_1's l2: 0.000265631
[174]	training's l2: 0.000201735	valid_1's l2: 0.000265689
[175]	training's l2: 0.00020145	valid_1's l2: 0.000265681
[176]	training's l2: 0.000201153	valid_1's l2: 0.000265725
[177]	training's l2: 0.000200772	valid_1's l2: 0.000265764
Did not meet early stopping. Best iteration is:
[177]	training's l2: 0.000200772	valid_1's l2: 0.000265764
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178404 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000388241	valid_1's l2: 0.00037804
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000379457	valid_1's l2: 0.000370139
[3]	training's l2: 0.00037164	valid_1's l2: 0.000363169
[4]	training's l2: 0.000364387	valid_1's l2: 0.000356654
[5]	training's l2: 0.000357561	valid_1's l2: 0.000351108
[6]	training's l2: 0.000351593	valid_1's l2: 0.000345629
[7]	training's l2: 0.000346028	valid_1's l2: 0.000340488
[8]	training's l2: 0.000340935	valid_1's l2: 0.000336317
[9]	training's l2: 0.000336154	valid_1's l2: 0.000331879
[10]	training's l2: 0.000331511	valid_1's l2: 0.000327855
[11]	training's l2: 0.000327547	valid_1's l2: 0.000324296
[12]	training's l2: 0.000323608	valid_1's l2: 0.000320976
[13]	training's l2: 0.00032019	valid_1's l2: 0.000318195
[14]	training's l2: 0.000316841	valid_1's l2: 0.000315347
[15]	training's l2: 0.000313639	valid_1's l2: 0.000312622
[16]	training's l2: 0.000310769	valid_1's l2: 0.000310116
[17]	training's l2: 0.000307996	valid_1's l2: 0.000307679
[18]	training's l2: 0.000305273	valid_1's l2: 0.000305593
[19]	training's l2: 0.000302923	valid_1's l2: 0.000303906
[20]	training's l2: 0.000300654	valid_1's l2: 0.000302315
[21]	training's l2: 0.000298503	valid_1's l2: 0.000300669
[22]	training's l2: 0.000296452	valid_1's l2: 0.000299272
[23]	training's l2: 0.000294481	valid_1's l2: 0.000297837
[24]	training's l2: 0.000292712	valid_1's l2: 0.000296739
[25]	training's l2: 0.000290943	valid_1's l2: 0.000295421
[26]	training's l2: 0.000289262	valid_1's l2: 0.000294435
[27]	training's l2: 0.000287748	valid_1's l2: 0.000293427
[28]	training's l2: 0.000286191	valid_1's l2: 0.00029235
[29]	training's l2: 0.000284676	valid_1's l2: 0.00029135
[30]	training's l2: 0.000283297	valid_1's l2: 0.000290443
[31]	training's l2: 0.000281843	valid_1's l2: 0.000289444
[32]	training's l2: 0.000280347	valid_1's l2: 0.000288381
[33]	training's l2: 0.000279227	valid_1's l2: 0.000287642
[34]	training's l2: 0.000277967	valid_1's l2: 0.000286954
[35]	training's l2: 0.00027694	valid_1's l2: 0.000286437
[36]	training's l2: 0.000275606	valid_1's l2: 0.0002857
[37]	training's l2: 0.000274687	valid_1's l2: 0.000285016
[38]	training's l2: 0.000273443	valid_1's l2: 0.000284099
[39]	training's l2: 0.000272484	valid_1's l2: 0.000283539
[40]	training's l2: 0.000271518	valid_1's l2: 0.000282914
[41]	training's l2: 0.000270384	valid_1's l2: 0.000282123
[42]	training's l2: 0.000269293	valid_1's l2: 0.000281449
[43]	training's l2: 0.000268427	valid_1's l2: 0.000280999
[44]	training's l2: 0.000267067	valid_1's l2: 0.000280222
[45]	training's l2: 0.000266115	valid_1's l2: 0.000279735
[46]	training's l2: 0.000265406	valid_1's l2: 0.000279353
[47]	training's l2: 0.000264464	valid_1's l2: 0.000278777
[48]	training's l2: 0.000263615	valid_1's l2: 0.000278429
[49]	training's l2: 0.000262916	valid_1's l2: 0.000278143
[50]	training's l2: 0.000262106	valid_1's l2: 0.000277894
[51]	training's l2: 0.000261321	valid_1's l2: 0.000277461
[52]	training's l2: 0.000260594	valid_1's l2: 0.000277093
[53]	training's l2: 0.000259613	valid_1's l2: 0.000276713
[54]	training's l2: 0.000258761	valid_1's l2: 0.00027625
[55]	training's l2: 0.000257948	valid_1's l2: 0.000275904
[56]	training's l2: 0.00025717	valid_1's l2: 0.000275486
[57]	training's l2: 0.000256289	valid_1's l2: 0.000275046
[58]	training's l2: 0.00025561	valid_1's l2: 0.000274708
[59]	training's l2: 0.000255056	valid_1's l2: 0.000274541
[60]	training's l2: 0.000254436	valid_1's l2: 0.000274342
[61]	training's l2: 0.00025383	valid_1's l2: 0.000274045
[62]	training's l2: 0.000253221	valid_1's l2: 0.000273889
[63]	training's l2: 0.000252483	valid_1's l2: 0.000273491
[64]	training's l2: 0.000251771	valid_1's l2: 0.000273209
[65]	training's l2: 0.000251208	valid_1's l2: 0.000272967
[66]	training's l2: 0.000250679	valid_1's l2: 0.000272876
[67]	training's l2: 0.000249906	valid_1's l2: 0.000272595
[68]	training's l2: 0.000249335	valid_1's l2: 0.00027242
[69]	training's l2: 0.000248772	valid_1's l2: 0.000272137
[70]	training's l2: 0.000248081	valid_1's l2: 0.000271929
[71]	training's l2: 0.000247517	valid_1's l2: 0.000271711
[72]	training's l2: 0.000246872	valid_1's l2: 0.000271569
[73]	training's l2: 0.000246152	valid_1's l2: 0.000271225
[74]	training's l2: 0.000245468	valid_1's l2: 0.000270928
[75]	training's l2: 0.000244966	valid_1's l2: 0.000270752
[76]	training's l2: 0.000244307	valid_1's l2: 0.000270543
[77]	training's l2: 0.000243807	valid_1's l2: 0.00027039
[78]	training's l2: 0.000243244	valid_1's l2: 0.00027032
[79]	training's l2: 0.000242636	valid_1's l2: 0.000270087
[80]	training's l2: 0.000242223	valid_1's l2: 0.000269964
[81]	training's l2: 0.000241764	valid_1's l2: 0.000269891
[82]	training's l2: 0.000241117	valid_1's l2: 0.000269644
[83]	training's l2: 0.000240626	valid_1's l2: 0.000269546
[84]	training's l2: 0.000240227	valid_1's l2: 0.000269461
[85]	training's l2: 0.00023973	valid_1's l2: 0.000269437
[86]	training's l2: 0.000239213	valid_1's l2: 0.000269306
[87]	training's l2: 0.000238784	valid_1's l2: 0.000269239
[88]	training's l2: 0.00023832	valid_1's l2: 0.000269124
[89]	training's l2: 0.000237753	valid_1's l2: 0.000268973
[90]	training's l2: 0.000237375	valid_1's l2: 0.000268907
[91]	training's l2: 0.000236962	valid_1's l2: 0.00026887
[92]	training's l2: 0.000236485	valid_1's l2: 0.000268874
[93]	training's l2: 0.000236113	valid_1's l2: 0.000268868
[94]	training's l2: 0.000235592	valid_1's l2: 0.000268718
[95]	training's l2: 0.00023507	valid_1's l2: 0.000268542
[96]	training's l2: 0.000234657	valid_1's l2: 0.000268485
[97]	training's l2: 0.000234224	valid_1's l2: 0.000268473
[98]	training's l2: 0.000233811	valid_1's l2: 0.000268362
[99]	training's l2: 0.000233338	valid_1's l2: 0.000268252
[100]	training's l2: 0.000232985	valid_1's l2: 0.000268229
[101]	training's l2: 0.000232647	valid_1's l2: 0.000268206
[102]	training's l2: 0.00023223	valid_1's l2: 0.000268143
[103]	training's l2: 0.000231783	valid_1's l2: 0.000268025
[104]	training's l2: 0.000231326	valid_1's l2: 0.000268
[105]	training's l2: 0.000230821	valid_1's l2: 0.000267911
[106]	training's l2: 0.000230451	valid_1's l2: 0.000267863
[107]	training's l2: 0.000230036	valid_1's l2: 0.000267841
[108]	training's l2: 0.000229566	valid_1's l2: 0.000267795
[109]	training's l2: 0.000229076	valid_1's l2: 0.000267717
[110]	training's l2: 0.000228743	valid_1's l2: 0.000267695
[111]	training's l2: 0.000228328	valid_1's l2: 0.000267545
[112]	training's l2: 0.00022795	valid_1's l2: 0.000267564
[113]	training's l2: 0.000227481	valid_1's l2: 0.000267414
[114]	training's l2: 0.000227122	valid_1's l2: 0.000267395
[115]	training's l2: 0.00022676	valid_1's l2: 0.00026742
[116]	training's l2: 0.000226409	valid_1's l2: 0.000267452
[117]	training's l2: 0.00022603	valid_1's l2: 0.000267322
[118]	training's l2: 0.00022558	valid_1's l2: 0.0002673
[119]	training's l2: 0.000225221	valid_1's l2: 0.000267248
[120]	training's l2: 0.000224859	valid_1's l2: 0.00026717
[121]	training's l2: 0.000224512	valid_1's l2: 0.000267116
[122]	training's l2: 0.000224155	valid_1's l2: 0.000267117
[123]	training's l2: 0.000223828	valid_1's l2: 0.000267094
[124]	training's l2: 0.00022338	valid_1's l2: 0.000266908
[125]	training's l2: 0.000223043	valid_1's l2: 0.00026694
[126]	training's l2: 0.000222621	valid_1's l2: 0.000266857
[127]	training's l2: 0.000222283	valid_1's l2: 0.000266795
[128]	training's l2: 0.000221896	valid_1's l2: 0.000266826
[129]	training's l2: 0.00022157	valid_1's l2: 0.000266827
[130]	training's l2: 0.000221241	valid_1's l2: 0.000266757
[131]	training's l2: 0.0002209	valid_1's l2: 0.000266771
[132]	training's l2: 0.000220533	valid_1's l2: 0.000266739
[133]	training's l2: 0.000220224	valid_1's l2: 0.000266738
[134]	training's l2: 0.000219898	valid_1's l2: 0.000266737
[135]	training's l2: 0.000219523	valid_1's l2: 0.000266614
[136]	training's l2: 0.000219185	valid_1's l2: 0.000266608
[137]	training's l2: 0.000218859	valid_1's l2: 0.000266559
[138]	training's l2: 0.000218493	valid_1's l2: 0.000266546
[139]	training's l2: 0.000218154	valid_1's l2: 0.000266505
[140]	training's l2: 0.000217835	valid_1's l2: 0.000266523
[141]	training's l2: 0.000217506	valid_1's l2: 0.000266544
[142]	training's l2: 0.000217177	valid_1's l2: 0.000266519
[143]	training's l2: 0.000216763	valid_1's l2: 0.000266368
[144]	training's l2: 0.000216359	valid_1's l2: 0.000266381
[145]	training's l2: 0.000216016	valid_1's l2: 0.000266373
[146]	training's l2: 0.000215689	valid_1's l2: 0.000266335
[147]	training's l2: 0.000215397	valid_1's l2: 0.000266303
[148]	training's l2: 0.00021503	valid_1's l2: 0.000266228
[149]	training's l2: 0.000214703	valid_1's l2: 0.000266154
[150]	training's l2: 0.000214386	valid_1's l2: 0.000266104
[151]	training's l2: 0.000214106	valid_1's l2: 0.000266098
[152]	training's l2: 0.0002138	valid_1's l2: 0.000266129
[153]	training's l2: 0.000213457	valid_1's l2: 0.000266086
[154]	training's l2: 0.000213127	valid_1's l2: 0.000266025
[155]	training's l2: 0.000212863	valid_1's l2: 0.000265996
[156]	training's l2: 0.000212521	valid_1's l2: 0.000266005
[157]	training's l2: 0.000212229	valid_1's l2: 0.000265994
[158]	training's l2: 0.00021189	valid_1's l2: 0.000265951
[159]	training's l2: 0.000211584	valid_1's l2: 0.000265962
[160]	training's l2: 0.000211287	valid_1's l2: 0.000265912
[161]	training's l2: 0.000211014	valid_1's l2: 0.000265965
[162]	training's l2: 0.000210721	valid_1's l2: 0.000265947
[163]	training's l2: 0.000210434	valid_1's l2: 0.000266002
[164]	training's l2: 0.000210163	valid_1's l2: 0.000265982
[165]	training's l2: 0.000209872	valid_1's l2: 0.000265904
[166]	training's l2: 0.00020962	valid_1's l2: 0.000265925
[167]	training's l2: 0.000209328	valid_1's l2: 0.000265922
[168]	training's l2: 0.000209001	valid_1's l2: 0.000265887
[169]	training's l2: 0.00020868	valid_1's l2: 0.000265928
[170]	training's l2: 0.000208358	valid_1's l2: 0.000265928
[171]	training's l2: 0.000208075	valid_1's l2: 0.000265976
[172]	training's l2: 0.000207814	valid_1's l2: 0.000265933
[173]	training's l2: 0.000207528	valid_1's l2: 0.000265916
[174]	training's l2: 0.000207262	valid_1's l2: 0.000265829
[175]	training's l2: 0.000206968	valid_1's l2: 0.000265824
[176]	training's l2: 0.000206643	valid_1's l2: 0.000265786
[177]	training's l2: 0.000206375	valid_1's l2: 0.000265759
[178]	training's l2: 0.000206064	valid_1's l2: 0.000265661
[179]	training's l2: 0.000205813	valid_1's l2: 0.000265697
[180]	training's l2: 0.000205529	valid_1's l2: 0.000265698
[181]	training's l2: 0.000205246	valid_1's l2: 0.000265659
[182]	training's l2: 0.00020497	valid_1's l2: 0.000265533
[183]	training's l2: 0.00020469	valid_1's l2: 0.00026557
[184]	training's l2: 0.000204433	valid_1's l2: 0.000265619
[185]	training's l2: 0.000204174	valid_1's l2: 0.000265589
[186]	training's l2: 0.000203889	valid_1's l2: 0.000265631
Did not meet early stopping. Best iteration is:
[186]	training's l2: 0.000203889	valid_1's l2: 0.000265631
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175766 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000387009	valid_1's l2: 0.000376631
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000377304	valid_1's l2: 0.000367595
[3]	training's l2: 0.000368687	valid_1's l2: 0.000359781
[4]	training's l2: 0.000360961	valid_1's l2: 0.000352474
[5]	training's l2: 0.000353946	valid_1's l2: 0.000346384
[6]	training's l2: 0.000347599	valid_1's l2: 0.000340382
[7]	training's l2: 0.000341843	valid_1's l2: 0.000335479
[8]	training's l2: 0.000336543	valid_1's l2: 0.000330765
[9]	training's l2: 0.000331474	valid_1's l2: 0.000326358
[10]	training's l2: 0.000327104	valid_1's l2: 0.000322711
[11]	training's l2: 0.000322947	valid_1's l2: 0.000318999
[12]	training's l2: 0.000319051	valid_1's l2: 0.00031545
[13]	training's l2: 0.000315495	valid_1's l2: 0.000312333
[14]	training's l2: 0.000312378	valid_1's l2: 0.000309742
[15]	training's l2: 0.000309248	valid_1's l2: 0.000307096
[16]	training's l2: 0.000306521	valid_1's l2: 0.000304997
[17]	training's l2: 0.000303809	valid_1's l2: 0.000302869
[18]	training's l2: 0.000301393	valid_1's l2: 0.000301018
[19]	training's l2: 0.000299139	valid_1's l2: 0.000299244
[20]	training's l2: 0.000296983	valid_1's l2: 0.000297582
[21]	training's l2: 0.000295056	valid_1's l2: 0.000296196
[22]	training's l2: 0.00029332	valid_1's l2: 0.000294855
[23]	training's l2: 0.000291483	valid_1's l2: 0.000293586
[24]	training's l2: 0.000289709	valid_1's l2: 0.000292287
[25]	training's l2: 0.000288019	valid_1's l2: 0.000291096
[26]	training's l2: 0.000286593	valid_1's l2: 0.000290045
[27]	training's l2: 0.000285215	valid_1's l2: 0.000289048
[28]	training's l2: 0.000283868	valid_1's l2: 0.000288227
[29]	training's l2: 0.000282636	valid_1's l2: 0.00028734
[30]	training's l2: 0.000281274	valid_1's l2: 0.000286694
[31]	training's l2: 0.000279872	valid_1's l2: 0.000285875
[32]	training's l2: 0.00027846	valid_1's l2: 0.000284911
[33]	training's l2: 0.000277437	valid_1's l2: 0.000284279
[34]	training's l2: 0.000276059	valid_1's l2: 0.00028349
[35]	training's l2: 0.000274879	valid_1's l2: 0.000282652
[36]	training's l2: 0.000273998	valid_1's l2: 0.000282139
[37]	training's l2: 0.00027306	valid_1's l2: 0.000281707
[38]	training's l2: 0.000271771	valid_1's l2: 0.00028091
[39]	training's l2: 0.00027074	valid_1's l2: 0.00028028
[40]	training's l2: 0.000269662	valid_1's l2: 0.000279637
[41]	training's l2: 0.000268722	valid_1's l2: 0.000279258
[42]	training's l2: 0.000267846	valid_1's l2: 0.000278663
[43]	training's l2: 0.000267039	valid_1's l2: 0.000278214
[44]	training's l2: 0.000266065	valid_1's l2: 0.000277635
[45]	training's l2: 0.000265318	valid_1's l2: 0.0002773
[46]	training's l2: 0.000264566	valid_1's l2: 0.000276953
[47]	training's l2: 0.000263681	valid_1's l2: 0.000276425
[48]	training's l2: 0.000262612	valid_1's l2: 0.000275963
[49]	training's l2: 0.000261822	valid_1's l2: 0.000275674
[50]	training's l2: 0.000261154	valid_1's l2: 0.00027547
[51]	training's l2: 0.000260435	valid_1's l2: 0.000275177
[52]	training's l2: 0.000259818	valid_1's l2: 0.00027504
[53]	training's l2: 0.000259173	valid_1's l2: 0.000274856
[54]	training's l2: 0.00025839	valid_1's l2: 0.000274504
[55]	training's l2: 0.000257605	valid_1's l2: 0.000274143
[56]	training's l2: 0.000256959	valid_1's l2: 0.000273904
[57]	training's l2: 0.00025632	valid_1's l2: 0.000273642
[58]	training's l2: 0.000255518	valid_1's l2: 0.000273141
[59]	training's l2: 0.000254689	valid_1's l2: 0.000272823
[60]	training's l2: 0.00025404	valid_1's l2: 0.000272512
[61]	training's l2: 0.000253457	valid_1's l2: 0.000272459
[62]	training's l2: 0.000252772	valid_1's l2: 0.000272308
[63]	training's l2: 0.000252084	valid_1's l2: 0.000271942
[64]	training's l2: 0.000251233	valid_1's l2: 0.000271481
[65]	training's l2: 0.000250602	valid_1's l2: 0.000271281
[66]	training's l2: 0.00025003	valid_1's l2: 0.000271057
[67]	training's l2: 0.000249396	valid_1's l2: 0.00027085
[68]	training's l2: 0.000248744	valid_1's l2: 0.000270668
[69]	training's l2: 0.000248206	valid_1's l2: 0.000270565
[70]	training's l2: 0.000247716	valid_1's l2: 0.000270457
[71]	training's l2: 0.000247148	valid_1's l2: 0.000270468
[72]	training's l2: 0.000246653	valid_1's l2: 0.000270364
[73]	training's l2: 0.00024603	valid_1's l2: 0.000270086
[74]	training's l2: 0.000245371	valid_1's l2: 0.000269822
[75]	training's l2: 0.000244912	valid_1's l2: 0.000269768
[76]	training's l2: 0.000244468	valid_1's l2: 0.000269627
[77]	training's l2: 0.000244005	valid_1's l2: 0.000269545
[78]	training's l2: 0.000243543	valid_1's l2: 0.000269509
[79]	training's l2: 0.000243022	valid_1's l2: 0.000269502
[80]	training's l2: 0.000242595	valid_1's l2: 0.000269473
[81]	training's l2: 0.000242013	valid_1's l2: 0.000269267
[82]	training's l2: 0.000241509	valid_1's l2: 0.000269148
[83]	training's l2: 0.000241132	valid_1's l2: 0.000269146
[84]	training's l2: 0.00024066	valid_1's l2: 0.000269061
[85]	training's l2: 0.000240164	valid_1's l2: 0.000268901
[86]	training's l2: 0.000239625	valid_1's l2: 0.00026874
[87]	training's l2: 0.000239059	valid_1's l2: 0.00026865
[88]	training's l2: 0.000238622	valid_1's l2: 0.000268506
[89]	training's l2: 0.000238158	valid_1's l2: 0.000268472
[90]	training's l2: 0.000237799	valid_1's l2: 0.000268417
[91]	training's l2: 0.000237301	valid_1's l2: 0.00026834
[92]	training's l2: 0.00023686	valid_1's l2: 0.000268307
[93]	training's l2: 0.00023637	valid_1's l2: 0.000268185
[94]	training's l2: 0.00023597	valid_1's l2: 0.000268146
[95]	training's l2: 0.000235417	valid_1's l2: 0.000267976
[96]	training's l2: 0.000235013	valid_1's l2: 0.00026803
[97]	training's l2: 0.000234563	valid_1's l2: 0.000267888
[98]	training's l2: 0.000234218	valid_1's l2: 0.000267846
[99]	training's l2: 0.000233758	valid_1's l2: 0.000267813
[100]	training's l2: 0.00023331	valid_1's l2: 0.000267779
[101]	training's l2: 0.000232902	valid_1's l2: 0.00026777
[102]	training's l2: 0.000232527	valid_1's l2: 0.00026763
[103]	training's l2: 0.000232073	valid_1's l2: 0.000267514
[104]	training's l2: 0.00023156	valid_1's l2: 0.000267434
[105]	training's l2: 0.000231107	valid_1's l2: 0.000267376
[106]	training's l2: 0.000230706	valid_1's l2: 0.000267339
[107]	training's l2: 0.000230243	valid_1's l2: 0.000267205
[108]	training's l2: 0.000229877	valid_1's l2: 0.000267231
[109]	training's l2: 0.000229514	valid_1's l2: 0.000267125
[110]	training's l2: 0.00022905	valid_1's l2: 0.000267033
[111]	training's l2: 0.000228632	valid_1's l2: 0.000267023
[112]	training's l2: 0.000228174	valid_1's l2: 0.000267029
[113]	training's l2: 0.000227815	valid_1's l2: 0.000266999
[114]	training's l2: 0.00022749	valid_1's l2: 0.000267008
[115]	training's l2: 0.00022716	valid_1's l2: 0.000266966
[116]	training's l2: 0.000226785	valid_1's l2: 0.000266892
[117]	training's l2: 0.000226419	valid_1's l2: 0.000266859
[118]	training's l2: 0.000226078	valid_1's l2: 0.000266956
[119]	training's l2: 0.000225726	valid_1's l2: 0.000266894
[120]	training's l2: 0.000225384	valid_1's l2: 0.000266873
[121]	training's l2: 0.000225025	valid_1's l2: 0.00026675
[122]	training's l2: 0.000224703	valid_1's l2: 0.000266695
[123]	training's l2: 0.000224258	valid_1's l2: 0.00026672
[124]	training's l2: 0.000223919	valid_1's l2: 0.000266734
[125]	training's l2: 0.000223605	valid_1's l2: 0.000266741
[126]	training's l2: 0.000223223	valid_1's l2: 0.000266726
[127]	training's l2: 0.000222849	valid_1's l2: 0.000266641
[128]	training's l2: 0.000222502	valid_1's l2: 0.000266582
[129]	training's l2: 0.000222161	valid_1's l2: 0.000266611
[130]	training's l2: 0.000221798	valid_1's l2: 0.000266564
[131]	training's l2: 0.000221451	valid_1's l2: 0.000266606
[132]	training's l2: 0.000221129	valid_1's l2: 0.000266611
[133]	training's l2: 0.000220795	valid_1's l2: 0.000266592
[134]	training's l2: 0.000220419	valid_1's l2: 0.000266544
[135]	training's l2: 0.000220061	valid_1's l2: 0.000266565
[136]	training's l2: 0.00021971	valid_1's l2: 0.000266457
[137]	training's l2: 0.000219411	valid_1's l2: 0.000266559
[138]	training's l2: 0.000219075	valid_1's l2: 0.000266573
[139]	training's l2: 0.000218729	valid_1's l2: 0.000266468
[140]	training's l2: 0.000218443	valid_1's l2: 0.000266488
[141]	training's l2: 0.000218054	valid_1's l2: 0.000266423
[142]	training's l2: 0.00021769	valid_1's l2: 0.000266389
[143]	training's l2: 0.000217361	valid_1's l2: 0.000266383
[144]	training's l2: 0.000217025	valid_1's l2: 0.000266373
[145]	training's l2: 0.000216707	valid_1's l2: 0.000266363
[146]	training's l2: 0.000216383	valid_1's l2: 0.000266308
[147]	training's l2: 0.000216104	valid_1's l2: 0.000266242
[148]	training's l2: 0.000215764	valid_1's l2: 0.000266171
[149]	training's l2: 0.000215471	valid_1's l2: 0.000266237
[150]	training's l2: 0.000215157	valid_1's l2: 0.000266223
[151]	training's l2: 0.000214834	valid_1's l2: 0.000266162
[152]	training's l2: 0.000214518	valid_1's l2: 0.000266122
[153]	training's l2: 0.000214231	valid_1's l2: 0.000266127
[154]	training's l2: 0.000213944	valid_1's l2: 0.000266114
[155]	training's l2: 0.000213684	valid_1's l2: 0.000266069
[156]	training's l2: 0.000213425	valid_1's l2: 0.000266089
[157]	training's l2: 0.000213119	valid_1's l2: 0.00026604
[158]	training's l2: 0.000212799	valid_1's l2: 0.000266056
[159]	training's l2: 0.000212492	valid_1's l2: 0.000266002
[160]	training's l2: 0.000212229	valid_1's l2: 0.000266002
[161]	training's l2: 0.000211935	valid_1's l2: 0.000265979
[162]	training's l2: 0.00021166	valid_1's l2: 0.00026603
[163]	training's l2: 0.000211387	valid_1's l2: 0.000266107
[164]	training's l2: 0.000211086	valid_1's l2: 0.000266089
[165]	training's l2: 0.000210761	valid_1's l2: 0.000266072
[166]	training's l2: 0.000210484	valid_1's l2: 0.000266085
[167]	training's l2: 0.000210186	valid_1's l2: 0.000266102
[168]	training's l2: 0.000209864	valid_1's l2: 0.000266049
[169]	training's l2: 0.000209575	valid_1's l2: 0.000266071
[170]	training's l2: 0.000209328	valid_1's l2: 0.000266037
[171]	training's l2: 0.000208939	valid_1's l2: 0.000266055
[172]	training's l2: 0.000208676	valid_1's l2: 0.000266068
[173]	training's l2: 0.000208398	valid_1's l2: 0.000266156
[174]	training's l2: 0.000208142	valid_1's l2: 0.000266126
[175]	training's l2: 0.000207842	valid_1's l2: 0.000266102
[176]	training's l2: 0.000207592	valid_1's l2: 0.000266049
[177]	training's l2: 0.000207283	valid_1's l2: 0.000266072
[178]	training's l2: 0.000206961	valid_1's l2: 0.000266041
[179]	training's l2: 0.00020664	valid_1's l2: 0.000265972
[180]	training's l2: 0.00020636	valid_1's l2: 0.000265924
[181]	training's l2: 0.000206041	valid_1's l2: 0.000265974
[182]	training's l2: 0.00020574	valid_1's l2: 0.000265997
[183]	training's l2: 0.000205511	valid_1's l2: 0.000266027
[184]	training's l2: 0.000205244	valid_1's l2: 0.000266134
Did not meet early stopping. Best iteration is:
[184]	training's l2: 0.000205244	valid_1's l2: 0.000266134
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177524 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000384183	valid_1's l2: 0.00037472
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000372195	valid_1's l2: 0.000364306
[3]	training's l2: 0.000361549	valid_1's l2: 0.000354801
[4]	training's l2: 0.000352054	valid_1's l2: 0.000346627
[5]	training's l2: 0.000343753	valid_1's l2: 0.000339489
[6]	training's l2: 0.000336365	valid_1's l2: 0.000333125
[7]	training's l2: 0.000329396	valid_1's l2: 0.000327006
[8]	training's l2: 0.000323474	valid_1's l2: 0.000322048
[9]	training's l2: 0.000317675	valid_1's l2: 0.000317695
[10]	training's l2: 0.000312844	valid_1's l2: 0.000313718
[11]	training's l2: 0.000307975	valid_1's l2: 0.000310065
[12]	training's l2: 0.000303573	valid_1's l2: 0.000306565
[13]	training's l2: 0.000299703	valid_1's l2: 0.000303638
[14]	training's l2: 0.000296008	valid_1's l2: 0.000300771
[15]	training's l2: 0.000292662	valid_1's l2: 0.000298568
[16]	training's l2: 0.000289677	valid_1's l2: 0.000296495
[17]	training's l2: 0.00028689	valid_1's l2: 0.000294778
[18]	training's l2: 0.000284265	valid_1's l2: 0.000292841
[19]	training's l2: 0.000281751	valid_1's l2: 0.000291321
[20]	training's l2: 0.000279478	valid_1's l2: 0.000290047
[21]	training's l2: 0.000277175	valid_1's l2: 0.00028856
[22]	training's l2: 0.000275023	valid_1's l2: 0.000287355
[23]	training's l2: 0.00027286	valid_1's l2: 0.0002862
[24]	training's l2: 0.000270922	valid_1's l2: 0.000285145
[25]	training's l2: 0.000269324	valid_1's l2: 0.000284212
[26]	training's l2: 0.000267471	valid_1's l2: 0.000283409
[27]	training's l2: 0.000265771	valid_1's l2: 0.0002825
[28]	training's l2: 0.000264132	valid_1's l2: 0.00028161
[29]	training's l2: 0.000262564	valid_1's l2: 0.000280747
[30]	training's l2: 0.000261113	valid_1's l2: 0.000280116
[31]	training's l2: 0.000259733	valid_1's l2: 0.00027948
[32]	training's l2: 0.000258288	valid_1's l2: 0.000278946
[33]	training's l2: 0.000256844	valid_1's l2: 0.000278294
[34]	training's l2: 0.000255206	valid_1's l2: 0.000277457
[35]	training's l2: 0.000253738	valid_1's l2: 0.0002768
[36]	training's l2: 0.000252127	valid_1's l2: 0.000276068
[37]	training's l2: 0.000250966	valid_1's l2: 0.000275754
[38]	training's l2: 0.000249767	valid_1's l2: 0.00027526
[39]	training's l2: 0.000248545	valid_1's l2: 0.000274915
[40]	training's l2: 0.00024745	valid_1's l2: 0.000274458
[41]	training's l2: 0.000245979	valid_1's l2: 0.000273715
[42]	training's l2: 0.000244999	valid_1's l2: 0.00027359
[43]	training's l2: 0.000243942	valid_1's l2: 0.000273419
[44]	training's l2: 0.000242589	valid_1's l2: 0.000272914
[45]	training's l2: 0.000241557	valid_1's l2: 0.000272642
[46]	training's l2: 0.000240523	valid_1's l2: 0.000272506
[47]	training's l2: 0.000239408	valid_1's l2: 0.000272027
[48]	training's l2: 0.000238479	valid_1's l2: 0.000271666
[49]	training's l2: 0.000237529	valid_1's l2: 0.000271478
[50]	training's l2: 0.000236331	valid_1's l2: 0.000271096
[51]	training's l2: 0.000235379	valid_1's l2: 0.000270918
[52]	training's l2: 0.000234452	valid_1's l2: 0.000270547
[53]	training's l2: 0.000233659	valid_1's l2: 0.000270381
[54]	training's l2: 0.000232684	valid_1's l2: 0.000270231
[55]	training's l2: 0.000231811	valid_1's l2: 0.000270115
[56]	training's l2: 0.000230706	valid_1's l2: 0.000269718
[57]	training's l2: 0.000229767	valid_1's l2: 0.000269554
[58]	training's l2: 0.000229019	valid_1's l2: 0.000269566
[59]	training's l2: 0.000228061	valid_1's l2: 0.000269253
[60]	training's l2: 0.000227233	valid_1's l2: 0.000269155
[61]	training's l2: 0.000226454	valid_1's l2: 0.000269038
[62]	training's l2: 0.000225649	valid_1's l2: 0.000268882
[63]	training's l2: 0.000224904	valid_1's l2: 0.000268795
[64]	training's l2: 0.000224078	valid_1's l2: 0.000268724
[65]	training's l2: 0.000223235	valid_1's l2: 0.000268503
[66]	training's l2: 0.000222552	valid_1's l2: 0.000268351
[67]	training's l2: 0.000221805	valid_1's l2: 0.000268367
[68]	training's l2: 0.000221053	valid_1's l2: 0.000268294
[69]	training's l2: 0.00022029	valid_1's l2: 0.000268173
[70]	training's l2: 0.000219448	valid_1's l2: 0.000267884
[71]	training's l2: 0.000218762	valid_1's l2: 0.000267856
[72]	training's l2: 0.00021807	valid_1's l2: 0.00026774
[73]	training's l2: 0.000217261	valid_1's l2: 0.000267628
[74]	training's l2: 0.000216576	valid_1's l2: 0.000267508
[75]	training's l2: 0.000215888	valid_1's l2: 0.000267561
[76]	training's l2: 0.000215148	valid_1's l2: 0.000267442
[77]	training's l2: 0.000214495	valid_1's l2: 0.000267268
[78]	training's l2: 0.000213799	valid_1's l2: 0.000267129
[79]	training's l2: 0.000213115	valid_1's l2: 0.000266967
[80]	training's l2: 0.000212417	valid_1's l2: 0.000266903
[81]	training's l2: 0.000211721	valid_1's l2: 0.000266729
[82]	training's l2: 0.000211116	valid_1's l2: 0.000266712
[83]	training's l2: 0.000210455	valid_1's l2: 0.000266807
[84]	training's l2: 0.000209807	valid_1's l2: 0.000266762
[85]	training's l2: 0.000209074	valid_1's l2: 0.000266654
[86]	training's l2: 0.000208334	valid_1's l2: 0.000266572
[87]	training's l2: 0.000207744	valid_1's l2: 0.000266577
[88]	training's l2: 0.000207054	valid_1's l2: 0.00026642
[89]	training's l2: 0.000206461	valid_1's l2: 0.000266304
[90]	training's l2: 0.000205904	valid_1's l2: 0.000266395
[91]	training's l2: 0.000205241	valid_1's l2: 0.000266349
[92]	training's l2: 0.000204613	valid_1's l2: 0.00026635
[93]	training's l2: 0.000203938	valid_1's l2: 0.000266277
[94]	training's l2: 0.000203379	valid_1's l2: 0.00026628
[95]	training's l2: 0.000202766	valid_1's l2: 0.000266105
[96]	training's l2: 0.000202203	valid_1's l2: 0.000266178
[97]	training's l2: 0.000201644	valid_1's l2: 0.000266304
[98]	training's l2: 0.000201062	valid_1's l2: 0.00026627
[99]	training's l2: 0.000200512	valid_1's l2: 0.000266279
[100]	training's l2: 0.0002	valid_1's l2: 0.000266293
[101]	training's l2: 0.000199479	valid_1's l2: 0.000266323
[102]	training's l2: 0.000198937	valid_1's l2: 0.000266366
[103]	training's l2: 0.000198406	valid_1's l2: 0.000266325
[104]	training's l2: 0.000197821	valid_1's l2: 0.000266349
[105]	training's l2: 0.000197299	valid_1's l2: 0.000266316
[106]	training's l2: 0.000196771	valid_1's l2: 0.000266316
[107]	training's l2: 0.000196156	valid_1's l2: 0.000266249
[108]	training's l2: 0.00019562	valid_1's l2: 0.000266215
[109]	training's l2: 0.000195076	valid_1's l2: 0.000266331
[110]	training's l2: 0.000194526	valid_1's l2: 0.00026635
[111]	training's l2: 0.000194079	valid_1's l2: 0.000266438
[112]	training's l2: 0.000193586	valid_1's l2: 0.000266472
[113]	training's l2: 0.000193137	valid_1's l2: 0.000266402
[114]	training's l2: 0.000192592	valid_1's l2: 0.000266303
[115]	training's l2: 0.000192078	valid_1's l2: 0.000266431
[116]	training's l2: 0.000191592	valid_1's l2: 0.000266576
[117]	training's l2: 0.000191145	valid_1's l2: 0.000266505
[118]	training's l2: 0.000190628	valid_1's l2: 0.000266495
[119]	training's l2: 0.000190145	valid_1's l2: 0.00026647
[120]	training's l2: 0.000189637	valid_1's l2: 0.000266501
[121]	training's l2: 0.000189127	valid_1's l2: 0.000266522
[122]	training's l2: 0.000188623	valid_1's l2: 0.00026657
[123]	training's l2: 0.000188156	valid_1's l2: 0.000266567
[124]	training's l2: 0.00018769	valid_1's l2: 0.000266502
[125]	training's l2: 0.000187223	valid_1's l2: 0.000266549
Early stopping, best iteration is:
[95]	training's l2: 0.000202766	valid_1's l2: 0.000266105
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183022 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000382982	valid_1's l2: 0.000372766
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000370344	valid_1's l2: 0.000360836
[3]	training's l2: 0.000359584	valid_1's l2: 0.000350955
[4]	training's l2: 0.000349989	valid_1's l2: 0.000343012
[5]	training's l2: 0.000341909	valid_1's l2: 0.000335422
[6]	training's l2: 0.000334358	valid_1's l2: 0.000328413
[7]	training's l2: 0.000328021	valid_1's l2: 0.000322482
[8]	training's l2: 0.000322232	valid_1's l2: 0.000317189
[9]	training's l2: 0.00031699	valid_1's l2: 0.000312375
[10]	training's l2: 0.000312313	valid_1's l2: 0.000308579
[11]	training's l2: 0.000308145	valid_1's l2: 0.000305321
[12]	training's l2: 0.000304221	valid_1's l2: 0.000302258
[13]	training's l2: 0.00030089	valid_1's l2: 0.000299768
[14]	training's l2: 0.000297854	valid_1's l2: 0.000297787
[15]	training's l2: 0.000295003	valid_1's l2: 0.000295721
[16]	training's l2: 0.000292423	valid_1's l2: 0.000293566
[17]	training's l2: 0.000290032	valid_1's l2: 0.000291656
[18]	training's l2: 0.000287912	valid_1's l2: 0.000290199
[19]	training's l2: 0.000285999	valid_1's l2: 0.000288708
[20]	training's l2: 0.000284059	valid_1's l2: 0.00028748
[21]	training's l2: 0.00028205	valid_1's l2: 0.000286267
[22]	training's l2: 0.000280174	valid_1's l2: 0.000285081
[23]	training's l2: 0.000278679	valid_1's l2: 0.000284178
[24]	training's l2: 0.000277057	valid_1's l2: 0.000283051
[25]	training's l2: 0.000275468	valid_1's l2: 0.000282159
[26]	training's l2: 0.00027364	valid_1's l2: 0.000281024
[27]	training's l2: 0.000272467	valid_1's l2: 0.000280379
[28]	training's l2: 0.000270969	valid_1's l2: 0.000279595
[29]	training's l2: 0.000269126	valid_1's l2: 0.000278577
[30]	training's l2: 0.000267712	valid_1's l2: 0.000277826
[31]	training's l2: 0.000266339	valid_1's l2: 0.000277016
[32]	training's l2: 0.0002652	valid_1's l2: 0.000276512
[33]	training's l2: 0.000264066	valid_1's l2: 0.000275937
[34]	training's l2: 0.000262848	valid_1's l2: 0.000275405
[35]	training's l2: 0.000261885	valid_1's l2: 0.000275019
[36]	training's l2: 0.000260934	valid_1's l2: 0.00027474
[37]	training's l2: 0.000259828	valid_1's l2: 0.000274125
[38]	training's l2: 0.000258418	valid_1's l2: 0.000273342
[39]	training's l2: 0.000257333	valid_1's l2: 0.000273012
[40]	training's l2: 0.000256431	valid_1's l2: 0.000272827
[41]	training's l2: 0.000255495	valid_1's l2: 0.000272535
[42]	training's l2: 0.000254707	valid_1's l2: 0.000272298
[43]	training's l2: 0.000253665	valid_1's l2: 0.000271923
[44]	training's l2: 0.000252888	valid_1's l2: 0.000271748
[45]	training's l2: 0.000251709	valid_1's l2: 0.000271222
[46]	training's l2: 0.000250875	valid_1's l2: 0.000271018
[47]	training's l2: 0.000249948	valid_1's l2: 0.000270689
[48]	training's l2: 0.000249177	valid_1's l2: 0.00027046
[49]	training's l2: 0.000248389	valid_1's l2: 0.000270231
[50]	training's l2: 0.000247705	valid_1's l2: 0.000270156
[51]	training's l2: 0.000246848	valid_1's l2: 0.000270036
[52]	training's l2: 0.000246136	valid_1's l2: 0.000269946
[53]	training's l2: 0.000245393	valid_1's l2: 0.00026973
[54]	training's l2: 0.000244648	valid_1's l2: 0.000269486
[55]	training's l2: 0.000243994	valid_1's l2: 0.000269321
[56]	training's l2: 0.000243103	valid_1's l2: 0.000269066
[57]	training's l2: 0.00024244	valid_1's l2: 0.00026893
[58]	training's l2: 0.000241787	valid_1's l2: 0.0002689
[59]	training's l2: 0.000241152	valid_1's l2: 0.000268789
[60]	training's l2: 0.000240459	valid_1's l2: 0.000268545
[61]	training's l2: 0.000239854	valid_1's l2: 0.000268369
[62]	training's l2: 0.000238952	valid_1's l2: 0.000268261
[63]	training's l2: 0.000238275	valid_1's l2: 0.000268002
[64]	training's l2: 0.00023763	valid_1's l2: 0.000267879
[65]	training's l2: 0.000237055	valid_1's l2: 0.000267847
[66]	training's l2: 0.000236324	valid_1's l2: 0.000267694
[67]	training's l2: 0.000235623	valid_1's l2: 0.000267602
[68]	training's l2: 0.000234953	valid_1's l2: 0.000267476
[69]	training's l2: 0.000234308	valid_1's l2: 0.000267348
[70]	training's l2: 0.000233614	valid_1's l2: 0.000267291
[71]	training's l2: 0.000233004	valid_1's l2: 0.000267194
[72]	training's l2: 0.000232441	valid_1's l2: 0.000267143
[73]	training's l2: 0.000231956	valid_1's l2: 0.000267147
[74]	training's l2: 0.000231429	valid_1's l2: 0.000267098
[75]	training's l2: 0.000230911	valid_1's l2: 0.000266934
[76]	training's l2: 0.00023013	valid_1's l2: 0.000266874
[77]	training's l2: 0.000229609	valid_1's l2: 0.000266831
[78]	training's l2: 0.000229035	valid_1's l2: 0.000266788
[79]	training's l2: 0.000228321	valid_1's l2: 0.000266574
[80]	training's l2: 0.000227742	valid_1's l2: 0.000266489
[81]	training's l2: 0.000227229	valid_1's l2: 0.000266484
[82]	training's l2: 0.000226644	valid_1's l2: 0.000266513
[83]	training's l2: 0.000226137	valid_1's l2: 0.000266323
[84]	training's l2: 0.000225643	valid_1's l2: 0.000266296
[85]	training's l2: 0.00022513	valid_1's l2: 0.000266362
[86]	training's l2: 0.000224575	valid_1's l2: 0.000266165
[87]	training's l2: 0.000223932	valid_1's l2: 0.000266147
[88]	training's l2: 0.00022347	valid_1's l2: 0.000266138
[89]	training's l2: 0.000223001	valid_1's l2: 0.000266224
[90]	training's l2: 0.000222515	valid_1's l2: 0.00026608
[91]	training's l2: 0.00022204	valid_1's l2: 0.000266126
[92]	training's l2: 0.000221519	valid_1's l2: 0.000266104
[93]	training's l2: 0.000220914	valid_1's l2: 0.000266022
[94]	training's l2: 0.000220312	valid_1's l2: 0.000265887
[95]	training's l2: 0.000219877	valid_1's l2: 0.000265945
[96]	training's l2: 0.000219385	valid_1's l2: 0.000265926
[97]	training's l2: 0.000218872	valid_1's l2: 0.000265851
[98]	training's l2: 0.000218457	valid_1's l2: 0.00026591
[99]	training's l2: 0.000218016	valid_1's l2: 0.000265809
[100]	training's l2: 0.000217458	valid_1's l2: 0.000265656
[101]	training's l2: 0.000216987	valid_1's l2: 0.000265537
[102]	training's l2: 0.000216549	valid_1's l2: 0.000265585
[103]	training's l2: 0.000216117	valid_1's l2: 0.000265632
[104]	training's l2: 0.00021564	valid_1's l2: 0.000265634
[105]	training's l2: 0.000215241	valid_1's l2: 0.000265681
[106]	training's l2: 0.000214808	valid_1's l2: 0.000265744
[107]	training's l2: 0.000214416	valid_1's l2: 0.00026576
[108]	training's l2: 0.00021397	valid_1's l2: 0.000265775
[109]	training's l2: 0.000213535	valid_1's l2: 0.000265641
[110]	training's l2: 0.000213063	valid_1's l2: 0.000265635
[111]	training's l2: 0.000212631	valid_1's l2: 0.00026559
[112]	training's l2: 0.00021212	valid_1's l2: 0.000265516
[113]	training's l2: 0.000211729	valid_1's l2: 0.000265591
[114]	training's l2: 0.000211299	valid_1's l2: 0.000265497
[115]	training's l2: 0.000210888	valid_1's l2: 0.000265401
[116]	training's l2: 0.000210465	valid_1's l2: 0.000265453
[117]	training's l2: 0.000210042	valid_1's l2: 0.000265406
Did not meet early stopping. Best iteration is:
[117]	training's l2: 0.000210042	valid_1's l2: 0.000265406
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183351 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.000308
[1]	training's l2: 0.000383709	valid_1's l2: 0.00037381
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000371492	valid_1's l2: 0.000362574
[3]	training's l2: 0.000360891	valid_1's l2: 0.000352616
[4]	training's l2: 0.000351536	valid_1's l2: 0.000344747
[5]	training's l2: 0.000343207	valid_1's l2: 0.000337071
[6]	training's l2: 0.000336013	valid_1's l2: 0.000330767
[7]	training's l2: 0.000329772	valid_1's l2: 0.000325155
[8]	training's l2: 0.000323804	valid_1's l2: 0.000319791
[9]	training's l2: 0.000318683	valid_1's l2: 0.00031502
[10]	training's l2: 0.00031394	valid_1's l2: 0.000310933
[11]	training's l2: 0.000309803	valid_1's l2: 0.000307534
[12]	training's l2: 0.000305855	valid_1's l2: 0.000304329
[13]	training's l2: 0.000302318	valid_1's l2: 0.000301856
[14]	training's l2: 0.000299059	valid_1's l2: 0.000298941
[15]	training's l2: 0.000296086	valid_1's l2: 0.000297022
[16]	training's l2: 0.000293425	valid_1's l2: 0.000295116
[17]	training's l2: 0.000290751	valid_1's l2: 0.000293242
[18]	training's l2: 0.000288388	valid_1's l2: 0.000291439
[19]	training's l2: 0.000285996	valid_1's l2: 0.000289897
[20]	training's l2: 0.000284138	valid_1's l2: 0.000288599
[21]	training's l2: 0.000282036	valid_1's l2: 0.000287292
[22]	training's l2: 0.000280372	valid_1's l2: 0.000286408
[23]	training's l2: 0.00027876	valid_1's l2: 0.000285399
[24]	training's l2: 0.000277056	valid_1's l2: 0.000284081
[25]	training's l2: 0.000275672	valid_1's l2: 0.000283077
[26]	training's l2: 0.000273722	valid_1's l2: 0.000281847
[27]	training's l2: 0.000272278	valid_1's l2: 0.000280962
[28]	training's l2: 0.000270672	valid_1's l2: 0.000280143
[29]	training's l2: 0.0002693	valid_1's l2: 0.00027944
[30]	training's l2: 0.000267886	valid_1's l2: 0.00027875
[31]	training's l2: 0.000266257	valid_1's l2: 0.000277826
[32]	training's l2: 0.000264916	valid_1's l2: 0.000277016
[33]	training's l2: 0.000263864	valid_1's l2: 0.000276541
[34]	training's l2: 0.000262773	valid_1's l2: 0.000276017
[35]	training's l2: 0.000261792	valid_1's l2: 0.000275664
[36]	training's l2: 0.000260237	valid_1's l2: 0.000274994
[37]	training's l2: 0.000259136	valid_1's l2: 0.000274518
[38]	training's l2: 0.000258128	valid_1's l2: 0.000274124
[39]	training's l2: 0.000256969	valid_1's l2: 0.000273496
[40]	training's l2: 0.000255786	valid_1's l2: 0.000272953
[41]	training's l2: 0.000254942	valid_1's l2: 0.000272746
[42]	training's l2: 0.000253678	valid_1's l2: 0.00027215
[43]	training's l2: 0.000252467	valid_1's l2: 0.000271615
[44]	training's l2: 0.000251596	valid_1's l2: 0.000271308
[45]	training's l2: 0.000250761	valid_1's l2: 0.000271134
[46]	training's l2: 0.00024976	valid_1's l2: 0.000270659
[47]	training's l2: 0.000249012	valid_1's l2: 0.000270531
[48]	training's l2: 0.000248048	valid_1's l2: 0.00027002
[49]	training's l2: 0.000246928	valid_1's l2: 0.000269635
[50]	training's l2: 0.000245939	valid_1's l2: 0.000269362
[51]	training's l2: 0.000245173	valid_1's l2: 0.000269002
[52]	training's l2: 0.000244311	valid_1's l2: 0.000268767
[53]	training's l2: 0.000243517	valid_1's l2: 0.000268596
[54]	training's l2: 0.000242812	valid_1's l2: 0.00026858
[55]	training's l2: 0.000242056	valid_1's l2: 0.000268404
[56]	training's l2: 0.000241086	valid_1's l2: 0.000267997
[57]	training's l2: 0.000240449	valid_1's l2: 0.000267939
[58]	training's l2: 0.000239855	valid_1's l2: 0.000267997
[59]	training's l2: 0.000239137	valid_1's l2: 0.000267838
[60]	training's l2: 0.000238479	valid_1's l2: 0.000267803
[61]	training's l2: 0.000237909	valid_1's l2: 0.000267684
[62]	training's l2: 0.000237271	valid_1's l2: 0.000267523
[63]	training's l2: 0.000236383	valid_1's l2: 0.000267297
[64]	training's l2: 0.000235684	valid_1's l2: 0.000267096
[65]	training's l2: 0.000235056	valid_1's l2: 0.000266963
[66]	training's l2: 0.000234484	valid_1's l2: 0.000267002
[67]	training's l2: 0.000233681	valid_1's l2: 0.000266656
[68]	training's l2: 0.000233095	valid_1's l2: 0.000266671
[69]	training's l2: 0.000232481	valid_1's l2: 0.00026658
[70]	training's l2: 0.000231891	valid_1's l2: 0.000266508
[71]	training's l2: 0.000231277	valid_1's l2: 0.000266405
[72]	training's l2: 0.00023069	valid_1's l2: 0.00026637
[73]	training's l2: 0.000230039	valid_1's l2: 0.000266305
[74]	training's l2: 0.000229514	valid_1's l2: 0.000266332
[75]	training's l2: 0.000228882	valid_1's l2: 0.000266175
[76]	training's l2: 0.000228195	valid_1's l2: 0.000266029
[77]	training's l2: 0.000227565	valid_1's l2: 0.000265994
[78]	training's l2: 0.000227018	valid_1's l2: 0.00026595
[79]	training's l2: 0.000226391	valid_1's l2: 0.000265818
[80]	training's l2: 0.000225775	valid_1's l2: 0.000265755
[81]	training's l2: 0.000225225	valid_1's l2: 0.00026572
[82]	training's l2: 0.000224626	valid_1's l2: 0.000265673
[83]	training's l2: 0.00022407	valid_1's l2: 0.000265649
[84]	training's l2: 0.000223585	valid_1's l2: 0.000265642
[85]	training's l2: 0.000222961	valid_1's l2: 0.000265517
[86]	training's l2: 0.000222407	valid_1's l2: 0.000265389
[87]	training's l2: 0.000221884	valid_1's l2: 0.000265347
[88]	training's l2: 0.000221322	valid_1's l2: 0.000265363
[89]	training's l2: 0.000220728	valid_1's l2: 0.000265357
[90]	training's l2: 0.000220268	valid_1's l2: 0.000265394
[91]	training's l2: 0.000219711	valid_1's l2: 0.000265345
[92]	training's l2: 0.000219205	valid_1's l2: 0.000265405
[93]	training's l2: 0.000218705	valid_1's l2: 0.000265303
[94]	training's l2: 0.000218214	valid_1's l2: 0.00026532
[95]	training's l2: 0.000217713	valid_1's l2: 0.000265255
[96]	training's l2: 0.000217173	valid_1's l2: 0.000265209
[97]	training's l2: 0.000216704	valid_1's l2: 0.00026528
[98]	training's l2: 0.000216184	valid_1's l2: 0.000265306
[99]	training's l2: 0.000215701	valid_1's l2: 0.000265319
[100]	training's l2: 0.000215237	valid_1's l2: 0.000265324
[101]	training's l2: 0.000214761	valid_1's l2: 0.000265295
[102]	training's l2: 0.000214298	valid_1's l2: 0.000265353
[103]	training's l2: 0.00021383	valid_1's l2: 0.000265441
[104]	training's l2: 0.000213389	valid_1's l2: 0.000265393
[105]	training's l2: 0.00021288	valid_1's l2: 0.000265336
[106]	training's l2: 0.000212398	valid_1's l2: 0.000265362
[107]	training's l2: 0.000211897	valid_1's l2: 0.000265251
[108]	training's l2: 0.000211431	valid_1's l2: 0.00026525
[109]	training's l2: 0.000210991	valid_1's l2: 0.000265284
[110]	training's l2: 0.00021045	valid_1's l2: 0.000265271
[111]	training's l2: 0.000209991	valid_1's l2: 0.000265203
[112]	training's l2: 0.000209505	valid_1's l2: 0.000265097
[113]	training's l2: 0.000209046	valid_1's l2: 0.000265176
[114]	training's l2: 0.000208663	valid_1's l2: 0.000265291
[115]	training's l2: 0.000208224	valid_1's l2: 0.00026516
[116]	training's l2: 0.000207678	valid_1's l2: 0.000265235
[117]	training's l2: 0.000207215	valid_1's l2: 0.000265213
[118]	training's l2: 0.000206717	valid_1's l2: 0.000265276
[119]	training's l2: 0.000206305	valid_1's l2: 0.000265219
[120]	training's l2: 0.000205926	valid_1's l2: 0.000265249
[121]	training's l2: 0.00020555	valid_1's l2: 0.000265377
[122]	training's l2: 0.000205116	valid_1's l2: 0.000265336
[123]	training's l2: 0.0002047	valid_1's l2: 0.000265391
[124]	training's l2: 0.000204303	valid_1's l2: 0.000265358
[125]	training's l2: 0.000203933	valid_1's l2: 0.000265317
[126]	training's l2: 0.000203511	valid_1's l2: 0.00026537
[127]	training's l2: 0.000203106	valid_1's l2: 0.000265278
[128]	training's l2: 0.000202706	valid_1's l2: 0.000265235
[129]	training's l2: 0.000202291	valid_1's l2: 0.000265301
[130]	training's l2: 0.000201882	valid_1's l2: 0.000265374
[131]	training's l2: 0.000201429	valid_1's l2: 0.00026535
[132]	training's l2: 0.000201039	valid_1's l2: 0.000265325
[133]	training's l2: 0.000200628	valid_1's l2: 0.000265389
[134]	training's l2: 0.000200224	valid_1's l2: 0.000265429
[135]	training's l2: 0.000199859	valid_1's l2: 0.00026549
[136]	training's l2: 0.000199417	valid_1's l2: 0.000265576
[137]	training's l2: 0.000198986	valid_1's l2: 0.000265591
[138]	training's l2: 0.000198629	valid_1's l2: 0.000265636
[139]	training's l2: 0.000197995	valid_1's l2: 0.000265358
[140]	training's l2: 0.000197509	valid_1's l2: 0.000265205
[141]	training's l2: 0.000197158	valid_1's l2: 0.000265161
[142]	training's l2: 0.000196779	valid_1's l2: 0.000265178
Early stopping, best iteration is:
[112]	training's l2: 0.000209505	valid_1's l2: 0.000265097
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178947 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000984436	valid_1's l2: 0.000892225
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000967887	valid_1's l2: 0.000880367
[3]	training's l2: 0.000952528	valid_1's l2: 0.00086841
[4]	training's l2: 0.000937962	valid_1's l2: 0.000857272
[5]	training's l2: 0.000925568	valid_1's l2: 0.000848384
[6]	training's l2: 0.00091426	valid_1's l2: 0.000839752
[7]	training's l2: 0.000904584	valid_1's l2: 0.000833611
[8]	training's l2: 0.000894839	valid_1's l2: 0.000826812
[9]	training's l2: 0.000885395	valid_1's l2: 0.00082025
[10]	training's l2: 0.000876603	valid_1's l2: 0.000813684
[11]	training's l2: 0.000868736	valid_1's l2: 0.000809012
[12]	training's l2: 0.0008627	valid_1's l2: 0.00080521
[13]	training's l2: 0.000856124	valid_1's l2: 0.000800365
[14]	training's l2: 0.000850028	valid_1's l2: 0.000796585
[15]	training's l2: 0.000843855	valid_1's l2: 0.000791819
[16]	training's l2: 0.000839336	valid_1's l2: 0.000788305
[17]	training's l2: 0.000834703	valid_1's l2: 0.000785973
[18]	training's l2: 0.000830053	valid_1's l2: 0.000782098
[19]	training's l2: 0.000825323	valid_1's l2: 0.000779442
[20]	training's l2: 0.000821687	valid_1's l2: 0.000778153
[21]	training's l2: 0.000817802	valid_1's l2: 0.000776013
[22]	training's l2: 0.000813358	valid_1's l2: 0.000773735
[23]	training's l2: 0.000809602	valid_1's l2: 0.000770777
[24]	training's l2: 0.000805373	valid_1's l2: 0.000768957
[25]	training's l2: 0.000801895	valid_1's l2: 0.000767355
[26]	training's l2: 0.000798774	valid_1's l2: 0.000764733
[27]	training's l2: 0.00079532	valid_1's l2: 0.00076269
[28]	training's l2: 0.000791705	valid_1's l2: 0.000760943
[29]	training's l2: 0.000788772	valid_1's l2: 0.000759671
[30]	training's l2: 0.000786226	valid_1's l2: 0.000758788
[31]	training's l2: 0.0007833	valid_1's l2: 0.000756629
[32]	training's l2: 0.000779922	valid_1's l2: 0.000755122
[33]	training's l2: 0.000776371	valid_1's l2: 0.000752908
[34]	training's l2: 0.000773257	valid_1's l2: 0.000751261
[35]	training's l2: 0.00076962	valid_1's l2: 0.000748085
[36]	training's l2: 0.000767091	valid_1's l2: 0.000746341
[37]	training's l2: 0.000765095	valid_1's l2: 0.000745614
[38]	training's l2: 0.000762212	valid_1's l2: 0.000744194
[39]	training's l2: 0.000759381	valid_1's l2: 0.00074278
[40]	training's l2: 0.000756354	valid_1's l2: 0.000741024
[41]	training's l2: 0.000754487	valid_1's l2: 0.000740711
[42]	training's l2: 0.000752099	valid_1's l2: 0.000739639
[43]	training's l2: 0.000749137	valid_1's l2: 0.000737792
[44]	training's l2: 0.000746608	valid_1's l2: 0.000737018
[45]	training's l2: 0.000744433	valid_1's l2: 0.000735989
[46]	training's l2: 0.000742063	valid_1's l2: 0.000734531
[47]	training's l2: 0.000738959	valid_1's l2: 0.000732399
[48]	training's l2: 0.000737196	valid_1's l2: 0.000731723
[49]	training's l2: 0.000735449	valid_1's l2: 0.000730783
[50]	training's l2: 0.000733231	valid_1's l2: 0.000730101
[51]	training's l2: 0.000730583	valid_1's l2: 0.000728151
[52]	training's l2: 0.000727923	valid_1's l2: 0.000727161
[53]	training's l2: 0.000725845	valid_1's l2: 0.000726809
[54]	training's l2: 0.000724173	valid_1's l2: 0.000726528
[55]	training's l2: 0.000722664	valid_1's l2: 0.000725738
[56]	training's l2: 0.000720546	valid_1's l2: 0.000725131
[57]	training's l2: 0.000718736	valid_1's l2: 0.000724985
[58]	training's l2: 0.000716989	valid_1's l2: 0.000724281
[59]	training's l2: 0.000715339	valid_1's l2: 0.000724199
[60]	training's l2: 0.000713672	valid_1's l2: 0.000723888
[61]	training's l2: 0.00071163	valid_1's l2: 0.000723368
[62]	training's l2: 0.000709945	valid_1's l2: 0.000722862
[63]	training's l2: 0.00070799	valid_1's l2: 0.000722555
[64]	training's l2: 0.000706063	valid_1's l2: 0.00072234
[65]	training's l2: 0.000703784	valid_1's l2: 0.00072058
[66]	training's l2: 0.000701601	valid_1's l2: 0.000719853
[67]	training's l2: 0.000700059	valid_1's l2: 0.000719895
[68]	training's l2: 0.000698535	valid_1's l2: 0.000719573
[69]	training's l2: 0.000696642	valid_1's l2: 0.000719446
[70]	training's l2: 0.000694995	valid_1's l2: 0.000719524
[71]	training's l2: 0.000693654	valid_1's l2: 0.00071917
[72]	training's l2: 0.000692008	valid_1's l2: 0.000719001
[73]	training's l2: 0.000690581	valid_1's l2: 0.000719132
[74]	training's l2: 0.000688522	valid_1's l2: 0.000717923
[75]	training's l2: 0.000687155	valid_1's l2: 0.000717802
[76]	training's l2: 0.000685618	valid_1's l2: 0.000717564
[77]	training's l2: 0.000684023	valid_1's l2: 0.000717141
[78]	training's l2: 0.000682366	valid_1's l2: 0.000717031
[79]	training's l2: 0.000680714	valid_1's l2: 0.00071675
[80]	training's l2: 0.000678666	valid_1's l2: 0.000715373
[81]	training's l2: 0.000677239	valid_1's l2: 0.000715078
[82]	training's l2: 0.000675872	valid_1's l2: 0.000714935
[83]	training's l2: 0.000674655	valid_1's l2: 0.000714687
[84]	training's l2: 0.000673305	valid_1's l2: 0.000714439
[85]	training's l2: 0.000671775	valid_1's l2: 0.00071425
[86]	training's l2: 0.000670361	valid_1's l2: 0.000714097
[87]	training's l2: 0.000668723	valid_1's l2: 0.000713387
[88]	training's l2: 0.000666643	valid_1's l2: 0.000712702
[89]	training's l2: 0.000665374	valid_1's l2: 0.000712298
[90]	training's l2: 0.000664096	valid_1's l2: 0.000712125
[91]	training's l2: 0.000662603	valid_1's l2: 0.000711766
[92]	training's l2: 0.000661231	valid_1's l2: 0.000711502
[93]	training's l2: 0.000659596	valid_1's l2: 0.000711476
[94]	training's l2: 0.000658362	valid_1's l2: 0.000711292
[95]	training's l2: 0.00065712	valid_1's l2: 0.000711235
[96]	training's l2: 0.000655796	valid_1's l2: 0.000711228
[97]	training's l2: 0.000654182	valid_1's l2: 0.000710432
[98]	training's l2: 0.000652931	valid_1's l2: 0.000710368
[99]	training's l2: 0.000651631	valid_1's l2: 0.000710464
[100]	training's l2: 0.00065039	valid_1's l2: 0.000710472
[101]	training's l2: 0.000649142	valid_1's l2: 0.000710408
[102]	training's l2: 0.000647578	valid_1's l2: 0.000710242
[103]	training's l2: 0.000646427	valid_1's l2: 0.000710347
[104]	training's l2: 0.000644964	valid_1's l2: 0.000710036
[105]	training's l2: 0.000643829	valid_1's l2: 0.000710218
[106]	training's l2: 0.00064254	valid_1's l2: 0.000710311
[107]	training's l2: 0.000641312	valid_1's l2: 0.000710081
[108]	training's l2: 0.000640038	valid_1's l2: 0.000709848
[109]	training's l2: 0.000638665	valid_1's l2: 0.000709669
[110]	training's l2: 0.000637454	valid_1's l2: 0.000709762
[111]	training's l2: 0.000636245	valid_1's l2: 0.000709861
[112]	training's l2: 0.000635069	valid_1's l2: 0.000709505
[113]	training's l2: 0.000633713	valid_1's l2: 0.000709022
[114]	training's l2: 0.000632467	valid_1's l2: 0.000708755
[115]	training's l2: 0.000631243	valid_1's l2: 0.000708991
[116]	training's l2: 0.000630123	valid_1's l2: 0.000708676
[117]	training's l2: 0.000628917	valid_1's l2: 0.000708636
[118]	training's l2: 0.000627802	valid_1's l2: 0.000708834
[119]	training's l2: 0.000626476	valid_1's l2: 0.000708526
[120]	training's l2: 0.00062547	valid_1's l2: 0.000708715
[121]	training's l2: 0.000624269	valid_1's l2: 0.000708908
[122]	training's l2: 0.000623088	valid_1's l2: 0.000708914
[123]	training's l2: 0.000622052	valid_1's l2: 0.000708882
[124]	training's l2: 0.000620672	valid_1's l2: 0.000708321
[125]	training's l2: 0.000619783	valid_1's l2: 0.000708311
[126]	training's l2: 0.000618639	valid_1's l2: 0.000707949
[127]	training's l2: 0.000617474	valid_1's l2: 0.000707998
[128]	training's l2: 0.00061635	valid_1's l2: 0.000707961
[129]	training's l2: 0.000615139	valid_1's l2: 0.000707887
[130]	training's l2: 0.000614055	valid_1's l2: 0.000708035
[131]	training's l2: 0.000612896	valid_1's l2: 0.000708036
[132]	training's l2: 0.000611969	valid_1's l2: 0.00070803
[133]	training's l2: 0.00061072	valid_1's l2: 0.000708028
[134]	training's l2: 0.000609514	valid_1's l2: 0.000707577
[135]	training's l2: 0.000608357	valid_1's l2: 0.000707556
[136]	training's l2: 0.000607195	valid_1's l2: 0.000707249
[137]	training's l2: 0.000606073	valid_1's l2: 0.000707329
[138]	training's l2: 0.000604972	valid_1's l2: 0.000707467
[139]	training's l2: 0.00060385	valid_1's l2: 0.000707541
[140]	training's l2: 0.000602714	valid_1's l2: 0.000707411
[141]	training's l2: 0.000601856	valid_1's l2: 0.000707489
[142]	training's l2: 0.000600364	valid_1's l2: 0.000707265
[143]	training's l2: 0.000599225	valid_1's l2: 0.000707374
[144]	training's l2: 0.000598071	valid_1's l2: 0.000707432
[145]	training's l2: 0.000596957	valid_1's l2: 0.000707427
[146]	training's l2: 0.000595857	valid_1's l2: 0.000707534
[147]	training's l2: 0.00059495	valid_1's l2: 0.000707509
[148]	training's l2: 0.000593951	valid_1's l2: 0.000707578
[149]	training's l2: 0.000593074	valid_1's l2: 0.000707658
[150]	training's l2: 0.000592228	valid_1's l2: 0.000707599
[151]	training's l2: 0.000590978	valid_1's l2: 0.000707166
[152]	training's l2: 0.000589864	valid_1's l2: 0.000707463
[153]	training's l2: 0.000588816	valid_1's l2: 0.000707571
[154]	training's l2: 0.000587899	valid_1's l2: 0.00070769
[155]	training's l2: 0.000587124	valid_1's l2: 0.000707687
[156]	training's l2: 0.000586251	valid_1's l2: 0.000707657
[157]	training's l2: 0.000585032	valid_1's l2: 0.000707735
[158]	training's l2: 0.000584162	valid_1's l2: 0.00070778
[159]	training's l2: 0.000583307	valid_1's l2: 0.00070774
[160]	training's l2: 0.000582236	valid_1's l2: 0.000707756
[161]	training's l2: 0.000581295	valid_1's l2: 0.000707573
[162]	training's l2: 0.000580261	valid_1's l2: 0.000707398
[163]	training's l2: 0.00057948	valid_1's l2: 0.000707347
[164]	training's l2: 0.000578647	valid_1's l2: 0.000707713
[165]	training's l2: 0.000577708	valid_1's l2: 0.000707697
[166]	training's l2: 0.000576719	valid_1's l2: 0.000707581
[167]	training's l2: 0.000575848	valid_1's l2: 0.000707443
[168]	training's l2: 0.000574794	valid_1's l2: 0.000707368
[169]	training's l2: 0.000573891	valid_1's l2: 0.000707265
[170]	training's l2: 0.000573159	valid_1's l2: 0.000707265
[171]	training's l2: 0.00057208	valid_1's l2: 0.000707054
[172]	training's l2: 0.000571119	valid_1's l2: 0.000707034
[173]	training's l2: 0.000570179	valid_1's l2: 0.00070736
[174]	training's l2: 0.000569165	valid_1's l2: 0.000707359
[175]	training's l2: 0.000568228	valid_1's l2: 0.000707306
[176]	training's l2: 0.000567373	valid_1's l2: 0.00070714
[177]	training's l2: 0.000566363	valid_1's l2: 0.00070718
[178]	training's l2: 0.000565437	valid_1's l2: 0.000707097
[179]	training's l2: 0.000564514	valid_1's l2: 0.000707166
[180]	training's l2: 0.000563778	valid_1's l2: 0.000707239
[181]	training's l2: 0.000562903	valid_1's l2: 0.000707284
[182]	training's l2: 0.000561993	valid_1's l2: 0.000707253
[183]	training's l2: 0.000561031	valid_1's l2: 0.000707178
[184]	training's l2: 0.00056025	valid_1's l2: 0.000707061
[185]	training's l2: 0.000559213	valid_1's l2: 0.000707265
[186]	training's l2: 0.000558449	valid_1's l2: 0.000707401
[187]	training's l2: 0.000557478	valid_1's l2: 0.000707453
[188]	training's l2: 0.000556798	valid_1's l2: 0.000707345
Did not meet early stopping. Best iteration is:
[188]	training's l2: 0.000556798	valid_1's l2: 0.000707345
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180520 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000979652	valid_1's l2: 0.000889581
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000958939	valid_1's l2: 0.000875262
[3]	training's l2: 0.000941031	valid_1's l2: 0.000862253
[4]	training's l2: 0.000924367	valid_1's l2: 0.000850113
[5]	training's l2: 0.000909812	valid_1's l2: 0.000839526
[6]	training's l2: 0.000897315	valid_1's l2: 0.000831755
[7]	training's l2: 0.000885271	valid_1's l2: 0.000824181
[8]	training's l2: 0.00087449	valid_1's l2: 0.000817217
[9]	training's l2: 0.000864443	valid_1's l2: 0.000811602
[10]	training's l2: 0.000854552	valid_1's l2: 0.000803996
[11]	training's l2: 0.00084597	valid_1's l2: 0.000799047
[12]	training's l2: 0.000837889	valid_1's l2: 0.000793447
[13]	training's l2: 0.000831331	valid_1's l2: 0.000789841
[14]	training's l2: 0.000825174	valid_1's l2: 0.000785913
[15]	training's l2: 0.000819068	valid_1's l2: 0.00078188
[16]	training's l2: 0.000812206	valid_1's l2: 0.000778341
[17]	training's l2: 0.000805776	valid_1's l2: 0.000775012
[18]	training's l2: 0.000799853	valid_1's l2: 0.000771955
[19]	training's l2: 0.000794322	valid_1's l2: 0.00076855
[20]	training's l2: 0.000789677	valid_1's l2: 0.000766173
[21]	training's l2: 0.000785534	valid_1's l2: 0.000764655
[22]	training's l2: 0.000781129	valid_1's l2: 0.00076216
[23]	training's l2: 0.000775936	valid_1's l2: 0.00075874
[24]	training's l2: 0.000771947	valid_1's l2: 0.000757204
[25]	training's l2: 0.000767803	valid_1's l2: 0.000754853
[26]	training's l2: 0.000763514	valid_1's l2: 0.000753439
[27]	training's l2: 0.000759644	valid_1's l2: 0.000751202
[28]	training's l2: 0.000756218	valid_1's l2: 0.000749073
[29]	training's l2: 0.000752065	valid_1's l2: 0.00074687
[30]	training's l2: 0.000748459	valid_1's l2: 0.000745769
[31]	training's l2: 0.00074477	valid_1's l2: 0.000744011
[32]	training's l2: 0.000741189	valid_1's l2: 0.00074227
[33]	training's l2: 0.000737111	valid_1's l2: 0.000740332
[34]	training's l2: 0.000733385	valid_1's l2: 0.00073813
[35]	training's l2: 0.000730169	valid_1's l2: 0.000737041
[36]	training's l2: 0.000726767	valid_1's l2: 0.000735518
[37]	training's l2: 0.000723515	valid_1's l2: 0.000734754
[38]	training's l2: 0.000720633	valid_1's l2: 0.000734206
[39]	training's l2: 0.00071717	valid_1's l2: 0.000732539
[40]	training's l2: 0.000714102	valid_1's l2: 0.000732383
[41]	training's l2: 0.000711456	valid_1's l2: 0.000731314
[42]	training's l2: 0.000707491	valid_1's l2: 0.000729035
[43]	training's l2: 0.000704271	valid_1's l2: 0.000727341
[44]	training's l2: 0.000701903	valid_1's l2: 0.000727274
[45]	training's l2: 0.000699652	valid_1's l2: 0.000726603
[46]	training's l2: 0.000696641	valid_1's l2: 0.000726035
[47]	training's l2: 0.000694337	valid_1's l2: 0.000725408
[48]	training's l2: 0.000692003	valid_1's l2: 0.000725033
[49]	training's l2: 0.000688893	valid_1's l2: 0.000723468
[50]	training's l2: 0.000686752	valid_1's l2: 0.000722937
[51]	training's l2: 0.000684237	valid_1's l2: 0.000722535
[52]	training's l2: 0.000681765	valid_1's l2: 0.000722133
[53]	training's l2: 0.000679076	valid_1's l2: 0.000721848
[54]	training's l2: 0.000676479	valid_1's l2: 0.000720546
[55]	training's l2: 0.00067394	valid_1's l2: 0.000720335
[56]	training's l2: 0.000671928	valid_1's l2: 0.000720349
[57]	training's l2: 0.000669279	valid_1's l2: 0.000719935
[58]	training's l2: 0.000666862	valid_1's l2: 0.000719452
[59]	training's l2: 0.000664977	valid_1's l2: 0.000718999
[60]	training's l2: 0.00066267	valid_1's l2: 0.000718433
[61]	training's l2: 0.000660295	valid_1's l2: 0.000718144
[62]	training's l2: 0.000658389	valid_1's l2: 0.000718136
[63]	training's l2: 0.000655793	valid_1's l2: 0.000716162
[64]	training's l2: 0.000653612	valid_1's l2: 0.000715925
[65]	training's l2: 0.000651075	valid_1's l2: 0.000715709
[66]	training's l2: 0.000648843	valid_1's l2: 0.000715946
[67]	training's l2: 0.000646862	valid_1's l2: 0.00071551
[68]	training's l2: 0.000644566	valid_1's l2: 0.000715588
[69]	training's l2: 0.0006425	valid_1's l2: 0.000715337
[70]	training's l2: 0.00064051	valid_1's l2: 0.000715178
[71]	training's l2: 0.000638528	valid_1's l2: 0.000715088
[72]	training's l2: 0.000636353	valid_1's l2: 0.000715141
[73]	training's l2: 0.000634139	valid_1's l2: 0.000715376
[74]	training's l2: 0.00063205	valid_1's l2: 0.000714535
[75]	training's l2: 0.000629953	valid_1's l2: 0.000714493
[76]	training's l2: 0.000627712	valid_1's l2: 0.000714571
[77]	training's l2: 0.000625567	valid_1's l2: 0.000713995
[78]	training's l2: 0.00062376	valid_1's l2: 0.000713991
[79]	training's l2: 0.000621503	valid_1's l2: 0.000713732
[80]	training's l2: 0.000619676	valid_1's l2: 0.000713741
[81]	training's l2: 0.000617713	valid_1's l2: 0.000713738
[82]	training's l2: 0.000615651	valid_1's l2: 0.00071324
[83]	training's l2: 0.000613809	valid_1's l2: 0.000713265
[84]	training's l2: 0.000612294	valid_1's l2: 0.00071341
[85]	training's l2: 0.000610246	valid_1's l2: 0.000713141
[86]	training's l2: 0.000608186	valid_1's l2: 0.000712967
[87]	training's l2: 0.000606177	valid_1's l2: 0.000712174
[88]	training's l2: 0.000604343	valid_1's l2: 0.000712098
[89]	training's l2: 0.000602564	valid_1's l2: 0.00071189
[90]	training's l2: 0.000600853	valid_1's l2: 0.000712058
[91]	training's l2: 0.000599227	valid_1's l2: 0.000712086
[92]	training's l2: 0.000597786	valid_1's l2: 0.000712328
[93]	training's l2: 0.000596372	valid_1's l2: 0.0007127
[94]	training's l2: 0.000594703	valid_1's l2: 0.000712662
[95]	training's l2: 0.000593098	valid_1's l2: 0.000712828
[96]	training's l2: 0.000591625	valid_1's l2: 0.000712749
[97]	training's l2: 0.000589676	valid_1's l2: 0.000712289
[98]	training's l2: 0.000588275	valid_1's l2: 0.000712238
[99]	training's l2: 0.000586579	valid_1's l2: 0.000712402
[100]	training's l2: 0.000585038	valid_1's l2: 0.000712158
[101]	training's l2: 0.000583153	valid_1's l2: 0.000712307
[102]	training's l2: 0.000581652	valid_1's l2: 0.000712815
[103]	training's l2: 0.000580061	valid_1's l2: 0.00071206
[104]	training's l2: 0.000578296	valid_1's l2: 0.000711634
[105]	training's l2: 0.000577004	valid_1's l2: 0.000711463
[106]	training's l2: 0.000575174	valid_1's l2: 0.000711204
[107]	training's l2: 0.000573382	valid_1's l2: 0.000711206
[108]	training's l2: 0.000571609	valid_1's l2: 0.000711392
[109]	training's l2: 0.000570102	valid_1's l2: 0.000711347
[110]	training's l2: 0.000568609	valid_1's l2: 0.000711708
[111]	training's l2: 0.000567247	valid_1's l2: 0.000711687
[112]	training's l2: 0.000565673	valid_1's l2: 0.00071175
[113]	training's l2: 0.000563975	valid_1's l2: 0.00071162
[114]	training's l2: 0.000562296	valid_1's l2: 0.000711524
[115]	training's l2: 0.000560938	valid_1's l2: 0.000711603
[116]	training's l2: 0.000559491	valid_1's l2: 0.00071153
[117]	training's l2: 0.000557846	valid_1's l2: 0.000711145
[118]	training's l2: 0.000556686	valid_1's l2: 0.000711354
[119]	training's l2: 0.000555311	valid_1's l2: 0.000711232
[120]	training's l2: 0.00055402	valid_1's l2: 0.000710964
[121]	training's l2: 0.000552045	valid_1's l2: 0.000710162
[122]	training's l2: 0.000550577	valid_1's l2: 0.000710319
[123]	training's l2: 0.000549004	valid_1's l2: 0.000710559
[124]	training's l2: 0.000547863	valid_1's l2: 0.000710633
[125]	training's l2: 0.000546103	valid_1's l2: 0.000710665
[126]	training's l2: 0.000544788	valid_1's l2: 0.000710763
[127]	training's l2: 0.000543469	valid_1's l2: 0.000710636
[128]	training's l2: 0.000542153	valid_1's l2: 0.000710739
[129]	training's l2: 0.000540567	valid_1's l2: 0.000711192
[130]	training's l2: 0.000539465	valid_1's l2: 0.000711236
[131]	training's l2: 0.000538096	valid_1's l2: 0.00071106
[132]	training's l2: 0.000536723	valid_1's l2: 0.000711073
[133]	training's l2: 0.000535229	valid_1's l2: 0.000710619
[134]	training's l2: 0.000533724	valid_1's l2: 0.000710176
[135]	training's l2: 0.000532745	valid_1's l2: 0.000710187
[136]	training's l2: 0.000531409	valid_1's l2: 0.000710507
[137]	training's l2: 0.000530201	valid_1's l2: 0.000710434
[138]	training's l2: 0.000528725	valid_1's l2: 0.000710135
[139]	training's l2: 0.000527669	valid_1's l2: 0.000710248
[140]	training's l2: 0.000526513	valid_1's l2: 0.000710327
[141]	training's l2: 0.000525081	valid_1's l2: 0.000710209
[142]	training's l2: 0.00052387	valid_1's l2: 0.000710267
[143]	training's l2: 0.000522755	valid_1's l2: 0.000710084
[144]	training's l2: 0.000521535	valid_1's l2: 0.000710107
[145]	training's l2: 0.000520104	valid_1's l2: 0.000710222
[146]	training's l2: 0.000519085	valid_1's l2: 0.000710224
[147]	training's l2: 0.000518031	valid_1's l2: 0.00071013
[148]	training's l2: 0.000516837	valid_1's l2: 0.000710267
[149]	training's l2: 0.000515734	valid_1's l2: 0.000710173
[150]	training's l2: 0.000514704	valid_1's l2: 0.000710123
[151]	training's l2: 0.000513578	valid_1's l2: 0.000710076
[152]	training's l2: 0.000512394	valid_1's l2: 0.00071016
[153]	training's l2: 0.000511392	valid_1's l2: 0.000710368
[154]	training's l2: 0.000510103	valid_1's l2: 0.000710663
[155]	training's l2: 0.000508951	valid_1's l2: 0.000710702
[156]	training's l2: 0.000507848	valid_1's l2: 0.00071065
[157]	training's l2: 0.000506745	valid_1's l2: 0.000710653
[158]	training's l2: 0.000505747	valid_1's l2: 0.000710907
[159]	training's l2: 0.000504643	valid_1's l2: 0.000710949
[160]	training's l2: 0.000503407	valid_1's l2: 0.000711072
[161]	training's l2: 0.000502417	valid_1's l2: 0.000711336
[162]	training's l2: 0.000501433	valid_1's l2: 0.000711416
[163]	training's l2: 0.000500374	valid_1's l2: 0.000711349
[164]	training's l2: 0.000499346	valid_1's l2: 0.000711298
[165]	training's l2: 0.000498164	valid_1's l2: 0.000711196
[166]	training's l2: 0.000496914	valid_1's l2: 0.000711113
[167]	training's l2: 0.000495711	valid_1's l2: 0.000711128
[168]	training's l2: 0.000494501	valid_1's l2: 0.00071125
[169]	training's l2: 0.000493411	valid_1's l2: 0.000711209
[170]	training's l2: 0.000492387	valid_1's l2: 0.000711244
[171]	training's l2: 0.000491048	valid_1's l2: 0.000711186
[172]	training's l2: 0.000489537	valid_1's l2: 0.00071101
[173]	training's l2: 0.000488431	valid_1's l2: 0.000710776
[174]	training's l2: 0.00048737	valid_1's l2: 0.000710655
[175]	training's l2: 0.000486376	valid_1's l2: 0.000710657
[176]	training's l2: 0.000485381	valid_1's l2: 0.000710752
[177]	training's l2: 0.000484243	valid_1's l2: 0.00071082
[178]	training's l2: 0.000483127	valid_1's l2: 0.000710783
[179]	training's l2: 0.00048189	valid_1's l2: 0.000710784
[180]	training's l2: 0.000480637	valid_1's l2: 0.0007105
[181]	training's l2: 0.000479645	valid_1's l2: 0.000710462
Early stopping, best iteration is:
[151]	training's l2: 0.000513578	valid_1's l2: 0.000710076
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176149 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000982627	valid_1's l2: 0.00089081
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000964032	valid_1's l2: 0.000876464
[3]	training's l2: 0.000947494	valid_1's l2: 0.000864281
[4]	training's l2: 0.000933776	valid_1's l2: 0.000854377
[5]	training's l2: 0.000921008	valid_1's l2: 0.00084522
[6]	training's l2: 0.000909313	valid_1's l2: 0.000837603
[7]	training's l2: 0.000898958	valid_1's l2: 0.000830947
[8]	training's l2: 0.000889236	valid_1's l2: 0.000824248
[9]	training's l2: 0.000879697	valid_1's l2: 0.000818606
[10]	training's l2: 0.000870442	valid_1's l2: 0.000811374
[11]	training's l2: 0.000861628	valid_1's l2: 0.000803874
[12]	training's l2: 0.000855027	valid_1's l2: 0.000801024
[13]	training's l2: 0.000848755	valid_1's l2: 0.000797439
[14]	training's l2: 0.000841967	valid_1's l2: 0.000793148
[15]	training's l2: 0.00083599	valid_1's l2: 0.000788621
[16]	training's l2: 0.000830249	valid_1's l2: 0.000784857
[17]	training's l2: 0.000825292	valid_1's l2: 0.000781601
[18]	training's l2: 0.000820426	valid_1's l2: 0.000778373
[19]	training's l2: 0.000816519	valid_1's l2: 0.000776353
[20]	training's l2: 0.000811959	valid_1's l2: 0.000774128
[21]	training's l2: 0.000808515	valid_1's l2: 0.000772728
[22]	training's l2: 0.000804197	valid_1's l2: 0.000769244
[23]	training's l2: 0.000799941	valid_1's l2: 0.000766651
[24]	training's l2: 0.000796288	valid_1's l2: 0.000764601
[25]	training's l2: 0.000792894	valid_1's l2: 0.000762948
[26]	training's l2: 0.000789495	valid_1's l2: 0.000760403
[27]	training's l2: 0.00078635	valid_1's l2: 0.000758584
[28]	training's l2: 0.000782589	valid_1's l2: 0.000756609
[29]	training's l2: 0.000779827	valid_1's l2: 0.000755592
[30]	training's l2: 0.000776574	valid_1's l2: 0.000753554
[31]	training's l2: 0.000773166	valid_1's l2: 0.000751391
[32]	training's l2: 0.000770007	valid_1's l2: 0.00074948
[33]	training's l2: 0.000766664	valid_1's l2: 0.000747775
[34]	training's l2: 0.00076367	valid_1's l2: 0.000745493
[35]	training's l2: 0.000760745	valid_1's l2: 0.000743724
[36]	training's l2: 0.000758005	valid_1's l2: 0.000742453
[37]	training's l2: 0.00075358	valid_1's l2: 0.000738755
[38]	training's l2: 0.000750243	valid_1's l2: 0.000737041
[39]	training's l2: 0.000747977	valid_1's l2: 0.00073609
[40]	training's l2: 0.000745158	valid_1's l2: 0.000734705
[41]	training's l2: 0.000742684	valid_1's l2: 0.000733167
[42]	training's l2: 0.000740156	valid_1's l2: 0.000731878
[43]	training's l2: 0.000737699	valid_1's l2: 0.000730753
[44]	training's l2: 0.000735026	valid_1's l2: 0.000728774
[45]	training's l2: 0.000732456	valid_1's l2: 0.00072754
[46]	training's l2: 0.00073042	valid_1's l2: 0.000726751
[47]	training's l2: 0.000728627	valid_1's l2: 0.000726402
[48]	training's l2: 0.000726638	valid_1's l2: 0.000726066
[49]	training's l2: 0.000724549	valid_1's l2: 0.00072542
[50]	training's l2: 0.000722301	valid_1's l2: 0.000725223
[51]	training's l2: 0.00072034	valid_1's l2: 0.00072485
[52]	training's l2: 0.000718608	valid_1's l2: 0.000724575
[53]	training's l2: 0.000715556	valid_1's l2: 0.000722318
[54]	training's l2: 0.000713574	valid_1's l2: 0.000721914
[55]	training's l2: 0.000711751	valid_1's l2: 0.000721327
[56]	training's l2: 0.000709828	valid_1's l2: 0.000720963
[57]	training's l2: 0.000707908	valid_1's l2: 0.000720435
[58]	training's l2: 0.000705599	valid_1's l2: 0.000720203
[59]	training's l2: 0.000703469	valid_1's l2: 0.000719557
[60]	training's l2: 0.000701823	valid_1's l2: 0.000718607
[61]	training's l2: 0.000699464	valid_1's l2: 0.000717542
[62]	training's l2: 0.000697122	valid_1's l2: 0.000715776
[63]	training's l2: 0.000695495	valid_1's l2: 0.000715665
[64]	training's l2: 0.000693362	valid_1's l2: 0.000715456
[65]	training's l2: 0.000691818	valid_1's l2: 0.000715518
[66]	training's l2: 0.000690228	valid_1's l2: 0.000715257
[67]	training's l2: 0.000688429	valid_1's l2: 0.000715402
[68]	training's l2: 0.000686751	valid_1's l2: 0.000714644
[69]	training's l2: 0.000685039	valid_1's l2: 0.000714648
[70]	training's l2: 0.000683348	valid_1's l2: 0.000713971
[71]	training's l2: 0.000681865	valid_1's l2: 0.000713719
[72]	training's l2: 0.000680352	valid_1's l2: 0.000713564
[73]	training's l2: 0.00067828	valid_1's l2: 0.00071274
[74]	training's l2: 0.000676382	valid_1's l2: 0.000712358
[75]	training's l2: 0.000673969	valid_1's l2: 0.000711469
[76]	training's l2: 0.000672434	valid_1's l2: 0.000711773
[77]	training's l2: 0.000671046	valid_1's l2: 0.000711717
[78]	training's l2: 0.000669556	valid_1's l2: 0.000711719
[79]	training's l2: 0.000668259	valid_1's l2: 0.000711053
[80]	training's l2: 0.000666598	valid_1's l2: 0.000710651
[81]	training's l2: 0.000664967	valid_1's l2: 0.000710636
[82]	training's l2: 0.000663478	valid_1's l2: 0.000710679
[83]	training's l2: 0.000661651	valid_1's l2: 0.00071034
[84]	training's l2: 0.000659965	valid_1's l2: 0.000709987
[85]	training's l2: 0.000657848	valid_1's l2: 0.000709603
[86]	training's l2: 0.000656352	valid_1's l2: 0.000709396
[87]	training's l2: 0.000654874	valid_1's l2: 0.000709458
[88]	training's l2: 0.000653491	valid_1's l2: 0.000709583
[89]	training's l2: 0.000652092	valid_1's l2: 0.00070962
[90]	training's l2: 0.000650749	valid_1's l2: 0.000709353
[91]	training's l2: 0.000649267	valid_1's l2: 0.000709466
[92]	training's l2: 0.000647945	valid_1's l2: 0.000709606
[93]	training's l2: 0.000646346	valid_1's l2: 0.000709421
[94]	training's l2: 0.000644838	valid_1's l2: 0.000709309
[95]	training's l2: 0.000643474	valid_1's l2: 0.000709124
[96]	training's l2: 0.000642181	valid_1's l2: 0.00070898
[97]	training's l2: 0.000640712	valid_1's l2: 0.000709312
[98]	training's l2: 0.000639133	valid_1's l2: 0.000708837
[99]	training's l2: 0.000637865	valid_1's l2: 0.000708525
[100]	training's l2: 0.000636374	valid_1's l2: 0.00070852
[101]	training's l2: 0.000634904	valid_1's l2: 0.000708501
[102]	training's l2: 0.000633451	valid_1's l2: 0.00070864
[103]	training's l2: 0.000632072	valid_1's l2: 0.000708744
[104]	training's l2: 0.00063065	valid_1's l2: 0.000708819
[105]	training's l2: 0.000629376	valid_1's l2: 0.000708573
[106]	training's l2: 0.000628019	valid_1's l2: 0.000708645
[107]	training's l2: 0.000626605	valid_1's l2: 0.000708727
[108]	training's l2: 0.000625462	valid_1's l2: 0.000708797
[109]	training's l2: 0.000624124	valid_1's l2: 0.000708538
[110]	training's l2: 0.000622335	valid_1's l2: 0.000707663
[111]	training's l2: 0.000620984	valid_1's l2: 0.000707237
[112]	training's l2: 0.000619607	valid_1's l2: 0.000707209
[113]	training's l2: 0.000618237	valid_1's l2: 0.000707295
[114]	training's l2: 0.000616985	valid_1's l2: 0.000707281
[115]	training's l2: 0.000615792	valid_1's l2: 0.000706925
[116]	training's l2: 0.000614553	valid_1's l2: 0.000707026
[117]	training's l2: 0.000613271	valid_1's l2: 0.000707075
[118]	training's l2: 0.000612075	valid_1's l2: 0.000706987
[119]	training's l2: 0.000610775	valid_1's l2: 0.000706895
[120]	training's l2: 0.000609532	valid_1's l2: 0.00070665
[121]	training's l2: 0.000608607	valid_1's l2: 0.00070678
[122]	training's l2: 0.000607226	valid_1's l2: 0.000706724
[123]	training's l2: 0.000605982	valid_1's l2: 0.000707085
[124]	training's l2: 0.000604715	valid_1's l2: 0.000707093
[125]	training's l2: 0.000603571	valid_1's l2: 0.000707004
[126]	training's l2: 0.000602459	valid_1's l2: 0.000707033
[127]	training's l2: 0.000601289	valid_1's l2: 0.000707516
[128]	training's l2: 0.000600097	valid_1's l2: 0.000707475
[129]	training's l2: 0.00059876	valid_1's l2: 0.000707251
[130]	training's l2: 0.00059765	valid_1's l2: 0.000707408
[131]	training's l2: 0.000596497	valid_1's l2: 0.000707524
[132]	training's l2: 0.000595301	valid_1's l2: 0.000707182
[133]	training's l2: 0.000594049	valid_1's l2: 0.000707484
[134]	training's l2: 0.000592552	valid_1's l2: 0.000707433
[135]	training's l2: 0.000591185	valid_1's l2: 0.000707543
[136]	training's l2: 0.000590175	valid_1's l2: 0.000707313
[137]	training's l2: 0.000588998	valid_1's l2: 0.000707008
[138]	training's l2: 0.000587824	valid_1's l2: 0.000707119
[139]	training's l2: 0.000586666	valid_1's l2: 0.000707352
[140]	training's l2: 0.000585769	valid_1's l2: 0.000707328
[141]	training's l2: 0.00058473	valid_1's l2: 0.000707196
[142]	training's l2: 0.000583423	valid_1's l2: 0.000706994
[143]	training's l2: 0.000582218	valid_1's l2: 0.000707053
[144]	training's l2: 0.000581103	valid_1's l2: 0.000707028
[145]	training's l2: 0.000579917	valid_1's l2: 0.000707205
[146]	training's l2: 0.000578726	valid_1's l2: 0.000707159
[147]	training's l2: 0.000577401	valid_1's l2: 0.000707118
[148]	training's l2: 0.000576182	valid_1's l2: 0.000706837
[149]	training's l2: 0.000574881	valid_1's l2: 0.000706795
[150]	training's l2: 0.00057373	valid_1's l2: 0.000706865
Early stopping, best iteration is:
[120]	training's l2: 0.000609532	valid_1's l2: 0.00070665
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172693 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000986179	valid_1's l2: 0.000895353
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000969518	valid_1's l2: 0.000883636
[3]	training's l2: 0.00095444	valid_1's l2: 0.000872154
[4]	training's l2: 0.000940952	valid_1's l2: 0.000862522
[5]	training's l2: 0.000928431	valid_1's l2: 0.000853005
[6]	training's l2: 0.000917335	valid_1's l2: 0.000844981
[7]	training's l2: 0.000907146	valid_1's l2: 0.000838415
[8]	training's l2: 0.000897207	valid_1's l2: 0.000832308
[9]	training's l2: 0.000888259	valid_1's l2: 0.000826259
[10]	training's l2: 0.000880391	valid_1's l2: 0.000821007
[11]	training's l2: 0.000873123	valid_1's l2: 0.000816091
[12]	training's l2: 0.000865591	valid_1's l2: 0.000811675
[13]	training's l2: 0.000859434	valid_1's l2: 0.000808214
[14]	training's l2: 0.000852934	valid_1's l2: 0.00080408
[15]	training's l2: 0.000847133	valid_1's l2: 0.000801074
[16]	training's l2: 0.000840521	valid_1's l2: 0.000795501
[17]	training's l2: 0.000834579	valid_1's l2: 0.000791677
[18]	training's l2: 0.000829164	valid_1's l2: 0.000787763
[19]	training's l2: 0.000824656	valid_1's l2: 0.00078581
[20]	training's l2: 0.000819927	valid_1's l2: 0.000783347
[21]	training's l2: 0.000815011	valid_1's l2: 0.000780007
[22]	training's l2: 0.000810638	valid_1's l2: 0.000778004
[23]	training's l2: 0.00080675	valid_1's l2: 0.000776117
[24]	training's l2: 0.000802671	valid_1's l2: 0.000773975
[25]	training's l2: 0.000799287	valid_1's l2: 0.000772627
[26]	training's l2: 0.000795432	valid_1's l2: 0.000770496
[27]	training's l2: 0.000791523	valid_1's l2: 0.000768946
[28]	training's l2: 0.000788617	valid_1's l2: 0.000767667
[29]	training's l2: 0.000784169	valid_1's l2: 0.000765537
[30]	training's l2: 0.000780895	valid_1's l2: 0.000763726
[31]	training's l2: 0.000777496	valid_1's l2: 0.000761165
[32]	training's l2: 0.000774497	valid_1's l2: 0.000759126
[33]	training's l2: 0.00077035	valid_1's l2: 0.000756065
[34]	training's l2: 0.000767336	valid_1's l2: 0.000754831
[35]	training's l2: 0.000764442	valid_1's l2: 0.000753613
[36]	training's l2: 0.000761623	valid_1's l2: 0.000752706
[37]	training's l2: 0.000758812	valid_1's l2: 0.00075167
[38]	training's l2: 0.000755448	valid_1's l2: 0.000750067
[39]	training's l2: 0.000752883	valid_1's l2: 0.000749093
[40]	training's l2: 0.000749886	valid_1's l2: 0.00074741
[41]	training's l2: 0.000746702	valid_1's l2: 0.000745727
[42]	training's l2: 0.000744177	valid_1's l2: 0.000744781
[43]	training's l2: 0.000741221	valid_1's l2: 0.000743222
[44]	training's l2: 0.000738694	valid_1's l2: 0.000742498
[45]	training's l2: 0.000736322	valid_1's l2: 0.000740992
[46]	training's l2: 0.000733577	valid_1's l2: 0.000739494
[47]	training's l2: 0.000731076	valid_1's l2: 0.000738614
[48]	training's l2: 0.000728555	valid_1's l2: 0.000738051
[49]	training's l2: 0.000726002	valid_1's l2: 0.000736498
[50]	training's l2: 0.000723901	valid_1's l2: 0.000735841
[51]	training's l2: 0.00072085	valid_1's l2: 0.000734583
[52]	training's l2: 0.00071818	valid_1's l2: 0.000732954
[53]	training's l2: 0.000715885	valid_1's l2: 0.000732292
[54]	training's l2: 0.000713868	valid_1's l2: 0.000731762
[55]	training's l2: 0.000712049	valid_1's l2: 0.000731628
[56]	training's l2: 0.000710341	valid_1's l2: 0.000731521
[57]	training's l2: 0.000708417	valid_1's l2: 0.0007311
[58]	training's l2: 0.00070572	valid_1's l2: 0.000729427
[59]	training's l2: 0.000703443	valid_1's l2: 0.000728918
[60]	training's l2: 0.000701123	valid_1's l2: 0.000728201
[61]	training's l2: 0.000698864	valid_1's l2: 0.000727689
[62]	training's l2: 0.000696988	valid_1's l2: 0.000727247
[63]	training's l2: 0.000695159	valid_1's l2: 0.000727141
[64]	training's l2: 0.000693155	valid_1's l2: 0.00072687
[65]	training's l2: 0.000691125	valid_1's l2: 0.000725985
[66]	training's l2: 0.000689146	valid_1's l2: 0.000725247
[67]	training's l2: 0.000687477	valid_1's l2: 0.000724959
[68]	training's l2: 0.000685458	valid_1's l2: 0.0007238
[69]	training's l2: 0.000683278	valid_1's l2: 0.000722776
[70]	training's l2: 0.000681393	valid_1's l2: 0.000722734
[71]	training's l2: 0.000679747	valid_1's l2: 0.000722868
[72]	training's l2: 0.000678007	valid_1's l2: 0.00072282
[73]	training's l2: 0.000676558	valid_1's l2: 0.00072245
[74]	training's l2: 0.000674641	valid_1's l2: 0.000721711
[75]	training's l2: 0.00067314	valid_1's l2: 0.000721707
[76]	training's l2: 0.000671113	valid_1's l2: 0.000721334
[77]	training's l2: 0.00066918	valid_1's l2: 0.000720377
[78]	training's l2: 0.000667605	valid_1's l2: 0.000720173
[79]	training's l2: 0.000665995	valid_1's l2: 0.000719985
[80]	training's l2: 0.000664112	valid_1's l2: 0.000719546
[81]	training's l2: 0.000662498	valid_1's l2: 0.000719507
[82]	training's l2: 0.000661123	valid_1's l2: 0.000719206
[83]	training's l2: 0.000659397	valid_1's l2: 0.000719176
[84]	training's l2: 0.000657772	valid_1's l2: 0.000718974
[85]	training's l2: 0.000656273	valid_1's l2: 0.000718994
[86]	training's l2: 0.00065454	valid_1's l2: 0.000718658
[87]	training's l2: 0.000652383	valid_1's l2: 0.000718461
[88]	training's l2: 0.00065091	valid_1's l2: 0.000718378
[89]	training's l2: 0.00064913	valid_1's l2: 0.000717641
[90]	training's l2: 0.00064717	valid_1's l2: 0.000716793
[91]	training's l2: 0.000645565	valid_1's l2: 0.000716624
[92]	training's l2: 0.000644175	valid_1's l2: 0.000716611
[93]	training's l2: 0.000642746	valid_1's l2: 0.000716513
[94]	training's l2: 0.000641425	valid_1's l2: 0.000716466
[95]	training's l2: 0.00063971	valid_1's l2: 0.000715973
[96]	training's l2: 0.000637867	valid_1's l2: 0.00071551
[97]	training's l2: 0.000636472	valid_1's l2: 0.00071554
[98]	training's l2: 0.000634814	valid_1's l2: 0.000715557
[99]	training's l2: 0.000633324	valid_1's l2: 0.000715464
[100]	training's l2: 0.000631386	valid_1's l2: 0.000714674
[101]	training's l2: 0.000629807	valid_1's l2: 0.000714174
[102]	training's l2: 0.000628498	valid_1's l2: 0.000714196
[103]	training's l2: 0.000626985	valid_1's l2: 0.000714133
[104]	training's l2: 0.000625615	valid_1's l2: 0.000714301
[105]	training's l2: 0.000624271	valid_1's l2: 0.000714154
[106]	training's l2: 0.000623058	valid_1's l2: 0.000714167
[107]	training's l2: 0.000621688	valid_1's l2: 0.000713958
[108]	training's l2: 0.000620257	valid_1's l2: 0.000713836
[109]	training's l2: 0.000618715	valid_1's l2: 0.000713351
[110]	training's l2: 0.000617285	valid_1's l2: 0.000712878
[111]	training's l2: 0.000615868	valid_1's l2: 0.000712614
[112]	training's l2: 0.000614146	valid_1's l2: 0.000712138
[113]	training's l2: 0.00061286	valid_1's l2: 0.000712374
[114]	training's l2: 0.000611461	valid_1's l2: 0.000712389
[115]	training's l2: 0.000610072	valid_1's l2: 0.000712505
[116]	training's l2: 0.000608869	valid_1's l2: 0.000712384
[117]	training's l2: 0.000607442	valid_1's l2: 0.000712212
[118]	training's l2: 0.000606057	valid_1's l2: 0.000711964
[119]	training's l2: 0.000604759	valid_1's l2: 0.000711943
[120]	training's l2: 0.000603396	valid_1's l2: 0.000711711
[121]	training's l2: 0.00060215	valid_1's l2: 0.000711703
[122]	training's l2: 0.000600847	valid_1's l2: 0.000712
[123]	training's l2: 0.000599431	valid_1's l2: 0.000711968
[124]	training's l2: 0.000598167	valid_1's l2: 0.000711887
[125]	training's l2: 0.000596832	valid_1's l2: 0.0007118
[126]	training's l2: 0.00059548	valid_1's l2: 0.000711933
[127]	training's l2: 0.0005943	valid_1's l2: 0.000711866
[128]	training's l2: 0.000593122	valid_1's l2: 0.000711813
[129]	training's l2: 0.000591964	valid_1's l2: 0.000711887
[130]	training's l2: 0.000590635	valid_1's l2: 0.000711982
[131]	training's l2: 0.000589444	valid_1's l2: 0.000711925
[132]	training's l2: 0.000588265	valid_1's l2: 0.000711925
[133]	training's l2: 0.000587041	valid_1's l2: 0.000711707
[134]	training's l2: 0.000585714	valid_1's l2: 0.000711852
[135]	training's l2: 0.000584542	valid_1's l2: 0.000711769
[136]	training's l2: 0.000583214	valid_1's l2: 0.000711923
[137]	training's l2: 0.000581756	valid_1's l2: 0.000711783
[138]	training's l2: 0.000580521	valid_1's l2: 0.000711767
[139]	training's l2: 0.000579281	valid_1's l2: 0.000711821
[140]	training's l2: 0.000577948	valid_1's l2: 0.000711768
[141]	training's l2: 0.000576841	valid_1's l2: 0.000711953
[142]	training's l2: 0.000575404	valid_1's l2: 0.00071168
[143]	training's l2: 0.00057435	valid_1's l2: 0.000711849
[144]	training's l2: 0.000573101	valid_1's l2: 0.000711767
[145]	training's l2: 0.000571971	valid_1's l2: 0.000711541
[146]	training's l2: 0.000570878	valid_1's l2: 0.000711481
[147]	training's l2: 0.000569638	valid_1's l2: 0.000711334
[148]	training's l2: 0.000568375	valid_1's l2: 0.000711326
[149]	training's l2: 0.000567208	valid_1's l2: 0.000711098
[150]	training's l2: 0.000566098	valid_1's l2: 0.000711204
[151]	training's l2: 0.000564876	valid_1's l2: 0.000711611
[152]	training's l2: 0.000563779	valid_1's l2: 0.000711496
[153]	training's l2: 0.000562649	valid_1's l2: 0.000711131
[154]	training's l2: 0.000561131	valid_1's l2: 0.000710778
[155]	training's l2: 0.000559863	valid_1's l2: 0.000710351
[156]	training's l2: 0.000558693	valid_1's l2: 0.000710404
[157]	training's l2: 0.000557525	valid_1's l2: 0.00071036
[158]	training's l2: 0.000556325	valid_1's l2: 0.000710271
[159]	training's l2: 0.000555107	valid_1's l2: 0.000710405
[160]	training's l2: 0.000553969	valid_1's l2: 0.000710234
[161]	training's l2: 0.000552935	valid_1's l2: 0.000710142
[162]	training's l2: 0.000551925	valid_1's l2: 0.000710184
[163]	training's l2: 0.000550775	valid_1's l2: 0.000710288
[164]	training's l2: 0.000549625	valid_1's l2: 0.000710419
[165]	training's l2: 0.000548549	valid_1's l2: 0.000710595
[166]	training's l2: 0.000547388	valid_1's l2: 0.000710553
[167]	training's l2: 0.000546205	valid_1's l2: 0.00071065
[168]	training's l2: 0.000545221	valid_1's l2: 0.000710622
[169]	training's l2: 0.000544129	valid_1's l2: 0.00071056
[170]	training's l2: 0.000543025	valid_1's l2: 0.000710758
[171]	training's l2: 0.000542023	valid_1's l2: 0.000710833
[172]	training's l2: 0.000541025	valid_1's l2: 0.000710854
[173]	training's l2: 0.000539876	valid_1's l2: 0.00071073
[174]	training's l2: 0.000538749	valid_1's l2: 0.000710837
[175]	training's l2: 0.000537617	valid_1's l2: 0.000710798
[176]	training's l2: 0.000536668	valid_1's l2: 0.000711056
[177]	training's l2: 0.000535562	valid_1's l2: 0.000710885
[178]	training's l2: 0.000534575	valid_1's l2: 0.000710903
[179]	training's l2: 0.000533623	valid_1's l2: 0.000710897
[180]	training's l2: 0.000532659	valid_1's l2: 0.000710973
[181]	training's l2: 0.000531662	valid_1's l2: 0.000710995
[182]	training's l2: 0.000530686	valid_1's l2: 0.000710796
[183]	training's l2: 0.000529778	valid_1's l2: 0.000710848
[184]	training's l2: 0.000528748	valid_1's l2: 0.00071081
[185]	training's l2: 0.000527872	valid_1's l2: 0.000710741
[186]	training's l2: 0.000526566	valid_1's l2: 0.000710333
[187]	training's l2: 0.000525506	valid_1's l2: 0.000710589
[188]	training's l2: 0.000524391	valid_1's l2: 0.000710638
[189]	training's l2: 0.000523327	valid_1's l2: 0.000711001
[190]	training's l2: 0.000522255	valid_1's l2: 0.000711117
[191]	training's l2: 0.000521283	valid_1's l2: 0.000710953
Early stopping, best iteration is:
[161]	training's l2: 0.000552935	valid_1's l2: 0.000710142
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172696 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000984089	valid_1's l2: 0.00089283
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000966716	valid_1's l2: 0.000880382
[3]	training's l2: 0.000950565	valid_1's l2: 0.000867896
[4]	training's l2: 0.000937296	valid_1's l2: 0.000857867
[5]	training's l2: 0.000925205	valid_1's l2: 0.000849485
[6]	training's l2: 0.00091435	valid_1's l2: 0.000842008
[7]	training's l2: 0.000904513	valid_1's l2: 0.000835222
[8]	training's l2: 0.000894374	valid_1's l2: 0.000828134
[9]	training's l2: 0.000886199	valid_1's l2: 0.000822599
[10]	training's l2: 0.000876698	valid_1's l2: 0.000814177
[11]	training's l2: 0.000868937	valid_1's l2: 0.000809368
[12]	training's l2: 0.000861897	valid_1's l2: 0.000804179
[13]	training's l2: 0.000855856	valid_1's l2: 0.000799993
[14]	training's l2: 0.000849956	valid_1's l2: 0.000796011
[15]	training's l2: 0.000844378	valid_1's l2: 0.000791895
[16]	training's l2: 0.000839007	valid_1's l2: 0.000789181
[17]	training's l2: 0.000834111	valid_1's l2: 0.000786723
[18]	training's l2: 0.000829303	valid_1's l2: 0.000782938
[19]	training's l2: 0.000824653	valid_1's l2: 0.000780128
[20]	training's l2: 0.00082085	valid_1's l2: 0.000778006
[21]	training's l2: 0.00081755	valid_1's l2: 0.000776482
[22]	training's l2: 0.0008132	valid_1's l2: 0.000774302
[23]	training's l2: 0.000809406	valid_1's l2: 0.000772106
[24]	training's l2: 0.000805968	valid_1's l2: 0.00077027
[25]	training's l2: 0.00080295	valid_1's l2: 0.000767984
[26]	training's l2: 0.000799229	valid_1's l2: 0.000765268
[27]	training's l2: 0.000796148	valid_1's l2: 0.000763458
[28]	training's l2: 0.000792862	valid_1's l2: 0.000761072
[29]	training's l2: 0.000789757	valid_1's l2: 0.000758972
[30]	training's l2: 0.00078744	valid_1's l2: 0.000757705
[31]	training's l2: 0.000784546	valid_1's l2: 0.000754391
[32]	training's l2: 0.000781388	valid_1's l2: 0.000752712
[33]	training's l2: 0.000778725	valid_1's l2: 0.000751512
[34]	training's l2: 0.000775428	valid_1's l2: 0.000749194
[35]	training's l2: 0.00077301	valid_1's l2: 0.000748028
[36]	training's l2: 0.000770201	valid_1's l2: 0.00074642
[37]	training's l2: 0.000767557	valid_1's l2: 0.000745824
[38]	training's l2: 0.000764598	valid_1's l2: 0.000744104
[39]	training's l2: 0.000762055	valid_1's l2: 0.000743271
[40]	training's l2: 0.000760251	valid_1's l2: 0.000741733
[41]	training's l2: 0.000757951	valid_1's l2: 0.000740589
[42]	training's l2: 0.000755747	valid_1's l2: 0.000739212
[43]	training's l2: 0.000753898	valid_1's l2: 0.000738092
[44]	training's l2: 0.000751195	valid_1's l2: 0.000736776
[45]	training's l2: 0.000747461	valid_1's l2: 0.000733831
[46]	training's l2: 0.000745302	valid_1's l2: 0.000733421
[47]	training's l2: 0.000743662	valid_1's l2: 0.000733103
[48]	training's l2: 0.000741862	valid_1's l2: 0.000732489
[49]	training's l2: 0.000740168	valid_1's l2: 0.000732096
[50]	training's l2: 0.000738569	valid_1's l2: 0.000731845
[51]	training's l2: 0.000736178	valid_1's l2: 0.000731295
[52]	training's l2: 0.000734478	valid_1's l2: 0.000730551
[53]	training's l2: 0.000732063	valid_1's l2: 0.000729773
[54]	training's l2: 0.000729953	valid_1's l2: 0.000729435
[55]	training's l2: 0.000728596	valid_1's l2: 0.00072886
[56]	training's l2: 0.000726938	valid_1's l2: 0.000728737
[57]	training's l2: 0.000724906	valid_1's l2: 0.000728534
[58]	training's l2: 0.000723281	valid_1's l2: 0.000728073
[59]	training's l2: 0.000721867	valid_1's l2: 0.00072805
[60]	training's l2: 0.000720185	valid_1's l2: 0.000727601
[61]	training's l2: 0.000717691	valid_1's l2: 0.000726467
[62]	training's l2: 0.000716278	valid_1's l2: 0.000726015
[63]	training's l2: 0.000714652	valid_1's l2: 0.000725471
[64]	training's l2: 0.000713036	valid_1's l2: 0.000725285
[65]	training's l2: 0.000710975	valid_1's l2: 0.00072476
[66]	training's l2: 0.000708948	valid_1's l2: 0.00072403
[67]	training's l2: 0.000707776	valid_1's l2: 0.000723424
[68]	training's l2: 0.000706078	valid_1's l2: 0.000723218
[69]	training's l2: 0.000704381	valid_1's l2: 0.000722442
[70]	training's l2: 0.000702763	valid_1's l2: 0.000722267
[71]	training's l2: 0.000701002	valid_1's l2: 0.000721452
[72]	training's l2: 0.000699538	valid_1's l2: 0.00072111
[73]	training's l2: 0.000698413	valid_1's l2: 0.000721142
[74]	training's l2: 0.000696948	valid_1's l2: 0.000721257
[75]	training's l2: 0.000695808	valid_1's l2: 0.000721216
[76]	training's l2: 0.000694521	valid_1's l2: 0.000721226
[77]	training's l2: 0.000693087	valid_1's l2: 0.000721163
[78]	training's l2: 0.000691974	valid_1's l2: 0.000721058
[79]	training's l2: 0.000690742	valid_1's l2: 0.000720683
[80]	training's l2: 0.000689526	valid_1's l2: 0.000720515
[81]	training's l2: 0.000688327	valid_1's l2: 0.000719991
[82]	training's l2: 0.000686288	valid_1's l2: 0.000719139
[83]	training's l2: 0.00068493	valid_1's l2: 0.000719048
[84]	training's l2: 0.00068357	valid_1's l2: 0.000718492
[85]	training's l2: 0.000682486	valid_1's l2: 0.000718425
[86]	training's l2: 0.000681349	valid_1's l2: 0.000718263
[87]	training's l2: 0.000680008	valid_1's l2: 0.000718072
[88]	training's l2: 0.000678643	valid_1's l2: 0.000717867
[89]	training's l2: 0.00067716	valid_1's l2: 0.000717835
[90]	training's l2: 0.000675896	valid_1's l2: 0.000717696
[91]	training's l2: 0.000674843	valid_1's l2: 0.000717657
[92]	training's l2: 0.000673567	valid_1's l2: 0.000717692
[93]	training's l2: 0.000672455	valid_1's l2: 0.000717605
[94]	training's l2: 0.000671317	valid_1's l2: 0.000717019
[95]	training's l2: 0.000670414	valid_1's l2: 0.000717103
[96]	training's l2: 0.000669054	valid_1's l2: 0.00071677
[97]	training's l2: 0.000667963	valid_1's l2: 0.000716378
[98]	training's l2: 0.000666684	valid_1's l2: 0.000715933
[99]	training's l2: 0.000665676	valid_1's l2: 0.000715745
[100]	training's l2: 0.000664747	valid_1's l2: 0.000715655
[101]	training's l2: 0.000663844	valid_1's l2: 0.000715543
[102]	training's l2: 0.000662705	valid_1's l2: 0.000714755
[103]	training's l2: 0.00066148	valid_1's l2: 0.000714135
[104]	training's l2: 0.000660023	valid_1's l2: 0.000714132
[105]	training's l2: 0.000658978	valid_1's l2: 0.000714085
[106]	training's l2: 0.000657468	valid_1's l2: 0.000714007
[107]	training's l2: 0.000656104	valid_1's l2: 0.000713994
[108]	training's l2: 0.000655138	valid_1's l2: 0.000713938
[109]	training's l2: 0.000654097	valid_1's l2: 0.00071409
[110]	training's l2: 0.000652992	valid_1's l2: 0.000713753
[111]	training's l2: 0.000652014	valid_1's l2: 0.000713484
[112]	training's l2: 0.000650539	valid_1's l2: 0.000713302
[113]	training's l2: 0.000649348	valid_1's l2: 0.000712604
[114]	training's l2: 0.000648184	valid_1's l2: 0.000712479
[115]	training's l2: 0.000647299	valid_1's l2: 0.000712494
[116]	training's l2: 0.000646444	valid_1's l2: 0.000712576
[117]	training's l2: 0.000645631	valid_1's l2: 0.000712533
[118]	training's l2: 0.000644479	valid_1's l2: 0.000712294
[119]	training's l2: 0.000643153	valid_1's l2: 0.000712221
[120]	training's l2: 0.000641996	valid_1's l2: 0.000712137
[121]	training's l2: 0.000640827	valid_1's l2: 0.000711797
[122]	training's l2: 0.000639913	valid_1's l2: 0.000711823
[123]	training's l2: 0.000638982	valid_1's l2: 0.0007117
[124]	training's l2: 0.000638006	valid_1's l2: 0.00071196
[125]	training's l2: 0.000636997	valid_1's l2: 0.00071202
[126]	training's l2: 0.000636214	valid_1's l2: 0.000711966
[127]	training's l2: 0.000635152	valid_1's l2: 0.000711847
[128]	training's l2: 0.000634083	valid_1's l2: 0.000711983
[129]	training's l2: 0.000633003	valid_1's l2: 0.000711738
[130]	training's l2: 0.000631986	valid_1's l2: 0.000711838
[131]	training's l2: 0.00063108	valid_1's l2: 0.000711749
[132]	training's l2: 0.000630296	valid_1's l2: 0.00071191
[133]	training's l2: 0.000629448	valid_1's l2: 0.000711758
[134]	training's l2: 0.000628599	valid_1's l2: 0.000711822
[135]	training's l2: 0.000627157	valid_1's l2: 0.000711307
[136]	training's l2: 0.000626078	valid_1's l2: 0.000710953
[137]	training's l2: 0.000625309	valid_1's l2: 0.000710874
[138]	training's l2: 0.000624387	valid_1's l2: 0.000710875
[139]	training's l2: 0.000623589	valid_1's l2: 0.000710829
[140]	training's l2: 0.000622498	valid_1's l2: 0.000710183
[141]	training's l2: 0.000621725	valid_1's l2: 0.000709987
[142]	training's l2: 0.000620896	valid_1's l2: 0.000710056
[143]	training's l2: 0.000620006	valid_1's l2: 0.000710009
[144]	training's l2: 0.000618944	valid_1's l2: 0.000709859
[145]	training's l2: 0.000618	valid_1's l2: 0.000709825
[146]	training's l2: 0.000616804	valid_1's l2: 0.00070975
[147]	training's l2: 0.000615787	valid_1's l2: 0.000709628
[148]	training's l2: 0.000614767	valid_1's l2: 0.000709687
[149]	training's l2: 0.000614078	valid_1's l2: 0.00070968
[150]	training's l2: 0.000613285	valid_1's l2: 0.000709557
[151]	training's l2: 0.000612127	valid_1's l2: 0.00070959
[152]	training's l2: 0.000611048	valid_1's l2: 0.000709596
[153]	training's l2: 0.000610279	valid_1's l2: 0.000709699
[154]	training's l2: 0.000609503	valid_1's l2: 0.000709758
[155]	training's l2: 0.000608682	valid_1's l2: 0.000709863
[156]	training's l2: 0.00060779	valid_1's l2: 0.000710197
[157]	training's l2: 0.000606952	valid_1's l2: 0.000710422
[158]	training's l2: 0.000606259	valid_1's l2: 0.000710507
[159]	training's l2: 0.000605471	valid_1's l2: 0.000710582
[160]	training's l2: 0.000604537	valid_1's l2: 0.000710535
[161]	training's l2: 0.000603635	valid_1's l2: 0.000710473
[162]	training's l2: 0.000602775	valid_1's l2: 0.000710397
[163]	training's l2: 0.000602032	valid_1's l2: 0.000710291
[164]	training's l2: 0.000601167	valid_1's l2: 0.000710312
[165]	training's l2: 0.000600381	valid_1's l2: 0.000710251
[166]	training's l2: 0.000599663	valid_1's l2: 0.000710198
[167]	training's l2: 0.000598728	valid_1's l2: 0.000710005
[168]	training's l2: 0.000597966	valid_1's l2: 0.000710058
[169]	training's l2: 0.000597224	valid_1's l2: 0.000709983
[170]	training's l2: 0.000596266	valid_1's l2: 0.000709973
[171]	training's l2: 0.000595274	valid_1's l2: 0.000709373
[172]	training's l2: 0.000594654	valid_1's l2: 0.000709506
[173]	training's l2: 0.000593893	valid_1's l2: 0.000709461
[174]	training's l2: 0.000593062	valid_1's l2: 0.000709304
[175]	training's l2: 0.00059222	valid_1's l2: 0.000709445
[176]	training's l2: 0.000591143	valid_1's l2: 0.000709362
[177]	training's l2: 0.00059044	valid_1's l2: 0.0007095
[178]	training's l2: 0.000589574	valid_1's l2: 0.000709311
[179]	training's l2: 0.000588762	valid_1's l2: 0.000709313
[180]	training's l2: 0.00058798	valid_1's l2: 0.000709459
[181]	training's l2: 0.000587289	valid_1's l2: 0.000709644
[182]	training's l2: 0.000586574	valid_1's l2: 0.000709583
[183]	training's l2: 0.000585796	valid_1's l2: 0.000709649
[184]	training's l2: 0.000584764	valid_1's l2: 0.000709342
[185]	training's l2: 0.000584015	valid_1's l2: 0.000709347
[186]	training's l2: 0.000583367	valid_1's l2: 0.000709381
[187]	training's l2: 0.00058277	valid_1's l2: 0.000709451
[188]	training's l2: 0.00058207	valid_1's l2: 0.000709097
[189]	training's l2: 0.000581364	valid_1's l2: 0.000709003
[190]	training's l2: 0.000580767	valid_1's l2: 0.000708891
[191]	training's l2: 0.000579989	valid_1's l2: 0.000708815
[192]	training's l2: 0.000579177	valid_1's l2: 0.00070882
[193]	training's l2: 0.000578607	valid_1's l2: 0.000708656
[194]	training's l2: 0.000577933	valid_1's l2: 0.000708525
[195]	training's l2: 0.000577315	valid_1's l2: 0.000708527
Did not meet early stopping. Best iteration is:
[195]	training's l2: 0.000577315	valid_1's l2: 0.000708527
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175372 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.00098532	valid_1's l2: 0.000894846
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000969242	valid_1's l2: 0.000884772
[3]	training's l2: 0.000954301	valid_1's l2: 0.000874565
[4]	training's l2: 0.000940392	valid_1's l2: 0.000864907
[5]	training's l2: 0.000927676	valid_1's l2: 0.000855573
[6]	training's l2: 0.000916102	valid_1's l2: 0.000848449
[7]	training's l2: 0.000905688	valid_1's l2: 0.000841152
[8]	training's l2: 0.000895894	valid_1's l2: 0.000834868
[9]	training's l2: 0.000886664	valid_1's l2: 0.000829421
[10]	training's l2: 0.00087827	valid_1's l2: 0.000823771
[11]	training's l2: 0.000868979	valid_1's l2: 0.000819196
[12]	training's l2: 0.000859377	valid_1's l2: 0.000813798
[13]	training's l2: 0.000851311	valid_1's l2: 0.00081002
[14]	training's l2: 0.00084247	valid_1's l2: 0.000805546
[15]	training's l2: 0.000835177	valid_1's l2: 0.00080167
[16]	training's l2: 0.000829029	valid_1's l2: 0.000798464
[17]	training's l2: 0.00082311	valid_1's l2: 0.000795951
[18]	training's l2: 0.000816462	valid_1's l2: 0.000793332
[19]	training's l2: 0.000809323	valid_1's l2: 0.000788619
[20]	training's l2: 0.000803446	valid_1's l2: 0.000785529
[21]	training's l2: 0.000798127	valid_1's l2: 0.00078278
[22]	training's l2: 0.000793123	valid_1's l2: 0.000780007
[23]	training's l2: 0.000788276	valid_1's l2: 0.000777739
[24]	training's l2: 0.000783794	valid_1's l2: 0.000775892
[25]	training's l2: 0.000779564	valid_1's l2: 0.000774285
[26]	training's l2: 0.000775277	valid_1's l2: 0.000771883
[27]	training's l2: 0.000770549	valid_1's l2: 0.000770384
[28]	training's l2: 0.000766148	valid_1's l2: 0.000768126
[29]	training's l2: 0.000761942	valid_1's l2: 0.000766114
[30]	training's l2: 0.00075828	valid_1's l2: 0.00076513
[31]	training's l2: 0.000755109	valid_1's l2: 0.00076382
[32]	training's l2: 0.000751728	valid_1's l2: 0.000761927
[33]	training's l2: 0.00074838	valid_1's l2: 0.000760481
[34]	training's l2: 0.000745357	valid_1's l2: 0.000758858
[35]	training's l2: 0.000741967	valid_1's l2: 0.000757019
[36]	training's l2: 0.000738398	valid_1's l2: 0.000755523
[37]	training's l2: 0.000735539	valid_1's l2: 0.000753818
[38]	training's l2: 0.000732242	valid_1's l2: 0.00075273
[39]	training's l2: 0.000729269	valid_1's l2: 0.000751817
[40]	training's l2: 0.00072656	valid_1's l2: 0.000750586
[41]	training's l2: 0.000723987	valid_1's l2: 0.000749656
[42]	training's l2: 0.000721547	valid_1's l2: 0.000749259
[43]	training's l2: 0.000718638	valid_1's l2: 0.000747038
[44]	training's l2: 0.000715128	valid_1's l2: 0.000745327
[45]	training's l2: 0.000712732	valid_1's l2: 0.000744323
[46]	training's l2: 0.000709841	valid_1's l2: 0.000742879
[47]	training's l2: 0.000706941	valid_1's l2: 0.000741839
[48]	training's l2: 0.000704335	valid_1's l2: 0.00074096
[49]	training's l2: 0.000701295	valid_1's l2: 0.000738827
[50]	training's l2: 0.000698489	valid_1's l2: 0.000738062
[51]	training's l2: 0.000695739	valid_1's l2: 0.000736541
[52]	training's l2: 0.0006931	valid_1's l2: 0.000735256
[53]	training's l2: 0.000690833	valid_1's l2: 0.00073451
[54]	training's l2: 0.000688507	valid_1's l2: 0.000733996
[55]	training's l2: 0.000686337	valid_1's l2: 0.000733465
[56]	training's l2: 0.000683774	valid_1's l2: 0.000732611
[57]	training's l2: 0.000681119	valid_1's l2: 0.000731677
[58]	training's l2: 0.000678779	valid_1's l2: 0.000731171
[59]	training's l2: 0.000676319	valid_1's l2: 0.000730456
[60]	training's l2: 0.000673962	valid_1's l2: 0.000729595
[61]	training's l2: 0.000671513	valid_1's l2: 0.000729146
[62]	training's l2: 0.000669261	valid_1's l2: 0.000728562
[63]	training's l2: 0.000667277	valid_1's l2: 0.000728234
[64]	training's l2: 0.00066545	valid_1's l2: 0.00072801
[65]	training's l2: 0.000662998	valid_1's l2: 0.000726401
[66]	training's l2: 0.000660756	valid_1's l2: 0.000726577
[67]	training's l2: 0.00065821	valid_1's l2: 0.000725815
[68]	training's l2: 0.000656188	valid_1's l2: 0.000725494
[69]	training's l2: 0.000654217	valid_1's l2: 0.000725234
[70]	training's l2: 0.000652345	valid_1's l2: 0.000724783
[71]	training's l2: 0.000650468	valid_1's l2: 0.000724572
[72]	training's l2: 0.000648692	valid_1's l2: 0.000724214
[73]	training's l2: 0.000646928	valid_1's l2: 0.000723537
[74]	training's l2: 0.000645194	valid_1's l2: 0.000723182
[75]	training's l2: 0.000643658	valid_1's l2: 0.000723214
[76]	training's l2: 0.000641704	valid_1's l2: 0.000722829
[77]	training's l2: 0.000639877	valid_1's l2: 0.000722666
[78]	training's l2: 0.000638274	valid_1's l2: 0.000722644
[79]	training's l2: 0.000636379	valid_1's l2: 0.000722428
[80]	training's l2: 0.000634205	valid_1's l2: 0.000721478
[81]	training's l2: 0.000632403	valid_1's l2: 0.000721132
[82]	training's l2: 0.000630814	valid_1's l2: 0.0007209
[83]	training's l2: 0.000628909	valid_1's l2: 0.000720797
[84]	training's l2: 0.000627463	valid_1's l2: 0.000720769
[85]	training's l2: 0.000625908	valid_1's l2: 0.000720649
[86]	training's l2: 0.000624354	valid_1's l2: 0.000720694
[87]	training's l2: 0.000622834	valid_1's l2: 0.000720261
[88]	training's l2: 0.000621348	valid_1's l2: 0.000719916
[89]	training's l2: 0.000619894	valid_1's l2: 0.0007199
[90]	training's l2: 0.000618364	valid_1's l2: 0.000719546
[91]	training's l2: 0.000616569	valid_1's l2: 0.000718653
[92]	training's l2: 0.000615259	valid_1's l2: 0.000718588
[93]	training's l2: 0.000613746	valid_1's l2: 0.000718415
[94]	training's l2: 0.00061232	valid_1's l2: 0.00071815
[95]	training's l2: 0.000610697	valid_1's l2: 0.000717897
[96]	training's l2: 0.000609217	valid_1's l2: 0.000717848
[97]	training's l2: 0.000607855	valid_1's l2: 0.000718097
[98]	training's l2: 0.000606472	valid_1's l2: 0.000717866
[99]	training's l2: 0.000604949	valid_1's l2: 0.000717757
[100]	training's l2: 0.000603164	valid_1's l2: 0.000716881
[101]	training's l2: 0.000601751	valid_1's l2: 0.000716764
[102]	training's l2: 0.000600527	valid_1's l2: 0.000716786
[103]	training's l2: 0.000599046	valid_1's l2: 0.000716661
[104]	training's l2: 0.000597782	valid_1's l2: 0.000716587
[105]	training's l2: 0.000596407	valid_1's l2: 0.000716614
[106]	training's l2: 0.00059505	valid_1's l2: 0.000716378
[107]	training's l2: 0.000593541	valid_1's l2: 0.000716075
[108]	training's l2: 0.000592225	valid_1's l2: 0.000716033
[109]	training's l2: 0.000590671	valid_1's l2: 0.000715634
[110]	training's l2: 0.000589172	valid_1's l2: 0.000715193
[111]	training's l2: 0.000587669	valid_1's l2: 0.000714927
[112]	training's l2: 0.000586362	valid_1's l2: 0.000714637
[113]	training's l2: 0.000585005	valid_1's l2: 0.000714599
[114]	training's l2: 0.000583749	valid_1's l2: 0.000714863
[115]	training's l2: 0.000582478	valid_1's l2: 0.000714742
[116]	training's l2: 0.000581276	valid_1's l2: 0.000714501
[117]	training's l2: 0.000579896	valid_1's l2: 0.000714352
[118]	training's l2: 0.000578333	valid_1's l2: 0.000713907
[119]	training's l2: 0.000576955	valid_1's l2: 0.000713845
[120]	training's l2: 0.000575417	valid_1's l2: 0.000713257
[121]	training's l2: 0.000574242	valid_1's l2: 0.00071316
[122]	training's l2: 0.000573089	valid_1's l2: 0.000713276
[123]	training's l2: 0.000571917	valid_1's l2: 0.0007131
[124]	training's l2: 0.000570464	valid_1's l2: 0.000712914
[125]	training's l2: 0.000569353	valid_1's l2: 0.000713023
[126]	training's l2: 0.000568099	valid_1's l2: 0.000712872
[127]	training's l2: 0.000566877	valid_1's l2: 0.000712718
[128]	training's l2: 0.000565478	valid_1's l2: 0.000712677
[129]	training's l2: 0.000564381	valid_1's l2: 0.000712873
[130]	training's l2: 0.000563074	valid_1's l2: 0.000712852
[131]	training's l2: 0.000561748	valid_1's l2: 0.000712491
[132]	training's l2: 0.000560395	valid_1's l2: 0.000712635
[133]	training's l2: 0.000559216	valid_1's l2: 0.000712538
[134]	training's l2: 0.000558128	valid_1's l2: 0.000712755
[135]	training's l2: 0.000556954	valid_1's l2: 0.000712592
[136]	training's l2: 0.000555861	valid_1's l2: 0.000712661
[137]	training's l2: 0.000554652	valid_1's l2: 0.000712557
[138]	training's l2: 0.000553575	valid_1's l2: 0.000712454
[139]	training's l2: 0.000552405	valid_1's l2: 0.000712507
[140]	training's l2: 0.000551081	valid_1's l2: 0.000712492
[141]	training's l2: 0.000550065	valid_1's l2: 0.000712428
[142]	training's l2: 0.000548825	valid_1's l2: 0.000712506
[143]	training's l2: 0.000547806	valid_1's l2: 0.000712553
[144]	training's l2: 0.000546577	valid_1's l2: 0.000712454
[145]	training's l2: 0.00054551	valid_1's l2: 0.000712475
[146]	training's l2: 0.00054437	valid_1's l2: 0.000712318
[147]	training's l2: 0.00054337	valid_1's l2: 0.000712302
[148]	training's l2: 0.00054226	valid_1's l2: 0.00071214
[149]	training's l2: 0.00054123	valid_1's l2: 0.000712173
[150]	training's l2: 0.000539927	valid_1's l2: 0.000712133
[151]	training's l2: 0.000538829	valid_1's l2: 0.000712365
[152]	training's l2: 0.000537775	valid_1's l2: 0.000712376
[153]	training's l2: 0.000536444	valid_1's l2: 0.000711602
[154]	training's l2: 0.00053528	valid_1's l2: 0.000711641
[155]	training's l2: 0.000534241	valid_1's l2: 0.000711724
[156]	training's l2: 0.00053316	valid_1's l2: 0.000711938
[157]	training's l2: 0.000532118	valid_1's l2: 0.000711712
[158]	training's l2: 0.000531094	valid_1's l2: 0.000711532
[159]	training's l2: 0.000530013	valid_1's l2: 0.000711407
[160]	training's l2: 0.000528924	valid_1's l2: 0.000711408
[161]	training's l2: 0.000527978	valid_1's l2: 0.00071137
[162]	training's l2: 0.000527016	valid_1's l2: 0.000711259
[163]	training's l2: 0.000526103	valid_1's l2: 0.000711283
[164]	training's l2: 0.000525117	valid_1's l2: 0.000711385
[165]	training's l2: 0.000524137	valid_1's l2: 0.000711204
[166]	training's l2: 0.000523069	valid_1's l2: 0.000711311
[167]	training's l2: 0.00052205	valid_1's l2: 0.000711275
[168]	training's l2: 0.000521044	valid_1's l2: 0.000711234
[169]	training's l2: 0.00052003	valid_1's l2: 0.000711202
[170]	training's l2: 0.000518874	valid_1's l2: 0.000711275
[171]	training's l2: 0.000517929	valid_1's l2: 0.000711464
[172]	training's l2: 0.000517063	valid_1's l2: 0.000711199
[173]	training's l2: 0.000516194	valid_1's l2: 0.000711227
[174]	training's l2: 0.000515353	valid_1's l2: 0.000711261
[175]	training's l2: 0.000514408	valid_1's l2: 0.000711309
[176]	training's l2: 0.000513329	valid_1's l2: 0.000711097
[177]	training's l2: 0.000512298	valid_1's l2: 0.000711095
[178]	training's l2: 0.00051127	valid_1's l2: 0.00071092
[179]	training's l2: 0.000510241	valid_1's l2: 0.000710717
[180]	training's l2: 0.000509176	valid_1's l2: 0.000710727
[181]	training's l2: 0.000508274	valid_1's l2: 0.000710717
[182]	training's l2: 0.000507355	valid_1's l2: 0.000710778
[183]	training's l2: 0.00050651	valid_1's l2: 0.000710664
[184]	training's l2: 0.000505533	valid_1's l2: 0.000710614
[185]	training's l2: 0.000504559	valid_1's l2: 0.00071074
[186]	training's l2: 0.000503659	valid_1's l2: 0.000710602
[187]	training's l2: 0.000502684	valid_1's l2: 0.000710495
Did not meet early stopping. Best iteration is:
[187]	training's l2: 0.000502684	valid_1's l2: 0.000710495
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182208 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000975685	valid_1's l2: 0.000889729
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000952268	valid_1's l2: 0.000875943
[3]	training's l2: 0.000931669	valid_1's l2: 0.00086179
[4]	training's l2: 0.000913944	valid_1's l2: 0.000851782
[5]	training's l2: 0.000897805	valid_1's l2: 0.000842363
[6]	training's l2: 0.000885105	valid_1's l2: 0.000833874
[7]	training's l2: 0.000871699	valid_1's l2: 0.000826561
[8]	training's l2: 0.000856995	valid_1's l2: 0.000818651
[9]	training's l2: 0.00084352	valid_1's l2: 0.000810815
[10]	training's l2: 0.00083229	valid_1's l2: 0.000806674
[11]	training's l2: 0.000823154	valid_1's l2: 0.000801545
[12]	training's l2: 0.000812238	valid_1's l2: 0.000794143
[13]	training's l2: 0.000803666	valid_1's l2: 0.000789815
[14]	training's l2: 0.000796957	valid_1's l2: 0.000785655
[15]	training's l2: 0.000788958	valid_1's l2: 0.000782104
[16]	training's l2: 0.000783079	valid_1's l2: 0.00077804
[17]	training's l2: 0.000775053	valid_1's l2: 0.000774149
[18]	training's l2: 0.000767785	valid_1's l2: 0.000770576
[19]	training's l2: 0.000762479	valid_1's l2: 0.000768351
[20]	training's l2: 0.000756291	valid_1's l2: 0.000765174
[21]	training's l2: 0.000751901	valid_1's l2: 0.000762595
[22]	training's l2: 0.000747349	valid_1's l2: 0.000761086
[23]	training's l2: 0.000742969	valid_1's l2: 0.000759791
[24]	training's l2: 0.000739292	valid_1's l2: 0.000758482
[25]	training's l2: 0.000735939	valid_1's l2: 0.000755805
[26]	training's l2: 0.000732632	valid_1's l2: 0.000754604
[27]	training's l2: 0.000728589	valid_1's l2: 0.000752538
[28]	training's l2: 0.00072387	valid_1's l2: 0.000750758
[29]	training's l2: 0.000720318	valid_1's l2: 0.000749932
[30]	training's l2: 0.000716735	valid_1's l2: 0.000747604
[31]	training's l2: 0.000713182	valid_1's l2: 0.000745286
[32]	training's l2: 0.000708879	valid_1's l2: 0.000742421
[33]	training's l2: 0.00070629	valid_1's l2: 0.000740877
[34]	training's l2: 0.000702983	valid_1's l2: 0.000738811
[35]	training's l2: 0.000699926	valid_1's l2: 0.000737979
[36]	training's l2: 0.000696855	valid_1's l2: 0.000736593
[37]	training's l2: 0.000694266	valid_1's l2: 0.000734807
[38]	training's l2: 0.000691677	valid_1's l2: 0.00073455
[39]	training's l2: 0.000688655	valid_1's l2: 0.000733659
[40]	training's l2: 0.000686291	valid_1's l2: 0.000732557
[41]	training's l2: 0.000684188	valid_1's l2: 0.000731874
[42]	training's l2: 0.000681655	valid_1's l2: 0.000730755
[43]	training's l2: 0.000679263	valid_1's l2: 0.000729884
[44]	training's l2: 0.000675811	valid_1's l2: 0.000728917
[45]	training's l2: 0.00067377	valid_1's l2: 0.000728707
[46]	training's l2: 0.000671461	valid_1's l2: 0.000728201
[47]	training's l2: 0.000668963	valid_1's l2: 0.000728176
[48]	training's l2: 0.000666864	valid_1's l2: 0.00072736
[49]	training's l2: 0.000664926	valid_1's l2: 0.0007271
[50]	training's l2: 0.000663053	valid_1's l2: 0.000726733
[51]	training's l2: 0.000660998	valid_1's l2: 0.000726423
[52]	training's l2: 0.000658175	valid_1's l2: 0.00072471
[53]	training's l2: 0.000655993	valid_1's l2: 0.000724695
[54]	training's l2: 0.000653934	valid_1's l2: 0.000724384
[55]	training's l2: 0.000652146	valid_1's l2: 0.00072422
[56]	training's l2: 0.00064985	valid_1's l2: 0.000723674
[57]	training's l2: 0.000648179	valid_1's l2: 0.000723241
[58]	training's l2: 0.000646106	valid_1's l2: 0.000722365
[59]	training's l2: 0.000644196	valid_1's l2: 0.000722556
[60]	training's l2: 0.000642448	valid_1's l2: 0.000722415
[61]	training's l2: 0.000640569	valid_1's l2: 0.000721799
[62]	training's l2: 0.000638045	valid_1's l2: 0.000720697
[63]	training's l2: 0.000635784	valid_1's l2: 0.000719276
[64]	training's l2: 0.000633363	valid_1's l2: 0.000718474
[65]	training's l2: 0.000631587	valid_1's l2: 0.000718063
[66]	training's l2: 0.000629828	valid_1's l2: 0.0007177
[67]	training's l2: 0.000628079	valid_1's l2: 0.000717608
[68]	training's l2: 0.000626235	valid_1's l2: 0.000717121
[69]	training's l2: 0.000624674	valid_1's l2: 0.000716684
[70]	training's l2: 0.000622495	valid_1's l2: 0.000715504
[71]	training's l2: 0.000620763	valid_1's l2: 0.00071526
[72]	training's l2: 0.000619064	valid_1's l2: 0.000714985
[73]	training's l2: 0.00061746	valid_1's l2: 0.000715218
[74]	training's l2: 0.00061576	valid_1's l2: 0.000715569
[75]	training's l2: 0.00061419	valid_1's l2: 0.00071525
[76]	training's l2: 0.000612544	valid_1's l2: 0.000715239
[77]	training's l2: 0.000610882	valid_1's l2: 0.000715317
[78]	training's l2: 0.000609366	valid_1's l2: 0.00071535
[79]	training's l2: 0.000607692	valid_1's l2: 0.000715168
[80]	training's l2: 0.000606204	valid_1's l2: 0.000715024
[81]	training's l2: 0.000604698	valid_1's l2: 0.000714963
[82]	training's l2: 0.000603255	valid_1's l2: 0.000715099
[83]	training's l2: 0.0006018	valid_1's l2: 0.000714691
[84]	training's l2: 0.000600066	valid_1's l2: 0.000714127
[85]	training's l2: 0.00059856	valid_1's l2: 0.000714201
[86]	training's l2: 0.000597104	valid_1's l2: 0.000714071
[87]	training's l2: 0.000595577	valid_1's l2: 0.000713853
[88]	training's l2: 0.000594153	valid_1's l2: 0.000714071
[89]	training's l2: 0.000592778	valid_1's l2: 0.000714143
[90]	training's l2: 0.000591341	valid_1's l2: 0.00071403
[91]	training's l2: 0.000589689	valid_1's l2: 0.000714297
[92]	training's l2: 0.000588362	valid_1's l2: 0.000714532
[93]	training's l2: 0.000586916	valid_1's l2: 0.000714454
[94]	training's l2: 0.000585294	valid_1's l2: 0.000713988
[95]	training's l2: 0.000583991	valid_1's l2: 0.000713992
[96]	training's l2: 0.000582569	valid_1's l2: 0.000714264
Did not meet early stopping. Best iteration is:
[96]	training's l2: 0.000582569	valid_1's l2: 0.000714264
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180262 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000986736	valid_1's l2: 0.000895152
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000971905	valid_1's l2: 0.000883667
[3]	training's l2: 0.000957981	valid_1's l2: 0.000872953
[4]	training's l2: 0.000945083	valid_1's l2: 0.00086324
[5]	training's l2: 0.000933508	valid_1's l2: 0.000854513
[6]	training's l2: 0.000923684	valid_1's l2: 0.000847956
[7]	training's l2: 0.000914087	valid_1's l2: 0.000840845
[8]	training's l2: 0.000905745	valid_1's l2: 0.000834953
[9]	training's l2: 0.000897325	valid_1's l2: 0.000828588
[10]	training's l2: 0.000889965	valid_1's l2: 0.000824235
[11]	training's l2: 0.000882862	valid_1's l2: 0.000818977
[12]	training's l2: 0.000874803	valid_1's l2: 0.000813516
[13]	training's l2: 0.000867524	valid_1's l2: 0.000807853
[14]	training's l2: 0.000861525	valid_1's l2: 0.000804116
[15]	training's l2: 0.000856097	valid_1's l2: 0.00080036
[16]	training's l2: 0.000850733	valid_1's l2: 0.000797629
[17]	training's l2: 0.000845732	valid_1's l2: 0.000794596
[18]	training's l2: 0.000841289	valid_1's l2: 0.000791557
[19]	training's l2: 0.000836874	valid_1's l2: 0.00078862
[20]	training's l2: 0.000832423	valid_1's l2: 0.000784973
[21]	training's l2: 0.000828381	valid_1's l2: 0.000782476
[22]	training's l2: 0.000823715	valid_1's l2: 0.000779234
[23]	training's l2: 0.000820786	valid_1's l2: 0.000778046
[24]	training's l2: 0.000816799	valid_1's l2: 0.000774731
[25]	training's l2: 0.000813275	valid_1's l2: 0.000772029
[26]	training's l2: 0.000810208	valid_1's l2: 0.000770622
[27]	training's l2: 0.000806868	valid_1's l2: 0.000768585
[28]	training's l2: 0.000803747	valid_1's l2: 0.000766646
[29]	training's l2: 0.000800732	valid_1's l2: 0.000765388
[30]	training's l2: 0.000798205	valid_1's l2: 0.000763846
[31]	training's l2: 0.000795198	valid_1's l2: 0.000761714
[32]	training's l2: 0.000792427	valid_1's l2: 0.000760552
[33]	training's l2: 0.000789475	valid_1's l2: 0.000758737
[34]	training's l2: 0.000787164	valid_1's l2: 0.000757434
[35]	training's l2: 0.000784312	valid_1's l2: 0.000755877
[36]	training's l2: 0.00078076	valid_1's l2: 0.000753193
[37]	training's l2: 0.000778525	valid_1's l2: 0.000751756
[38]	training's l2: 0.000775934	valid_1's l2: 0.000750787
[39]	training's l2: 0.000773549	valid_1's l2: 0.000749557
[40]	training's l2: 0.000770637	valid_1's l2: 0.000747642
[41]	training's l2: 0.000768458	valid_1's l2: 0.000746724
[42]	training's l2: 0.000765284	valid_1's l2: 0.000744077
[43]	training's l2: 0.0007622	valid_1's l2: 0.000740764
[44]	training's l2: 0.000760162	valid_1's l2: 0.000739793
[45]	training's l2: 0.000757843	valid_1's l2: 0.000737905
[46]	training's l2: 0.000755771	valid_1's l2: 0.000736427
[47]	training's l2: 0.000754131	valid_1's l2: 0.000735835
[48]	training's l2: 0.000752269	valid_1's l2: 0.00073526
[49]	training's l2: 0.000750219	valid_1's l2: 0.000733977
[50]	training's l2: 0.000748244	valid_1's l2: 0.000733122
[51]	training's l2: 0.000746499	valid_1's l2: 0.000732445
[52]	training's l2: 0.000744419	valid_1's l2: 0.00073201
[53]	training's l2: 0.000742828	valid_1's l2: 0.000731519
[54]	training's l2: 0.000741375	valid_1's l2: 0.000731119
[55]	training's l2: 0.000739406	valid_1's l2: 0.000730369
[56]	training's l2: 0.000737377	valid_1's l2: 0.000729483
[57]	training's l2: 0.000735507	valid_1's l2: 0.000728294
[58]	training's l2: 0.000733758	valid_1's l2: 0.000727664
[59]	training's l2: 0.000732266	valid_1's l2: 0.000727329
[60]	training's l2: 0.000730273	valid_1's l2: 0.000725569
[61]	training's l2: 0.000728304	valid_1's l2: 0.000724904
[62]	training's l2: 0.000726676	valid_1's l2: 0.000723927
[63]	training's l2: 0.000725218	valid_1's l2: 0.000723813
[64]	training's l2: 0.000723946	valid_1's l2: 0.000723531
[65]	training's l2: 0.000722516	valid_1's l2: 0.000722918
[66]	training's l2: 0.000720657	valid_1's l2: 0.000721876
[67]	training's l2: 0.000718735	valid_1's l2: 0.000721548
[68]	training's l2: 0.000716803	valid_1's l2: 0.000720931
[69]	training's l2: 0.000715163	valid_1's l2: 0.0007207
[70]	training's l2: 0.000713948	valid_1's l2: 0.000720376
[71]	training's l2: 0.000712204	valid_1's l2: 0.000720066
[72]	training's l2: 0.000710965	valid_1's l2: 0.000719737
[73]	training's l2: 0.000709353	valid_1's l2: 0.00071954
[74]	training's l2: 0.000707896	valid_1's l2: 0.000719324
[75]	training's l2: 0.000706389	valid_1's l2: 0.000719068
[76]	training's l2: 0.000705001	valid_1's l2: 0.000718917
[77]	training's l2: 0.000703627	valid_1's l2: 0.000718629
[78]	training's l2: 0.000701923	valid_1's l2: 0.000717549
[79]	training's l2: 0.000700434	valid_1's l2: 0.000717153
[80]	training's l2: 0.000699121	valid_1's l2: 0.000717322
[81]	training's l2: 0.000697625	valid_1's l2: 0.000717164
[82]	training's l2: 0.000696289	valid_1's l2: 0.000717075
[83]	training's l2: 0.000694952	valid_1's l2: 0.000716824
[84]	training's l2: 0.000693571	valid_1's l2: 0.000716727
[85]	training's l2: 0.000692329	valid_1's l2: 0.000716413
[86]	training's l2: 0.000690869	valid_1's l2: 0.000715724
[87]	training's l2: 0.000689354	valid_1's l2: 0.000715176
[88]	training's l2: 0.000687832	valid_1's l2: 0.00071417
[89]	training's l2: 0.000686552	valid_1's l2: 0.000713813
[90]	training's l2: 0.000685327	valid_1's l2: 0.000713614
[91]	training's l2: 0.000684263	valid_1's l2: 0.000713471
[92]	training's l2: 0.000683191	valid_1's l2: 0.000713407
[93]	training's l2: 0.000681951	valid_1's l2: 0.000713231
[94]	training's l2: 0.000680691	valid_1's l2: 0.000713092
[95]	training's l2: 0.000679459	valid_1's l2: 0.000713194
[96]	training's l2: 0.000678152	valid_1's l2: 0.000712927
[97]	training's l2: 0.000677096	valid_1's l2: 0.000712827
[98]	training's l2: 0.000675867	valid_1's l2: 0.000712653
[99]	training's l2: 0.00067472	valid_1's l2: 0.000712501
[100]	training's l2: 0.000673698	valid_1's l2: 0.000712463
[101]	training's l2: 0.000672596	valid_1's l2: 0.000712332
[102]	training's l2: 0.000671486	valid_1's l2: 0.000712373
[103]	training's l2: 0.000670147	valid_1's l2: 0.000712645
[104]	training's l2: 0.000668723	valid_1's l2: 0.000712257
[105]	training's l2: 0.000667498	valid_1's l2: 0.000712106
[106]	training's l2: 0.000666414	valid_1's l2: 0.000712171
[107]	training's l2: 0.000665376	valid_1's l2: 0.00071208
[108]	training's l2: 0.000664269	valid_1's l2: 0.000712043
[109]	training's l2: 0.000663044	valid_1's l2: 0.000712031
[110]	training's l2: 0.000662038	valid_1's l2: 0.000711924
[111]	training's l2: 0.000660939	valid_1's l2: 0.00071187
[112]	training's l2: 0.000659645	valid_1's l2: 0.000711226
[113]	training's l2: 0.000658672	valid_1's l2: 0.000711107
[114]	training's l2: 0.000657417	valid_1's l2: 0.000710995
[115]	training's l2: 0.000656236	valid_1's l2: 0.000710849
[116]	training's l2: 0.000655067	valid_1's l2: 0.000710552
[117]	training's l2: 0.000653794	valid_1's l2: 0.00071021
[118]	training's l2: 0.000652568	valid_1's l2: 0.000710211
[119]	training's l2: 0.000651454	valid_1's l2: 0.000710241
[120]	training's l2: 0.000650423	valid_1's l2: 0.000710001
[121]	training's l2: 0.000649504	valid_1's l2: 0.000710147
[122]	training's l2: 0.000648474	valid_1's l2: 0.000710411
[123]	training's l2: 0.000647351	valid_1's l2: 0.000710131
[124]	training's l2: 0.000646411	valid_1's l2: 0.000710132
[125]	training's l2: 0.000644963	valid_1's l2: 0.000709689
[126]	training's l2: 0.000643954	valid_1's l2: 0.000709768
[127]	training's l2: 0.000642864	valid_1's l2: 0.000709535
[128]	training's l2: 0.000641844	valid_1's l2: 0.000709463
[129]	training's l2: 0.000640858	valid_1's l2: 0.000709339
[130]	training's l2: 0.000639852	valid_1's l2: 0.000709399
[131]	training's l2: 0.000639025	valid_1's l2: 0.000709453
[132]	training's l2: 0.000638207	valid_1's l2: 0.00070965
[133]	training's l2: 0.000636705	valid_1's l2: 0.00070855
[134]	training's l2: 0.00063569	valid_1's l2: 0.000708443
[135]	training's l2: 0.000634649	valid_1's l2: 0.000708426
[136]	training's l2: 0.000633765	valid_1's l2: 0.000708397
[137]	training's l2: 0.000632821	valid_1's l2: 0.000708469
[138]	training's l2: 0.00063188	valid_1's l2: 0.000708462
[139]	training's l2: 0.000630913	valid_1's l2: 0.000708431
[140]	training's l2: 0.000629854	valid_1's l2: 0.000708321
[141]	training's l2: 0.000628944	valid_1's l2: 0.000708043
[142]	training's l2: 0.000628166	valid_1's l2: 0.000708059
[143]	training's l2: 0.000627094	valid_1's l2: 0.000707854
[144]	training's l2: 0.00062598	valid_1's l2: 0.000707616
[145]	training's l2: 0.000625088	valid_1's l2: 0.00070765
[146]	training's l2: 0.000624224	valid_1's l2: 0.000707581
[147]	training's l2: 0.000623371	valid_1's l2: 0.000707444
[148]	training's l2: 0.000622393	valid_1's l2: 0.000707418
[149]	training's l2: 0.00062139	valid_1's l2: 0.000707348
[150]	training's l2: 0.000620439	valid_1's l2: 0.000707307
[151]	training's l2: 0.000619599	valid_1's l2: 0.000707432
[152]	training's l2: 0.000618772	valid_1's l2: 0.000707219
[153]	training's l2: 0.000617869	valid_1's l2: 0.000707052
[154]	training's l2: 0.000616932	valid_1's l2: 0.000706826
[155]	training's l2: 0.000615843	valid_1's l2: 0.000706764
[156]	training's l2: 0.000614963	valid_1's l2: 0.000706878
[157]	training's l2: 0.000614015	valid_1's l2: 0.00070663
[158]	training's l2: 0.000612961	valid_1's l2: 0.000706632
[159]	training's l2: 0.000611802	valid_1's l2: 0.000706288
[160]	training's l2: 0.000610975	valid_1's l2: 0.000706374
[161]	training's l2: 0.000610202	valid_1's l2: 0.000706466
[162]	training's l2: 0.000609254	valid_1's l2: 0.000706473
[163]	training's l2: 0.000608263	valid_1's l2: 0.000706669
[164]	training's l2: 0.000607295	valid_1's l2: 0.000706552
[165]	training's l2: 0.000606376	valid_1's l2: 0.000706473
[166]	training's l2: 0.000605293	valid_1's l2: 0.000706573
[167]	training's l2: 0.00060442	valid_1's l2: 0.000706608
[168]	training's l2: 0.000603474	valid_1's l2: 0.000706675
[169]	training's l2: 0.000602757	valid_1's l2: 0.000706586
[170]	training's l2: 0.000601984	valid_1's l2: 0.000706608
[171]	training's l2: 0.000601066	valid_1's l2: 0.000706654
[172]	training's l2: 0.000600224	valid_1's l2: 0.000706432
[173]	training's l2: 0.000599092	valid_1's l2: 0.000706196
[174]	training's l2: 0.000598043	valid_1's l2: 0.00070608
[175]	training's l2: 0.000597028	valid_1's l2: 0.000706026
[176]	training's l2: 0.000596252	valid_1's l2: 0.000706067
[177]	training's l2: 0.00059521	valid_1's l2: 0.000706013
[178]	training's l2: 0.000594441	valid_1's l2: 0.000706024
[179]	training's l2: 0.000593644	valid_1's l2: 0.000705999
[180]	training's l2: 0.000592727	valid_1's l2: 0.000706124
[181]	training's l2: 0.000591802	valid_1's l2: 0.000706126
[182]	training's l2: 0.000591014	valid_1's l2: 0.000706364
[183]	training's l2: 0.000590097	valid_1's l2: 0.000706273
[184]	training's l2: 0.000589239	valid_1's l2: 0.000706329
[185]	training's l2: 0.000588403	valid_1's l2: 0.000706354
[186]	training's l2: 0.000587548	valid_1's l2: 0.000706084
[187]	training's l2: 0.000586733	valid_1's l2: 0.000706152
[188]	training's l2: 0.000585965	valid_1's l2: 0.000706265
[189]	training's l2: 0.000585194	valid_1's l2: 0.000706115
[190]	training's l2: 0.000584256	valid_1's l2: 0.000705943
[191]	training's l2: 0.000583479	valid_1's l2: 0.00070586
Did not meet early stopping. Best iteration is:
[191]	training's l2: 0.000583479	valid_1's l2: 0.00070586
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.175500 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.00098055	valid_1's l2: 0.000891589
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.000960452	valid_1's l2: 0.000879266
[3]	training's l2: 0.000942669	valid_1's l2: 0.000867873
[4]	training's l2: 0.000926433	valid_1's l2: 0.000857931
[5]	training's l2: 0.000910915	valid_1's l2: 0.00084785
[6]	training's l2: 0.000897996	valid_1's l2: 0.000839784
[7]	training's l2: 0.000885612	valid_1's l2: 0.000831484
[8]	training's l2: 0.000874271	valid_1's l2: 0.000824977
[9]	training's l2: 0.000863825	valid_1's l2: 0.000817724
[10]	training's l2: 0.000854655	valid_1's l2: 0.000812621
[11]	training's l2: 0.000845468	valid_1's l2: 0.000807792
[12]	training's l2: 0.000836439	valid_1's l2: 0.000803009
[13]	training's l2: 0.000828786	valid_1's l2: 0.00079887
[14]	training's l2: 0.000820717	valid_1's l2: 0.000795322
[15]	training's l2: 0.000814255	valid_1's l2: 0.00079197
[16]	training's l2: 0.000806657	valid_1's l2: 0.000788044
[17]	training's l2: 0.000798949	valid_1's l2: 0.000783295
[18]	training's l2: 0.000793206	valid_1's l2: 0.000780449
[19]	training's l2: 0.000786985	valid_1's l2: 0.000776717
[20]	training's l2: 0.00078122	valid_1's l2: 0.000774717
[21]	training's l2: 0.000775857	valid_1's l2: 0.000771962
[22]	training's l2: 0.000770275	valid_1's l2: 0.000769827
[23]	training's l2: 0.00076546	valid_1's l2: 0.000768683
[24]	training's l2: 0.000760553	valid_1's l2: 0.000766866
[25]	training's l2: 0.000756383	valid_1's l2: 0.000765537
[26]	training's l2: 0.000752042	valid_1's l2: 0.000764052
[27]	training's l2: 0.000747502	valid_1's l2: 0.000762303
[28]	training's l2: 0.000742177	valid_1's l2: 0.00075972
[29]	training's l2: 0.00073812	valid_1's l2: 0.000758125
[30]	training's l2: 0.000733731	valid_1's l2: 0.00075486
[31]	training's l2: 0.000729813	valid_1's l2: 0.00075276
[32]	training's l2: 0.000725407	valid_1's l2: 0.000751443
[33]	training's l2: 0.000721469	valid_1's l2: 0.000750049
[34]	training's l2: 0.000717466	valid_1's l2: 0.000748461
[35]	training's l2: 0.00071362	valid_1's l2: 0.000746486
[36]	training's l2: 0.000710155	valid_1's l2: 0.00074564
[37]	training's l2: 0.000706188	valid_1's l2: 0.000744379
[38]	training's l2: 0.000702464	valid_1's l2: 0.000743896
[39]	training's l2: 0.000698495	valid_1's l2: 0.000740832
[40]	training's l2: 0.000694	valid_1's l2: 0.0007388
[41]	training's l2: 0.000690322	valid_1's l2: 0.000737323
[42]	training's l2: 0.000686803	valid_1's l2: 0.000735242
[43]	training's l2: 0.000683219	valid_1's l2: 0.000733658
[44]	training's l2: 0.000680095	valid_1's l2: 0.000732326
[45]	training's l2: 0.000676838	valid_1's l2: 0.000731129
[46]	training's l2: 0.000673563	valid_1's l2: 0.000730138
[47]	training's l2: 0.000670527	valid_1's l2: 0.000729198
[48]	training's l2: 0.000667915	valid_1's l2: 0.000728805
[49]	training's l2: 0.000664962	valid_1's l2: 0.000727615
[50]	training's l2: 0.000662057	valid_1's l2: 0.000727663
[51]	training's l2: 0.000659349	valid_1's l2: 0.000727816
[52]	training's l2: 0.000656457	valid_1's l2: 0.00072707
[53]	training's l2: 0.00065344	valid_1's l2: 0.000725476
[54]	training's l2: 0.000650637	valid_1's l2: 0.000724801
[55]	training's l2: 0.000647592	valid_1's l2: 0.000724238
[56]	training's l2: 0.000645144	valid_1's l2: 0.000723944
[57]	training's l2: 0.000642474	valid_1's l2: 0.000723534
[58]	training's l2: 0.000639791	valid_1's l2: 0.000723564
[59]	training's l2: 0.000637114	valid_1's l2: 0.000722858
[60]	training's l2: 0.000634755	valid_1's l2: 0.000722929
[61]	training's l2: 0.000632408	valid_1's l2: 0.000722542
[62]	training's l2: 0.000629802	valid_1's l2: 0.000722191
[63]	training's l2: 0.000627254	valid_1's l2: 0.000722025
[64]	training's l2: 0.000625043	valid_1's l2: 0.000721666
[65]	training's l2: 0.000622731	valid_1's l2: 0.000721596
[66]	training's l2: 0.000619791	valid_1's l2: 0.000720659
[67]	training's l2: 0.000617372	valid_1's l2: 0.000720708
[68]	training's l2: 0.000614702	valid_1's l2: 0.000720523
[69]	training's l2: 0.000611982	valid_1's l2: 0.000720067
[70]	training's l2: 0.000609778	valid_1's l2: 0.000719955
[71]	training's l2: 0.000607425	valid_1's l2: 0.000719594
[72]	training's l2: 0.000605478	valid_1's l2: 0.000719466
[73]	training's l2: 0.000603355	valid_1's l2: 0.000719568
[74]	training's l2: 0.000601049	valid_1's l2: 0.000718996
[75]	training's l2: 0.000598736	valid_1's l2: 0.000719182
[76]	training's l2: 0.000596333	valid_1's l2: 0.000719279
[77]	training's l2: 0.000594046	valid_1's l2: 0.000719554
[78]	training's l2: 0.000591844	valid_1's l2: 0.000719659
[79]	training's l2: 0.000589718	valid_1's l2: 0.000719518
[80]	training's l2: 0.000587194	valid_1's l2: 0.000718859
[81]	training's l2: 0.000585144	valid_1's l2: 0.000718858
[82]	training's l2: 0.000582961	valid_1's l2: 0.000718717
[83]	training's l2: 0.000581077	valid_1's l2: 0.000718531
[84]	training's l2: 0.000578818	valid_1's l2: 0.000718651
[85]	training's l2: 0.000576712	valid_1's l2: 0.000718533
[86]	training's l2: 0.000574325	valid_1's l2: 0.000717951
[87]	training's l2: 0.000572428	valid_1's l2: 0.000718151
[88]	training's l2: 0.000570438	valid_1's l2: 0.000717854
[89]	training's l2: 0.000568757	valid_1's l2: 0.000717585
[90]	training's l2: 0.000566952	valid_1's l2: 0.000717635
[91]	training's l2: 0.000565003	valid_1's l2: 0.000717245
[92]	training's l2: 0.000562906	valid_1's l2: 0.000716805
[93]	training's l2: 0.000560816	valid_1's l2: 0.000716585
[94]	training's l2: 0.000559006	valid_1's l2: 0.00071689
[95]	training's l2: 0.0005571	valid_1's l2: 0.000716794
[96]	training's l2: 0.000555287	valid_1's l2: 0.000716631
[97]	training's l2: 0.000553397	valid_1's l2: 0.000716927
[98]	training's l2: 0.000551256	valid_1's l2: 0.000716816
[99]	training's l2: 0.000549425	valid_1's l2: 0.000716307
[100]	training's l2: 0.000547312	valid_1's l2: 0.000715662
[101]	training's l2: 0.000545615	valid_1's l2: 0.000715615
[102]	training's l2: 0.000543791	valid_1's l2: 0.00071549
[103]	training's l2: 0.000542089	valid_1's l2: 0.000715621
[104]	training's l2: 0.000540467	valid_1's l2: 0.000715704
[105]	training's l2: 0.000538651	valid_1's l2: 0.00071562
[106]	training's l2: 0.00053675	valid_1's l2: 0.000715249
[107]	training's l2: 0.000535035	valid_1's l2: 0.00071503
[108]	training's l2: 0.0005333	valid_1's l2: 0.000714868
[109]	training's l2: 0.000531579	valid_1's l2: 0.000714683
[110]	training's l2: 0.000529812	valid_1's l2: 0.000714214
[111]	training's l2: 0.000528043	valid_1's l2: 0.000713809
[112]	training's l2: 0.000526532	valid_1's l2: 0.000713783
[113]	training's l2: 0.000524829	valid_1's l2: 0.000713758
[114]	training's l2: 0.000523258	valid_1's l2: 0.000713845
[115]	training's l2: 0.000521452	valid_1's l2: 0.000713876
[116]	training's l2: 0.000519869	valid_1's l2: 0.000713877
[117]	training's l2: 0.000518102	valid_1's l2: 0.000713954
[118]	training's l2: 0.000516702	valid_1's l2: 0.000714078
[119]	training's l2: 0.000515207	valid_1's l2: 0.000714254
[120]	training's l2: 0.000513481	valid_1's l2: 0.000713938
[121]	training's l2: 0.000512009	valid_1's l2: 0.000713936
[122]	training's l2: 0.000510236	valid_1's l2: 0.00071381
[123]	training's l2: 0.000508883	valid_1's l2: 0.000713885
[124]	training's l2: 0.000507262	valid_1's l2: 0.000713603
[125]	training's l2: 0.000505784	valid_1's l2: 0.000713633
[126]	training's l2: 0.000504352	valid_1's l2: 0.000713847
[127]	training's l2: 0.000502966	valid_1's l2: 0.000713687
[128]	training's l2: 0.000501526	valid_1's l2: 0.000713495
[129]	training's l2: 0.000500053	valid_1's l2: 0.000713047
[130]	training's l2: 0.000498318	valid_1's l2: 0.000712836
[131]	training's l2: 0.000496869	valid_1's l2: 0.000712978
[132]	training's l2: 0.000495598	valid_1's l2: 0.0007131
[133]	training's l2: 0.000494207	valid_1's l2: 0.000713031
[134]	training's l2: 0.000492883	valid_1's l2: 0.000712911
[135]	training's l2: 0.00049125	valid_1's l2: 0.00071302
[136]	training's l2: 0.000489738	valid_1's l2: 0.000713116
[137]	training's l2: 0.000488109	valid_1's l2: 0.000713031
[138]	training's l2: 0.000486783	valid_1's l2: 0.000713116
[139]	training's l2: 0.000485235	valid_1's l2: 0.000713077
[140]	training's l2: 0.000483783	valid_1's l2: 0.000712896
[141]	training's l2: 0.000482407	valid_1's l2: 0.000712816
[142]	training's l2: 0.000481078	valid_1's l2: 0.000712673
[143]	training's l2: 0.000479797	valid_1's l2: 0.000712475
[144]	training's l2: 0.000478214	valid_1's l2: 0.000712494
[145]	training's l2: 0.000476996	valid_1's l2: 0.000712496
[146]	training's l2: 0.000475581	valid_1's l2: 0.000712328
[147]	training's l2: 0.000474228	valid_1's l2: 0.000712442
[148]	training's l2: 0.000472906	valid_1's l2: 0.000712389
[149]	training's l2: 0.000471719	valid_1's l2: 0.000712351
[150]	training's l2: 0.000470258	valid_1's l2: 0.000712367
Did not meet early stopping. Best iteration is:
[150]	training's l2: 0.000470258	valid_1's l2: 0.000712367
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180997 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.000757
[1]	training's l2: 0.000977632	valid_1's l2: 0.000889417
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.0009564	valid_1's l2: 0.000873747
[3]	training's l2: 0.000936697	valid_1's l2: 0.000859232
[4]	training's l2: 0.00092125	valid_1's l2: 0.000848558
[5]	training's l2: 0.000907121	valid_1's l2: 0.000838282
[6]	training's l2: 0.000893804	valid_1's l2: 0.000830163
[7]	training's l2: 0.0008824	valid_1's l2: 0.000823747
[8]	training's l2: 0.000870508	valid_1's l2: 0.000818249
[9]	training's l2: 0.000860251	valid_1's l2: 0.000812974
[10]	training's l2: 0.000849779	valid_1's l2: 0.000805826
[11]	training's l2: 0.000840705	valid_1's l2: 0.000800187
[12]	training's l2: 0.000832533	valid_1's l2: 0.000794701
[13]	training's l2: 0.000825231	valid_1's l2: 0.000790071
[14]	training's l2: 0.000818374	valid_1's l2: 0.000785821
[15]	training's l2: 0.000811982	valid_1's l2: 0.000781954
[16]	training's l2: 0.000805198	valid_1's l2: 0.000778253
[17]	training's l2: 0.000799261	valid_1's l2: 0.000775066
[18]	training's l2: 0.000794337	valid_1's l2: 0.000773238
[19]	training's l2: 0.000789641	valid_1's l2: 0.000770419
[20]	training's l2: 0.000785276	valid_1's l2: 0.000768052
[21]	training's l2: 0.000780177	valid_1's l2: 0.000763788
[22]	training's l2: 0.000776006	valid_1's l2: 0.000762557
[23]	training's l2: 0.000771632	valid_1's l2: 0.00076092
[24]	training's l2: 0.000767104	valid_1's l2: 0.000759242
[25]	training's l2: 0.000762596	valid_1's l2: 0.00075713
[26]	training's l2: 0.000758109	valid_1's l2: 0.000754337
[27]	training's l2: 0.000752934	valid_1's l2: 0.000752172
[28]	training's l2: 0.00074826	valid_1's l2: 0.000749757
[29]	training's l2: 0.000744901	valid_1's l2: 0.00074911
[30]	training's l2: 0.000741613	valid_1's l2: 0.000747121
[31]	training's l2: 0.00073759	valid_1's l2: 0.000744851
[32]	training's l2: 0.000734143	valid_1's l2: 0.000744089
[33]	training's l2: 0.000729931	valid_1's l2: 0.000741256
[34]	training's l2: 0.00072631	valid_1's l2: 0.000739411
[35]	training's l2: 0.000722269	valid_1's l2: 0.000738058
[36]	training's l2: 0.000719401	valid_1's l2: 0.000736963
[37]	training's l2: 0.000716596	valid_1's l2: 0.000735714
[38]	training's l2: 0.000713723	valid_1's l2: 0.000734584
[39]	training's l2: 0.000710945	valid_1's l2: 0.000733758
[40]	training's l2: 0.000708264	valid_1's l2: 0.000732622
[41]	training's l2: 0.000706041	valid_1's l2: 0.000732597
[42]	training's l2: 0.000702477	valid_1's l2: 0.000730736
[43]	training's l2: 0.000699748	valid_1's l2: 0.000730094
[44]	training's l2: 0.000696829	valid_1's l2: 0.000728045
[45]	training's l2: 0.000694441	valid_1's l2: 0.00072703
[46]	training's l2: 0.000691806	valid_1's l2: 0.000725925
[47]	training's l2: 0.000689664	valid_1's l2: 0.000725639
[48]	training's l2: 0.000687135	valid_1's l2: 0.000724896
[49]	training's l2: 0.00068496	valid_1's l2: 0.000724869
[50]	training's l2: 0.000682362	valid_1's l2: 0.000722905
[51]	training's l2: 0.000679827	valid_1's l2: 0.000722459
[52]	training's l2: 0.000677605	valid_1's l2: 0.0007221
[53]	training's l2: 0.000675263	valid_1's l2: 0.000720838
[54]	training's l2: 0.000672956	valid_1's l2: 0.00072045
[55]	training's l2: 0.000670735	valid_1's l2: 0.00072027
[56]	training's l2: 0.000668102	valid_1's l2: 0.000720049
[57]	training's l2: 0.00066608	valid_1's l2: 0.000720478
[58]	training's l2: 0.000663907	valid_1's l2: 0.000720803
[59]	training's l2: 0.000661953	valid_1's l2: 0.000720862
[60]	training's l2: 0.000659852	valid_1's l2: 0.000721209
[61]	training's l2: 0.000657824	valid_1's l2: 0.000721093
[62]	training's l2: 0.000655825	valid_1's l2: 0.000720855
[63]	training's l2: 0.000653567	valid_1's l2: 0.000720654
[64]	training's l2: 0.000650573	valid_1's l2: 0.000719496
[65]	training's l2: 0.000648562	valid_1's l2: 0.000719753
[66]	training's l2: 0.000646795	valid_1's l2: 0.000719126
[67]	training's l2: 0.00064458	valid_1's l2: 0.000718453
[68]	training's l2: 0.000642603	valid_1's l2: 0.000718597
[69]	training's l2: 0.000640707	valid_1's l2: 0.000718436
[70]	training's l2: 0.000638994	valid_1's l2: 0.000718167
[71]	training's l2: 0.000637021	valid_1's l2: 0.000718591
[72]	training's l2: 0.000634821	valid_1's l2: 0.000718153
[73]	training's l2: 0.000633081	valid_1's l2: 0.000718166
[74]	training's l2: 0.000631323	valid_1's l2: 0.000717963
[75]	training's l2: 0.000629307	valid_1's l2: 0.000717876
[76]	training's l2: 0.000627592	valid_1's l2: 0.000717899
[77]	training's l2: 0.00062625	valid_1's l2: 0.00071757
[78]	training's l2: 0.000624778	valid_1's l2: 0.000717572
[79]	training's l2: 0.000623034	valid_1's l2: 0.000717664
[80]	training's l2: 0.000621285	valid_1's l2: 0.000717656
[81]	training's l2: 0.000619346	valid_1's l2: 0.000717626
[82]	training's l2: 0.000617873	valid_1's l2: 0.000717769
[83]	training's l2: 0.00061588	valid_1's l2: 0.000717554
[84]	training's l2: 0.00061405	valid_1's l2: 0.000717544
[85]	training's l2: 0.00061227	valid_1's l2: 0.000717077
[86]	training's l2: 0.000610647	valid_1's l2: 0.000716859
[87]	training's l2: 0.000608944	valid_1's l2: 0.000716802
[88]	training's l2: 0.000607377	valid_1's l2: 0.000716587
[89]	training's l2: 0.000605819	valid_1's l2: 0.000716672
[90]	training's l2: 0.000604143	valid_1's l2: 0.000716481
[91]	training's l2: 0.000602538	valid_1's l2: 0.000716628
[92]	training's l2: 0.000600403	valid_1's l2: 0.000715623
[93]	training's l2: 0.000599034	valid_1's l2: 0.000715467
[94]	training's l2: 0.000597362	valid_1's l2: 0.000715475
[95]	training's l2: 0.000596008	valid_1's l2: 0.000715617
[96]	training's l2: 0.00059444	valid_1's l2: 0.00071552
[97]	training's l2: 0.00059307	valid_1's l2: 0.000714884
[98]	training's l2: 0.000591377	valid_1's l2: 0.000714541
[99]	training's l2: 0.000589974	valid_1's l2: 0.000714829
[100]	training's l2: 0.000588441	valid_1's l2: 0.00071484
[101]	training's l2: 0.000586706	valid_1's l2: 0.000714945
[102]	training's l2: 0.000585242	valid_1's l2: 0.000714956
[103]	training's l2: 0.000583411	valid_1's l2: 0.000714789
[104]	training's l2: 0.000582048	valid_1's l2: 0.000714834
[105]	training's l2: 0.0005801	valid_1's l2: 0.000714766
[106]	training's l2: 0.00057878	valid_1's l2: 0.000714539
[107]	training's l2: 0.000577611	valid_1's l2: 0.000714464
[108]	training's l2: 0.000575703	valid_1's l2: 0.000713519
[109]	training's l2: 0.000574521	valid_1's l2: 0.000713515
[110]	training's l2: 0.00057327	valid_1's l2: 0.000713162
[111]	training's l2: 0.00057195	valid_1's l2: 0.000713092
[112]	training's l2: 0.000570643	valid_1's l2: 0.000712971
[113]	training's l2: 0.000569327	valid_1's l2: 0.000712872
[114]	training's l2: 0.000567684	valid_1's l2: 0.000712835
[115]	training's l2: 0.000566314	valid_1's l2: 0.000712845
[116]	training's l2: 0.000564887	valid_1's l2: 0.000712756
[117]	training's l2: 0.000563573	valid_1's l2: 0.000712804
[118]	training's l2: 0.000562494	valid_1's l2: 0.000712947
[119]	training's l2: 0.000561067	valid_1's l2: 0.000713099
[120]	training's l2: 0.000559834	valid_1's l2: 0.000713326
[121]	training's l2: 0.00055827	valid_1's l2: 0.000713625
[122]	training's l2: 0.000557193	valid_1's l2: 0.000713713
[123]	training's l2: 0.00055599	valid_1's l2: 0.000713775
[124]	training's l2: 0.000554698	valid_1's l2: 0.000713788
[125]	training's l2: 0.000553553	valid_1's l2: 0.000713846
[126]	training's l2: 0.000552453	valid_1's l2: 0.000713879
[127]	training's l2: 0.00055093	valid_1's l2: 0.000714021
[128]	training's l2: 0.00054969	valid_1's l2: 0.000713626
[129]	training's l2: 0.000548494	valid_1's l2: 0.000713463
[130]	training's l2: 0.000547401	valid_1's l2: 0.000713757
[131]	training's l2: 0.000546112	valid_1's l2: 0.000713828
[132]	training's l2: 0.000544843	valid_1's l2: 0.000713723
[133]	training's l2: 0.00054357	valid_1's l2: 0.000713747
[134]	training's l2: 0.000542266	valid_1's l2: 0.000713579
[135]	training's l2: 0.00054103	valid_1's l2: 0.000713394
[136]	training's l2: 0.000539705	valid_1's l2: 0.000713419
[137]	training's l2: 0.000538544	valid_1's l2: 0.000713178
[138]	training's l2: 0.000537264	valid_1's l2: 0.000713133
[139]	training's l2: 0.000536263	valid_1's l2: 0.000713367
[140]	training's l2: 0.000534976	valid_1's l2: 0.000713336
[141]	training's l2: 0.000533666	valid_1's l2: 0.000713302
[142]	training's l2: 0.000532322	valid_1's l2: 0.000713255
[143]	training's l2: 0.000531043	valid_1's l2: 0.000713517
[144]	training's l2: 0.000529811	valid_1's l2: 0.000713571
[145]	training's l2: 0.000528475	valid_1's l2: 0.000713473
[146]	training's l2: 0.000527264	valid_1's l2: 0.000713292
Early stopping, best iteration is:
[116]	training's l2: 0.000564887	valid_1's l2: 0.000712756
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.172005 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230921	valid_1's l2: 0.00233286
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00227329	valid_1's l2: 0.00230357
[3]	training's l2: 0.00224013	valid_1's l2: 0.00227515
[4]	training's l2: 0.00221128	valid_1's l2: 0.00225282
[5]	training's l2: 0.00218574	valid_1's l2: 0.00223085
[6]	training's l2: 0.00216299	valid_1's l2: 0.00221548
[7]	training's l2: 0.0021411	valid_1's l2: 0.00219903
[8]	training's l2: 0.0021222	valid_1's l2: 0.00218127
[9]	training's l2: 0.00210343	valid_1's l2: 0.00216628
[10]	training's l2: 0.00208639	valid_1's l2: 0.00215425
[11]	training's l2: 0.00207295	valid_1's l2: 0.00214211
[12]	training's l2: 0.0020588	valid_1's l2: 0.0021322
[13]	training's l2: 0.00204752	valid_1's l2: 0.00212303
[14]	training's l2: 0.00203551	valid_1's l2: 0.00211585
[15]	training's l2: 0.00202583	valid_1's l2: 0.00210944
[16]	training's l2: 0.00201643	valid_1's l2: 0.00210015
[17]	training's l2: 0.00200607	valid_1's l2: 0.00209428
[18]	training's l2: 0.00199661	valid_1's l2: 0.00208813
[19]	training's l2: 0.0019878	valid_1's l2: 0.00208351
[20]	training's l2: 0.0019796	valid_1's l2: 0.00208027
[21]	training's l2: 0.00197256	valid_1's l2: 0.00207257
[22]	training's l2: 0.00196517	valid_1's l2: 0.0020689
[23]	training's l2: 0.00195916	valid_1's l2: 0.00206547
[24]	training's l2: 0.00195253	valid_1's l2: 0.00206235
[25]	training's l2: 0.00194703	valid_1's l2: 0.00205653
[26]	training's l2: 0.00193985	valid_1's l2: 0.00205268
[27]	training's l2: 0.00193433	valid_1's l2: 0.00205006
[28]	training's l2: 0.00192864	valid_1's l2: 0.00204782
[29]	training's l2: 0.0019234	valid_1's l2: 0.00204459
[30]	training's l2: 0.00191823	valid_1's l2: 0.00204165
[31]	training's l2: 0.00191373	valid_1's l2: 0.00203956
[32]	training's l2: 0.00190922	valid_1's l2: 0.0020342
[33]	training's l2: 0.00190389	valid_1's l2: 0.00203192
[34]	training's l2: 0.00189983	valid_1's l2: 0.00202848
[35]	training's l2: 0.00189565	valid_1's l2: 0.00202635
[36]	training's l2: 0.00189095	valid_1's l2: 0.00202208
[37]	training's l2: 0.0018872	valid_1's l2: 0.00201821
[38]	training's l2: 0.00188109	valid_1's l2: 0.00201485
[39]	training's l2: 0.00187675	valid_1's l2: 0.00201291
[40]	training's l2: 0.00187319	valid_1's l2: 0.00201116
[41]	training's l2: 0.00186767	valid_1's l2: 0.00200746
[42]	training's l2: 0.0018636	valid_1's l2: 0.00200353
[43]	training's l2: 0.00185876	valid_1's l2: 0.00200099
[44]	training's l2: 0.00185487	valid_1's l2: 0.00199825
[45]	training's l2: 0.00185138	valid_1's l2: 0.0019954
[46]	training's l2: 0.0018479	valid_1's l2: 0.00199407
[47]	training's l2: 0.001845	valid_1's l2: 0.00199201
[48]	training's l2: 0.00184132	valid_1's l2: 0.00199031
[49]	training's l2: 0.00183816	valid_1's l2: 0.0019885
[50]	training's l2: 0.00183393	valid_1's l2: 0.00198574
[51]	training's l2: 0.00183066	valid_1's l2: 0.00198468
[52]	training's l2: 0.00182788	valid_1's l2: 0.00198373
[53]	training's l2: 0.0018245	valid_1's l2: 0.00198223
[54]	training's l2: 0.00182166	valid_1's l2: 0.00198128
[55]	training's l2: 0.00181828	valid_1's l2: 0.00197984
[56]	training's l2: 0.00181561	valid_1's l2: 0.00197726
[57]	training's l2: 0.00181233	valid_1's l2: 0.0019758
[58]	training's l2: 0.00180985	valid_1's l2: 0.00197394
[59]	training's l2: 0.0018063	valid_1's l2: 0.00197267
[60]	training's l2: 0.00180351	valid_1's l2: 0.00197026
[61]	training's l2: 0.00180111	valid_1's l2: 0.00196821
[62]	training's l2: 0.00179881	valid_1's l2: 0.00196795
[63]	training's l2: 0.00179632	valid_1's l2: 0.00196673
[64]	training's l2: 0.00179389	valid_1's l2: 0.00196514
[65]	training's l2: 0.00179061	valid_1's l2: 0.00196356
[66]	training's l2: 0.00178792	valid_1's l2: 0.00196271
[67]	training's l2: 0.00178566	valid_1's l2: 0.00196272
[68]	training's l2: 0.00178321	valid_1's l2: 0.00196207
[69]	training's l2: 0.0017807	valid_1's l2: 0.00196086
[70]	training's l2: 0.0017784	valid_1's l2: 0.00196033
[71]	training's l2: 0.00177633	valid_1's l2: 0.0019598
[72]	training's l2: 0.00177401	valid_1's l2: 0.00195873
[73]	training's l2: 0.00177117	valid_1's l2: 0.00195751
[74]	training's l2: 0.0017687	valid_1's l2: 0.00195677
[75]	training's l2: 0.00176487	valid_1's l2: 0.00195492
[76]	training's l2: 0.00176262	valid_1's l2: 0.00195478
[77]	training's l2: 0.0017605	valid_1's l2: 0.00195397
[78]	training's l2: 0.0017583	valid_1's l2: 0.00195404
[79]	training's l2: 0.00175601	valid_1's l2: 0.00195322
[80]	training's l2: 0.00175277	valid_1's l2: 0.00195154
[81]	training's l2: 0.00175078	valid_1's l2: 0.00195051
[82]	training's l2: 0.0017485	valid_1's l2: 0.00194961
[83]	training's l2: 0.00174661	valid_1's l2: 0.00194947
[84]	training's l2: 0.00174379	valid_1's l2: 0.00194822
[85]	training's l2: 0.0017418	valid_1's l2: 0.00194805
[86]	training's l2: 0.0017397	valid_1's l2: 0.00194704
[87]	training's l2: 0.0017377	valid_1's l2: 0.00194676
[88]	training's l2: 0.0017357	valid_1's l2: 0.00194628
[89]	training's l2: 0.001733	valid_1's l2: 0.00194549
[90]	training's l2: 0.00173102	valid_1's l2: 0.00194478
[91]	training's l2: 0.00172915	valid_1's l2: 0.00194439
[92]	training's l2: 0.00172722	valid_1's l2: 0.00194396
[93]	training's l2: 0.00172512	valid_1's l2: 0.00194371
[94]	training's l2: 0.00172341	valid_1's l2: 0.00194276
[95]	training's l2: 0.00172096	valid_1's l2: 0.00194217
[96]	training's l2: 0.00171917	valid_1's l2: 0.00194182
[97]	training's l2: 0.00171699	valid_1's l2: 0.00194089
[98]	training's l2: 0.00171516	valid_1's l2: 0.00194097
[99]	training's l2: 0.00171318	valid_1's l2: 0.00194091
[100]	training's l2: 0.0017108	valid_1's l2: 0.00194037
[101]	training's l2: 0.00170904	valid_1's l2: 0.00193953
[102]	training's l2: 0.00170714	valid_1's l2: 0.00193868
[103]	training's l2: 0.00170492	valid_1's l2: 0.00193794
[104]	training's l2: 0.00170304	valid_1's l2: 0.00193707
[105]	training's l2: 0.00170129	valid_1's l2: 0.00193638
[106]	training's l2: 0.00169955	valid_1's l2: 0.001936
[107]	training's l2: 0.00169785	valid_1's l2: 0.00193559
[108]	training's l2: 0.00169621	valid_1's l2: 0.0019354
[109]	training's l2: 0.00169454	valid_1's l2: 0.00193545
[110]	training's l2: 0.00169247	valid_1's l2: 0.0019337
[111]	training's l2: 0.00169067	valid_1's l2: 0.00193338
[112]	training's l2: 0.00168885	valid_1's l2: 0.00193282
[113]	training's l2: 0.00168714	valid_1's l2: 0.0019329
[114]	training's l2: 0.00168555	valid_1's l2: 0.00193308
[115]	training's l2: 0.00168355	valid_1's l2: 0.00193319
[116]	training's l2: 0.00168154	valid_1's l2: 0.00193228
[117]	training's l2: 0.00167982	valid_1's l2: 0.00193168
[118]	training's l2: 0.00167811	valid_1's l2: 0.00193193
[119]	training's l2: 0.00167658	valid_1's l2: 0.00193134
[120]	training's l2: 0.00167502	valid_1's l2: 0.00193135
[121]	training's l2: 0.00167343	valid_1's l2: 0.00193103
[122]	training's l2: 0.00167173	valid_1's l2: 0.0019305
[123]	training's l2: 0.00167002	valid_1's l2: 0.00193011
[124]	training's l2: 0.00166832	valid_1's l2: 0.00193048
[125]	training's l2: 0.00166648	valid_1's l2: 0.00193095
[126]	training's l2: 0.00166483	valid_1's l2: 0.00193062
[127]	training's l2: 0.00166319	valid_1's l2: 0.00193051
[128]	training's l2: 0.00166113	valid_1's l2: 0.00192946
[129]	training's l2: 0.00165957	valid_1's l2: 0.00192961
[130]	training's l2: 0.0016582	valid_1's l2: 0.00192974
[131]	training's l2: 0.00165653	valid_1's l2: 0.00192904
[132]	training's l2: 0.00165504	valid_1's l2: 0.00192864
[133]	training's l2: 0.00165339	valid_1's l2: 0.00192806
[134]	training's l2: 0.00165193	valid_1's l2: 0.00192782
[135]	training's l2: 0.0016503	valid_1's l2: 0.00192825
[136]	training's l2: 0.00164893	valid_1's l2: 0.00192791
[137]	training's l2: 0.00164744	valid_1's l2: 0.00192757
[138]	training's l2: 0.00164572	valid_1's l2: 0.0019278
[139]	training's l2: 0.00164407	valid_1's l2: 0.00192793
[140]	training's l2: 0.00164246	valid_1's l2: 0.00192802
[141]	training's l2: 0.00164067	valid_1's l2: 0.00192736
[142]	training's l2: 0.00163903	valid_1's l2: 0.00192692
[143]	training's l2: 0.00163737	valid_1's l2: 0.00192677
[144]	training's l2: 0.00163596	valid_1's l2: 0.00192694
[145]	training's l2: 0.00163447	valid_1's l2: 0.00192694
[146]	training's l2: 0.00163295	valid_1's l2: 0.00192687
[147]	training's l2: 0.00163154	valid_1's l2: 0.00192703
[148]	training's l2: 0.00163035	valid_1's l2: 0.0019269
[149]	training's l2: 0.00162902	valid_1's l2: 0.00192721
[150]	training's l2: 0.00162756	valid_1's l2: 0.00192721
[151]	training's l2: 0.00162606	valid_1's l2: 0.00192741
[152]	training's l2: 0.00162442	valid_1's l2: 0.00192736
[153]	training's l2: 0.00162275	valid_1's l2: 0.00192704
[154]	training's l2: 0.00162143	valid_1's l2: 0.00192684
[155]	training's l2: 0.00161983	valid_1's l2: 0.00192642
[156]	training's l2: 0.00161829	valid_1's l2: 0.0019262
[157]	training's l2: 0.00161686	valid_1's l2: 0.00192643
[158]	training's l2: 0.00161546	valid_1's l2: 0.00192639
[159]	training's l2: 0.00161403	valid_1's l2: 0.00192623
[160]	training's l2: 0.0016129	valid_1's l2: 0.00192627
[161]	training's l2: 0.00161158	valid_1's l2: 0.00192606
[162]	training's l2: 0.00161016	valid_1's l2: 0.00192562
[163]	training's l2: 0.00160861	valid_1's l2: 0.00192588
[164]	training's l2: 0.00160719	valid_1's l2: 0.00192583
[165]	training's l2: 0.00160594	valid_1's l2: 0.00192593
[166]	training's l2: 0.00160439	valid_1's l2: 0.00192535
[167]	training's l2: 0.00160296	valid_1's l2: 0.00192532
[168]	training's l2: 0.00160136	valid_1's l2: 0.00192525
[169]	training's l2: 0.00159988	valid_1's l2: 0.00192502
[170]	training's l2: 0.00159856	valid_1's l2: 0.00192493
[171]	training's l2: 0.00159697	valid_1's l2: 0.00192519
[172]	training's l2: 0.00159536	valid_1's l2: 0.00192469
[173]	training's l2: 0.00159408	valid_1's l2: 0.00192427
[174]	training's l2: 0.00159284	valid_1's l2: 0.00192411
[175]	training's l2: 0.00159136	valid_1's l2: 0.00192348
[176]	training's l2: 0.00159006	valid_1's l2: 0.00192315
[177]	training's l2: 0.0015887	valid_1's l2: 0.00192273
[178]	training's l2: 0.00158736	valid_1's l2: 0.00192251
[179]	training's l2: 0.00158606	valid_1's l2: 0.00192227
[180]	training's l2: 0.00158475	valid_1's l2: 0.00192247
[181]	training's l2: 0.0015835	valid_1's l2: 0.00192224
[182]	training's l2: 0.00158229	valid_1's l2: 0.00192244
[183]	training's l2: 0.00158074	valid_1's l2: 0.0019223
[184]	training's l2: 0.00157954	valid_1's l2: 0.00192217
[185]	training's l2: 0.00157845	valid_1's l2: 0.00192225
[186]	training's l2: 0.00157721	valid_1's l2: 0.00192224
[187]	training's l2: 0.001576	valid_1's l2: 0.00192201
[188]	training's l2: 0.0015748	valid_1's l2: 0.00192231
[189]	training's l2: 0.0015732	valid_1's l2: 0.00192227
[190]	training's l2: 0.0015719	valid_1's l2: 0.00192263
[191]	training's l2: 0.00157066	valid_1's l2: 0.00192251
[192]	training's l2: 0.00156922	valid_1's l2: 0.00192251
[193]	training's l2: 0.00156785	valid_1's l2: 0.00192254
[194]	training's l2: 0.00156673	valid_1's l2: 0.00192246
[195]	training's l2: 0.00156561	valid_1's l2: 0.00192244
Did not meet early stopping. Best iteration is:
[195]	training's l2: 0.00156561	valid_1's l2: 0.00192244
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185419 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230477	valid_1's l2: 0.00232928
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00226299	valid_1's l2: 0.00229531
[3]	training's l2: 0.00222611	valid_1's l2: 0.00226667
[4]	training's l2: 0.0021917	valid_1's l2: 0.00224135
[5]	training's l2: 0.00216118	valid_1's l2: 0.00221816
[6]	training's l2: 0.00213333	valid_1's l2: 0.00219962
[7]	training's l2: 0.00210681	valid_1's l2: 0.0021825
[8]	training's l2: 0.00208399	valid_1's l2: 0.00216796
[9]	training's l2: 0.00206183	valid_1's l2: 0.00215319
[10]	training's l2: 0.0020408	valid_1's l2: 0.00214252
[11]	training's l2: 0.00202231	valid_1's l2: 0.00213174
[12]	training's l2: 0.00200463	valid_1's l2: 0.0021212
[13]	training's l2: 0.0019891	valid_1's l2: 0.00210909
[14]	training's l2: 0.00197322	valid_1's l2: 0.00209977
[15]	training's l2: 0.00195943	valid_1's l2: 0.00209107
[16]	training's l2: 0.00194539	valid_1's l2: 0.00208296
[17]	training's l2: 0.00193267	valid_1's l2: 0.00207791
[18]	training's l2: 0.00192129	valid_1's l2: 0.00207082
[19]	training's l2: 0.00190975	valid_1's l2: 0.00206515
[20]	training's l2: 0.00189862	valid_1's l2: 0.00206012
[21]	training's l2: 0.0018893	valid_1's l2: 0.00205513
[22]	training's l2: 0.00187947	valid_1's l2: 0.00205115
[23]	training's l2: 0.00187004	valid_1's l2: 0.00204557
[24]	training's l2: 0.00186179	valid_1's l2: 0.0020409
[25]	training's l2: 0.00185273	valid_1's l2: 0.00203666
[26]	training's l2: 0.00184332	valid_1's l2: 0.00203276
[27]	training's l2: 0.00183453	valid_1's l2: 0.00203048
[28]	training's l2: 0.00182651	valid_1's l2: 0.00202784
[29]	training's l2: 0.0018175	valid_1's l2: 0.0020244
[30]	training's l2: 0.00181036	valid_1's l2: 0.00202116
[31]	training's l2: 0.00180321	valid_1's l2: 0.00201786
[32]	training's l2: 0.00179486	valid_1's l2: 0.0020146
[33]	training's l2: 0.00178775	valid_1's l2: 0.00201141
[34]	training's l2: 0.00178066	valid_1's l2: 0.00200892
[35]	training's l2: 0.00177391	valid_1's l2: 0.00200663
[36]	training's l2: 0.00176739	valid_1's l2: 0.00200431
[37]	training's l2: 0.00176117	valid_1's l2: 0.00200138
[38]	training's l2: 0.0017548	valid_1's l2: 0.00199931
[39]	training's l2: 0.001749	valid_1's l2: 0.00199749
[40]	training's l2: 0.00174238	valid_1's l2: 0.00199564
[41]	training's l2: 0.00173627	valid_1's l2: 0.00199356
[42]	training's l2: 0.00173048	valid_1's l2: 0.00199039
[43]	training's l2: 0.00172398	valid_1's l2: 0.00198858
[44]	training's l2: 0.00171883	valid_1's l2: 0.00198788
[45]	training's l2: 0.0017123	valid_1's l2: 0.00198598
[46]	training's l2: 0.00170707	valid_1's l2: 0.00198351
[47]	training's l2: 0.00170144	valid_1's l2: 0.00198012
[48]	training's l2: 0.0016951	valid_1's l2: 0.00197711
[49]	training's l2: 0.00169002	valid_1's l2: 0.00197434
[50]	training's l2: 0.00168385	valid_1's l2: 0.00197264
[51]	training's l2: 0.00167915	valid_1's l2: 0.00197161
[52]	training's l2: 0.00167347	valid_1's l2: 0.00196983
[53]	training's l2: 0.00166826	valid_1's l2: 0.00196838
[54]	training's l2: 0.00166279	valid_1's l2: 0.00196696
[55]	training's l2: 0.00165823	valid_1's l2: 0.0019659
[56]	training's l2: 0.00165368	valid_1's l2: 0.00196519
[57]	training's l2: 0.00164887	valid_1's l2: 0.00196381
[58]	training's l2: 0.00164426	valid_1's l2: 0.00196293
[59]	training's l2: 0.00163963	valid_1's l2: 0.00196194
[60]	training's l2: 0.00163497	valid_1's l2: 0.00196125
[61]	training's l2: 0.0016305	valid_1's l2: 0.00196042
[62]	training's l2: 0.00162628	valid_1's l2: 0.00196005
[63]	training's l2: 0.00162193	valid_1's l2: 0.0019595
[64]	training's l2: 0.00161752	valid_1's l2: 0.00195824
[65]	training's l2: 0.00161324	valid_1's l2: 0.00195742
[66]	training's l2: 0.00160865	valid_1's l2: 0.00195684
[67]	training's l2: 0.00160418	valid_1's l2: 0.0019558
[68]	training's l2: 0.00160038	valid_1's l2: 0.00195558
[69]	training's l2: 0.00159621	valid_1's l2: 0.00195468
[70]	training's l2: 0.00159214	valid_1's l2: 0.00195402
[71]	training's l2: 0.00158785	valid_1's l2: 0.00195315
[72]	training's l2: 0.00158371	valid_1's l2: 0.00195323
[73]	training's l2: 0.00157954	valid_1's l2: 0.00195234
[74]	training's l2: 0.00157564	valid_1's l2: 0.00195138
[75]	training's l2: 0.00157144	valid_1's l2: 0.00195148
[76]	training's l2: 0.00156743	valid_1's l2: 0.00195104
[77]	training's l2: 0.00156361	valid_1's l2: 0.00195058
[78]	training's l2: 0.00155989	valid_1's l2: 0.00194974
[79]	training's l2: 0.00155556	valid_1's l2: 0.00194844
[80]	training's l2: 0.0015519	valid_1's l2: 0.00194821
[81]	training's l2: 0.00154765	valid_1's l2: 0.00194737
[82]	training's l2: 0.00154383	valid_1's l2: 0.00194694
[83]	training's l2: 0.00153971	valid_1's l2: 0.00194594
[84]	training's l2: 0.00153597	valid_1's l2: 0.00194573
[85]	training's l2: 0.00153232	valid_1's l2: 0.00194599
[86]	training's l2: 0.00152824	valid_1's l2: 0.00194488
[87]	training's l2: 0.00152475	valid_1's l2: 0.00194471
[88]	training's l2: 0.00152092	valid_1's l2: 0.00194468
[89]	training's l2: 0.00151751	valid_1's l2: 0.00194434
[90]	training's l2: 0.00151382	valid_1's l2: 0.00194347
[91]	training's l2: 0.00151013	valid_1's l2: 0.00194288
[92]	training's l2: 0.0015064	valid_1's l2: 0.00194294
[93]	training's l2: 0.00150299	valid_1's l2: 0.00194275
[94]	training's l2: 0.00149969	valid_1's l2: 0.00194264
[95]	training's l2: 0.0014961	valid_1's l2: 0.00194213
[96]	training's l2: 0.0014927	valid_1's l2: 0.00194236
[97]	training's l2: 0.00148924	valid_1's l2: 0.00194231
[98]	training's l2: 0.00148583	valid_1's l2: 0.00194236
[99]	training's l2: 0.0014826	valid_1's l2: 0.0019424
[100]	training's l2: 0.00147867	valid_1's l2: 0.00194188
[101]	training's l2: 0.00147524	valid_1's l2: 0.00194165
[102]	training's l2: 0.00147168	valid_1's l2: 0.00194156
[103]	training's l2: 0.00146819	valid_1's l2: 0.00194105
[104]	training's l2: 0.00146479	valid_1's l2: 0.00194064
[105]	training's l2: 0.00146155	valid_1's l2: 0.00193979
[106]	training's l2: 0.001458	valid_1's l2: 0.001939
[107]	training's l2: 0.00145451	valid_1's l2: 0.00193941
[108]	training's l2: 0.00145112	valid_1's l2: 0.00193901
[109]	training's l2: 0.00144773	valid_1's l2: 0.00193934
[110]	training's l2: 0.00144457	valid_1's l2: 0.00193959
[111]	training's l2: 0.00144125	valid_1's l2: 0.00193953
[112]	training's l2: 0.00143807	valid_1's l2: 0.00193952
[113]	training's l2: 0.00143484	valid_1's l2: 0.00193924
[114]	training's l2: 0.0014314	valid_1's l2: 0.00193956
[115]	training's l2: 0.00142815	valid_1's l2: 0.00193942
[116]	training's l2: 0.00142473	valid_1's l2: 0.00193897
[117]	training's l2: 0.00142151	valid_1's l2: 0.00193835
[118]	training's l2: 0.00141815	valid_1's l2: 0.00193838
[119]	training's l2: 0.00141512	valid_1's l2: 0.00193815
[120]	training's l2: 0.00141207	valid_1's l2: 0.00193773
[121]	training's l2: 0.00140884	valid_1's l2: 0.00193773
[122]	training's l2: 0.00140586	valid_1's l2: 0.00193773
[123]	training's l2: 0.00140285	valid_1's l2: 0.00193718
[124]	training's l2: 0.00139973	valid_1's l2: 0.00193686
[125]	training's l2: 0.00139677	valid_1's l2: 0.00193672
[126]	training's l2: 0.00139376	valid_1's l2: 0.00193612
[127]	training's l2: 0.00139073	valid_1's l2: 0.00193648
[128]	training's l2: 0.00138762	valid_1's l2: 0.00193621
[129]	training's l2: 0.00138458	valid_1's l2: 0.00193589
[130]	training's l2: 0.00138158	valid_1's l2: 0.00193587
[131]	training's l2: 0.00137873	valid_1's l2: 0.00193595
[132]	training's l2: 0.00137581	valid_1's l2: 0.00193554
[133]	training's l2: 0.00137301	valid_1's l2: 0.00193539
[134]	training's l2: 0.00136987	valid_1's l2: 0.00193541
[135]	training's l2: 0.00136721	valid_1's l2: 0.00193504
[136]	training's l2: 0.00136428	valid_1's l2: 0.00193535
[137]	training's l2: 0.00136139	valid_1's l2: 0.00193502
[138]	training's l2: 0.00135864	valid_1's l2: 0.00193517
[139]	training's l2: 0.00135578	valid_1's l2: 0.00193537
[140]	training's l2: 0.00135284	valid_1's l2: 0.00193499
[141]	training's l2: 0.00134995	valid_1's l2: 0.00193485
[142]	training's l2: 0.00134716	valid_1's l2: 0.00193427
[143]	training's l2: 0.0013443	valid_1's l2: 0.00193409
[144]	training's l2: 0.00134149	valid_1's l2: 0.00193458
[145]	training's l2: 0.00133852	valid_1's l2: 0.00193419
[146]	training's l2: 0.00133583	valid_1's l2: 0.00193454
[147]	training's l2: 0.00133307	valid_1's l2: 0.00193437
[148]	training's l2: 0.00133035	valid_1's l2: 0.00193438
[149]	training's l2: 0.00132758	valid_1's l2: 0.00193439
[150]	training's l2: 0.00132485	valid_1's l2: 0.00193431
[151]	training's l2: 0.00132186	valid_1's l2: 0.00193427
[152]	training's l2: 0.00131924	valid_1's l2: 0.00193458
[153]	training's l2: 0.00131654	valid_1's l2: 0.00193394
[154]	training's l2: 0.00131373	valid_1's l2: 0.00193479
[155]	training's l2: 0.00131122	valid_1's l2: 0.00193493
[156]	training's l2: 0.00130857	valid_1's l2: 0.00193518
[157]	training's l2: 0.00130584	valid_1's l2: 0.00193537
[158]	training's l2: 0.00130332	valid_1's l2: 0.00193528
[159]	training's l2: 0.00130047	valid_1's l2: 0.00193526
[160]	training's l2: 0.0012977	valid_1's l2: 0.00193465
[161]	training's l2: 0.00129513	valid_1's l2: 0.00193431
[162]	training's l2: 0.00129245	valid_1's l2: 0.00193441
[163]	training's l2: 0.00128995	valid_1's l2: 0.00193379
[164]	training's l2: 0.00128747	valid_1's l2: 0.00193378
[165]	training's l2: 0.00128505	valid_1's l2: 0.00193413
[166]	training's l2: 0.00128244	valid_1's l2: 0.00193449
Did not meet early stopping. Best iteration is:
[166]	training's l2: 0.00128244	valid_1's l2: 0.00193449
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180864 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00229476	valid_1's l2: 0.002321
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00225017	valid_1's l2: 0.00228507
[3]	training's l2: 0.00220924	valid_1's l2: 0.00224945
[4]	training's l2: 0.00217572	valid_1's l2: 0.00222358
[5]	training's l2: 0.00214781	valid_1's l2: 0.0022015
[6]	training's l2: 0.00212139	valid_1's l2: 0.00218008
[7]	training's l2: 0.00210061	valid_1's l2: 0.00216685
[8]	training's l2: 0.00208157	valid_1's l2: 0.00214892
[9]	training's l2: 0.00206441	valid_1's l2: 0.00213852
[10]	training's l2: 0.00205055	valid_1's l2: 0.00212475
[11]	training's l2: 0.00203688	valid_1's l2: 0.00211622
[12]	training's l2: 0.00202396	valid_1's l2: 0.00210653
[13]	training's l2: 0.00201288	valid_1's l2: 0.00209816
[14]	training's l2: 0.0020017	valid_1's l2: 0.00209163
[15]	training's l2: 0.00199171	valid_1's l2: 0.00208584
[16]	training's l2: 0.00198316	valid_1's l2: 0.00207979
[17]	training's l2: 0.00197531	valid_1's l2: 0.00207203
[18]	training's l2: 0.00196709	valid_1's l2: 0.00206393
[19]	training's l2: 0.00195714	valid_1's l2: 0.00205743
[20]	training's l2: 0.00195089	valid_1's l2: 0.0020543
[21]	training's l2: 0.0019438	valid_1's l2: 0.00205055
[22]	training's l2: 0.00193795	valid_1's l2: 0.0020443
[23]	training's l2: 0.00193315	valid_1's l2: 0.00204224
[24]	training's l2: 0.00192666	valid_1's l2: 0.00203604
[25]	training's l2: 0.00192112	valid_1's l2: 0.00203337
[26]	training's l2: 0.00191548	valid_1's l2: 0.00202853
[27]	training's l2: 0.00191086	valid_1's l2: 0.00202575
[28]	training's l2: 0.00190218	valid_1's l2: 0.00202052
[29]	training's l2: 0.00189746	valid_1's l2: 0.00201412
[30]	training's l2: 0.00189268	valid_1's l2: 0.00200928
[31]	training's l2: 0.00188528	valid_1's l2: 0.0020048
[32]	training's l2: 0.0018805	valid_1's l2: 0.00200302
[33]	training's l2: 0.00187562	valid_1's l2: 0.00199947
[34]	training's l2: 0.00187201	valid_1's l2: 0.00199795
[35]	training's l2: 0.0018675	valid_1's l2: 0.00199594
[36]	training's l2: 0.001862	valid_1's l2: 0.00199265
[37]	training's l2: 0.00185814	valid_1's l2: 0.00198991
[38]	training's l2: 0.00185469	valid_1's l2: 0.00198751
[39]	training's l2: 0.0018507	valid_1's l2: 0.00198618
[40]	training's l2: 0.00184587	valid_1's l2: 0.00198228
[41]	training's l2: 0.00184178	valid_1's l2: 0.00197964
[42]	training's l2: 0.00183857	valid_1's l2: 0.00197845
[43]	training's l2: 0.001836	valid_1's l2: 0.00197724
[44]	training's l2: 0.00183301	valid_1's l2: 0.00197629
[45]	training's l2: 0.00182877	valid_1's l2: 0.00197502
[46]	training's l2: 0.00182597	valid_1's l2: 0.00197429
[47]	training's l2: 0.00182282	valid_1's l2: 0.00197162
[48]	training's l2: 0.00181947	valid_1's l2: 0.00197028
[49]	training's l2: 0.00181555	valid_1's l2: 0.00196815
[50]	training's l2: 0.00181234	valid_1's l2: 0.00196733
[51]	training's l2: 0.0018098	valid_1's l2: 0.00196622
[52]	training's l2: 0.00180714	valid_1's l2: 0.00196353
[53]	training's l2: 0.00180375	valid_1's l2: 0.00196352
[54]	training's l2: 0.00180105	valid_1's l2: 0.00196196
[55]	training's l2: 0.00179845	valid_1's l2: 0.0019609
[56]	training's l2: 0.00179568	valid_1's l2: 0.00196051
[57]	training's l2: 0.00179124	valid_1's l2: 0.00195774
[58]	training's l2: 0.00178818	valid_1's l2: 0.00195583
[59]	training's l2: 0.0017855	valid_1's l2: 0.00195459
[60]	training's l2: 0.00178321	valid_1's l2: 0.0019542
[61]	training's l2: 0.00178077	valid_1's l2: 0.00195316
[62]	training's l2: 0.00177843	valid_1's l2: 0.00195262
[63]	training's l2: 0.00177648	valid_1's l2: 0.00195212
[64]	training's l2: 0.0017741	valid_1's l2: 0.00195123
[65]	training's l2: 0.00177164	valid_1's l2: 0.001951
[66]	training's l2: 0.00176946	valid_1's l2: 0.00195058
[67]	training's l2: 0.00176706	valid_1's l2: 0.00194988
[68]	training's l2: 0.00176505	valid_1's l2: 0.00194926
[69]	training's l2: 0.00176317	valid_1's l2: 0.001948
[70]	training's l2: 0.00176114	valid_1's l2: 0.00194816
[71]	training's l2: 0.00175891	valid_1's l2: 0.00194763
[72]	training's l2: 0.00175705	valid_1's l2: 0.00194727
[73]	training's l2: 0.00175519	valid_1's l2: 0.00194621
[74]	training's l2: 0.00175223	valid_1's l2: 0.00194492
[75]	training's l2: 0.00174985	valid_1's l2: 0.00194415
[76]	training's l2: 0.0017479	valid_1's l2: 0.00194303
[77]	training's l2: 0.00174523	valid_1's l2: 0.00194178
[78]	training's l2: 0.00174334	valid_1's l2: 0.00194126
[79]	training's l2: 0.00174126	valid_1's l2: 0.0019409
[80]	training's l2: 0.00173929	valid_1's l2: 0.0019399
[81]	training's l2: 0.00173692	valid_1's l2: 0.00193858
[82]	training's l2: 0.00173466	valid_1's l2: 0.00193728
[83]	training's l2: 0.00173273	valid_1's l2: 0.00193699
[84]	training's l2: 0.00173088	valid_1's l2: 0.00193648
[85]	training's l2: 0.00172881	valid_1's l2: 0.00193603
[86]	training's l2: 0.00172698	valid_1's l2: 0.00193568
[87]	training's l2: 0.00172475	valid_1's l2: 0.0019358
[88]	training's l2: 0.00172277	valid_1's l2: 0.00193483
[89]	training's l2: 0.00172049	valid_1's l2: 0.00193437
[90]	training's l2: 0.00171885	valid_1's l2: 0.00193458
[91]	training's l2: 0.00171698	valid_1's l2: 0.00193384
[92]	training's l2: 0.00171513	valid_1's l2: 0.00193349
[93]	training's l2: 0.00171335	valid_1's l2: 0.00193324
[94]	training's l2: 0.00171185	valid_1's l2: 0.00193366
[95]	training's l2: 0.0017102	valid_1's l2: 0.00193337
[96]	training's l2: 0.00170851	valid_1's l2: 0.00193309
[97]	training's l2: 0.00170664	valid_1's l2: 0.00193255
[98]	training's l2: 0.00170489	valid_1's l2: 0.00193214
[99]	training's l2: 0.00170284	valid_1's l2: 0.00193207
[100]	training's l2: 0.00170121	valid_1's l2: 0.00193197
[101]	training's l2: 0.00169919	valid_1's l2: 0.00193109
[102]	training's l2: 0.00169744	valid_1's l2: 0.00193041
[103]	training's l2: 0.00169571	valid_1's l2: 0.00192974
[104]	training's l2: 0.0016943	valid_1's l2: 0.00192973
[105]	training's l2: 0.00169269	valid_1's l2: 0.00192925
[106]	training's l2: 0.00169119	valid_1's l2: 0.00192906
[107]	training's l2: 0.0016893	valid_1's l2: 0.00193005
[108]	training's l2: 0.00168744	valid_1's l2: 0.00192953
[109]	training's l2: 0.00168598	valid_1's l2: 0.00192939
[110]	training's l2: 0.00168435	valid_1's l2: 0.00192926
[111]	training's l2: 0.0016828	valid_1's l2: 0.00192902
[112]	training's l2: 0.00168118	valid_1's l2: 0.00192952
[113]	training's l2: 0.00167956	valid_1's l2: 0.00192923
[114]	training's l2: 0.00167806	valid_1's l2: 0.00192855
[115]	training's l2: 0.00167639	valid_1's l2: 0.00192826
[116]	training's l2: 0.00167487	valid_1's l2: 0.0019277
[117]	training's l2: 0.00167371	valid_1's l2: 0.00192799
[118]	training's l2: 0.00167234	valid_1's l2: 0.00192758
[119]	training's l2: 0.00167087	valid_1's l2: 0.00192773
[120]	training's l2: 0.0016693	valid_1's l2: 0.00192746
[121]	training's l2: 0.00166743	valid_1's l2: 0.00192666
[122]	training's l2: 0.00166595	valid_1's l2: 0.00192608
[123]	training's l2: 0.00166445	valid_1's l2: 0.00192633
[124]	training's l2: 0.00166284	valid_1's l2: 0.00192695
[125]	training's l2: 0.00166141	valid_1's l2: 0.00192685
[126]	training's l2: 0.00166026	valid_1's l2: 0.00192669
[127]	training's l2: 0.00165906	valid_1's l2: 0.00192673
[128]	training's l2: 0.00165756	valid_1's l2: 0.00192671
[129]	training's l2: 0.00165628	valid_1's l2: 0.00192627
[130]	training's l2: 0.0016546	valid_1's l2: 0.0019253
[131]	training's l2: 0.00165319	valid_1's l2: 0.00192495
[132]	training's l2: 0.00165145	valid_1's l2: 0.00192469
[133]	training's l2: 0.0016502	valid_1's l2: 0.00192456
[134]	training's l2: 0.00164882	valid_1's l2: 0.00192469
[135]	training's l2: 0.00164742	valid_1's l2: 0.00192456
[136]	training's l2: 0.00164601	valid_1's l2: 0.00192447
[137]	training's l2: 0.00164464	valid_1's l2: 0.00192545
[138]	training's l2: 0.0016428	valid_1's l2: 0.00192468
[139]	training's l2: 0.00164141	valid_1's l2: 0.00192404
[140]	training's l2: 0.00163996	valid_1's l2: 0.00192467
[141]	training's l2: 0.00163874	valid_1's l2: 0.001925
[142]	training's l2: 0.00163707	valid_1's l2: 0.00192429
[143]	training's l2: 0.00163585	valid_1's l2: 0.00192455
[144]	training's l2: 0.00163445	valid_1's l2: 0.00192434
[145]	training's l2: 0.00163301	valid_1's l2: 0.00192451
[146]	training's l2: 0.00163181	valid_1's l2: 0.00192424
[147]	training's l2: 0.00163047	valid_1's l2: 0.00192409
[148]	training's l2: 0.00162946	valid_1's l2: 0.00192409
[149]	training's l2: 0.00162808	valid_1's l2: 0.00192468
[150]	training's l2: 0.00162681	valid_1's l2: 0.00192467
[151]	training's l2: 0.00162532	valid_1's l2: 0.00192488
[152]	training's l2: 0.00162389	valid_1's l2: 0.00192471
[153]	training's l2: 0.00162267	valid_1's l2: 0.00192447
[154]	training's l2: 0.00162085	valid_1's l2: 0.00192475
[155]	training's l2: 0.0016197	valid_1's l2: 0.00192493
[156]	training's l2: 0.00161825	valid_1's l2: 0.00192434
[157]	training's l2: 0.00161659	valid_1's l2: 0.00192417
[158]	training's l2: 0.001615	valid_1's l2: 0.00192378
[159]	training's l2: 0.00161374	valid_1's l2: 0.00192367
[160]	training's l2: 0.0016123	valid_1's l2: 0.00192299
[161]	training's l2: 0.0016106	valid_1's l2: 0.00192279
[162]	training's l2: 0.00160932	valid_1's l2: 0.00192263
[163]	training's l2: 0.00160815	valid_1's l2: 0.00192288
[164]	training's l2: 0.00160675	valid_1's l2: 0.00192259
[165]	training's l2: 0.00160549	valid_1's l2: 0.00192245
[166]	training's l2: 0.00160402	valid_1's l2: 0.001923
[167]	training's l2: 0.00160255	valid_1's l2: 0.00192333
[168]	training's l2: 0.00160143	valid_1's l2: 0.00192311
[169]	training's l2: 0.00159975	valid_1's l2: 0.00192335
[170]	training's l2: 0.00159816	valid_1's l2: 0.00192375
[171]	training's l2: 0.00159723	valid_1's l2: 0.00192365
[172]	training's l2: 0.00159588	valid_1's l2: 0.00192356
[173]	training's l2: 0.0015943	valid_1's l2: 0.00192364
[174]	training's l2: 0.00159303	valid_1's l2: 0.00192379
[175]	training's l2: 0.00159185	valid_1's l2: 0.00192366
[176]	training's l2: 0.00159048	valid_1's l2: 0.00192352
[177]	training's l2: 0.00158908	valid_1's l2: 0.00192303
[178]	training's l2: 0.00158789	valid_1's l2: 0.00192334
[179]	training's l2: 0.00158678	valid_1's l2: 0.00192341
[180]	training's l2: 0.00158558	valid_1's l2: 0.00192351
[181]	training's l2: 0.00158453	valid_1's l2: 0.00192356
[182]	training's l2: 0.00158326	valid_1's l2: 0.0019235
[183]	training's l2: 0.00158218	valid_1's l2: 0.00192367
[184]	training's l2: 0.00158111	valid_1's l2: 0.00192377
[185]	training's l2: 0.00157976	valid_1's l2: 0.00192355
[186]	training's l2: 0.00157851	valid_1's l2: 0.00192367
[187]	training's l2: 0.00157727	valid_1's l2: 0.00192362
[188]	training's l2: 0.00157628	valid_1's l2: 0.00192347
Did not meet early stopping. Best iteration is:
[188]	training's l2: 0.00157628	valid_1's l2: 0.00192347
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.178473 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230283	valid_1's l2: 0.00232782
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00226396	valid_1's l2: 0.00229612
[3]	training's l2: 0.00222943	valid_1's l2: 0.0022667
[4]	training's l2: 0.00219894	valid_1's l2: 0.00224261
[5]	training's l2: 0.002174	valid_1's l2: 0.00222174
[6]	training's l2: 0.00215103	valid_1's l2: 0.00220606
[7]	training's l2: 0.00213253	valid_1's l2: 0.00218698
[8]	training's l2: 0.00211354	valid_1's l2: 0.00217368
[9]	training's l2: 0.00209815	valid_1's l2: 0.00216219
[10]	training's l2: 0.00208433	valid_1's l2: 0.00214826
[11]	training's l2: 0.00207032	valid_1's l2: 0.00213938
[12]	training's l2: 0.00205873	valid_1's l2: 0.00213472
[13]	training's l2: 0.00204822	valid_1's l2: 0.00213013
[14]	training's l2: 0.00203824	valid_1's l2: 0.00212414
[15]	training's l2: 0.00202952	valid_1's l2: 0.00211506
[16]	training's l2: 0.00201932	valid_1's l2: 0.00210928
[17]	training's l2: 0.00200906	valid_1's l2: 0.00210235
[18]	training's l2: 0.00200117	valid_1's l2: 0.00209872
[19]	training's l2: 0.00199378	valid_1's l2: 0.00209388
[20]	training's l2: 0.00198775	valid_1's l2: 0.00208582
[21]	training's l2: 0.00198225	valid_1's l2: 0.00208038
[22]	training's l2: 0.00197641	valid_1's l2: 0.00207304
[23]	training's l2: 0.00197087	valid_1's l2: 0.00206806
[24]	training's l2: 0.00196524	valid_1's l2: 0.00206097
[25]	training's l2: 0.00196072	valid_1's l2: 0.00205453
[26]	training's l2: 0.0019558	valid_1's l2: 0.00205177
[27]	training's l2: 0.00195144	valid_1's l2: 0.00204662
[28]	training's l2: 0.00194368	valid_1's l2: 0.00204173
[29]	training's l2: 0.0019395	valid_1's l2: 0.00203805
[30]	training's l2: 0.00193484	valid_1's l2: 0.00203476
[31]	training's l2: 0.00193137	valid_1's l2: 0.00203082
[32]	training's l2: 0.00192683	valid_1's l2: 0.00202694
[33]	training's l2: 0.00192155	valid_1's l2: 0.00202484
[34]	training's l2: 0.00191599	valid_1's l2: 0.00202087
[35]	training's l2: 0.00191288	valid_1's l2: 0.00201939
[36]	training's l2: 0.00190889	valid_1's l2: 0.00201506
[37]	training's l2: 0.00190533	valid_1's l2: 0.00201436
[38]	training's l2: 0.00190061	valid_1's l2: 0.00201181
[39]	training's l2: 0.0018977	valid_1's l2: 0.00200699
[40]	training's l2: 0.00189451	valid_1's l2: 0.00200453
[41]	training's l2: 0.00189038	valid_1's l2: 0.00200073
[42]	training's l2: 0.00188701	valid_1's l2: 0.00199939
[43]	training's l2: 0.00188278	valid_1's l2: 0.00199651
[44]	training's l2: 0.00187974	valid_1's l2: 0.00199361
[45]	training's l2: 0.00187698	valid_1's l2: 0.00199255
[46]	training's l2: 0.00187428	valid_1's l2: 0.00199063
[47]	training's l2: 0.00187125	valid_1's l2: 0.00198831
[48]	training's l2: 0.00186858	valid_1's l2: 0.00198746
[49]	training's l2: 0.00186618	valid_1's l2: 0.00198546
[50]	training's l2: 0.00186255	valid_1's l2: 0.00198423
[51]	training's l2: 0.00185999	valid_1's l2: 0.00198294
[52]	training's l2: 0.00185676	valid_1's l2: 0.00198128
[53]	training's l2: 0.00185417	valid_1's l2: 0.00197874
[54]	training's l2: 0.0018518	valid_1's l2: 0.00197792
[55]	training's l2: 0.00184966	valid_1's l2: 0.00197721
[56]	training's l2: 0.0018473	valid_1's l2: 0.00197609
[57]	training's l2: 0.00184527	valid_1's l2: 0.00197557
[58]	training's l2: 0.00184317	valid_1's l2: 0.00197299
[59]	training's l2: 0.00184059	valid_1's l2: 0.00197196
[60]	training's l2: 0.00183841	valid_1's l2: 0.0019705
[61]	training's l2: 0.00183646	valid_1's l2: 0.00197012
[62]	training's l2: 0.00183457	valid_1's l2: 0.0019697
[63]	training's l2: 0.00183174	valid_1's l2: 0.00196896
[64]	training's l2: 0.00182979	valid_1's l2: 0.00196818
[65]	training's l2: 0.00182792	valid_1's l2: 0.00196676
[66]	training's l2: 0.00182594	valid_1's l2: 0.00196629
[67]	training's l2: 0.00182279	valid_1's l2: 0.00196382
[68]	training's l2: 0.00182037	valid_1's l2: 0.0019626
[69]	training's l2: 0.00181834	valid_1's l2: 0.0019621
[70]	training's l2: 0.00181683	valid_1's l2: 0.00196134
[71]	training's l2: 0.00181444	valid_1's l2: 0.0019599
[72]	training's l2: 0.00181257	valid_1's l2: 0.00195876
[73]	training's l2: 0.00181049	valid_1's l2: 0.00195797
[74]	training's l2: 0.00180861	valid_1's l2: 0.00195754
[75]	training's l2: 0.00180613	valid_1's l2: 0.00195586
[76]	training's l2: 0.00180434	valid_1's l2: 0.00195551
[77]	training's l2: 0.00180262	valid_1's l2: 0.00195506
[78]	training's l2: 0.00180081	valid_1's l2: 0.00195455
[79]	training's l2: 0.00179936	valid_1's l2: 0.00195396
[80]	training's l2: 0.00179788	valid_1's l2: 0.00195417
[81]	training's l2: 0.00179612	valid_1's l2: 0.00195347
[82]	training's l2: 0.00179405	valid_1's l2: 0.00195301
[83]	training's l2: 0.00179258	valid_1's l2: 0.00195254
[84]	training's l2: 0.00179107	valid_1's l2: 0.00195193
[85]	training's l2: 0.00178882	valid_1's l2: 0.00195117
[86]	training's l2: 0.0017874	valid_1's l2: 0.00195035
[87]	training's l2: 0.00178598	valid_1's l2: 0.00194937
[88]	training's l2: 0.00178463	valid_1's l2: 0.00194831
[89]	training's l2: 0.00178312	valid_1's l2: 0.00194813
[90]	training's l2: 0.00178058	valid_1's l2: 0.00194704
[91]	training's l2: 0.00177896	valid_1's l2: 0.00194675
[92]	training's l2: 0.00177729	valid_1's l2: 0.00194637
[93]	training's l2: 0.00177604	valid_1's l2: 0.00194588
[94]	training's l2: 0.00177466	valid_1's l2: 0.00194572
[95]	training's l2: 0.00177314	valid_1's l2: 0.00194524
[96]	training's l2: 0.00177181	valid_1's l2: 0.00194532
[97]	training's l2: 0.00177055	valid_1's l2: 0.00194518
[98]	training's l2: 0.00176907	valid_1's l2: 0.00194473
[99]	training's l2: 0.00176761	valid_1's l2: 0.0019448
[100]	training's l2: 0.00176579	valid_1's l2: 0.00194416
[101]	training's l2: 0.00176445	valid_1's l2: 0.00194506
[102]	training's l2: 0.00176307	valid_1's l2: 0.00194482
[103]	training's l2: 0.00176179	valid_1's l2: 0.00194458
[104]	training's l2: 0.00176011	valid_1's l2: 0.0019447
[105]	training's l2: 0.00175877	valid_1's l2: 0.00194426
[106]	training's l2: 0.00175744	valid_1's l2: 0.00194411
[107]	training's l2: 0.00175591	valid_1's l2: 0.00194386
[108]	training's l2: 0.00175473	valid_1's l2: 0.00194345
[109]	training's l2: 0.0017535	valid_1's l2: 0.0019422
[110]	training's l2: 0.00175218	valid_1's l2: 0.00194222
[111]	training's l2: 0.00175089	valid_1's l2: 0.00194254
[112]	training's l2: 0.0017496	valid_1's l2: 0.00194249
[113]	training's l2: 0.00174815	valid_1's l2: 0.00194251
[114]	training's l2: 0.00174658	valid_1's l2: 0.00194235
[115]	training's l2: 0.00174524	valid_1's l2: 0.00194224
[116]	training's l2: 0.00174398	valid_1's l2: 0.00194213
[117]	training's l2: 0.00174273	valid_1's l2: 0.00194156
[118]	training's l2: 0.00174085	valid_1's l2: 0.00194065
[119]	training's l2: 0.00173964	valid_1's l2: 0.0019404
[120]	training's l2: 0.00173847	valid_1's l2: 0.00194085
[121]	training's l2: 0.00173693	valid_1's l2: 0.00194005
[122]	training's l2: 0.00173554	valid_1's l2: 0.00193984
[123]	training's l2: 0.00173452	valid_1's l2: 0.00194002
[124]	training's l2: 0.00173321	valid_1's l2: 0.00193977
[125]	training's l2: 0.0017321	valid_1's l2: 0.00193998
[126]	training's l2: 0.00173107	valid_1's l2: 0.00193983
[127]	training's l2: 0.00172987	valid_1's l2: 0.00193957
[128]	training's l2: 0.00172865	valid_1's l2: 0.00193954
[129]	training's l2: 0.00172744	valid_1's l2: 0.00193946
[130]	training's l2: 0.00172645	valid_1's l2: 0.00193916
[131]	training's l2: 0.00172505	valid_1's l2: 0.00193919
[132]	training's l2: 0.00172408	valid_1's l2: 0.00193954
[133]	training's l2: 0.00172276	valid_1's l2: 0.00193992
[134]	training's l2: 0.00172145	valid_1's l2: 0.0019396
[135]	training's l2: 0.00172038	valid_1's l2: 0.0019393
[136]	training's l2: 0.00171925	valid_1's l2: 0.00193943
[137]	training's l2: 0.00171824	valid_1's l2: 0.00193928
[138]	training's l2: 0.00171696	valid_1's l2: 0.0019396
[139]	training's l2: 0.00171599	valid_1's l2: 0.00193984
[140]	training's l2: 0.00171469	valid_1's l2: 0.00193954
[141]	training's l2: 0.00171329	valid_1's l2: 0.00193878
[142]	training's l2: 0.00171202	valid_1's l2: 0.00193877
[143]	training's l2: 0.00171116	valid_1's l2: 0.00193817
[144]	training's l2: 0.00170998	valid_1's l2: 0.00193799
[145]	training's l2: 0.00170878	valid_1's l2: 0.00193801
[146]	training's l2: 0.00170789	valid_1's l2: 0.00193785
[147]	training's l2: 0.00170675	valid_1's l2: 0.00193769
[148]	training's l2: 0.0017054	valid_1's l2: 0.00193713
[149]	training's l2: 0.0017043	valid_1's l2: 0.0019375
[150]	training's l2: 0.00170311	valid_1's l2: 0.00193726
[151]	training's l2: 0.00170198	valid_1's l2: 0.00193749
[152]	training's l2: 0.00170062	valid_1's l2: 0.00193695
[153]	training's l2: 0.00169967	valid_1's l2: 0.001937
[154]	training's l2: 0.00169871	valid_1's l2: 0.00193666
[155]	training's l2: 0.00169786	valid_1's l2: 0.00193661
[156]	training's l2: 0.00169689	valid_1's l2: 0.0019364
[157]	training's l2: 0.00169573	valid_1's l2: 0.00193603
[158]	training's l2: 0.00169478	valid_1's l2: 0.00193592
[159]	training's l2: 0.00169363	valid_1's l2: 0.00193583
[160]	training's l2: 0.00169258	valid_1's l2: 0.00193597
[161]	training's l2: 0.00169165	valid_1's l2: 0.00193568
[162]	training's l2: 0.00169074	valid_1's l2: 0.00193602
[163]	training's l2: 0.0016899	valid_1's l2: 0.00193596
[164]	training's l2: 0.00168864	valid_1's l2: 0.00193538
[165]	training's l2: 0.0016878	valid_1's l2: 0.0019355
[166]	training's l2: 0.00168648	valid_1's l2: 0.00193581
[167]	training's l2: 0.00168537	valid_1's l2: 0.00193571
[168]	training's l2: 0.00168451	valid_1's l2: 0.00193539
[169]	training's l2: 0.00168336	valid_1's l2: 0.00193458
[170]	training's l2: 0.00168241	valid_1's l2: 0.00193464
[171]	training's l2: 0.00168156	valid_1's l2: 0.00193473
[172]	training's l2: 0.00168023	valid_1's l2: 0.00193403
[173]	training's l2: 0.00167916	valid_1's l2: 0.0019337
[174]	training's l2: 0.00167804	valid_1's l2: 0.00193368
[175]	training's l2: 0.00167687	valid_1's l2: 0.00193331
[176]	training's l2: 0.00167574	valid_1's l2: 0.00193281
[177]	training's l2: 0.00167469	valid_1's l2: 0.00193307
[178]	training's l2: 0.00167368	valid_1's l2: 0.00193321
[179]	training's l2: 0.00167261	valid_1's l2: 0.00193342
[180]	training's l2: 0.0016714	valid_1's l2: 0.00193295
[181]	training's l2: 0.00167046	valid_1's l2: 0.00193334
[182]	training's l2: 0.00166955	valid_1's l2: 0.00193378
[183]	training's l2: 0.00166857	valid_1's l2: 0.00193394
Did not meet early stopping. Best iteration is:
[183]	training's l2: 0.00166857	valid_1's l2: 0.00193394
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183388 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230044	valid_1's l2: 0.00232466
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00225857	valid_1's l2: 0.00229057
[3]	training's l2: 0.00221987	valid_1's l2: 0.00226057
[4]	training's l2: 0.00218827	valid_1's l2: 0.00223463
[5]	training's l2: 0.00216057	valid_1's l2: 0.0022148
[6]	training's l2: 0.00213356	valid_1's l2: 0.00219412
[7]	training's l2: 0.00211156	valid_1's l2: 0.00217403
[8]	training's l2: 0.00209018	valid_1's l2: 0.00215975
[9]	training's l2: 0.00207229	valid_1's l2: 0.00214538
[10]	training's l2: 0.00205478	valid_1's l2: 0.0021327
[11]	training's l2: 0.00203991	valid_1's l2: 0.00212376
[12]	training's l2: 0.00202753	valid_1's l2: 0.00211523
[13]	training's l2: 0.00201368	valid_1's l2: 0.00210555
[14]	training's l2: 0.00200093	valid_1's l2: 0.00209855
[15]	training's l2: 0.00199105	valid_1's l2: 0.00209072
[16]	training's l2: 0.00198085	valid_1's l2: 0.00208355
[17]	training's l2: 0.00197254	valid_1's l2: 0.00207567
[18]	training's l2: 0.00196308	valid_1's l2: 0.00207104
[19]	training's l2: 0.00195307	valid_1's l2: 0.00206747
[20]	training's l2: 0.00194422	valid_1's l2: 0.0020608
[21]	training's l2: 0.0019366	valid_1's l2: 0.00205875
[22]	training's l2: 0.00192923	valid_1's l2: 0.00205398
[23]	training's l2: 0.0019227	valid_1's l2: 0.00204921
[24]	training's l2: 0.00191549	valid_1's l2: 0.00204449
[25]	training's l2: 0.00191017	valid_1's l2: 0.00204274
[26]	training's l2: 0.00190435	valid_1's l2: 0.00203875
[27]	training's l2: 0.00189774	valid_1's l2: 0.00203492
[28]	training's l2: 0.00189181	valid_1's l2: 0.00203142
[29]	training's l2: 0.00188638	valid_1's l2: 0.00202635
[30]	training's l2: 0.0018814	valid_1's l2: 0.00202399
[31]	training's l2: 0.00187691	valid_1's l2: 0.00202086
[32]	training's l2: 0.00187231	valid_1's l2: 0.00201685
[33]	training's l2: 0.0018648	valid_1's l2: 0.00201243
[34]	training's l2: 0.00185978	valid_1's l2: 0.00201043
[35]	training's l2: 0.00185456	valid_1's l2: 0.00200681
[36]	training's l2: 0.00185053	valid_1's l2: 0.00200502
[37]	training's l2: 0.0018462	valid_1's l2: 0.00200125
[38]	training's l2: 0.00183976	valid_1's l2: 0.00199754
[39]	training's l2: 0.00183552	valid_1's l2: 0.00199581
[40]	training's l2: 0.00183136	valid_1's l2: 0.00199339
[41]	training's l2: 0.00182631	valid_1's l2: 0.00199072
[42]	training's l2: 0.00182273	valid_1's l2: 0.00198752
[43]	training's l2: 0.00181839	valid_1's l2: 0.00198556
[44]	training's l2: 0.00181497	valid_1's l2: 0.00198426
[45]	training's l2: 0.00181169	valid_1's l2: 0.0019831
[46]	training's l2: 0.00180644	valid_1's l2: 0.00198057
[47]	training's l2: 0.00180358	valid_1's l2: 0.00197938
[48]	training's l2: 0.00180004	valid_1's l2: 0.00197719
[49]	training's l2: 0.00179609	valid_1's l2: 0.00197508
[50]	training's l2: 0.00179288	valid_1's l2: 0.0019737
[51]	training's l2: 0.00179005	valid_1's l2: 0.00197238
[52]	training's l2: 0.00178686	valid_1's l2: 0.00197211
[53]	training's l2: 0.00178349	valid_1's l2: 0.00197104
[54]	training's l2: 0.00178043	valid_1's l2: 0.00197033
[55]	training's l2: 0.00177722	valid_1's l2: 0.00196963
[56]	training's l2: 0.00177427	valid_1's l2: 0.00196866
[57]	training's l2: 0.00177157	valid_1's l2: 0.00196788
[58]	training's l2: 0.00176687	valid_1's l2: 0.00196487
[59]	training's l2: 0.00176321	valid_1's l2: 0.00196245
[60]	training's l2: 0.0017604	valid_1's l2: 0.00196082
[61]	training's l2: 0.00175775	valid_1's l2: 0.00195861
[62]	training's l2: 0.0017551	valid_1's l2: 0.00195635
[63]	training's l2: 0.00175232	valid_1's l2: 0.00195595
[64]	training's l2: 0.0017485	valid_1's l2: 0.00195418
[65]	training's l2: 0.00174564	valid_1's l2: 0.0019538
[66]	training's l2: 0.00174314	valid_1's l2: 0.00195356
[67]	training's l2: 0.00174026	valid_1's l2: 0.00195324
[68]	training's l2: 0.00173767	valid_1's l2: 0.00195224
[69]	training's l2: 0.00173425	valid_1's l2: 0.00195062
[70]	training's l2: 0.00173156	valid_1's l2: 0.00195029
[71]	training's l2: 0.00172838	valid_1's l2: 0.00194883
[72]	training's l2: 0.00172619	valid_1's l2: 0.0019481
[73]	training's l2: 0.001724	valid_1's l2: 0.00194747
[74]	training's l2: 0.00172154	valid_1's l2: 0.00194703
[75]	training's l2: 0.0017187	valid_1's l2: 0.0019454
[76]	training's l2: 0.00171611	valid_1's l2: 0.00194534
[77]	training's l2: 0.00171392	valid_1's l2: 0.00194505
[78]	training's l2: 0.00171134	valid_1's l2: 0.00194455
[79]	training's l2: 0.00170858	valid_1's l2: 0.00194397
[80]	training's l2: 0.00170645	valid_1's l2: 0.00194368
[81]	training's l2: 0.00170363	valid_1's l2: 0.00194335
[82]	training's l2: 0.00170116	valid_1's l2: 0.00194227
[83]	training's l2: 0.00169885	valid_1's l2: 0.0019419
[84]	training's l2: 0.00169665	valid_1's l2: 0.00194153
[85]	training's l2: 0.00169427	valid_1's l2: 0.00194031
[86]	training's l2: 0.00169219	valid_1's l2: 0.00193915
[87]	training's l2: 0.00168966	valid_1's l2: 0.00193812
[88]	training's l2: 0.00168754	valid_1's l2: 0.00193814
[89]	training's l2: 0.00168529	valid_1's l2: 0.00193841
[90]	training's l2: 0.00168276	valid_1's l2: 0.00193764
[91]	training's l2: 0.0016807	valid_1's l2: 0.00193747
[92]	training's l2: 0.00167839	valid_1's l2: 0.00193667
[93]	training's l2: 0.00167629	valid_1's l2: 0.00193609
[94]	training's l2: 0.00167415	valid_1's l2: 0.00193652
[95]	training's l2: 0.00167203	valid_1's l2: 0.00193679
[96]	training's l2: 0.00166959	valid_1's l2: 0.00193639
[97]	training's l2: 0.00166766	valid_1's l2: 0.00193612
[98]	training's l2: 0.00166539	valid_1's l2: 0.00193455
[99]	training's l2: 0.00166333	valid_1's l2: 0.00193393
[100]	training's l2: 0.00166116	valid_1's l2: 0.00193377
[101]	training's l2: 0.00165912	valid_1's l2: 0.00193372
[102]	training's l2: 0.00165732	valid_1's l2: 0.00193406
[103]	training's l2: 0.00165532	valid_1's l2: 0.00193346
[104]	training's l2: 0.00165361	valid_1's l2: 0.00193307
[105]	training's l2: 0.00165184	valid_1's l2: 0.00193295
[106]	training's l2: 0.00164998	valid_1's l2: 0.00193281
[107]	training's l2: 0.00164801	valid_1's l2: 0.00193327
[108]	training's l2: 0.00164578	valid_1's l2: 0.00193372
[109]	training's l2: 0.00164375	valid_1's l2: 0.00193313
[110]	training's l2: 0.00164159	valid_1's l2: 0.00193368
[111]	training's l2: 0.00163957	valid_1's l2: 0.00193336
[112]	training's l2: 0.00163767	valid_1's l2: 0.00193307
[113]	training's l2: 0.00163588	valid_1's l2: 0.00193307
[114]	training's l2: 0.00163391	valid_1's l2: 0.00193289
[115]	training's l2: 0.00163206	valid_1's l2: 0.00193313
[116]	training's l2: 0.00163015	valid_1's l2: 0.00193268
[117]	training's l2: 0.00162809	valid_1's l2: 0.001932
[118]	training's l2: 0.00162612	valid_1's l2: 0.00193193
[119]	training's l2: 0.00162421	valid_1's l2: 0.00193166
[120]	training's l2: 0.00162243	valid_1's l2: 0.00193189
[121]	training's l2: 0.00162076	valid_1's l2: 0.00193168
[122]	training's l2: 0.00161903	valid_1's l2: 0.00193141
[123]	training's l2: 0.001617	valid_1's l2: 0.00193113
[124]	training's l2: 0.00161533	valid_1's l2: 0.0019304
[125]	training's l2: 0.00161364	valid_1's l2: 0.00193031
[126]	training's l2: 0.00161195	valid_1's l2: 0.00193071
[127]	training's l2: 0.00160985	valid_1's l2: 0.00193094
[128]	training's l2: 0.00160822	valid_1's l2: 0.00193094
[129]	training's l2: 0.00160646	valid_1's l2: 0.00193111
[130]	training's l2: 0.0016048	valid_1's l2: 0.00193089
[131]	training's l2: 0.00160319	valid_1's l2: 0.0019306
[132]	training's l2: 0.00160142	valid_1's l2: 0.00193026
[133]	training's l2: 0.00159987	valid_1's l2: 0.00193028
[134]	training's l2: 0.00159818	valid_1's l2: 0.00193032
[135]	training's l2: 0.00159665	valid_1's l2: 0.00193042
[136]	training's l2: 0.00159499	valid_1's l2: 0.00193023
[137]	training's l2: 0.00159323	valid_1's l2: 0.00192977
[138]	training's l2: 0.00159133	valid_1's l2: 0.00192936
[139]	training's l2: 0.00158982	valid_1's l2: 0.00192925
[140]	training's l2: 0.00158782	valid_1's l2: 0.0019288
[141]	training's l2: 0.00158612	valid_1's l2: 0.00192869
[142]	training's l2: 0.00158437	valid_1's l2: 0.0019292
[143]	training's l2: 0.00158241	valid_1's l2: 0.00192909
[144]	training's l2: 0.00158084	valid_1's l2: 0.00192888
[145]	training's l2: 0.00157918	valid_1's l2: 0.00192887
[146]	training's l2: 0.00157754	valid_1's l2: 0.00192884
[147]	training's l2: 0.00157581	valid_1's l2: 0.00192916
[148]	training's l2: 0.00157405	valid_1's l2: 0.00192868
[149]	training's l2: 0.00157228	valid_1's l2: 0.00192852
[150]	training's l2: 0.00157042	valid_1's l2: 0.0019281
[151]	training's l2: 0.00156884	valid_1's l2: 0.00192835
[152]	training's l2: 0.00156713	valid_1's l2: 0.0019284
[153]	training's l2: 0.0015655	valid_1's l2: 0.00192894
[154]	training's l2: 0.00156394	valid_1's l2: 0.00192862
[155]	training's l2: 0.00156231	valid_1's l2: 0.00192849
[156]	training's l2: 0.00156078	valid_1's l2: 0.00192838
[157]	training's l2: 0.00155918	valid_1's l2: 0.00192809
[158]	training's l2: 0.00155754	valid_1's l2: 0.00192761
[159]	training's l2: 0.00155607	valid_1's l2: 0.00192732
[160]	training's l2: 0.00155431	valid_1's l2: 0.00192743
[161]	training's l2: 0.00155271	valid_1's l2: 0.00192749
[162]	training's l2: 0.00155118	valid_1's l2: 0.00192747
[163]	training's l2: 0.00154976	valid_1's l2: 0.00192745
[164]	training's l2: 0.00154811	valid_1's l2: 0.00192726
[165]	training's l2: 0.00154668	valid_1's l2: 0.00192721
[166]	training's l2: 0.00154526	valid_1's l2: 0.0019264
[167]	training's l2: 0.00154347	valid_1's l2: 0.00192637
[168]	training's l2: 0.00154219	valid_1's l2: 0.00192638
[169]	training's l2: 0.00154089	valid_1's l2: 0.00192673
[170]	training's l2: 0.00153935	valid_1's l2: 0.0019266
[171]	training's l2: 0.00153786	valid_1's l2: 0.00192635
[172]	training's l2: 0.00153627	valid_1's l2: 0.00192675
[173]	training's l2: 0.0015346	valid_1's l2: 0.00192622
[174]	training's l2: 0.00153312	valid_1's l2: 0.00192611
[175]	training's l2: 0.00153165	valid_1's l2: 0.00192617
[176]	training's l2: 0.00153003	valid_1's l2: 0.0019266
[177]	training's l2: 0.00152838	valid_1's l2: 0.00192655
[178]	training's l2: 0.00152703	valid_1's l2: 0.00192642
[179]	training's l2: 0.00152552	valid_1's l2: 0.00192613
[180]	training's l2: 0.00152423	valid_1's l2: 0.00192615
[181]	training's l2: 0.0015228	valid_1's l2: 0.00192604
[182]	training's l2: 0.00152137	valid_1's l2: 0.00192651
[183]	training's l2: 0.00151987	valid_1's l2: 0.0019265
Did not meet early stopping. Best iteration is:
[183]	training's l2: 0.00151987	valid_1's l2: 0.0019265
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189082 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230499	valid_1's l2: 0.00232887
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00226708	valid_1's l2: 0.00229821
[3]	training's l2: 0.00223211	valid_1's l2: 0.00226792
[4]	training's l2: 0.00220148	valid_1's l2: 0.00224401
[5]	training's l2: 0.00217535	valid_1's l2: 0.00222241
[6]	training's l2: 0.00215257	valid_1's l2: 0.00220693
[7]	training's l2: 0.00213037	valid_1's l2: 0.00218939
[8]	training's l2: 0.00211263	valid_1's l2: 0.00217276
[9]	training's l2: 0.00209477	valid_1's l2: 0.00216137
[10]	training's l2: 0.00207893	valid_1's l2: 0.00215018
[11]	training's l2: 0.00206553	valid_1's l2: 0.00214128
[12]	training's l2: 0.00205363	valid_1's l2: 0.00213122
[13]	training's l2: 0.0020416	valid_1's l2: 0.0021229
[14]	training's l2: 0.00203117	valid_1's l2: 0.00211337
[15]	training's l2: 0.0020208	valid_1's l2: 0.0021079
[16]	training's l2: 0.00201199	valid_1's l2: 0.00209999
[17]	training's l2: 0.00200171	valid_1's l2: 0.0020948
[18]	training's l2: 0.00199244	valid_1's l2: 0.0020885
[19]	training's l2: 0.00198432	valid_1's l2: 0.00208476
[20]	training's l2: 0.00197547	valid_1's l2: 0.00207785
[21]	training's l2: 0.0019693	valid_1's l2: 0.00207046
[22]	training's l2: 0.0019614	valid_1's l2: 0.00206482
[23]	training's l2: 0.0019546	valid_1's l2: 0.00206064
[24]	training's l2: 0.00194842	valid_1's l2: 0.00205818
[25]	training's l2: 0.00194308	valid_1's l2: 0.00205185
[26]	training's l2: 0.00193808	valid_1's l2: 0.00204828
[27]	training's l2: 0.0019328	valid_1's l2: 0.00204333
[28]	training's l2: 0.001926	valid_1's l2: 0.00203849
[29]	training's l2: 0.00192103	valid_1's l2: 0.0020342
[30]	training's l2: 0.00191595	valid_1's l2: 0.00202875
[31]	training's l2: 0.00191185	valid_1's l2: 0.00202677
[32]	training's l2: 0.00190766	valid_1's l2: 0.00202493
[33]	training's l2: 0.00190366	valid_1's l2: 0.00202022
[34]	training's l2: 0.00189798	valid_1's l2: 0.00201649
[35]	training's l2: 0.00189326	valid_1's l2: 0.00201468
[36]	training's l2: 0.00188988	valid_1's l2: 0.00201225
[37]	training's l2: 0.00188667	valid_1's l2: 0.0020087
[38]	training's l2: 0.00188122	valid_1's l2: 0.00200528
[39]	training's l2: 0.00187676	valid_1's l2: 0.00200389
[40]	training's l2: 0.00187319	valid_1's l2: 0.00200104
[41]	training's l2: 0.00186981	valid_1's l2: 0.00199892
[42]	training's l2: 0.00186509	valid_1's l2: 0.00199579
[43]	training's l2: 0.00186154	valid_1's l2: 0.00199309
[44]	training's l2: 0.00185827	valid_1's l2: 0.00198987
[45]	training's l2: 0.00185524	valid_1's l2: 0.00198883
[46]	training's l2: 0.00185079	valid_1's l2: 0.00198662
[47]	training's l2: 0.00184802	valid_1's l2: 0.00198524
[48]	training's l2: 0.0018454	valid_1's l2: 0.00198391
[49]	training's l2: 0.00184244	valid_1's l2: 0.00198261
[50]	training's l2: 0.00183848	valid_1's l2: 0.00198033
[51]	training's l2: 0.00183579	valid_1's l2: 0.00197853
[52]	training's l2: 0.00183342	valid_1's l2: 0.00197824
[53]	training's l2: 0.00183038	valid_1's l2: 0.00197716
[54]	training's l2: 0.00182755	valid_1's l2: 0.00197544
[55]	training's l2: 0.00182424	valid_1's l2: 0.00197316
[56]	training's l2: 0.00182184	valid_1's l2: 0.00197208
[57]	training's l2: 0.00181918	valid_1's l2: 0.00197068
[58]	training's l2: 0.00181631	valid_1's l2: 0.00196903
[59]	training's l2: 0.00181378	valid_1's l2: 0.00196755
[60]	training's l2: 0.00181134	valid_1's l2: 0.00196613
[61]	training's l2: 0.0018089	valid_1's l2: 0.00196466
[62]	training's l2: 0.00180602	valid_1's l2: 0.00196306
[63]	training's l2: 0.00180387	valid_1's l2: 0.00196238
[64]	training's l2: 0.00180142	valid_1's l2: 0.00196122
[65]	training's l2: 0.00179879	valid_1's l2: 0.00196007
[66]	training's l2: 0.0017963	valid_1's l2: 0.00195949
[67]	training's l2: 0.00179434	valid_1's l2: 0.00195877
[68]	training's l2: 0.00179202	valid_1's l2: 0.0019568
[69]	training's l2: 0.00179007	valid_1's l2: 0.00195592
[70]	training's l2: 0.00178791	valid_1's l2: 0.00195544
[71]	training's l2: 0.00178508	valid_1's l2: 0.00195538
[72]	training's l2: 0.00178313	valid_1's l2: 0.001955
[73]	training's l2: 0.00178108	valid_1's l2: 0.00195379
[74]	training's l2: 0.00177894	valid_1's l2: 0.00195351
[75]	training's l2: 0.00177553	valid_1's l2: 0.00195122
[76]	training's l2: 0.0017733	valid_1's l2: 0.0019503
[77]	training's l2: 0.00177147	valid_1's l2: 0.00194974
[78]	training's l2: 0.00176968	valid_1's l2: 0.00194938
[79]	training's l2: 0.00176792	valid_1's l2: 0.0019483
[80]	training's l2: 0.00176588	valid_1's l2: 0.00194766
[81]	training's l2: 0.0017639	valid_1's l2: 0.00194744
[82]	training's l2: 0.00176215	valid_1's l2: 0.00194667
[83]	training's l2: 0.00176044	valid_1's l2: 0.001946
[84]	training's l2: 0.00175875	valid_1's l2: 0.00194595
[85]	training's l2: 0.00175608	valid_1's l2: 0.00194395
[86]	training's l2: 0.00175453	valid_1's l2: 0.00194358
[87]	training's l2: 0.00175293	valid_1's l2: 0.00194299
[88]	training's l2: 0.0017509	valid_1's l2: 0.00194242
[89]	training's l2: 0.00174874	valid_1's l2: 0.00194153
[90]	training's l2: 0.00174686	valid_1's l2: 0.00194133
[91]	training's l2: 0.00174516	valid_1's l2: 0.00194106
[92]	training's l2: 0.00174364	valid_1's l2: 0.00194069
[93]	training's l2: 0.00174195	valid_1's l2: 0.00193981
[94]	training's l2: 0.00173947	valid_1's l2: 0.0019388
[95]	training's l2: 0.00173772	valid_1's l2: 0.00193883
[96]	training's l2: 0.00173619	valid_1's l2: 0.00193832
[97]	training's l2: 0.00173457	valid_1's l2: 0.00193814
[98]	training's l2: 0.00173294	valid_1's l2: 0.00193801
[99]	training's l2: 0.00173099	valid_1's l2: 0.00193665
[100]	training's l2: 0.00172929	valid_1's l2: 0.0019371
[101]	training's l2: 0.00172773	valid_1's l2: 0.00193655
[102]	training's l2: 0.00172596	valid_1's l2: 0.00193623
[103]	training's l2: 0.00172419	valid_1's l2: 0.00193595
[104]	training's l2: 0.00172262	valid_1's l2: 0.0019356
[105]	training's l2: 0.00172098	valid_1's l2: 0.0019353
[106]	training's l2: 0.00171923	valid_1's l2: 0.00193489
[107]	training's l2: 0.00171772	valid_1's l2: 0.00193474
[108]	training's l2: 0.00171619	valid_1's l2: 0.00193499
[109]	training's l2: 0.00171465	valid_1's l2: 0.00193507
[110]	training's l2: 0.00171306	valid_1's l2: 0.00193522
[111]	training's l2: 0.0017115	valid_1's l2: 0.00193485
[112]	training's l2: 0.00170947	valid_1's l2: 0.00193365
[113]	training's l2: 0.00170757	valid_1's l2: 0.00193373
[114]	training's l2: 0.00170603	valid_1's l2: 0.00193339
[115]	training's l2: 0.00170465	valid_1's l2: 0.00193268
[116]	training's l2: 0.00170299	valid_1's l2: 0.00193171
[117]	training's l2: 0.00170147	valid_1's l2: 0.00193127
[118]	training's l2: 0.00169986	valid_1's l2: 0.00193155
[119]	training's l2: 0.00169837	valid_1's l2: 0.00193098
[120]	training's l2: 0.00169693	valid_1's l2: 0.00193072
[121]	training's l2: 0.00169527	valid_1's l2: 0.0019308
[122]	training's l2: 0.00169377	valid_1's l2: 0.00193096
[123]	training's l2: 0.00169236	valid_1's l2: 0.00193059
[124]	training's l2: 0.00169091	valid_1's l2: 0.00193029
[125]	training's l2: 0.0016896	valid_1's l2: 0.00193023
[126]	training's l2: 0.00168823	valid_1's l2: 0.00193004
[127]	training's l2: 0.00168678	valid_1's l2: 0.00193006
[128]	training's l2: 0.0016853	valid_1's l2: 0.00192957
[129]	training's l2: 0.00168379	valid_1's l2: 0.00192965
[130]	training's l2: 0.00168252	valid_1's l2: 0.00192947
[131]	training's l2: 0.00168126	valid_1's l2: 0.00192942
[132]	training's l2: 0.00167987	valid_1's l2: 0.00192875
[133]	training's l2: 0.00167825	valid_1's l2: 0.00192807
[134]	training's l2: 0.00167714	valid_1's l2: 0.00192817
[135]	training's l2: 0.00167595	valid_1's l2: 0.001928
[136]	training's l2: 0.00167467	valid_1's l2: 0.00192812
[137]	training's l2: 0.00167337	valid_1's l2: 0.00192833
[138]	training's l2: 0.00167213	valid_1's l2: 0.00192782
[139]	training's l2: 0.00167093	valid_1's l2: 0.00192772
[140]	training's l2: 0.00166964	valid_1's l2: 0.00192735
[141]	training's l2: 0.00166846	valid_1's l2: 0.00192751
[142]	training's l2: 0.00166713	valid_1's l2: 0.00192775
[143]	training's l2: 0.00166587	valid_1's l2: 0.00192761
[144]	training's l2: 0.00166452	valid_1's l2: 0.00192736
[145]	training's l2: 0.00166326	valid_1's l2: 0.00192681
[146]	training's l2: 0.00166207	valid_1's l2: 0.00192631
[147]	training's l2: 0.00166084	valid_1's l2: 0.00192593
[148]	training's l2: 0.00165957	valid_1's l2: 0.00192589
[149]	training's l2: 0.00165823	valid_1's l2: 0.00192583
[150]	training's l2: 0.00165698	valid_1's l2: 0.00192558
[151]	training's l2: 0.00165583	valid_1's l2: 0.00192531
[152]	training's l2: 0.00165488	valid_1's l2: 0.00192538
[153]	training's l2: 0.0016536	valid_1's l2: 0.00192527
[154]	training's l2: 0.00165242	valid_1's l2: 0.00192502
[155]	training's l2: 0.00165129	valid_1's l2: 0.00192468
[156]	training's l2: 0.00164995	valid_1's l2: 0.00192431
[157]	training's l2: 0.00164855	valid_1's l2: 0.00192375
[158]	training's l2: 0.00164732	valid_1's l2: 0.00192375
[159]	training's l2: 0.00164606	valid_1's l2: 0.001924
[160]	training's l2: 0.0016449	valid_1's l2: 0.00192379
[161]	training's l2: 0.00164363	valid_1's l2: 0.0019238
[162]	training's l2: 0.00164223	valid_1's l2: 0.00192384
[163]	training's l2: 0.0016412	valid_1's l2: 0.00192356
[164]	training's l2: 0.00163995	valid_1's l2: 0.00192332
[165]	training's l2: 0.00163891	valid_1's l2: 0.00192357
[166]	training's l2: 0.00163784	valid_1's l2: 0.00192325
[167]	training's l2: 0.00163678	valid_1's l2: 0.00192364
[168]	training's l2: 0.00163539	valid_1's l2: 0.00192346
[169]	training's l2: 0.00163427	valid_1's l2: 0.00192392
[170]	training's l2: 0.00163321	valid_1's l2: 0.00192412
[171]	training's l2: 0.00163178	valid_1's l2: 0.0019241
[172]	training's l2: 0.00163069	valid_1's l2: 0.00192437
[173]	training's l2: 0.00162946	valid_1's l2: 0.00192434
[174]	training's l2: 0.00162816	valid_1's l2: 0.0019246
[175]	training's l2: 0.00162691	valid_1's l2: 0.00192424
[176]	training's l2: 0.00162604	valid_1's l2: 0.00192437
[177]	training's l2: 0.00162519	valid_1's l2: 0.0019244
[178]	training's l2: 0.00162395	valid_1's l2: 0.0019243
[179]	training's l2: 0.00162262	valid_1's l2: 0.00192372
[180]	training's l2: 0.00162134	valid_1's l2: 0.00192357
[181]	training's l2: 0.00162008	valid_1's l2: 0.00192363
[182]	training's l2: 0.00161883	valid_1's l2: 0.00192307
[183]	training's l2: 0.00161762	valid_1's l2: 0.00192289
[184]	training's l2: 0.00161664	valid_1's l2: 0.0019226
[185]	training's l2: 0.00161559	valid_1's l2: 0.00192242
[186]	training's l2: 0.00161451	valid_1's l2: 0.00192215
[187]	training's l2: 0.0016134	valid_1's l2: 0.00192192
[188]	training's l2: 0.00161236	valid_1's l2: 0.00192174
[189]	training's l2: 0.00161117	valid_1's l2: 0.00192182
[190]	training's l2: 0.0016102	valid_1's l2: 0.0019223
[191]	training's l2: 0.00160922	valid_1's l2: 0.00192232
[192]	training's l2: 0.00160832	valid_1's l2: 0.00192231
[193]	training's l2: 0.00160744	valid_1's l2: 0.00192254
Did not meet early stopping. Best iteration is:
[193]	training's l2: 0.00160744	valid_1's l2: 0.00192254
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186902 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230734	valid_1's l2: 0.00233179
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00227082	valid_1's l2: 0.00230108
[3]	training's l2: 0.00223724	valid_1's l2: 0.00227161
[4]	training's l2: 0.00220745	valid_1's l2: 0.00224723
[5]	training's l2: 0.00218267	valid_1's l2: 0.00222779
[6]	training's l2: 0.00216128	valid_1's l2: 0.00220874
[7]	training's l2: 0.00213952	valid_1's l2: 0.00219275
[8]	training's l2: 0.00212058	valid_1's l2: 0.00217888
[9]	training's l2: 0.00210535	valid_1's l2: 0.0021647
[10]	training's l2: 0.0020897	valid_1's l2: 0.0021533
[11]	training's l2: 0.00207705	valid_1's l2: 0.00214122
[12]	training's l2: 0.00206434	valid_1's l2: 0.00213411
[13]	training's l2: 0.0020524	valid_1's l2: 0.00212811
[14]	training's l2: 0.00204042	valid_1's l2: 0.00211821
[15]	training's l2: 0.00202893	valid_1's l2: 0.00211022
[16]	training's l2: 0.0020203	valid_1's l2: 0.00210193
[17]	training's l2: 0.0020127	valid_1's l2: 0.00209601
[18]	training's l2: 0.00200405	valid_1's l2: 0.00209088
[19]	training's l2: 0.0019971	valid_1's l2: 0.00208344
[20]	training's l2: 0.00199052	valid_1's l2: 0.00207817
[21]	training's l2: 0.00198193	valid_1's l2: 0.00207108
[22]	training's l2: 0.00197506	valid_1's l2: 0.00206453
[23]	training's l2: 0.00196768	valid_1's l2: 0.00205821
[24]	training's l2: 0.00196184	valid_1's l2: 0.00205582
[25]	training's l2: 0.0019562	valid_1's l2: 0.00205297
[26]	training's l2: 0.00195076	valid_1's l2: 0.00204919
[27]	training's l2: 0.00194602	valid_1's l2: 0.00204618
[28]	training's l2: 0.00194106	valid_1's l2: 0.00204153
[29]	training's l2: 0.00193622	valid_1's l2: 0.00203615
[30]	training's l2: 0.00193111	valid_1's l2: 0.00203244
[31]	training's l2: 0.0019271	valid_1's l2: 0.00202737
[32]	training's l2: 0.00192195	valid_1's l2: 0.00202334
[33]	training's l2: 0.00191729	valid_1's l2: 0.00201991
[34]	training's l2: 0.00191298	valid_1's l2: 0.0020182
[35]	training's l2: 0.00190896	valid_1's l2: 0.00201609
[36]	training's l2: 0.00190253	valid_1's l2: 0.00201286
[37]	training's l2: 0.00189929	valid_1's l2: 0.00200833
[38]	training's l2: 0.00189412	valid_1's l2: 0.00200535
[39]	training's l2: 0.00189022	valid_1's l2: 0.00200136
[40]	training's l2: 0.00188655	valid_1's l2: 0.00199865
[41]	training's l2: 0.00188247	valid_1's l2: 0.00199584
[42]	training's l2: 0.00187782	valid_1's l2: 0.0019922
[43]	training's l2: 0.00187441	valid_1's l2: 0.00199015
[44]	training's l2: 0.00187109	valid_1's l2: 0.00198781
[45]	training's l2: 0.00186691	valid_1's l2: 0.00198559
[46]	training's l2: 0.00186367	valid_1's l2: 0.00198332
[47]	training's l2: 0.00186069	valid_1's l2: 0.00198146
[48]	training's l2: 0.00185742	valid_1's l2: 0.00197903
[49]	training's l2: 0.00185508	valid_1's l2: 0.00197841
[50]	training's l2: 0.00185081	valid_1's l2: 0.00197743
[51]	training's l2: 0.00184807	valid_1's l2: 0.00197596
[52]	training's l2: 0.00184535	valid_1's l2: 0.00197394
[53]	training's l2: 0.00184286	valid_1's l2: 0.00197378
[54]	training's l2: 0.00183996	valid_1's l2: 0.00197183
[55]	training's l2: 0.00183762	valid_1's l2: 0.00197094
[56]	training's l2: 0.00183413	valid_1's l2: 0.0019701
[57]	training's l2: 0.00183157	valid_1's l2: 0.00196989
[58]	training's l2: 0.00182892	valid_1's l2: 0.00196881
[59]	training's l2: 0.00182637	valid_1's l2: 0.00196802
[60]	training's l2: 0.00182391	valid_1's l2: 0.00196717
[61]	training's l2: 0.00182167	valid_1's l2: 0.00196594
[62]	training's l2: 0.00181937	valid_1's l2: 0.00196526
[63]	training's l2: 0.00181673	valid_1's l2: 0.00196454
[64]	training's l2: 0.00181421	valid_1's l2: 0.00196391
[65]	training's l2: 0.00181183	valid_1's l2: 0.00196322
[66]	training's l2: 0.00180905	valid_1's l2: 0.0019629
[67]	training's l2: 0.00180675	valid_1's l2: 0.00196243
[68]	training's l2: 0.00180433	valid_1's l2: 0.00196153
[69]	training's l2: 0.00180157	valid_1's l2: 0.00195915
[70]	training's l2: 0.00179937	valid_1's l2: 0.00195859
[71]	training's l2: 0.00179657	valid_1's l2: 0.0019586
[72]	training's l2: 0.00179464	valid_1's l2: 0.00195823
[73]	training's l2: 0.00179265	valid_1's l2: 0.00195806
[74]	training's l2: 0.00179073	valid_1's l2: 0.00195742
[75]	training's l2: 0.00178873	valid_1's l2: 0.00195698
[76]	training's l2: 0.00178669	valid_1's l2: 0.00195569
[77]	training's l2: 0.00178489	valid_1's l2: 0.00195469
[78]	training's l2: 0.00178282	valid_1's l2: 0.00195414
[79]	training's l2: 0.00178114	valid_1's l2: 0.00195367
[80]	training's l2: 0.00177932	valid_1's l2: 0.00195325
[81]	training's l2: 0.00177748	valid_1's l2: 0.00195276
[82]	training's l2: 0.00177553	valid_1's l2: 0.0019525
[83]	training's l2: 0.0017724	valid_1's l2: 0.00195057
[84]	training's l2: 0.00177061	valid_1's l2: 0.00195025
[85]	training's l2: 0.00176905	valid_1's l2: 0.00194987
[86]	training's l2: 0.00176714	valid_1's l2: 0.00194999
[87]	training's l2: 0.00176555	valid_1's l2: 0.001949
[88]	training's l2: 0.00176302	valid_1's l2: 0.00194741
[89]	training's l2: 0.00176132	valid_1's l2: 0.00194627
[90]	training's l2: 0.00175976	valid_1's l2: 0.00194606
[91]	training's l2: 0.00175759	valid_1's l2: 0.00194611
[92]	training's l2: 0.00175609	valid_1's l2: 0.00194558
[93]	training's l2: 0.00175456	valid_1's l2: 0.00194556
[94]	training's l2: 0.00175262	valid_1's l2: 0.00194559
[95]	training's l2: 0.00175099	valid_1's l2: 0.00194533
[96]	training's l2: 0.00174924	valid_1's l2: 0.00194473
[97]	training's l2: 0.00174745	valid_1's l2: 0.0019446
[98]	training's l2: 0.00174586	valid_1's l2: 0.00194409
[99]	training's l2: 0.0017444	valid_1's l2: 0.00194382
[100]	training's l2: 0.00174278	valid_1's l2: 0.00194395
[101]	training's l2: 0.00174106	valid_1's l2: 0.00194341
[102]	training's l2: 0.00173914	valid_1's l2: 0.0019425
[103]	training's l2: 0.00173733	valid_1's l2: 0.00194232
[104]	training's l2: 0.00173489	valid_1's l2: 0.00194178
[105]	training's l2: 0.00173315	valid_1's l2: 0.00194109
[106]	training's l2: 0.00173145	valid_1's l2: 0.00194119
[107]	training's l2: 0.00172996	valid_1's l2: 0.00194101
[108]	training's l2: 0.00172788	valid_1's l2: 0.0019404
[109]	training's l2: 0.00172627	valid_1's l2: 0.00194031
[110]	training's l2: 0.00172459	valid_1's l2: 0.00193954
[111]	training's l2: 0.0017227	valid_1's l2: 0.00193991
[112]	training's l2: 0.00172131	valid_1's l2: 0.00194029
[113]	training's l2: 0.00171936	valid_1's l2: 0.0019398
[114]	training's l2: 0.00171759	valid_1's l2: 0.00193882
[115]	training's l2: 0.00171612	valid_1's l2: 0.0019393
[116]	training's l2: 0.0017145	valid_1's l2: 0.00193912
[117]	training's l2: 0.00171318	valid_1's l2: 0.00193912
[118]	training's l2: 0.00171192	valid_1's l2: 0.00193885
[119]	training's l2: 0.00171022	valid_1's l2: 0.00193819
[120]	training's l2: 0.00170868	valid_1's l2: 0.00193822
[121]	training's l2: 0.00170736	valid_1's l2: 0.00193774
[122]	training's l2: 0.00170588	valid_1's l2: 0.00193729
[123]	training's l2: 0.00170413	valid_1's l2: 0.00193686
[124]	training's l2: 0.0017025	valid_1's l2: 0.00193708
[125]	training's l2: 0.00170113	valid_1's l2: 0.00193664
[126]	training's l2: 0.00169983	valid_1's l2: 0.001937
[127]	training's l2: 0.0016984	valid_1's l2: 0.00193608
[128]	training's l2: 0.00169697	valid_1's l2: 0.00193544
[129]	training's l2: 0.00169558	valid_1's l2: 0.00193543
[130]	training's l2: 0.00169407	valid_1's l2: 0.00193488
[131]	training's l2: 0.00169281	valid_1's l2: 0.00193478
[132]	training's l2: 0.00169153	valid_1's l2: 0.001934
[133]	training's l2: 0.00168997	valid_1's l2: 0.00193368
[134]	training's l2: 0.00168877	valid_1's l2: 0.00193352
[135]	training's l2: 0.00168727	valid_1's l2: 0.00193305
[136]	training's l2: 0.00168587	valid_1's l2: 0.00193274
[137]	training's l2: 0.00168428	valid_1's l2: 0.00193192
[138]	training's l2: 0.00168311	valid_1's l2: 0.00193188
[139]	training's l2: 0.00168191	valid_1's l2: 0.00193229
[140]	training's l2: 0.0016804	valid_1's l2: 0.00193299
[141]	training's l2: 0.00167904	valid_1's l2: 0.00193255
[142]	training's l2: 0.00167759	valid_1's l2: 0.00193256
[143]	training's l2: 0.00167611	valid_1's l2: 0.00193189
[144]	training's l2: 0.001675	valid_1's l2: 0.0019318
[145]	training's l2: 0.00167359	valid_1's l2: 0.00193212
[146]	training's l2: 0.00167239	valid_1's l2: 0.00193217
[147]	training's l2: 0.00167113	valid_1's l2: 0.00193185
[148]	training's l2: 0.0016699	valid_1's l2: 0.00193184
[149]	training's l2: 0.00166867	valid_1's l2: 0.00193197
[150]	training's l2: 0.00166723	valid_1's l2: 0.00193149
[151]	training's l2: 0.00166583	valid_1's l2: 0.00193127
[152]	training's l2: 0.00166475	valid_1's l2: 0.0019315
[153]	training's l2: 0.00166334	valid_1's l2: 0.0019318
[154]	training's l2: 0.00166231	valid_1's l2: 0.00193192
[155]	training's l2: 0.00166137	valid_1's l2: 0.00193189
[156]	training's l2: 0.00166024	valid_1's l2: 0.00193239
[157]	training's l2: 0.00165892	valid_1's l2: 0.00193205
[158]	training's l2: 0.00165744	valid_1's l2: 0.00193169
[159]	training's l2: 0.00165612	valid_1's l2: 0.00193112
[160]	training's l2: 0.0016549	valid_1's l2: 0.00193095
[161]	training's l2: 0.00165382	valid_1's l2: 0.00193103
[162]	training's l2: 0.00165272	valid_1's l2: 0.00193102
[163]	training's l2: 0.00165143	valid_1's l2: 0.00193128
[164]	training's l2: 0.0016502	valid_1's l2: 0.00193137
[165]	training's l2: 0.00164895	valid_1's l2: 0.00193149
[166]	training's l2: 0.00164787	valid_1's l2: 0.0019313
[167]	training's l2: 0.00164673	valid_1's l2: 0.00193135
[168]	training's l2: 0.00164545	valid_1's l2: 0.0019314
[169]	training's l2: 0.00164421	valid_1's l2: 0.00193101
[170]	training's l2: 0.00164299	valid_1's l2: 0.00193071
[171]	training's l2: 0.00164161	valid_1's l2: 0.00193063
[172]	training's l2: 0.00164037	valid_1's l2: 0.00193029
[173]	training's l2: 0.00163919	valid_1's l2: 0.00193044
[174]	training's l2: 0.00163803	valid_1's l2: 0.00193044
[175]	training's l2: 0.00163705	valid_1's l2: 0.00193013
[176]	training's l2: 0.00163589	valid_1's l2: 0.00192978
[177]	training's l2: 0.0016347	valid_1's l2: 0.00192979
Did not meet early stopping. Best iteration is:
[177]	training's l2: 0.0016347	valid_1's l2: 0.00192979
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.189519 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.0023081	valid_1's l2: 0.00233254
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.002269	valid_1's l2: 0.00230067
[3]	training's l2: 0.00223375	valid_1's l2: 0.00227407
[4]	training's l2: 0.00219982	valid_1's l2: 0.00225029
[5]	training's l2: 0.00217062	valid_1's l2: 0.00222958
[6]	training's l2: 0.00214338	valid_1's l2: 0.00220931
[7]	training's l2: 0.00211766	valid_1's l2: 0.00219283
[8]	training's l2: 0.00209446	valid_1's l2: 0.00217871
[9]	training's l2: 0.00207282	valid_1's l2: 0.00216422
[10]	training's l2: 0.00205226	valid_1's l2: 0.00215022
[11]	training's l2: 0.00203236	valid_1's l2: 0.00213932
[12]	training's l2: 0.00201469	valid_1's l2: 0.00212929
[13]	training's l2: 0.00199747	valid_1's l2: 0.00212021
[14]	training's l2: 0.00198186	valid_1's l2: 0.00211167
[15]	training's l2: 0.0019663	valid_1's l2: 0.00210301
[16]	training's l2: 0.00195268	valid_1's l2: 0.00209471
[17]	training's l2: 0.00193909	valid_1's l2: 0.00208694
[18]	training's l2: 0.00192582	valid_1's l2: 0.00207945
[19]	training's l2: 0.00191402	valid_1's l2: 0.00207276
[20]	training's l2: 0.00190316	valid_1's l2: 0.00206841
[21]	training's l2: 0.00189187	valid_1's l2: 0.00206306
[22]	training's l2: 0.00188119	valid_1's l2: 0.00205867
[23]	training's l2: 0.00187186	valid_1's l2: 0.00205424
[24]	training's l2: 0.00186225	valid_1's l2: 0.00205028
[25]	training's l2: 0.00185229	valid_1's l2: 0.00204615
[26]	training's l2: 0.00184364	valid_1's l2: 0.00204338
[27]	training's l2: 0.00183534	valid_1's l2: 0.00203973
[28]	training's l2: 0.00182722	valid_1's l2: 0.00203608
[29]	training's l2: 0.00181864	valid_1's l2: 0.00203324
[30]	training's l2: 0.00181038	valid_1's l2: 0.00202938
[31]	training's l2: 0.00180299	valid_1's l2: 0.0020277
[32]	training's l2: 0.00179565	valid_1's l2: 0.00202369
[33]	training's l2: 0.00178773	valid_1's l2: 0.00202125
[34]	training's l2: 0.00178074	valid_1's l2: 0.00201823
[35]	training's l2: 0.00177263	valid_1's l2: 0.00201545
[36]	training's l2: 0.00176572	valid_1's l2: 0.00201293
[37]	training's l2: 0.00175915	valid_1's l2: 0.00201135
[38]	training's l2: 0.00175252	valid_1's l2: 0.00200813
[39]	training's l2: 0.00174586	valid_1's l2: 0.00200592
[40]	training's l2: 0.00173938	valid_1's l2: 0.00200352
[41]	training's l2: 0.00173277	valid_1's l2: 0.00200182
[42]	training's l2: 0.001727	valid_1's l2: 0.00200037
[43]	training's l2: 0.00172058	valid_1's l2: 0.00199869
[44]	training's l2: 0.00171449	valid_1's l2: 0.00199669
[45]	training's l2: 0.00170808	valid_1's l2: 0.00199504
[46]	training's l2: 0.00170287	valid_1's l2: 0.00199291
[47]	training's l2: 0.00169722	valid_1's l2: 0.00198988
[48]	training's l2: 0.00169149	valid_1's l2: 0.00198873
[49]	training's l2: 0.00168578	valid_1's l2: 0.00198676
[50]	training's l2: 0.00167917	valid_1's l2: 0.00198392
[51]	training's l2: 0.00167385	valid_1's l2: 0.00198161
[52]	training's l2: 0.00166876	valid_1's l2: 0.00198083
[53]	training's l2: 0.00166354	valid_1's l2: 0.00197792
[54]	training's l2: 0.00165767	valid_1's l2: 0.0019756
[55]	training's l2: 0.001652	valid_1's l2: 0.00197451
[56]	training's l2: 0.00164667	valid_1's l2: 0.00197325
[57]	training's l2: 0.0016417	valid_1's l2: 0.00197187
[58]	training's l2: 0.00163615	valid_1's l2: 0.0019701
[59]	training's l2: 0.00163156	valid_1's l2: 0.00196994
[60]	training's l2: 0.00162658	valid_1's l2: 0.0019687
[61]	training's l2: 0.00162189	valid_1's l2: 0.00196748
[62]	training's l2: 0.00161628	valid_1's l2: 0.00196578
[63]	training's l2: 0.00161179	valid_1's l2: 0.00196525
[64]	training's l2: 0.0016073	valid_1's l2: 0.00196446
[65]	training's l2: 0.00160248	valid_1's l2: 0.0019633
[66]	training's l2: 0.00159797	valid_1's l2: 0.00196117
[67]	training's l2: 0.00159315	valid_1's l2: 0.00196016
[68]	training's l2: 0.00158836	valid_1's l2: 0.00195885
[69]	training's l2: 0.00158364	valid_1's l2: 0.00195747
[70]	training's l2: 0.00157935	valid_1's l2: 0.00195703
[71]	training's l2: 0.00157482	valid_1's l2: 0.00195607
[72]	training's l2: 0.00157015	valid_1's l2: 0.00195509
[73]	training's l2: 0.00156536	valid_1's l2: 0.00195392
[74]	training's l2: 0.00156104	valid_1's l2: 0.00195276
[75]	training's l2: 0.00155667	valid_1's l2: 0.00195191
[76]	training's l2: 0.00155255	valid_1's l2: 0.00195174
[77]	training's l2: 0.00154826	valid_1's l2: 0.00195124
[78]	training's l2: 0.00154393	valid_1's l2: 0.00195032
[79]	training's l2: 0.0015397	valid_1's l2: 0.00195039
[80]	training's l2: 0.00153544	valid_1's l2: 0.00195059
[81]	training's l2: 0.00153129	valid_1's l2: 0.0019503
[82]	training's l2: 0.00152757	valid_1's l2: 0.0019493
[83]	training's l2: 0.00152369	valid_1's l2: 0.00194972
[84]	training's l2: 0.00151954	valid_1's l2: 0.0019485
[85]	training's l2: 0.00151559	valid_1's l2: 0.00194867
[86]	training's l2: 0.00151141	valid_1's l2: 0.00194714
[87]	training's l2: 0.00150765	valid_1's l2: 0.00194711
[88]	training's l2: 0.00150396	valid_1's l2: 0.00194599
[89]	training's l2: 0.00149997	valid_1's l2: 0.00194608
[90]	training's l2: 0.00149627	valid_1's l2: 0.00194577
[91]	training's l2: 0.00149242	valid_1's l2: 0.00194555
[92]	training's l2: 0.0014885	valid_1's l2: 0.00194523
[93]	training's l2: 0.00148469	valid_1's l2: 0.00194534
[94]	training's l2: 0.0014804	valid_1's l2: 0.00194408
[95]	training's l2: 0.00147668	valid_1's l2: 0.00194375
[96]	training's l2: 0.00147287	valid_1's l2: 0.00194431
[97]	training's l2: 0.00146921	valid_1's l2: 0.00194433
[98]	training's l2: 0.0014651	valid_1's l2: 0.00194355
[99]	training's l2: 0.00146135	valid_1's l2: 0.00194374
[100]	training's l2: 0.00145732	valid_1's l2: 0.00194286
[101]	training's l2: 0.00145394	valid_1's l2: 0.00194237
[102]	training's l2: 0.00145015	valid_1's l2: 0.00194129
[103]	training's l2: 0.00144655	valid_1's l2: 0.00194091
[104]	training's l2: 0.00144307	valid_1's l2: 0.00194035
[105]	training's l2: 0.0014393	valid_1's l2: 0.00194015
[106]	training's l2: 0.00143585	valid_1's l2: 0.00194019
[107]	training's l2: 0.00143196	valid_1's l2: 0.00193975
[108]	training's l2: 0.00142835	valid_1's l2: 0.00193941
[109]	training's l2: 0.00142457	valid_1's l2: 0.00194019
[110]	training's l2: 0.00142127	valid_1's l2: 0.00194044
[111]	training's l2: 0.0014175	valid_1's l2: 0.00193928
[112]	training's l2: 0.00141412	valid_1's l2: 0.00193876
[113]	training's l2: 0.00141061	valid_1's l2: 0.00193882
[114]	training's l2: 0.0014072	valid_1's l2: 0.00193867
[115]	training's l2: 0.00140356	valid_1's l2: 0.00193801
[116]	training's l2: 0.00140015	valid_1's l2: 0.00193758
[117]	training's l2: 0.00139673	valid_1's l2: 0.00193694
[118]	training's l2: 0.00139318	valid_1's l2: 0.00193731
[119]	training's l2: 0.00139011	valid_1's l2: 0.0019369
[120]	training's l2: 0.00138697	valid_1's l2: 0.00193675
[121]	training's l2: 0.00138378	valid_1's l2: 0.00193658
[122]	training's l2: 0.00138074	valid_1's l2: 0.00193624
[123]	training's l2: 0.00137738	valid_1's l2: 0.0019355
[124]	training's l2: 0.00137414	valid_1's l2: 0.00193512
[125]	training's l2: 0.00137061	valid_1's l2: 0.00193492
[126]	training's l2: 0.0013674	valid_1's l2: 0.00193442
[127]	training's l2: 0.00136393	valid_1's l2: 0.00193445
[128]	training's l2: 0.00136084	valid_1's l2: 0.0019341
[129]	training's l2: 0.00135752	valid_1's l2: 0.00193412
[130]	training's l2: 0.00135417	valid_1's l2: 0.00193371
[131]	training's l2: 0.00135097	valid_1's l2: 0.00193382
[132]	training's l2: 0.00134809	valid_1's l2: 0.00193344
[133]	training's l2: 0.00134519	valid_1's l2: 0.00193349
[134]	training's l2: 0.00134201	valid_1's l2: 0.00193317
[135]	training's l2: 0.00133883	valid_1's l2: 0.00193273
[136]	training's l2: 0.00133575	valid_1's l2: 0.00193246
[137]	training's l2: 0.00133257	valid_1's l2: 0.00193201
[138]	training's l2: 0.00132939	valid_1's l2: 0.00193169
[139]	training's l2: 0.0013265	valid_1's l2: 0.00193143
[140]	training's l2: 0.0013235	valid_1's l2: 0.00193108
[141]	training's l2: 0.00132058	valid_1's l2: 0.00193078
[142]	training's l2: 0.00131755	valid_1's l2: 0.00193064
[143]	training's l2: 0.00131441	valid_1's l2: 0.00193062
[144]	training's l2: 0.0013115	valid_1's l2: 0.0019308
[145]	training's l2: 0.00130845	valid_1's l2: 0.00193045
[146]	training's l2: 0.0013055	valid_1's l2: 0.00193059
[147]	training's l2: 0.00130255	valid_1's l2: 0.00193054
[148]	training's l2: 0.00129968	valid_1's l2: 0.00193039
[149]	training's l2: 0.00129685	valid_1's l2: 0.00192988
[150]	training's l2: 0.00129433	valid_1's l2: 0.0019298
[151]	training's l2: 0.00129163	valid_1's l2: 0.00192969
[152]	training's l2: 0.00128891	valid_1's l2: 0.00193008
[153]	training's l2: 0.00128628	valid_1's l2: 0.00193003
[154]	training's l2: 0.00128361	valid_1's l2: 0.0019298
[155]	training's l2: 0.00128089	valid_1's l2: 0.00192962
[156]	training's l2: 0.00127806	valid_1's l2: 0.00192968
[157]	training's l2: 0.00127539	valid_1's l2: 0.00192936
[158]	training's l2: 0.00127251	valid_1's l2: 0.00192955
[159]	training's l2: 0.00126989	valid_1's l2: 0.0019297
[160]	training's l2: 0.00126703	valid_1's l2: 0.0019292
[161]	training's l2: 0.00126434	valid_1's l2: 0.00192874
[162]	training's l2: 0.0012615	valid_1's l2: 0.00192826
[163]	training's l2: 0.00125863	valid_1's l2: 0.00192818
[164]	training's l2: 0.00125609	valid_1's l2: 0.0019277
[165]	training's l2: 0.00125367	valid_1's l2: 0.00192776
[166]	training's l2: 0.00125085	valid_1's l2: 0.0019274
[167]	training's l2: 0.00124797	valid_1's l2: 0.00192742
[168]	training's l2: 0.00124528	valid_1's l2: 0.00192693
[169]	training's l2: 0.00124287	valid_1's l2: 0.00192711
[170]	training's l2: 0.00124016	valid_1's l2: 0.00192704
[171]	training's l2: 0.0012375	valid_1's l2: 0.00192685
[172]	training's l2: 0.00123529	valid_1's l2: 0.00192724
[173]	training's l2: 0.00123278	valid_1's l2: 0.001927
[174]	training's l2: 0.00123019	valid_1's l2: 0.00192656
[175]	training's l2: 0.0012278	valid_1's l2: 0.00192613
[176]	training's l2: 0.00122533	valid_1's l2: 0.00192598
[177]	training's l2: 0.00122271	valid_1's l2: 0.00192647
[178]	training's l2: 0.00122039	valid_1's l2: 0.00192666
[179]	training's l2: 0.00121774	valid_1's l2: 0.00192705
[180]	training's l2: 0.00121531	valid_1's l2: 0.00192734
[181]	training's l2: 0.00121291	valid_1's l2: 0.00192729
[182]	training's l2: 0.0012105	valid_1's l2: 0.00192712
[183]	training's l2: 0.00120793	valid_1's l2: 0.00192727
[184]	training's l2: 0.00120577	valid_1's l2: 0.00192759
[185]	training's l2: 0.00120335	valid_1's l2: 0.00192774
Did not meet early stopping. Best iteration is:
[185]	training's l2: 0.00120335	valid_1's l2: 0.00192774
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183449 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00230698	valid_1's l2: 0.00233058
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00226921	valid_1's l2: 0.00229949
[3]	training's l2: 0.00223514	valid_1's l2: 0.00227242
[4]	training's l2: 0.00220546	valid_1's l2: 0.00224694
[5]	training's l2: 0.00217729	valid_1's l2: 0.00222471
[6]	training's l2: 0.00215415	valid_1's l2: 0.00220722
[7]	training's l2: 0.00213132	valid_1's l2: 0.00219138
[8]	training's l2: 0.00211269	valid_1's l2: 0.00217613
[9]	training's l2: 0.00209533	valid_1's l2: 0.00216397
[10]	training's l2: 0.00207754	valid_1's l2: 0.00214928
[11]	training's l2: 0.00206179	valid_1's l2: 0.00213812
[12]	training's l2: 0.00204826	valid_1's l2: 0.00212607
[13]	training's l2: 0.0020359	valid_1's l2: 0.00211658
[14]	training's l2: 0.00202387	valid_1's l2: 0.00210899
[15]	training's l2: 0.00201365	valid_1's l2: 0.00210162
[16]	training's l2: 0.00200317	valid_1's l2: 0.0020967
[17]	training's l2: 0.00199368	valid_1's l2: 0.00208969
[18]	training's l2: 0.00198582	valid_1's l2: 0.0020838
[19]	training's l2: 0.00197733	valid_1's l2: 0.00207859
[20]	training's l2: 0.00196846	valid_1's l2: 0.00207488
[21]	training's l2: 0.00196071	valid_1's l2: 0.00206943
[22]	training's l2: 0.00195193	valid_1's l2: 0.00206357
[23]	training's l2: 0.00194575	valid_1's l2: 0.00205852
[24]	training's l2: 0.00194003	valid_1's l2: 0.00205514
[25]	training's l2: 0.00193369	valid_1's l2: 0.00205319
[26]	training's l2: 0.00192814	valid_1's l2: 0.00204885
[27]	training's l2: 0.00192229	valid_1's l2: 0.0020459
[28]	training's l2: 0.00191656	valid_1's l2: 0.00204183
[29]	training's l2: 0.00191103	valid_1's l2: 0.00203969
[30]	training's l2: 0.00190646	valid_1's l2: 0.00203667
[31]	training's l2: 0.00190198	valid_1's l2: 0.00203507
[32]	training's l2: 0.00189697	valid_1's l2: 0.00203207
[33]	training's l2: 0.00189182	valid_1's l2: 0.00202922
[34]	training's l2: 0.00188715	valid_1's l2: 0.00202518
[35]	training's l2: 0.00188281	valid_1's l2: 0.00202089
[36]	training's l2: 0.00187875	valid_1's l2: 0.00201633
[37]	training's l2: 0.00187225	valid_1's l2: 0.00201205
[38]	training's l2: 0.00186831	valid_1's l2: 0.00201107
[39]	training's l2: 0.00186311	valid_1's l2: 0.00200741
[40]	training's l2: 0.00185943	valid_1's l2: 0.00200426
[41]	training's l2: 0.00185406	valid_1's l2: 0.00200124
[42]	training's l2: 0.00184999	valid_1's l2: 0.00199812
[43]	training's l2: 0.00184612	valid_1's l2: 0.00199537
[44]	training's l2: 0.00184112	valid_1's l2: 0.00199161
[45]	training's l2: 0.00183704	valid_1's l2: 0.00198929
[46]	training's l2: 0.00183315	valid_1's l2: 0.00198878
[47]	training's l2: 0.00183026	valid_1's l2: 0.00198793
[48]	training's l2: 0.00182684	valid_1's l2: 0.00198589
[49]	training's l2: 0.0018232	valid_1's l2: 0.00198465
[50]	training's l2: 0.00182006	valid_1's l2: 0.00198311
[51]	training's l2: 0.00181601	valid_1's l2: 0.00198084
[52]	training's l2: 0.00181297	valid_1's l2: 0.00197912
[53]	training's l2: 0.00181001	valid_1's l2: 0.00197723
[54]	training's l2: 0.00180714	valid_1's l2: 0.00197511
[55]	training's l2: 0.0018039	valid_1's l2: 0.00197372
[56]	training's l2: 0.00179984	valid_1's l2: 0.00197273
[57]	training's l2: 0.00179707	valid_1's l2: 0.00197143
[58]	training's l2: 0.00179421	valid_1's l2: 0.00197016
[59]	training's l2: 0.00179091	valid_1's l2: 0.00196866
[60]	training's l2: 0.00178776	valid_1's l2: 0.00196805
[61]	training's l2: 0.00178486	valid_1's l2: 0.00196596
[62]	training's l2: 0.00178212	valid_1's l2: 0.00196453
[63]	training's l2: 0.00177941	valid_1's l2: 0.00196377
[64]	training's l2: 0.00177619	valid_1's l2: 0.00196243
[65]	training's l2: 0.0017735	valid_1's l2: 0.00196224
[66]	training's l2: 0.00177086	valid_1's l2: 0.00196052
[67]	training's l2: 0.00176857	valid_1's l2: 0.0019598
[68]	training's l2: 0.00176535	valid_1's l2: 0.0019586
[69]	training's l2: 0.00176301	valid_1's l2: 0.00195768
[70]	training's l2: 0.00176065	valid_1's l2: 0.00195731
[71]	training's l2: 0.00175838	valid_1's l2: 0.00195664
[72]	training's l2: 0.00175592	valid_1's l2: 0.00195598
[73]	training's l2: 0.00175368	valid_1's l2: 0.00195521
[74]	training's l2: 0.00175134	valid_1's l2: 0.00195439
[75]	training's l2: 0.00174901	valid_1's l2: 0.00195359
[76]	training's l2: 0.00174674	valid_1's l2: 0.00195362
[77]	training's l2: 0.0017441	valid_1's l2: 0.00195205
[78]	training's l2: 0.00174125	valid_1's l2: 0.00195198
[79]	training's l2: 0.00173906	valid_1's l2: 0.00195082
[80]	training's l2: 0.00173687	valid_1's l2: 0.00195105
[81]	training's l2: 0.00173492	valid_1's l2: 0.00195034
[82]	training's l2: 0.00173241	valid_1's l2: 0.00194962
[83]	training's l2: 0.00173004	valid_1's l2: 0.00194956
[84]	training's l2: 0.00172801	valid_1's l2: 0.00194926
[85]	training's l2: 0.00172599	valid_1's l2: 0.00194891
[86]	training's l2: 0.00172282	valid_1's l2: 0.00194768
[87]	training's l2: 0.00172057	valid_1's l2: 0.00194664
[88]	training's l2: 0.00171836	valid_1's l2: 0.00194619
[89]	training's l2: 0.00171644	valid_1's l2: 0.00194482
[90]	training's l2: 0.00171447	valid_1's l2: 0.00194456
[91]	training's l2: 0.00171221	valid_1's l2: 0.00194393
[92]	training's l2: 0.00171026	valid_1's l2: 0.00194399
[93]	training's l2: 0.00170846	valid_1's l2: 0.00194351
[94]	training's l2: 0.00170621	valid_1's l2: 0.00194359
[95]	training's l2: 0.00170411	valid_1's l2: 0.00194353
[96]	training's l2: 0.00170235	valid_1's l2: 0.00194332
[97]	training's l2: 0.00169967	valid_1's l2: 0.00194264
[98]	training's l2: 0.00169785	valid_1's l2: 0.00194259
[99]	training's l2: 0.00169584	valid_1's l2: 0.00194213
[100]	training's l2: 0.00169328	valid_1's l2: 0.00194149
[101]	training's l2: 0.00169118	valid_1's l2: 0.00194109
[102]	training's l2: 0.00168925	valid_1's l2: 0.00194111
[103]	training's l2: 0.00168713	valid_1's l2: 0.00194163
[104]	training's l2: 0.00168496	valid_1's l2: 0.00194067
[105]	training's l2: 0.0016832	valid_1's l2: 0.0019403
[106]	training's l2: 0.00168117	valid_1's l2: 0.00194029
[107]	training's l2: 0.00167907	valid_1's l2: 0.00193991
[108]	training's l2: 0.00167714	valid_1's l2: 0.00193948
[109]	training's l2: 0.00167538	valid_1's l2: 0.00193895
[110]	training's l2: 0.0016735	valid_1's l2: 0.0019388
[111]	training's l2: 0.00167162	valid_1's l2: 0.0019386
[112]	training's l2: 0.00166973	valid_1's l2: 0.00193883
[113]	training's l2: 0.00166789	valid_1's l2: 0.001938
[114]	training's l2: 0.00166604	valid_1's l2: 0.00193806
[115]	training's l2: 0.00166437	valid_1's l2: 0.00193811
[116]	training's l2: 0.00166249	valid_1's l2: 0.00193739
[117]	training's l2: 0.00166035	valid_1's l2: 0.00193678
[118]	training's l2: 0.00165869	valid_1's l2: 0.00193603
[119]	training's l2: 0.00165676	valid_1's l2: 0.00193636
[120]	training's l2: 0.00165511	valid_1's l2: 0.00193606
[121]	training's l2: 0.00165339	valid_1's l2: 0.00193543
[122]	training's l2: 0.00165172	valid_1's l2: 0.00193571
[123]	training's l2: 0.00164995	valid_1's l2: 0.00193591
[124]	training's l2: 0.001648	valid_1's l2: 0.00193497
[125]	training's l2: 0.00164623	valid_1's l2: 0.00193465
[126]	training's l2: 0.00164454	valid_1's l2: 0.00193398
[127]	training's l2: 0.00164297	valid_1's l2: 0.00193369
[128]	training's l2: 0.00164126	valid_1's l2: 0.00193325
[129]	training's l2: 0.00163949	valid_1's l2: 0.00193362
[130]	training's l2: 0.00163776	valid_1's l2: 0.00193325
[131]	training's l2: 0.00163605	valid_1's l2: 0.00193298
[132]	training's l2: 0.00163439	valid_1's l2: 0.00193248
[133]	training's l2: 0.00163288	valid_1's l2: 0.00193221
[134]	training's l2: 0.00163115	valid_1's l2: 0.00193198
[135]	training's l2: 0.00162974	valid_1's l2: 0.00193179
[136]	training's l2: 0.00162788	valid_1's l2: 0.00193084
[137]	training's l2: 0.00162614	valid_1's l2: 0.00193077
[138]	training's l2: 0.0016246	valid_1's l2: 0.00193046
[139]	training's l2: 0.00162318	valid_1's l2: 0.00193029
[140]	training's l2: 0.00162156	valid_1's l2: 0.00193015
[141]	training's l2: 0.00162005	valid_1's l2: 0.00193014
[142]	training's l2: 0.00161867	valid_1's l2: 0.00193
[143]	training's l2: 0.00161709	valid_1's l2: 0.00192974
[144]	training's l2: 0.00161557	valid_1's l2: 0.00192962
[145]	training's l2: 0.00161418	valid_1's l2: 0.00192941
[146]	training's l2: 0.00161274	valid_1's l2: 0.00192894
[147]	training's l2: 0.00161097	valid_1's l2: 0.00192954
[148]	training's l2: 0.00160965	valid_1's l2: 0.00192977
[149]	training's l2: 0.00160803	valid_1's l2: 0.00192963
[150]	training's l2: 0.00160645	valid_1's l2: 0.00192928
[151]	training's l2: 0.00160488	valid_1's l2: 0.00192955
[152]	training's l2: 0.00160309	valid_1's l2: 0.00192982
[153]	training's l2: 0.00160168	valid_1's l2: 0.00192958
[154]	training's l2: 0.0016003	valid_1's l2: 0.00192934
[155]	training's l2: 0.00159887	valid_1's l2: 0.00192932
[156]	training's l2: 0.00159729	valid_1's l2: 0.00192905
[157]	training's l2: 0.00159582	valid_1's l2: 0.00192886
[158]	training's l2: 0.0015942	valid_1's l2: 0.00192866
[159]	training's l2: 0.0015928	valid_1's l2: 0.00192861
[160]	training's l2: 0.00159136	valid_1's l2: 0.00192874
[161]	training's l2: 0.00158979	valid_1's l2: 0.00192876
[162]	training's l2: 0.00158868	valid_1's l2: 0.00192901
[163]	training's l2: 0.00158709	valid_1's l2: 0.00192904
[164]	training's l2: 0.00158587	valid_1's l2: 0.00192924
[165]	training's l2: 0.00158442	valid_1's l2: 0.00192882
[166]	training's l2: 0.00158307	valid_1's l2: 0.00192853
[167]	training's l2: 0.00158162	valid_1's l2: 0.00192808
[168]	training's l2: 0.00158023	valid_1's l2: 0.00192792
[169]	training's l2: 0.00157885	valid_1's l2: 0.0019277
[170]	training's l2: 0.00157754	valid_1's l2: 0.001927
[171]	training's l2: 0.00157632	valid_1's l2: 0.00192729
[172]	training's l2: 0.00157512	valid_1's l2: 0.00192718
[173]	training's l2: 0.00157377	valid_1's l2: 0.00192721
[174]	training's l2: 0.00157243	valid_1's l2: 0.00192727
[175]	training's l2: 0.00157087	valid_1's l2: 0.00192734
[176]	training's l2: 0.00156969	valid_1's l2: 0.00192732
[177]	training's l2: 0.00156807	valid_1's l2: 0.00192748
[178]	training's l2: 0.00156658	valid_1's l2: 0.00192715
[179]	training's l2: 0.00156506	valid_1's l2: 0.00192732
[180]	training's l2: 0.00156371	valid_1's l2: 0.00192722
[181]	training's l2: 0.00156206	valid_1's l2: 0.00192748
[182]	training's l2: 0.00156057	valid_1's l2: 0.00192728
[183]	training's l2: 0.00155932	valid_1's l2: 0.0019269
[184]	training's l2: 0.00155805	valid_1's l2: 0.00192729
[185]	training's l2: 0.00155667	valid_1's l2: 0.00192744
[186]	training's l2: 0.00155539	valid_1's l2: 0.00192745
[187]	training's l2: 0.00155382	valid_1's l2: 0.00192674
[188]	training's l2: 0.00155243	valid_1's l2: 0.0019267
[189]	training's l2: 0.00155113	valid_1's l2: 0.0019265
[190]	training's l2: 0.00154967	valid_1's l2: 0.00192637
[191]	training's l2: 0.00154827	valid_1's l2: 0.00192631
[192]	training's l2: 0.00154714	valid_1's l2: 0.00192623
[193]	training's l2: 0.00154598	valid_1's l2: 0.00192633
Did not meet early stopping. Best iteration is:
[193]	training's l2: 0.00154598	valid_1's l2: 0.00192633
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183597 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.002246
[1]	training's l2: 0.00231203	valid_1's l2: 0.00233532
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00227612	valid_1's l2: 0.00230572
[3]	training's l2: 0.00224361	valid_1's l2: 0.00228106
[4]	training's l2: 0.00221213	valid_1's l2: 0.00225739
[5]	training's l2: 0.00218452	valid_1's l2: 0.00223542
[6]	training's l2: 0.00215929	valid_1's l2: 0.00221726
[7]	training's l2: 0.00213493	valid_1's l2: 0.00220116
[8]	training's l2: 0.00211295	valid_1's l2: 0.00218667
[9]	training's l2: 0.00209166	valid_1's l2: 0.00217148
[10]	training's l2: 0.00207136	valid_1's l2: 0.00215933
[11]	training's l2: 0.00205322	valid_1's l2: 0.00214792
[12]	training's l2: 0.00203626	valid_1's l2: 0.0021369
[13]	training's l2: 0.00201931	valid_1's l2: 0.00212737
[14]	training's l2: 0.00200347	valid_1's l2: 0.00211825
[15]	training's l2: 0.00198922	valid_1's l2: 0.00210997
[16]	training's l2: 0.00197505	valid_1's l2: 0.00210202
[17]	training's l2: 0.00196279	valid_1's l2: 0.00209403
[18]	training's l2: 0.00195115	valid_1's l2: 0.00208626
[19]	training's l2: 0.00193896	valid_1's l2: 0.00207936
[20]	training's l2: 0.00192688	valid_1's l2: 0.00207252
[21]	training's l2: 0.00191693	valid_1's l2: 0.00206821
[22]	training's l2: 0.0019067	valid_1's l2: 0.00206111
[23]	training's l2: 0.00189678	valid_1's l2: 0.00205662
[24]	training's l2: 0.00188816	valid_1's l2: 0.00205211
[25]	training's l2: 0.00187903	valid_1's l2: 0.00204843
[26]	training's l2: 0.00187063	valid_1's l2: 0.00204383
[27]	training's l2: 0.00186246	valid_1's l2: 0.00204131
[28]	training's l2: 0.00185412	valid_1's l2: 0.00203796
[29]	training's l2: 0.0018461	valid_1's l2: 0.00203448
[30]	training's l2: 0.00183902	valid_1's l2: 0.00203097
[31]	training's l2: 0.00183111	valid_1's l2: 0.00202706
[32]	training's l2: 0.00182374	valid_1's l2: 0.00202322
[33]	training's l2: 0.00181559	valid_1's l2: 0.00201857
[34]	training's l2: 0.00180834	valid_1's l2: 0.00201582
[35]	training's l2: 0.00180198	valid_1's l2: 0.0020127
[36]	training's l2: 0.00179558	valid_1's l2: 0.00201013
[37]	training's l2: 0.00178926	valid_1's l2: 0.00200777
[38]	training's l2: 0.00178199	valid_1's l2: 0.00200485
[39]	training's l2: 0.00177621	valid_1's l2: 0.00200318
[40]	training's l2: 0.00177056	valid_1's l2: 0.00200173
[41]	training's l2: 0.00176483	valid_1's l2: 0.00199864
[42]	training's l2: 0.00175899	valid_1's l2: 0.00199656
[43]	training's l2: 0.00175303	valid_1's l2: 0.00199481
[44]	training's l2: 0.00174738	valid_1's l2: 0.00199285
[45]	training's l2: 0.00174196	valid_1's l2: 0.00199112
[46]	training's l2: 0.00173585	valid_1's l2: 0.00198971
[47]	training's l2: 0.00173057	valid_1's l2: 0.00198855
[48]	training's l2: 0.00172513	valid_1's l2: 0.00198582
[49]	training's l2: 0.00171991	valid_1's l2: 0.00198314
[50]	training's l2: 0.00171465	valid_1's l2: 0.00198161
[51]	training's l2: 0.00170961	valid_1's l2: 0.00198
[52]	training's l2: 0.00170348	valid_1's l2: 0.00197806
[53]	training's l2: 0.00169858	valid_1's l2: 0.00197689
[54]	training's l2: 0.00169337	valid_1's l2: 0.00197575
[55]	training's l2: 0.00168818	valid_1's l2: 0.00197439
[56]	training's l2: 0.00168268	valid_1's l2: 0.00197293
[57]	training's l2: 0.00167754	valid_1's l2: 0.00197096
[58]	training's l2: 0.00167257	valid_1's l2: 0.00196858
[59]	training's l2: 0.00166717	valid_1's l2: 0.00196741
[60]	training's l2: 0.00166272	valid_1's l2: 0.00196612
[61]	training's l2: 0.00165824	valid_1's l2: 0.00196543
[62]	training's l2: 0.00165385	valid_1's l2: 0.00196406
[63]	training's l2: 0.00164914	valid_1's l2: 0.00196315
[64]	training's l2: 0.00164414	valid_1's l2: 0.00196113
[65]	training's l2: 0.00163952	valid_1's l2: 0.00196061
[66]	training's l2: 0.00163568	valid_1's l2: 0.00196005
[67]	training's l2: 0.00163101	valid_1's l2: 0.00195893
[68]	training's l2: 0.00162676	valid_1's l2: 0.00195793
[69]	training's l2: 0.00162253	valid_1's l2: 0.00195709
[70]	training's l2: 0.00161815	valid_1's l2: 0.00195745
[71]	training's l2: 0.00161417	valid_1's l2: 0.00195627
[72]	training's l2: 0.0016097	valid_1's l2: 0.00195504
[73]	training's l2: 0.00160599	valid_1's l2: 0.00195456
[74]	training's l2: 0.00160156	valid_1's l2: 0.00195386
[75]	training's l2: 0.00159746	valid_1's l2: 0.00195275
[76]	training's l2: 0.00159361	valid_1's l2: 0.00195171
[77]	training's l2: 0.00158966	valid_1's l2: 0.00195052
[78]	training's l2: 0.00158507	valid_1's l2: 0.0019494
[79]	training's l2: 0.00158107	valid_1's l2: 0.001949
[80]	training's l2: 0.00157728	valid_1's l2: 0.00194854
[81]	training's l2: 0.00157359	valid_1's l2: 0.00194773
[82]	training's l2: 0.00156919	valid_1's l2: 0.00194599
[83]	training's l2: 0.00156534	valid_1's l2: 0.00194574
[84]	training's l2: 0.00156151	valid_1's l2: 0.00194491
[85]	training's l2: 0.00155771	valid_1's l2: 0.00194401
[86]	training's l2: 0.00155386	valid_1's l2: 0.00194348
[87]	training's l2: 0.00155014	valid_1's l2: 0.00194344
[88]	training's l2: 0.00154676	valid_1's l2: 0.00194366
[89]	training's l2: 0.0015431	valid_1's l2: 0.00194299
[90]	training's l2: 0.00153944	valid_1's l2: 0.00194281
[91]	training's l2: 0.00153565	valid_1's l2: 0.00194177
[92]	training's l2: 0.00153218	valid_1's l2: 0.0019413
[93]	training's l2: 0.00152867	valid_1's l2: 0.00194083
[94]	training's l2: 0.00152533	valid_1's l2: 0.00193993
[95]	training's l2: 0.00152174	valid_1's l2: 0.00193944
[96]	training's l2: 0.00151837	valid_1's l2: 0.00193935
[97]	training's l2: 0.00151476	valid_1's l2: 0.00193862
[98]	training's l2: 0.00151148	valid_1's l2: 0.00193792
[99]	training's l2: 0.00150807	valid_1's l2: 0.00193724
[100]	training's l2: 0.00150459	valid_1's l2: 0.00193729
[101]	training's l2: 0.00150135	valid_1's l2: 0.00193715
[102]	training's l2: 0.00149777	valid_1's l2: 0.00193656
[103]	training's l2: 0.00149423	valid_1's l2: 0.0019363
[104]	training's l2: 0.00149051	valid_1's l2: 0.00193519
[105]	training's l2: 0.001487	valid_1's l2: 0.00193495
[106]	training's l2: 0.00148388	valid_1's l2: 0.00193502
[107]	training's l2: 0.00148048	valid_1's l2: 0.00193485
[108]	training's l2: 0.00147679	valid_1's l2: 0.00193421
[109]	training's l2: 0.00147332	valid_1's l2: 0.00193459
[110]	training's l2: 0.00147	valid_1's l2: 0.00193412
[111]	training's l2: 0.00146655	valid_1's l2: 0.0019339
[112]	training's l2: 0.00146359	valid_1's l2: 0.00193406
[113]	training's l2: 0.00146022	valid_1's l2: 0.00193367
[114]	training's l2: 0.00145701	valid_1's l2: 0.00193372
[115]	training's l2: 0.00145364	valid_1's l2: 0.00193342
[116]	training's l2: 0.00145053	valid_1's l2: 0.00193273
[117]	training's l2: 0.00144721	valid_1's l2: 0.0019325
[118]	training's l2: 0.00144423	valid_1's l2: 0.00193231
[119]	training's l2: 0.00144113	valid_1's l2: 0.00193213
[120]	training's l2: 0.00143815	valid_1's l2: 0.00193174
[121]	training's l2: 0.0014349	valid_1's l2: 0.00193174
[122]	training's l2: 0.00143184	valid_1's l2: 0.00193087
[123]	training's l2: 0.00142875	valid_1's l2: 0.00193023
[124]	training's l2: 0.00142563	valid_1's l2: 0.0019297
[125]	training's l2: 0.00142255	valid_1's l2: 0.00193
[126]	training's l2: 0.00141941	valid_1's l2: 0.00192938
[127]	training's l2: 0.0014164	valid_1's l2: 0.00192923
[128]	training's l2: 0.00141321	valid_1's l2: 0.00192821
[129]	training's l2: 0.00141016	valid_1's l2: 0.00192793
[130]	training's l2: 0.00140708	valid_1's l2: 0.00192759
[131]	training's l2: 0.00140436	valid_1's l2: 0.00192723
[132]	training's l2: 0.00140121	valid_1's l2: 0.0019268
[133]	training's l2: 0.00139803	valid_1's l2: 0.00192708
[134]	training's l2: 0.00139518	valid_1's l2: 0.00192674
[135]	training's l2: 0.00139231	valid_1's l2: 0.00192655
[136]	training's l2: 0.00138952	valid_1's l2: 0.00192669
[137]	training's l2: 0.00138647	valid_1's l2: 0.00192591
[138]	training's l2: 0.00138343	valid_1's l2: 0.00192613
[139]	training's l2: 0.00138057	valid_1's l2: 0.0019262
[140]	training's l2: 0.00137778	valid_1's l2: 0.00192565
[141]	training's l2: 0.00137507	valid_1's l2: 0.00192595
[142]	training's l2: 0.00137208	valid_1's l2: 0.00192579
[143]	training's l2: 0.00136938	valid_1's l2: 0.00192573
[144]	training's l2: 0.00136673	valid_1's l2: 0.00192582
[145]	training's l2: 0.00136395	valid_1's l2: 0.00192647
[146]	training's l2: 0.00136112	valid_1's l2: 0.00192618
[147]	training's l2: 0.00135822	valid_1's l2: 0.0019261
[148]	training's l2: 0.00135543	valid_1's l2: 0.00192594
[149]	training's l2: 0.0013529	valid_1's l2: 0.00192544
[150]	training's l2: 0.0013499	valid_1's l2: 0.00192528
[151]	training's l2: 0.00134713	valid_1's l2: 0.00192463
[152]	training's l2: 0.0013444	valid_1's l2: 0.00192475
[153]	training's l2: 0.00134174	valid_1's l2: 0.00192503
[154]	training's l2: 0.00133907	valid_1's l2: 0.00192449
[155]	training's l2: 0.00133647	valid_1's l2: 0.0019251
[156]	training's l2: 0.00133371	valid_1's l2: 0.0019244
[157]	training's l2: 0.00133106	valid_1's l2: 0.00192404
[158]	training's l2: 0.00132833	valid_1's l2: 0.00192408
[159]	training's l2: 0.00132595	valid_1's l2: 0.00192367
[160]	training's l2: 0.00132362	valid_1's l2: 0.00192339
[161]	training's l2: 0.00132112	valid_1's l2: 0.00192317
[162]	training's l2: 0.00131853	valid_1's l2: 0.00192287
[163]	training's l2: 0.00131592	valid_1's l2: 0.00192241
[164]	training's l2: 0.00131344	valid_1's l2: 0.00192235
[165]	training's l2: 0.00131076	valid_1's l2: 0.00192157
[166]	training's l2: 0.0013084	valid_1's l2: 0.00192176
[167]	training's l2: 0.00130617	valid_1's l2: 0.00192186
[168]	training's l2: 0.00130356	valid_1's l2: 0.0019219
[169]	training's l2: 0.00130106	valid_1's l2: 0.00192132
[170]	training's l2: 0.0012987	valid_1's l2: 0.00192104
[171]	training's l2: 0.00129625	valid_1's l2: 0.00192133
[172]	training's l2: 0.00129373	valid_1's l2: 0.00192123
[173]	training's l2: 0.00129151	valid_1's l2: 0.00192114
[174]	training's l2: 0.00128903	valid_1's l2: 0.00192129
[175]	training's l2: 0.00128657	valid_1's l2: 0.00192125
[176]	training's l2: 0.00128432	valid_1's l2: 0.00192088
[177]	training's l2: 0.00128197	valid_1's l2: 0.00192072
[178]	training's l2: 0.00127959	valid_1's l2: 0.00192093
[179]	training's l2: 0.0012774	valid_1's l2: 0.00192106
[180]	training's l2: 0.00127519	valid_1's l2: 0.00192154
[181]	training's l2: 0.00127292	valid_1's l2: 0.00192163
[182]	training's l2: 0.00127055	valid_1's l2: 0.00192163
[183]	training's l2: 0.0012683	valid_1's l2: 0.00192171
[184]	training's l2: 0.0012657	valid_1's l2: 0.00192194
[185]	training's l2: 0.0012633	valid_1's l2: 0.0019221
Did not meet early stopping. Best iteration is:
[185]	training's l2: 0.0012633	valid_1's l2: 0.0019221
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182190 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.003979	valid_1's l2: 0.00368613
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00389881	valid_1's l2: 0.00363111
[3]	training's l2: 0.00382681	valid_1's l2: 0.00358021
[4]	training's l2: 0.00376521	valid_1's l2: 0.00354174
[5]	training's l2: 0.00370731	valid_1's l2: 0.00350668
[6]	training's l2: 0.00365131	valid_1's l2: 0.00346984
[7]	training's l2: 0.00360322	valid_1's l2: 0.00343923
[8]	training's l2: 0.00355733	valid_1's l2: 0.00341242
[9]	training's l2: 0.0035146	valid_1's l2: 0.0033858
[10]	training's l2: 0.00347438	valid_1's l2: 0.00336346
[11]	training's l2: 0.00343837	valid_1's l2: 0.00334362
[12]	training's l2: 0.00340147	valid_1's l2: 0.00332604
[13]	training's l2: 0.00336967	valid_1's l2: 0.00331014
[14]	training's l2: 0.00333574	valid_1's l2: 0.003291
[15]	training's l2: 0.00330617	valid_1's l2: 0.00327729
[16]	training's l2: 0.00327933	valid_1's l2: 0.00326586
[17]	training's l2: 0.00325225	valid_1's l2: 0.00325487
[18]	training's l2: 0.00322904	valid_1's l2: 0.00324687
[19]	training's l2: 0.00320586	valid_1's l2: 0.003239
[20]	training's l2: 0.00318529	valid_1's l2: 0.00323352
[21]	training's l2: 0.00316267	valid_1's l2: 0.00322506
[22]	training's l2: 0.00314281	valid_1's l2: 0.00321701
[23]	training's l2: 0.003124	valid_1's l2: 0.0032112
[24]	training's l2: 0.00310708	valid_1's l2: 0.00320573
[25]	training's l2: 0.00308801	valid_1's l2: 0.0031993
[26]	training's l2: 0.00307157	valid_1's l2: 0.00319447
[27]	training's l2: 0.00305421	valid_1's l2: 0.00319011
[28]	training's l2: 0.00303806	valid_1's l2: 0.00318618
[29]	training's l2: 0.0030222	valid_1's l2: 0.00318291
[30]	training's l2: 0.00300721	valid_1's l2: 0.00317874
[31]	training's l2: 0.00299288	valid_1's l2: 0.00317575
[32]	training's l2: 0.00298005	valid_1's l2: 0.00317356
[33]	training's l2: 0.00296748	valid_1's l2: 0.00317166
[34]	training's l2: 0.00295425	valid_1's l2: 0.00316787
[35]	training's l2: 0.00294194	valid_1's l2: 0.00316575
[36]	training's l2: 0.00293162	valid_1's l2: 0.0031646
[37]	training's l2: 0.0029203	valid_1's l2: 0.00316265
[38]	training's l2: 0.00290921	valid_1's l2: 0.00316101
[39]	training's l2: 0.0028985	valid_1's l2: 0.00315926
[40]	training's l2: 0.00288784	valid_1's l2: 0.00315824
[41]	training's l2: 0.0028778	valid_1's l2: 0.00315755
[42]	training's l2: 0.00286566	valid_1's l2: 0.00315509
[43]	training's l2: 0.00285437	valid_1's l2: 0.00315231
[44]	training's l2: 0.0028435	valid_1's l2: 0.00314972
[45]	training's l2: 0.00283168	valid_1's l2: 0.00314677
[46]	training's l2: 0.00282025	valid_1's l2: 0.00314364
[47]	training's l2: 0.00280848	valid_1's l2: 0.00314211
[48]	training's l2: 0.00279783	valid_1's l2: 0.00313915
[49]	training's l2: 0.00278834	valid_1's l2: 0.00313905
[50]	training's l2: 0.0027786	valid_1's l2: 0.00313731
[51]	training's l2: 0.00276845	valid_1's l2: 0.00313549
[52]	training's l2: 0.00275881	valid_1's l2: 0.00313091
[53]	training's l2: 0.00275057	valid_1's l2: 0.00312901
[54]	training's l2: 0.00274093	valid_1's l2: 0.00312548
[55]	training's l2: 0.00273197	valid_1's l2: 0.003124
[56]	training's l2: 0.0027216	valid_1's l2: 0.00312081
[57]	training's l2: 0.00271298	valid_1's l2: 0.00311861
[58]	training's l2: 0.00270485	valid_1's l2: 0.00311807
[59]	training's l2: 0.00269457	valid_1's l2: 0.00311405
[60]	training's l2: 0.00268647	valid_1's l2: 0.00311176
[61]	training's l2: 0.00267771	valid_1's l2: 0.00311073
[62]	training's l2: 0.00266832	valid_1's l2: 0.00310751
[63]	training's l2: 0.00265957	valid_1's l2: 0.00310748
[64]	training's l2: 0.00265127	valid_1's l2: 0.00310776
[65]	training's l2: 0.00264151	valid_1's l2: 0.00310562
[66]	training's l2: 0.00263274	valid_1's l2: 0.00310435
[67]	training's l2: 0.00262349	valid_1's l2: 0.00310316
[68]	training's l2: 0.00261561	valid_1's l2: 0.00310181
[69]	training's l2: 0.00260755	valid_1's l2: 0.00310163
[70]	training's l2: 0.00259843	valid_1's l2: 0.00309938
[71]	training's l2: 0.00259005	valid_1's l2: 0.00309754
[72]	training's l2: 0.00258228	valid_1's l2: 0.00309747
[73]	training's l2: 0.00257436	valid_1's l2: 0.00309627
[74]	training's l2: 0.00256538	valid_1's l2: 0.00309403
[75]	training's l2: 0.00255723	valid_1's l2: 0.00309264
[76]	training's l2: 0.00254944	valid_1's l2: 0.00309097
[77]	training's l2: 0.00254194	valid_1's l2: 0.00309082
[78]	training's l2: 0.00253502	valid_1's l2: 0.00308953
[79]	training's l2: 0.0025274	valid_1's l2: 0.00308943
[80]	training's l2: 0.00252052	valid_1's l2: 0.00308894
[81]	training's l2: 0.00251286	valid_1's l2: 0.00308909
[82]	training's l2: 0.00250535	valid_1's l2: 0.00308846
[83]	training's l2: 0.00249748	valid_1's l2: 0.00308847
[84]	training's l2: 0.00249054	valid_1's l2: 0.00308776
[85]	training's l2: 0.00248345	valid_1's l2: 0.00308592
[86]	training's l2: 0.00247673	valid_1's l2: 0.00308586
[87]	training's l2: 0.00246954	valid_1's l2: 0.00308562
[88]	training's l2: 0.002463	valid_1's l2: 0.00308522
[89]	training's l2: 0.00245578	valid_1's l2: 0.00308534
[90]	training's l2: 0.00244913	valid_1's l2: 0.00308378
[91]	training's l2: 0.00244243	valid_1's l2: 0.00308272
[92]	training's l2: 0.00243543	valid_1's l2: 0.00308206
[93]	training's l2: 0.00242898	valid_1's l2: 0.00308137
[94]	training's l2: 0.00242295	valid_1's l2: 0.00308014
[95]	training's l2: 0.0024165	valid_1's l2: 0.00308044
[96]	training's l2: 0.00241005	valid_1's l2: 0.00308031
[97]	training's l2: 0.00240366	valid_1's l2: 0.0030792
[98]	training's l2: 0.00239764	valid_1's l2: 0.00307925
[99]	training's l2: 0.00239086	valid_1's l2: 0.00307899
[100]	training's l2: 0.00238445	valid_1's l2: 0.0030785
[101]	training's l2: 0.00237784	valid_1's l2: 0.00307858
[102]	training's l2: 0.00237178	valid_1's l2: 0.00307953
[103]	training's l2: 0.00236523	valid_1's l2: 0.00307964
[104]	training's l2: 0.0023588	valid_1's l2: 0.0030801
[105]	training's l2: 0.00235236	valid_1's l2: 0.00307927
[106]	training's l2: 0.00234577	valid_1's l2: 0.00307918
[107]	training's l2: 0.00233976	valid_1's l2: 0.00307921
[108]	training's l2: 0.00233401	valid_1's l2: 0.00307845
[109]	training's l2: 0.0023281	valid_1's l2: 0.00307851
[110]	training's l2: 0.00232174	valid_1's l2: 0.00307953
[111]	training's l2: 0.00231556	valid_1's l2: 0.00307931
[112]	training's l2: 0.00230993	valid_1's l2: 0.00307849
[113]	training's l2: 0.00230419	valid_1's l2: 0.00307864
[114]	training's l2: 0.00229798	valid_1's l2: 0.00307794
[115]	training's l2: 0.00229227	valid_1's l2: 0.00307704
[116]	training's l2: 0.00228658	valid_1's l2: 0.0030767
[117]	training's l2: 0.00228067	valid_1's l2: 0.00307729
[118]	training's l2: 0.00227553	valid_1's l2: 0.00307623
[119]	training's l2: 0.00226951	valid_1's l2: 0.00307561
[120]	training's l2: 0.00226375	valid_1's l2: 0.0030754
[121]	training's l2: 0.00225819	valid_1's l2: 0.00307472
[122]	training's l2: 0.00225276	valid_1's l2: 0.0030747
[123]	training's l2: 0.0022468	valid_1's l2: 0.00307307
[124]	training's l2: 0.00224113	valid_1's l2: 0.0030726
[125]	training's l2: 0.00223546	valid_1's l2: 0.00307144
[126]	training's l2: 0.00222988	valid_1's l2: 0.00307127
[127]	training's l2: 0.00222393	valid_1's l2: 0.00307136
[128]	training's l2: 0.00221781	valid_1's l2: 0.00307016
[129]	training's l2: 0.00221246	valid_1's l2: 0.00306957
[130]	training's l2: 0.00220678	valid_1's l2: 0.00306884
[131]	training's l2: 0.00220118	valid_1's l2: 0.00306863
[132]	training's l2: 0.0021954	valid_1's l2: 0.00306883
[133]	training's l2: 0.00218995	valid_1's l2: 0.00306861
[134]	training's l2: 0.00218511	valid_1's l2: 0.00306937
[135]	training's l2: 0.00217989	valid_1's l2: 0.00306929
[136]	training's l2: 0.00217419	valid_1's l2: 0.00306882
[137]	training's l2: 0.0021684	valid_1's l2: 0.00306892
[138]	training's l2: 0.00216285	valid_1's l2: 0.003069
[139]	training's l2: 0.00215777	valid_1's l2: 0.0030687
[140]	training's l2: 0.00215235	valid_1's l2: 0.00306884
[141]	training's l2: 0.00214664	valid_1's l2: 0.00306922
[142]	training's l2: 0.00214114	valid_1's l2: 0.00306891
[143]	training's l2: 0.00213625	valid_1's l2: 0.00306909
[144]	training's l2: 0.00213096	valid_1's l2: 0.00306965
[145]	training's l2: 0.00212579	valid_1's l2: 0.00306994
[146]	training's l2: 0.0021205	valid_1's l2: 0.00306951
[147]	training's l2: 0.00211557	valid_1's l2: 0.00306909
[148]	training's l2: 0.00211029	valid_1's l2: 0.00306823
[149]	training's l2: 0.00210497	valid_1's l2: 0.00306827
[150]	training's l2: 0.00210028	valid_1's l2: 0.00306836
[151]	training's l2: 0.00209523	valid_1's l2: 0.00306848
[152]	training's l2: 0.00209068	valid_1's l2: 0.00306836
[153]	training's l2: 0.00208605	valid_1's l2: 0.00306874
[154]	training's l2: 0.00208068	valid_1's l2: 0.00306825
[155]	training's l2: 0.00207565	valid_1's l2: 0.00306758
[156]	training's l2: 0.00207055	valid_1's l2: 0.00306806
[157]	training's l2: 0.00206558	valid_1's l2: 0.0030682
[158]	training's l2: 0.00206037	valid_1's l2: 0.00306874
[159]	training's l2: 0.00205574	valid_1's l2: 0.00306865
[160]	training's l2: 0.00205127	valid_1's l2: 0.00306839
[161]	training's l2: 0.00204669	valid_1's l2: 0.00306828
[162]	training's l2: 0.00204231	valid_1's l2: 0.00306824
[163]	training's l2: 0.00203785	valid_1's l2: 0.0030668
[164]	training's l2: 0.002033	valid_1's l2: 0.00306611
[165]	training's l2: 0.00202792	valid_1's l2: 0.00306611
[166]	training's l2: 0.0020231	valid_1's l2: 0.00306638
[167]	training's l2: 0.00201827	valid_1's l2: 0.00306642
[168]	training's l2: 0.00201362	valid_1's l2: 0.00306645
[169]	training's l2: 0.00200885	valid_1's l2: 0.00306765
[170]	training's l2: 0.0020041	valid_1's l2: 0.00306721
[171]	training's l2: 0.00199952	valid_1's l2: 0.00306765
[172]	training's l2: 0.00199491	valid_1's l2: 0.00306802
Did not meet early stopping. Best iteration is:
[172]	training's l2: 0.00199491	valid_1's l2: 0.00306802
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180476 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.0039546	valid_1's l2: 0.00366811
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00385759	valid_1's l2: 0.00359898
[3]	training's l2: 0.00378116	valid_1's l2: 0.00354626
[4]	training's l2: 0.00371082	valid_1's l2: 0.00350127
[5]	training's l2: 0.00364777	valid_1's l2: 0.0034603
[6]	training's l2: 0.00359167	valid_1's l2: 0.00342665
[7]	training's l2: 0.00354251	valid_1's l2: 0.00339772
[8]	training's l2: 0.00349495	valid_1's l2: 0.00337556
[9]	training's l2: 0.0034484	valid_1's l2: 0.00334699
[10]	training's l2: 0.00341301	valid_1's l2: 0.00333099
[11]	training's l2: 0.00337756	valid_1's l2: 0.00331582
[12]	training's l2: 0.00334586	valid_1's l2: 0.00329911
[13]	training's l2: 0.00331472	valid_1's l2: 0.00328392
[14]	training's l2: 0.00328838	valid_1's l2: 0.00327039
[15]	training's l2: 0.00326573	valid_1's l2: 0.00326214
[16]	training's l2: 0.00324335	valid_1's l2: 0.00325166
[17]	training's l2: 0.00321909	valid_1's l2: 0.00324175
[18]	training's l2: 0.00319853	valid_1's l2: 0.0032354
[19]	training's l2: 0.00317986	valid_1's l2: 0.00322955
[20]	training's l2: 0.0031638	valid_1's l2: 0.00322486
[21]	training's l2: 0.00314783	valid_1's l2: 0.00322142
[22]	training's l2: 0.00313005	valid_1's l2: 0.00321154
[23]	training's l2: 0.00311372	valid_1's l2: 0.00320771
[24]	training's l2: 0.00310041	valid_1's l2: 0.00320494
[25]	training's l2: 0.00308639	valid_1's l2: 0.00320041
[26]	training's l2: 0.00307239	valid_1's l2: 0.00319444
[27]	training's l2: 0.0030592	valid_1's l2: 0.00319058
[28]	training's l2: 0.00304676	valid_1's l2: 0.00318669
[29]	training's l2: 0.00303542	valid_1's l2: 0.00318364
[30]	training's l2: 0.00302272	valid_1's l2: 0.0031796
[31]	training's l2: 0.00301264	valid_1's l2: 0.0031772
[32]	training's l2: 0.00300063	valid_1's l2: 0.00317281
[33]	training's l2: 0.002989	valid_1's l2: 0.00316982
[34]	training's l2: 0.00297859	valid_1's l2: 0.00316673
[35]	training's l2: 0.00296812	valid_1's l2: 0.00316435
[36]	training's l2: 0.00295691	valid_1's l2: 0.0031566
[37]	training's l2: 0.00294818	valid_1's l2: 0.00315532
[38]	training's l2: 0.0029354	valid_1's l2: 0.00314877
[39]	training's l2: 0.00292489	valid_1's l2: 0.00314738
[40]	training's l2: 0.00291569	valid_1's l2: 0.00314462
[41]	training's l2: 0.00290688	valid_1's l2: 0.00314168
[42]	training's l2: 0.00289676	valid_1's l2: 0.00313588
[43]	training's l2: 0.0028866	valid_1's l2: 0.00313509
[44]	training's l2: 0.00287866	valid_1's l2: 0.00313447
[45]	training's l2: 0.00287064	valid_1's l2: 0.00313337
[46]	training's l2: 0.00286246	valid_1's l2: 0.00312936
[47]	training's l2: 0.00285398	valid_1's l2: 0.00312591
[48]	training's l2: 0.00284589	valid_1's l2: 0.00312421
[49]	training's l2: 0.00283563	valid_1's l2: 0.00312159
[50]	training's l2: 0.00282721	valid_1's l2: 0.0031197
[51]	training's l2: 0.00281945	valid_1's l2: 0.00311965
[52]	training's l2: 0.00281286	valid_1's l2: 0.00311828
[53]	training's l2: 0.00280618	valid_1's l2: 0.00311793
[54]	training's l2: 0.00279867	valid_1's l2: 0.00311449
[55]	training's l2: 0.00279052	valid_1's l2: 0.00311422
[56]	training's l2: 0.00278268	valid_1's l2: 0.00311199
[57]	training's l2: 0.00277623	valid_1's l2: 0.00311112
[58]	training's l2: 0.00276931	valid_1's l2: 0.00310983
[59]	training's l2: 0.00276239	valid_1's l2: 0.00310974
[60]	training's l2: 0.00275475	valid_1's l2: 0.00310891
[61]	training's l2: 0.00274851	valid_1's l2: 0.00310808
[62]	training's l2: 0.00274185	valid_1's l2: 0.00310765
[63]	training's l2: 0.00273563	valid_1's l2: 0.00310774
[64]	training's l2: 0.00272975	valid_1's l2: 0.00310821
[65]	training's l2: 0.00272382	valid_1's l2: 0.00310745
[66]	training's l2: 0.00271753	valid_1's l2: 0.00310701
[67]	training's l2: 0.00271122	valid_1's l2: 0.00310555
[68]	training's l2: 0.00270432	valid_1's l2: 0.00310398
[69]	training's l2: 0.00269846	valid_1's l2: 0.00310444
[70]	training's l2: 0.00269267	valid_1's l2: 0.00310428
[71]	training's l2: 0.002686	valid_1's l2: 0.00310117
[72]	training's l2: 0.00268003	valid_1's l2: 0.00309989
[73]	training's l2: 0.00267406	valid_1's l2: 0.00310034
[74]	training's l2: 0.00266748	valid_1's l2: 0.00309879
[75]	training's l2: 0.00266154	valid_1's l2: 0.00309965
[76]	training's l2: 0.0026552	valid_1's l2: 0.003099
[77]	training's l2: 0.00264869	valid_1's l2: 0.00309831
[78]	training's l2: 0.00264263	valid_1's l2: 0.00309676
[79]	training's l2: 0.00263735	valid_1's l2: 0.00309551
[80]	training's l2: 0.0026313	valid_1's l2: 0.00309413
[81]	training's l2: 0.00262525	valid_1's l2: 0.00309301
[82]	training's l2: 0.00261966	valid_1's l2: 0.00309336
[83]	training's l2: 0.00261368	valid_1's l2: 0.00309269
[84]	training's l2: 0.00260827	valid_1's l2: 0.00309143
[85]	training's l2: 0.00260287	valid_1's l2: 0.00309053
[86]	training's l2: 0.0025972	valid_1's l2: 0.00308998
[87]	training's l2: 0.00259174	valid_1's l2: 0.00308794
[88]	training's l2: 0.00258679	valid_1's l2: 0.00308767
[89]	training's l2: 0.00258114	valid_1's l2: 0.00308695
[90]	training's l2: 0.00257546	valid_1's l2: 0.00308566
[91]	training's l2: 0.00257003	valid_1's l2: 0.00308549
[92]	training's l2: 0.00256384	valid_1's l2: 0.00308474
[93]	training's l2: 0.00255928	valid_1's l2: 0.00308502
[94]	training's l2: 0.00255421	valid_1's l2: 0.00308517
[95]	training's l2: 0.00254913	valid_1's l2: 0.00308445
[96]	training's l2: 0.0025438	valid_1's l2: 0.00308357
[97]	training's l2: 0.0025386	valid_1's l2: 0.00308304
[98]	training's l2: 0.00253382	valid_1's l2: 0.00308155
[99]	training's l2: 0.0025286	valid_1's l2: 0.00308179
[100]	training's l2: 0.00252406	valid_1's l2: 0.00308179
[101]	training's l2: 0.00251833	valid_1's l2: 0.00308117
[102]	training's l2: 0.00251329	valid_1's l2: 0.00308193
[103]	training's l2: 0.00250864	valid_1's l2: 0.00308229
[104]	training's l2: 0.00250446	valid_1's l2: 0.0030824
[105]	training's l2: 0.00249989	valid_1's l2: 0.00308332
[106]	training's l2: 0.00249495	valid_1's l2: 0.00308189
[107]	training's l2: 0.00249031	valid_1's l2: 0.00308188
[108]	training's l2: 0.00248555	valid_1's l2: 0.00308206
[109]	training's l2: 0.00248121	valid_1's l2: 0.00308285
[110]	training's l2: 0.00247653	valid_1's l2: 0.00308212
[111]	training's l2: 0.002472	valid_1's l2: 0.00308205
[112]	training's l2: 0.00246708	valid_1's l2: 0.0030811
[113]	training's l2: 0.00246241	valid_1's l2: 0.00308152
[114]	training's l2: 0.00245778	valid_1's l2: 0.00308103
[115]	training's l2: 0.00245366	valid_1's l2: 0.00308088
[116]	training's l2: 0.00244892	valid_1's l2: 0.00308008
[117]	training's l2: 0.00244442	valid_1's l2: 0.00307983
[118]	training's l2: 0.00243953	valid_1's l2: 0.00307973
[119]	training's l2: 0.00243506	valid_1's l2: 0.00307937
[120]	training's l2: 0.00243097	valid_1's l2: 0.00307981
[121]	training's l2: 0.00242621	valid_1's l2: 0.00308012
[122]	training's l2: 0.00242238	valid_1's l2: 0.00307961
[123]	training's l2: 0.00241798	valid_1's l2: 0.00307958
[124]	training's l2: 0.00241361	valid_1's l2: 0.00308014
[125]	training's l2: 0.00240885	valid_1's l2: 0.00308018
[126]	training's l2: 0.0024051	valid_1's l2: 0.0030803
[127]	training's l2: 0.00240058	valid_1's l2: 0.00307962
[128]	training's l2: 0.00239659	valid_1's l2: 0.00307945
[129]	training's l2: 0.0023918	valid_1's l2: 0.00307915
[130]	training's l2: 0.00238719	valid_1's l2: 0.00307883
[131]	training's l2: 0.00238233	valid_1's l2: 0.00307962
[132]	training's l2: 0.00237772	valid_1's l2: 0.00307887
[133]	training's l2: 0.00237347	valid_1's l2: 0.00307889
[134]	training's l2: 0.00236921	valid_1's l2: 0.00307924
[135]	training's l2: 0.00236551	valid_1's l2: 0.00307858
[136]	training's l2: 0.00236125	valid_1's l2: 0.00307815
[137]	training's l2: 0.00235737	valid_1's l2: 0.00307867
[138]	training's l2: 0.00235351	valid_1's l2: 0.00308
[139]	training's l2: 0.00234864	valid_1's l2: 0.0030804
[140]	training's l2: 0.00234403	valid_1's l2: 0.00307912
[141]	training's l2: 0.00233938	valid_1's l2: 0.00307988
[142]	training's l2: 0.00233479	valid_1's l2: 0.00308009
[143]	training's l2: 0.00232938	valid_1's l2: 0.0030787
[144]	training's l2: 0.00232547	valid_1's l2: 0.00307906
[145]	training's l2: 0.00232147	valid_1's l2: 0.00307943
[146]	training's l2: 0.00231744	valid_1's l2: 0.00308025
[147]	training's l2: 0.00231384	valid_1's l2: 0.00308047
[148]	training's l2: 0.00230998	valid_1's l2: 0.00308067
[149]	training's l2: 0.00230548	valid_1's l2: 0.0030812
[150]	training's l2: 0.00230179	valid_1's l2: 0.00308134
[151]	training's l2: 0.00229861	valid_1's l2: 0.00308167
[152]	training's l2: 0.00229522	valid_1's l2: 0.00308186
[153]	training's l2: 0.00229176	valid_1's l2: 0.00308198
[154]	training's l2: 0.00228771	valid_1's l2: 0.00308181
[155]	training's l2: 0.00228381	valid_1's l2: 0.0030822
[156]	training's l2: 0.0022797	valid_1's l2: 0.0030822
[157]	training's l2: 0.00227551	valid_1's l2: 0.00308218
[158]	training's l2: 0.0022723	valid_1's l2: 0.0030826
[159]	training's l2: 0.00226885	valid_1's l2: 0.00308218
[160]	training's l2: 0.00226498	valid_1's l2: 0.00308293
[161]	training's l2: 0.00226091	valid_1's l2: 0.00308192
[162]	training's l2: 0.00225712	valid_1's l2: 0.00308181
[163]	training's l2: 0.0022535	valid_1's l2: 0.00308277
[164]	training's l2: 0.00225	valid_1's l2: 0.00308257
[165]	training's l2: 0.00224626	valid_1's l2: 0.0030824
[166]	training's l2: 0.00224203	valid_1's l2: 0.00308263
Early stopping, best iteration is:
[136]	training's l2: 0.00236125	valid_1's l2: 0.00307815
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.171483 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00397205	valid_1's l2: 0.00367984
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00388879	valid_1's l2: 0.00362011
[3]	training's l2: 0.00381218	valid_1's l2: 0.00356774
[4]	training's l2: 0.00374221	valid_1's l2: 0.00352189
[5]	training's l2: 0.0036835	valid_1's l2: 0.00348518
[6]	training's l2: 0.00362916	valid_1's l2: 0.0034525
[7]	training's l2: 0.00358062	valid_1's l2: 0.00342286
[8]	training's l2: 0.00353398	valid_1's l2: 0.00339537
[9]	training's l2: 0.00349197	valid_1's l2: 0.00337142
[10]	training's l2: 0.00345066	valid_1's l2: 0.00334845
[11]	training's l2: 0.0034117	valid_1's l2: 0.00332706
[12]	training's l2: 0.00337762	valid_1's l2: 0.00331178
[13]	training's l2: 0.00334588	valid_1's l2: 0.00329451
[14]	training's l2: 0.00331568	valid_1's l2: 0.0032816
[15]	training's l2: 0.00328725	valid_1's l2: 0.00326725
[16]	training's l2: 0.00325978	valid_1's l2: 0.00325381
[17]	training's l2: 0.00323566	valid_1's l2: 0.00324325
[18]	training's l2: 0.00321084	valid_1's l2: 0.00323047
[19]	training's l2: 0.00318913	valid_1's l2: 0.00322233
[20]	training's l2: 0.00316804	valid_1's l2: 0.00321304
[21]	training's l2: 0.00314702	valid_1's l2: 0.00320541
[22]	training's l2: 0.00312755	valid_1's l2: 0.00319665
[23]	training's l2: 0.00310935	valid_1's l2: 0.00319191
[24]	training's l2: 0.00309144	valid_1's l2: 0.00318467
[25]	training's l2: 0.00307449	valid_1's l2: 0.00317804
[26]	training's l2: 0.00305733	valid_1's l2: 0.0031731
[27]	training's l2: 0.00304099	valid_1's l2: 0.00316694
[28]	training's l2: 0.00302628	valid_1's l2: 0.00316362
[29]	training's l2: 0.00301275	valid_1's l2: 0.00316026
[30]	training's l2: 0.00299908	valid_1's l2: 0.00315691
[31]	training's l2: 0.00298558	valid_1's l2: 0.00315449
[32]	training's l2: 0.0029727	valid_1's l2: 0.00315291
[33]	training's l2: 0.00296106	valid_1's l2: 0.00315111
[34]	training's l2: 0.00294959	valid_1's l2: 0.00315002
[35]	training's l2: 0.002938	valid_1's l2: 0.00314777
[36]	training's l2: 0.00292608	valid_1's l2: 0.00314604
[37]	training's l2: 0.00291555	valid_1's l2: 0.0031453
[38]	training's l2: 0.00290432	valid_1's l2: 0.00314248
[39]	training's l2: 0.00289393	valid_1's l2: 0.00313972
[40]	training's l2: 0.0028816	valid_1's l2: 0.0031368
[41]	training's l2: 0.00287092	valid_1's l2: 0.00313568
[42]	training's l2: 0.00286031	valid_1's l2: 0.00313202
[43]	training's l2: 0.00284915	valid_1's l2: 0.00312904
[44]	training's l2: 0.00283939	valid_1's l2: 0.00312745
[45]	training's l2: 0.00282942	valid_1's l2: 0.00312616
[46]	training's l2: 0.00281958	valid_1's l2: 0.00312518
[47]	training's l2: 0.00280877	valid_1's l2: 0.00312126
[48]	training's l2: 0.00279788	valid_1's l2: 0.00311689
[49]	training's l2: 0.00278873	valid_1's l2: 0.00311614
[50]	training's l2: 0.00277927	valid_1's l2: 0.00311529
[51]	training's l2: 0.00276948	valid_1's l2: 0.00311384
[52]	training's l2: 0.00276002	valid_1's l2: 0.00311231
[53]	training's l2: 0.002751	valid_1's l2: 0.00311129
[54]	training's l2: 0.00274009	valid_1's l2: 0.0031097
[55]	training's l2: 0.00273076	valid_1's l2: 0.0031091
[56]	training's l2: 0.00272071	valid_1's l2: 0.00310638
[57]	training's l2: 0.0027106	valid_1's l2: 0.0031029
[58]	training's l2: 0.00270242	valid_1's l2: 0.00310166
[59]	training's l2: 0.00269438	valid_1's l2: 0.00310106
[60]	training's l2: 0.00268492	valid_1's l2: 0.00309861
[61]	training's l2: 0.00267585	valid_1's l2: 0.00309658
[62]	training's l2: 0.00266791	valid_1's l2: 0.00309552
[63]	training's l2: 0.00265994	valid_1's l2: 0.00309416
[64]	training's l2: 0.00265174	valid_1's l2: 0.00309283
[65]	training's l2: 0.00264388	valid_1's l2: 0.0030916
[66]	training's l2: 0.00263654	valid_1's l2: 0.00309016
[67]	training's l2: 0.00262841	valid_1's l2: 0.00309011
[68]	training's l2: 0.00262039	valid_1's l2: 0.00308859
[69]	training's l2: 0.00261271	valid_1's l2: 0.00308823
[70]	training's l2: 0.00260548	valid_1's l2: 0.00308727
[71]	training's l2: 0.00259824	valid_1's l2: 0.00308764
[72]	training's l2: 0.00259061	valid_1's l2: 0.00308803
[73]	training's l2: 0.00258319	valid_1's l2: 0.00308759
[74]	training's l2: 0.00257609	valid_1's l2: 0.00308816
[75]	training's l2: 0.00256929	valid_1's l2: 0.00308817
[76]	training's l2: 0.00256149	valid_1's l2: 0.00308585
[77]	training's l2: 0.00255492	valid_1's l2: 0.00308484
[78]	training's l2: 0.00254775	valid_1's l2: 0.00308402
[79]	training's l2: 0.00254093	valid_1's l2: 0.00308401
[80]	training's l2: 0.00253447	valid_1's l2: 0.00308344
[81]	training's l2: 0.00252705	valid_1's l2: 0.00308212
[82]	training's l2: 0.00251982	valid_1's l2: 0.0030811
[83]	training's l2: 0.00251272	valid_1's l2: 0.00308129
[84]	training's l2: 0.00250486	valid_1's l2: 0.00307951
[85]	training's l2: 0.00249852	valid_1's l2: 0.00307918
[86]	training's l2: 0.0024919	valid_1's l2: 0.00307889
[87]	training's l2: 0.0024852	valid_1's l2: 0.00307878
[88]	training's l2: 0.00247842	valid_1's l2: 0.00307741
[89]	training's l2: 0.00247193	valid_1's l2: 0.00307815
[90]	training's l2: 0.0024659	valid_1's l2: 0.0030783
[91]	training's l2: 0.00245974	valid_1's l2: 0.00307735
[92]	training's l2: 0.00245352	valid_1's l2: 0.0030778
[93]	training's l2: 0.0024467	valid_1's l2: 0.00307659
[94]	training's l2: 0.00243955	valid_1's l2: 0.00307571
[95]	training's l2: 0.00243303	valid_1's l2: 0.00307611
[96]	training's l2: 0.00242649	valid_1's l2: 0.00307573
[97]	training's l2: 0.00242069	valid_1's l2: 0.00307562
[98]	training's l2: 0.00241497	valid_1's l2: 0.00307569
[99]	training's l2: 0.00240969	valid_1's l2: 0.00307501
[100]	training's l2: 0.00240342	valid_1's l2: 0.00307441
[101]	training's l2: 0.00239655	valid_1's l2: 0.00307214
[102]	training's l2: 0.00239057	valid_1's l2: 0.00307134
[103]	training's l2: 0.00238483	valid_1's l2: 0.0030725
[104]	training's l2: 0.00237886	valid_1's l2: 0.00307129
[105]	training's l2: 0.00237363	valid_1's l2: 0.00307164
[106]	training's l2: 0.00236818	valid_1's l2: 0.00307151
[107]	training's l2: 0.00236245	valid_1's l2: 0.00307123
[108]	training's l2: 0.00235638	valid_1's l2: 0.003071
[109]	training's l2: 0.00235067	valid_1's l2: 0.00306966
[110]	training's l2: 0.00234576	valid_1's l2: 0.00307029
[111]	training's l2: 0.0023402	valid_1's l2: 0.00307039
[112]	training's l2: 0.00233422	valid_1's l2: 0.00307032
[113]	training's l2: 0.00232927	valid_1's l2: 0.00306999
[114]	training's l2: 0.00232319	valid_1's l2: 0.0030688
[115]	training's l2: 0.00231776	valid_1's l2: 0.00306782
[116]	training's l2: 0.0023117	valid_1's l2: 0.00306835
[117]	training's l2: 0.0023067	valid_1's l2: 0.00306921
[118]	training's l2: 0.00230188	valid_1's l2: 0.00306897
[119]	training's l2: 0.00229669	valid_1's l2: 0.00306857
[120]	training's l2: 0.00229106	valid_1's l2: 0.00306869
[121]	training's l2: 0.00228525	valid_1's l2: 0.00306671
[122]	training's l2: 0.00227957	valid_1's l2: 0.00306563
[123]	training's l2: 0.00227444	valid_1's l2: 0.00306593
[124]	training's l2: 0.00226875	valid_1's l2: 0.0030652
[125]	training's l2: 0.00226304	valid_1's l2: 0.00306366
[126]	training's l2: 0.00225778	valid_1's l2: 0.00306374
[127]	training's l2: 0.00225261	valid_1's l2: 0.00306306
[128]	training's l2: 0.0022471	valid_1's l2: 0.0030627
[129]	training's l2: 0.00224168	valid_1's l2: 0.00306223
[130]	training's l2: 0.00223675	valid_1's l2: 0.00306306
[131]	training's l2: 0.00223072	valid_1's l2: 0.00306337
[132]	training's l2: 0.00222594	valid_1's l2: 0.00306346
[133]	training's l2: 0.00222077	valid_1's l2: 0.00306407
[134]	training's l2: 0.00221579	valid_1's l2: 0.00306448
[135]	training's l2: 0.00221032	valid_1's l2: 0.00306434
[136]	training's l2: 0.00220451	valid_1's l2: 0.00306453
[137]	training's l2: 0.00219978	valid_1's l2: 0.0030641
[138]	training's l2: 0.00219542	valid_1's l2: 0.00306413
[139]	training's l2: 0.00219074	valid_1's l2: 0.00306442
[140]	training's l2: 0.00218551	valid_1's l2: 0.0030638
[141]	training's l2: 0.00218076	valid_1's l2: 0.00306412
[142]	training's l2: 0.00217558	valid_1's l2: 0.0030638
[143]	training's l2: 0.00217032	valid_1's l2: 0.00306319
[144]	training's l2: 0.0021654	valid_1's l2: 0.00306322
[145]	training's l2: 0.00216043	valid_1's l2: 0.00306281
[146]	training's l2: 0.00215649	valid_1's l2: 0.00306246
[147]	training's l2: 0.00215131	valid_1's l2: 0.0030614
[148]	training's l2: 0.00214723	valid_1's l2: 0.00306127
[149]	training's l2: 0.00214241	valid_1's l2: 0.00306085
[150]	training's l2: 0.00213819	valid_1's l2: 0.00306053
[151]	training's l2: 0.00213412	valid_1's l2: 0.00306086
[152]	training's l2: 0.00213025	valid_1's l2: 0.00306139
Did not meet early stopping. Best iteration is:
[152]	training's l2: 0.00213025	valid_1's l2: 0.00306139
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183847 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00397316	valid_1's l2: 0.00368328
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00389107	valid_1's l2: 0.0036266
[3]	training's l2: 0.00381465	valid_1's l2: 0.0035761
[4]	training's l2: 0.00374561	valid_1's l2: 0.00353032
[5]	training's l2: 0.00368771	valid_1's l2: 0.00349453
[6]	training's l2: 0.00363354	valid_1's l2: 0.00346197
[7]	training's l2: 0.00358429	valid_1's l2: 0.00343205
[8]	training's l2: 0.0035373	valid_1's l2: 0.00340426
[9]	training's l2: 0.00349408	valid_1's l2: 0.00337935
[10]	training's l2: 0.00345183	valid_1's l2: 0.00335679
[11]	training's l2: 0.00341595	valid_1's l2: 0.00333857
[12]	training's l2: 0.0033801	valid_1's l2: 0.0033201
[13]	training's l2: 0.00334771	valid_1's l2: 0.00330234
[14]	training's l2: 0.00331719	valid_1's l2: 0.00328882
[15]	training's l2: 0.00329025	valid_1's l2: 0.00327495
[16]	training's l2: 0.00326225	valid_1's l2: 0.00326303
[17]	training's l2: 0.0032375	valid_1's l2: 0.00325479
[18]	training's l2: 0.00321432	valid_1's l2: 0.0032443
[19]	training's l2: 0.00319282	valid_1's l2: 0.0032351
[20]	training's l2: 0.00317353	valid_1's l2: 0.00322834
[21]	training's l2: 0.00315238	valid_1's l2: 0.00322051
[22]	training's l2: 0.00313463	valid_1's l2: 0.00321367
[23]	training's l2: 0.00311594	valid_1's l2: 0.00320768
[24]	training's l2: 0.00309792	valid_1's l2: 0.00320184
[25]	training's l2: 0.00308188	valid_1's l2: 0.00319963
[26]	training's l2: 0.00306629	valid_1's l2: 0.00319659
[27]	training's l2: 0.00305172	valid_1's l2: 0.00319409
[28]	training's l2: 0.00303625	valid_1's l2: 0.00318946
[29]	training's l2: 0.00302325	valid_1's l2: 0.00318679
[30]	training's l2: 0.00300984	valid_1's l2: 0.00318454
[31]	training's l2: 0.00299693	valid_1's l2: 0.00318094
[32]	training's l2: 0.0029828	valid_1's l2: 0.00317611
[33]	training's l2: 0.00296987	valid_1's l2: 0.00317574
[34]	training's l2: 0.00295803	valid_1's l2: 0.00317343
[35]	training's l2: 0.00294604	valid_1's l2: 0.00316985
[36]	training's l2: 0.00293372	valid_1's l2: 0.00316554
[37]	training's l2: 0.00292204	valid_1's l2: 0.00316315
[38]	training's l2: 0.00291224	valid_1's l2: 0.00316251
[39]	training's l2: 0.00290235	valid_1's l2: 0.00316091
[40]	training's l2: 0.00289136	valid_1's l2: 0.00315759
[41]	training's l2: 0.00287972	valid_1's l2: 0.00315678
[42]	training's l2: 0.00286879	valid_1's l2: 0.0031539
[43]	training's l2: 0.00285824	valid_1's l2: 0.00315134
[44]	training's l2: 0.00284883	valid_1's l2: 0.00314885
[45]	training's l2: 0.00283865	valid_1's l2: 0.00314647
[46]	training's l2: 0.00282952	valid_1's l2: 0.00314429
[47]	training's l2: 0.00282072	valid_1's l2: 0.00314261
[48]	training's l2: 0.00281035	valid_1's l2: 0.00313974
[49]	training's l2: 0.00280095	valid_1's l2: 0.00313878
[50]	training's l2: 0.00279189	valid_1's l2: 0.00313709
[51]	training's l2: 0.002783	valid_1's l2: 0.00313597
[52]	training's l2: 0.00277393	valid_1's l2: 0.00313503
[53]	training's l2: 0.00276464	valid_1's l2: 0.00313246
[54]	training's l2: 0.00275468	valid_1's l2: 0.00312921
[55]	training's l2: 0.00274643	valid_1's l2: 0.00312627
[56]	training's l2: 0.0027366	valid_1's l2: 0.00312576
[57]	training's l2: 0.0027284	valid_1's l2: 0.00312386
[58]	training's l2: 0.00272072	valid_1's l2: 0.00312287
[59]	training's l2: 0.00271257	valid_1's l2: 0.00312136
[60]	training's l2: 0.00270499	valid_1's l2: 0.00312099
[61]	training's l2: 0.00269574	valid_1's l2: 0.00311802
[62]	training's l2: 0.00268821	valid_1's l2: 0.00311655
[63]	training's l2: 0.00267919	valid_1's l2: 0.00311244
[64]	training's l2: 0.0026712	valid_1's l2: 0.00311094
[65]	training's l2: 0.00266302	valid_1's l2: 0.00310969
[66]	training's l2: 0.00265528	valid_1's l2: 0.0031096
[67]	training's l2: 0.00264834	valid_1's l2: 0.0031093
[68]	training's l2: 0.00264051	valid_1's l2: 0.00310684
[69]	training's l2: 0.00263233	valid_1's l2: 0.00310594
[70]	training's l2: 0.00262511	valid_1's l2: 0.00310546
[71]	training's l2: 0.00261794	valid_1's l2: 0.00310524
[72]	training's l2: 0.00261139	valid_1's l2: 0.00310589
[73]	training's l2: 0.00260466	valid_1's l2: 0.00310506
[74]	training's l2: 0.00259774	valid_1's l2: 0.00310567
[75]	training's l2: 0.00259134	valid_1's l2: 0.00310388
[76]	training's l2: 0.00258428	valid_1's l2: 0.00310149
[77]	training's l2: 0.00257778	valid_1's l2: 0.00310029
[78]	training's l2: 0.00256994	valid_1's l2: 0.00309894
[79]	training's l2: 0.00256353	valid_1's l2: 0.00309854
[80]	training's l2: 0.00255684	valid_1's l2: 0.00309708
[81]	training's l2: 0.00255019	valid_1's l2: 0.00309724
[82]	training's l2: 0.00254467	valid_1's l2: 0.00309687
[83]	training's l2: 0.00253828	valid_1's l2: 0.00309568
[84]	training's l2: 0.00253202	valid_1's l2: 0.00309532
[85]	training's l2: 0.00252588	valid_1's l2: 0.00309585
[86]	training's l2: 0.00251987	valid_1's l2: 0.00309578
[87]	training's l2: 0.00251312	valid_1's l2: 0.00309527
[88]	training's l2: 0.0025069	valid_1's l2: 0.00309478
[89]	training's l2: 0.00250116	valid_1's l2: 0.00309425
[90]	training's l2: 0.00249499	valid_1's l2: 0.00309305
[91]	training's l2: 0.00248817	valid_1's l2: 0.00309273
[92]	training's l2: 0.00248247	valid_1's l2: 0.00309231
[93]	training's l2: 0.00247597	valid_1's l2: 0.00309202
[94]	training's l2: 0.00246976	valid_1's l2: 0.00309223
[95]	training's l2: 0.00246407	valid_1's l2: 0.0030917
[96]	training's l2: 0.00245822	valid_1's l2: 0.00309178
[97]	training's l2: 0.00245182	valid_1's l2: 0.00309138
[98]	training's l2: 0.00244576	valid_1's l2: 0.00309028
[99]	training's l2: 0.00243956	valid_1's l2: 0.00309018
[100]	training's l2: 0.00243365	valid_1's l2: 0.00308864
[101]	training's l2: 0.00242809	valid_1's l2: 0.00308825
[102]	training's l2: 0.00242268	valid_1's l2: 0.00308808
[103]	training's l2: 0.00241709	valid_1's l2: 0.00308763
[104]	training's l2: 0.00241132	valid_1's l2: 0.0030881
[105]	training's l2: 0.00240583	valid_1's l2: 0.00308679
[106]	training's l2: 0.00240013	valid_1's l2: 0.00308723
[107]	training's l2: 0.00239476	valid_1's l2: 0.00308743
[108]	training's l2: 0.00238893	valid_1's l2: 0.00308673
[109]	training's l2: 0.0023834	valid_1's l2: 0.00308721
[110]	training's l2: 0.00237768	valid_1's l2: 0.00308785
[111]	training's l2: 0.00237199	valid_1's l2: 0.00308855
[112]	training's l2: 0.00236671	valid_1's l2: 0.00308831
[113]	training's l2: 0.00236127	valid_1's l2: 0.00308821
[114]	training's l2: 0.00235578	valid_1's l2: 0.00308838
[115]	training's l2: 0.00235046	valid_1's l2: 0.00308788
[116]	training's l2: 0.00234558	valid_1's l2: 0.00308723
[117]	training's l2: 0.0023405	valid_1's l2: 0.00308735
[118]	training's l2: 0.00233509	valid_1's l2: 0.00308708
[119]	training's l2: 0.00232996	valid_1's l2: 0.00308774
[120]	training's l2: 0.00232492	valid_1's l2: 0.00308675
[121]	training's l2: 0.00232017	valid_1's l2: 0.00308626
[122]	training's l2: 0.00231534	valid_1's l2: 0.00308622
[123]	training's l2: 0.00230984	valid_1's l2: 0.00308549
[124]	training's l2: 0.00230456	valid_1's l2: 0.00308531
[125]	training's l2: 0.00229919	valid_1's l2: 0.00308565
[126]	training's l2: 0.00229449	valid_1's l2: 0.00308511
[127]	training's l2: 0.00228946	valid_1's l2: 0.00308525
[128]	training's l2: 0.00228385	valid_1's l2: 0.00308375
[129]	training's l2: 0.00227844	valid_1's l2: 0.00308334
[130]	training's l2: 0.0022738	valid_1's l2: 0.00308389
[131]	training's l2: 0.00226964	valid_1's l2: 0.00308399
[132]	training's l2: 0.00226521	valid_1's l2: 0.00308395
[133]	training's l2: 0.00226059	valid_1's l2: 0.00308387
[134]	training's l2: 0.00225536	valid_1's l2: 0.00308313
[135]	training's l2: 0.00225036	valid_1's l2: 0.00308272
[136]	training's l2: 0.00224544	valid_1's l2: 0.00308234
[137]	training's l2: 0.00224073	valid_1's l2: 0.0030826
[138]	training's l2: 0.00223607	valid_1's l2: 0.00308254
[139]	training's l2: 0.0022319	valid_1's l2: 0.00308292
[140]	training's l2: 0.00222712	valid_1's l2: 0.00308103
[141]	training's l2: 0.00222276	valid_1's l2: 0.00308099
[142]	training's l2: 0.00221826	valid_1's l2: 0.00308105
[143]	training's l2: 0.00221371	valid_1's l2: 0.00308125
[144]	training's l2: 0.0022098	valid_1's l2: 0.00308131
[145]	training's l2: 0.0022053	valid_1's l2: 0.00308144
[146]	training's l2: 0.00220061	valid_1's l2: 0.00308188
[147]	training's l2: 0.00219559	valid_1's l2: 0.00308076
[148]	training's l2: 0.00219085	valid_1's l2: 0.00307924
[149]	training's l2: 0.00218663	valid_1's l2: 0.00307842
[150]	training's l2: 0.00218205	valid_1's l2: 0.00307854
[151]	training's l2: 0.00217787	valid_1's l2: 0.00307833
[152]	training's l2: 0.00217378	valid_1's l2: 0.00307856
[153]	training's l2: 0.0021694	valid_1's l2: 0.00307918
[154]	training's l2: 0.00216543	valid_1's l2: 0.00307935
[155]	training's l2: 0.00216119	valid_1's l2: 0.0030802
[156]	training's l2: 0.00215714	valid_1's l2: 0.0030798
[157]	training's l2: 0.00215337	valid_1's l2: 0.00308004
[158]	training's l2: 0.00214872	valid_1's l2: 0.00307958
[159]	training's l2: 0.00214414	valid_1's l2: 0.00307881
[160]	training's l2: 0.00214049	valid_1's l2: 0.00307952
[161]	training's l2: 0.0021365	valid_1's l2: 0.00307887
[162]	training's l2: 0.00213194	valid_1's l2: 0.00307853
[163]	training's l2: 0.00212752	valid_1's l2: 0.00307832
[164]	training's l2: 0.00212338	valid_1's l2: 0.00307853
[165]	training's l2: 0.0021188	valid_1's l2: 0.00307777
[166]	training's l2: 0.0021147	valid_1's l2: 0.00307795
[167]	training's l2: 0.00211127	valid_1's l2: 0.00307797
[168]	training's l2: 0.00210766	valid_1's l2: 0.00307751
[169]	training's l2: 0.00210281	valid_1's l2: 0.00307708
[170]	training's l2: 0.0020979	valid_1's l2: 0.00307635
[171]	training's l2: 0.0020933	valid_1's l2: 0.00307524
[172]	training's l2: 0.00208911	valid_1's l2: 0.00307529
[173]	training's l2: 0.00208476	valid_1's l2: 0.00307521
[174]	training's l2: 0.00208138	valid_1's l2: 0.00307532
[175]	training's l2: 0.00207767	valid_1's l2: 0.00307516
[176]	training's l2: 0.0020736	valid_1's l2: 0.00307482
[177]	training's l2: 0.00206951	valid_1's l2: 0.00307499
[178]	training's l2: 0.00206561	valid_1's l2: 0.00307484
[179]	training's l2: 0.00206102	valid_1's l2: 0.00307548
[180]	training's l2: 0.00205705	valid_1's l2: 0.00307461
[181]	training's l2: 0.00205323	valid_1's l2: 0.00307439
[182]	training's l2: 0.00204916	valid_1's l2: 0.00307422
[183]	training's l2: 0.00204545	valid_1's l2: 0.00307428
[184]	training's l2: 0.00204136	valid_1's l2: 0.00307481
[185]	training's l2: 0.00203806	valid_1's l2: 0.00307434
[186]	training's l2: 0.00203432	valid_1's l2: 0.00307362
[187]	training's l2: 0.00203003	valid_1's l2: 0.00307429
[188]	training's l2: 0.00202657	valid_1's l2: 0.00307499
[189]	training's l2: 0.00202269	valid_1's l2: 0.00307506
Did not meet early stopping. Best iteration is:
[189]	training's l2: 0.00202269	valid_1's l2: 0.00307506
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179110 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00397465	valid_1's l2: 0.00368151
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00389159	valid_1's l2: 0.00362231
[3]	training's l2: 0.00381603	valid_1's l2: 0.00357141
[4]	training's l2: 0.00375272	valid_1's l2: 0.00352811
[5]	training's l2: 0.00369512	valid_1's l2: 0.00349264
[6]	training's l2: 0.00363812	valid_1's l2: 0.00345552
[7]	training's l2: 0.00358897	valid_1's l2: 0.00342551
[8]	training's l2: 0.00354448	valid_1's l2: 0.00339932
[9]	training's l2: 0.00350148	valid_1's l2: 0.00337451
[10]	training's l2: 0.00345871	valid_1's l2: 0.00335161
[11]	training's l2: 0.00342124	valid_1's l2: 0.00333168
[12]	training's l2: 0.00338717	valid_1's l2: 0.00331518
[13]	training's l2: 0.00335273	valid_1's l2: 0.00330158
[14]	training's l2: 0.00332378	valid_1's l2: 0.0032871
[15]	training's l2: 0.00329573	valid_1's l2: 0.00327466
[16]	training's l2: 0.00327058	valid_1's l2: 0.0032644
[17]	training's l2: 0.00324262	valid_1's l2: 0.00325142
[18]	training's l2: 0.00321918	valid_1's l2: 0.00323919
[19]	training's l2: 0.00319819	valid_1's l2: 0.00323325
[20]	training's l2: 0.00317736	valid_1's l2: 0.00322576
[21]	training's l2: 0.00315751	valid_1's l2: 0.00321921
[22]	training's l2: 0.00313675	valid_1's l2: 0.00321187
[23]	training's l2: 0.00311816	valid_1's l2: 0.00320739
[24]	training's l2: 0.00310139	valid_1's l2: 0.00320466
[25]	training's l2: 0.00308531	valid_1's l2: 0.00319926
[26]	training's l2: 0.00306846	valid_1's l2: 0.00319429
[27]	training's l2: 0.00305229	valid_1's l2: 0.00318822
[28]	training's l2: 0.00303829	valid_1's l2: 0.00318715
[29]	training's l2: 0.00302496	valid_1's l2: 0.00318592
[30]	training's l2: 0.0030106	valid_1's l2: 0.00318264
[31]	training's l2: 0.00299673	valid_1's l2: 0.00317914
[32]	training's l2: 0.00298519	valid_1's l2: 0.00317796
[33]	training's l2: 0.0029745	valid_1's l2: 0.00317581
[34]	training's l2: 0.00296072	valid_1's l2: 0.00317254
[35]	training's l2: 0.0029497	valid_1's l2: 0.0031717
[36]	training's l2: 0.00293824	valid_1's l2: 0.00316892
[37]	training's l2: 0.00292603	valid_1's l2: 0.00316438
[38]	training's l2: 0.00291492	valid_1's l2: 0.00316234
[39]	training's l2: 0.00290334	valid_1's l2: 0.00315948
[40]	training's l2: 0.00289128	valid_1's l2: 0.00315657
[41]	training's l2: 0.00288075	valid_1's l2: 0.00315509
[42]	training's l2: 0.00286928	valid_1's l2: 0.00315167
[43]	training's l2: 0.00285935	valid_1's l2: 0.00314991
[44]	training's l2: 0.00284846	valid_1's l2: 0.00314588
[45]	training's l2: 0.00283842	valid_1's l2: 0.00314457
[46]	training's l2: 0.00282879	valid_1's l2: 0.00314218
[47]	training's l2: 0.00281926	valid_1's l2: 0.00313958
[48]	training's l2: 0.00280922	valid_1's l2: 0.00313843
[49]	training's l2: 0.00279904	valid_1's l2: 0.00313495
[50]	training's l2: 0.00278937	valid_1's l2: 0.00313359
[51]	training's l2: 0.00277971	valid_1's l2: 0.00313089
[52]	training's l2: 0.00277116	valid_1's l2: 0.00313046
[53]	training's l2: 0.00276309	valid_1's l2: 0.00313037
[54]	training's l2: 0.00275427	valid_1's l2: 0.00312848
[55]	training's l2: 0.00274385	valid_1's l2: 0.00312493
[56]	training's l2: 0.00273524	valid_1's l2: 0.00312239
[57]	training's l2: 0.00272664	valid_1's l2: 0.003121
[58]	training's l2: 0.00271742	valid_1's l2: 0.00311856
[59]	training's l2: 0.00270813	valid_1's l2: 0.00311406
[60]	training's l2: 0.00269959	valid_1's l2: 0.00311332
[61]	training's l2: 0.00269094	valid_1's l2: 0.00311209
[62]	training's l2: 0.00268232	valid_1's l2: 0.00311135
[63]	training's l2: 0.00267453	valid_1's l2: 0.00311166
[64]	training's l2: 0.0026657	valid_1's l2: 0.00311002
[65]	training's l2: 0.00265864	valid_1's l2: 0.0031093
[66]	training's l2: 0.0026501	valid_1's l2: 0.00310799
[67]	training's l2: 0.00264256	valid_1's l2: 0.00310497
[68]	training's l2: 0.0026349	valid_1's l2: 0.00310582
[69]	training's l2: 0.00262756	valid_1's l2: 0.00310516
[70]	training's l2: 0.00261937	valid_1's l2: 0.00310366
[71]	training's l2: 0.00261156	valid_1's l2: 0.00310299
[72]	training's l2: 0.00260478	valid_1's l2: 0.00310153
[73]	training's l2: 0.00259782	valid_1's l2: 0.00310084
[74]	training's l2: 0.00259035	valid_1's l2: 0.00310095
[75]	training's l2: 0.0025833	valid_1's l2: 0.00310077
[76]	training's l2: 0.00257598	valid_1's l2: 0.00309953
[77]	training's l2: 0.00256944	valid_1's l2: 0.00309938
[78]	training's l2: 0.00256247	valid_1's l2: 0.00309988
[79]	training's l2: 0.00255514	valid_1's l2: 0.00309836
[80]	training's l2: 0.00254781	valid_1's l2: 0.00309704
[81]	training's l2: 0.0025407	valid_1's l2: 0.0030974
[82]	training's l2: 0.00253383	valid_1's l2: 0.0030967
[83]	training's l2: 0.00252729	valid_1's l2: 0.00309657
[84]	training's l2: 0.0025206	valid_1's l2: 0.00309677
[85]	training's l2: 0.00251416	valid_1's l2: 0.00309749
[86]	training's l2: 0.00250825	valid_1's l2: 0.00309734
[87]	training's l2: 0.00250163	valid_1's l2: 0.00309591
[88]	training's l2: 0.00249548	valid_1's l2: 0.00309553
[89]	training's l2: 0.00248904	valid_1's l2: 0.00309459
[90]	training's l2: 0.00248254	valid_1's l2: 0.00309482
[91]	training's l2: 0.00247623	valid_1's l2: 0.00309478
[92]	training's l2: 0.00246983	valid_1's l2: 0.00309392
[93]	training's l2: 0.00246339	valid_1's l2: 0.00309388
[94]	training's l2: 0.00245648	valid_1's l2: 0.00309249
[95]	training's l2: 0.00244985	valid_1's l2: 0.00309158
[96]	training's l2: 0.00244333	valid_1's l2: 0.00309195
[97]	training's l2: 0.00243701	valid_1's l2: 0.0030915
[98]	training's l2: 0.00242998	valid_1's l2: 0.00309157
[99]	training's l2: 0.00242395	valid_1's l2: 0.00309201
[100]	training's l2: 0.00241766	valid_1's l2: 0.0030917
[101]	training's l2: 0.00241127	valid_1's l2: 0.00309152
[102]	training's l2: 0.00240458	valid_1's l2: 0.00309118
[103]	training's l2: 0.00239847	valid_1's l2: 0.0030912
[104]	training's l2: 0.00239239	valid_1's l2: 0.00309042
[105]	training's l2: 0.00238628	valid_1's l2: 0.00309051
[106]	training's l2: 0.00238016	valid_1's l2: 0.00308958
[107]	training's l2: 0.00237389	valid_1's l2: 0.00309004
[108]	training's l2: 0.00236845	valid_1's l2: 0.00308983
[109]	training's l2: 0.00236222	valid_1's l2: 0.00309041
[110]	training's l2: 0.0023565	valid_1's l2: 0.00309042
[111]	training's l2: 0.00235145	valid_1's l2: 0.00308995
[112]	training's l2: 0.00234582	valid_1's l2: 0.00308967
[113]	training's l2: 0.0023401	valid_1's l2: 0.00308977
[114]	training's l2: 0.00233418	valid_1's l2: 0.00308958
[115]	training's l2: 0.00232871	valid_1's l2: 0.00308901
[116]	training's l2: 0.00232254	valid_1's l2: 0.00308834
[117]	training's l2: 0.00231692	valid_1's l2: 0.00308827
[118]	training's l2: 0.00231156	valid_1's l2: 0.00308837
[119]	training's l2: 0.00230614	valid_1's l2: 0.00308777
[120]	training's l2: 0.00230094	valid_1's l2: 0.00308766
[121]	training's l2: 0.00229596	valid_1's l2: 0.00308715
[122]	training's l2: 0.00228966	valid_1's l2: 0.00308534
[123]	training's l2: 0.0022841	valid_1's l2: 0.00308587
[124]	training's l2: 0.00227845	valid_1's l2: 0.00308603
[125]	training's l2: 0.00227278	valid_1's l2: 0.00308715
[126]	training's l2: 0.00226716	valid_1's l2: 0.00308614
[127]	training's l2: 0.00226175	valid_1's l2: 0.00308637
[128]	training's l2: 0.0022565	valid_1's l2: 0.00308634
[129]	training's l2: 0.00225058	valid_1's l2: 0.00308583
[130]	training's l2: 0.00224535	valid_1's l2: 0.00308526
[131]	training's l2: 0.00223995	valid_1's l2: 0.00308468
[132]	training's l2: 0.00223416	valid_1's l2: 0.00308342
[133]	training's l2: 0.00222831	valid_1's l2: 0.00308337
[134]	training's l2: 0.00222262	valid_1's l2: 0.0030827
[135]	training's l2: 0.00221755	valid_1's l2: 0.00308336
[136]	training's l2: 0.00221211	valid_1's l2: 0.0030835
[137]	training's l2: 0.00220654	valid_1's l2: 0.00308399
[138]	training's l2: 0.00220143	valid_1's l2: 0.00308304
[139]	training's l2: 0.00219598	valid_1's l2: 0.00308357
[140]	training's l2: 0.00219152	valid_1's l2: 0.00308373
[141]	training's l2: 0.00218627	valid_1's l2: 0.00308242
[142]	training's l2: 0.00218112	valid_1's l2: 0.0030824
[143]	training's l2: 0.00217578	valid_1's l2: 0.0030817
[144]	training's l2: 0.0021709	valid_1's l2: 0.00308191
[145]	training's l2: 0.002166	valid_1's l2: 0.00308203
[146]	training's l2: 0.00216134	valid_1's l2: 0.00308212
[147]	training's l2: 0.00215608	valid_1's l2: 0.00308171
[148]	training's l2: 0.00215091	valid_1's l2: 0.00308188
[149]	training's l2: 0.00214652	valid_1's l2: 0.00308229
[150]	training's l2: 0.00214165	valid_1's l2: 0.00308243
[151]	training's l2: 0.00213671	valid_1's l2: 0.00308183
[152]	training's l2: 0.00213199	valid_1's l2: 0.00308251
[153]	training's l2: 0.00212713	valid_1's l2: 0.00308294
[154]	training's l2: 0.00212212	valid_1's l2: 0.00308216
[155]	training's l2: 0.00211736	valid_1's l2: 0.00308207
[156]	training's l2: 0.00211285	valid_1's l2: 0.00308248
[157]	training's l2: 0.00210867	valid_1's l2: 0.00308291
[158]	training's l2: 0.00210368	valid_1's l2: 0.0030833
[159]	training's l2: 0.00209913	valid_1's l2: 0.00308358
[160]	training's l2: 0.00209437	valid_1's l2: 0.00308418
[161]	training's l2: 0.00208946	valid_1's l2: 0.00308495
[162]	training's l2: 0.00208457	valid_1's l2: 0.00308486
[163]	training's l2: 0.00208036	valid_1's l2: 0.00308438
[164]	training's l2: 0.00207553	valid_1's l2: 0.00308359
[165]	training's l2: 0.00207087	valid_1's l2: 0.00308322
[166]	training's l2: 0.00206598	valid_1's l2: 0.00308294
[167]	training's l2: 0.00206155	valid_1's l2: 0.00308327
[168]	training's l2: 0.00205678	valid_1's l2: 0.00308356
[169]	training's l2: 0.00205281	valid_1's l2: 0.0030836
[170]	training's l2: 0.00204837	valid_1's l2: 0.00308384
[171]	training's l2: 0.00204404	valid_1's l2: 0.00308414
[172]	training's l2: 0.00203946	valid_1's l2: 0.00308486
[173]	training's l2: 0.00203498	valid_1's l2: 0.00308383
Early stopping, best iteration is:
[143]	training's l2: 0.00217578	valid_1's l2: 0.0030817
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180116 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00397549	valid_1's l2: 0.00368416
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00389297	valid_1's l2: 0.00362663
[3]	training's l2: 0.00381783	valid_1's l2: 0.0035776
[4]	training's l2: 0.00375502	valid_1's l2: 0.00353784
[5]	training's l2: 0.00369571	valid_1's l2: 0.00350161
[6]	training's l2: 0.00363813	valid_1's l2: 0.00346491
[7]	training's l2: 0.00358967	valid_1's l2: 0.00343672
[8]	training's l2: 0.00354379	valid_1's l2: 0.00341028
[9]	training's l2: 0.00350023	valid_1's l2: 0.00338511
[10]	training's l2: 0.0034602	valid_1's l2: 0.00336247
[11]	training's l2: 0.00342041	valid_1's l2: 0.00334324
[12]	training's l2: 0.00338602	valid_1's l2: 0.00332516
[13]	training's l2: 0.00335258	valid_1's l2: 0.00330835
[14]	training's l2: 0.0033217	valid_1's l2: 0.00329411
[15]	training's l2: 0.00329312	valid_1's l2: 0.00328199
[16]	training's l2: 0.00326559	valid_1's l2: 0.00327006
[17]	training's l2: 0.00323852	valid_1's l2: 0.0032557
[18]	training's l2: 0.00321276	valid_1's l2: 0.00324722
[19]	training's l2: 0.00318955	valid_1's l2: 0.00324047
[20]	training's l2: 0.00316624	valid_1's l2: 0.00323004
[21]	training's l2: 0.00314541	valid_1's l2: 0.00322423
[22]	training's l2: 0.00312531	valid_1's l2: 0.00321568
[23]	training's l2: 0.00310733	valid_1's l2: 0.00320983
[24]	training's l2: 0.00308962	valid_1's l2: 0.00320771
[25]	training's l2: 0.00307114	valid_1's l2: 0.0032007
[26]	training's l2: 0.00305522	valid_1's l2: 0.00319858
[27]	training's l2: 0.00303911	valid_1's l2: 0.00319233
[28]	training's l2: 0.00302278	valid_1's l2: 0.00318676
[29]	training's l2: 0.00300822	valid_1's l2: 0.00318298
[30]	training's l2: 0.00299436	valid_1's l2: 0.00318151
[31]	training's l2: 0.0029786	valid_1's l2: 0.0031756
[32]	training's l2: 0.00296639	valid_1's l2: 0.00317277
[33]	training's l2: 0.00295206	valid_1's l2: 0.00316998
[34]	training's l2: 0.0029401	valid_1's l2: 0.00316943
[35]	training's l2: 0.00292896	valid_1's l2: 0.00316703
[36]	training's l2: 0.0029172	valid_1's l2: 0.00316496
[37]	training's l2: 0.00290573	valid_1's l2: 0.00316214
[38]	training's l2: 0.00289309	valid_1's l2: 0.00315992
[39]	training's l2: 0.00288242	valid_1's l2: 0.00315823
[40]	training's l2: 0.00287138	valid_1's l2: 0.00315661
[41]	training's l2: 0.00285975	valid_1's l2: 0.0031539
[42]	training's l2: 0.00284984	valid_1's l2: 0.00315233
[43]	training's l2: 0.00283962	valid_1's l2: 0.00315261
[44]	training's l2: 0.00282751	valid_1's l2: 0.00314762
[45]	training's l2: 0.00281645	valid_1's l2: 0.00314295
[46]	training's l2: 0.00280663	valid_1's l2: 0.00314328
[47]	training's l2: 0.00279597	valid_1's l2: 0.00313895
[48]	training's l2: 0.00278573	valid_1's l2: 0.00313668
[49]	training's l2: 0.00277475	valid_1's l2: 0.00313252
[50]	training's l2: 0.00276583	valid_1's l2: 0.00313043
[51]	training's l2: 0.00275576	valid_1's l2: 0.00312664
[52]	training's l2: 0.00274662	valid_1's l2: 0.00312541
[53]	training's l2: 0.00273603	valid_1's l2: 0.00312116
[54]	training's l2: 0.00272726	valid_1's l2: 0.00311981
[55]	training's l2: 0.00271743	valid_1's l2: 0.00312048
[56]	training's l2: 0.0027086	valid_1's l2: 0.0031178
[57]	training's l2: 0.00270036	valid_1's l2: 0.00311749
[58]	training's l2: 0.00269008	valid_1's l2: 0.00311363
[59]	training's l2: 0.00268236	valid_1's l2: 0.00311311
[60]	training's l2: 0.00267393	valid_1's l2: 0.00311286
[61]	training's l2: 0.00266461	valid_1's l2: 0.00310876
[62]	training's l2: 0.00265653	valid_1's l2: 0.00310727
[63]	training's l2: 0.00264877	valid_1's l2: 0.00310602
[64]	training's l2: 0.00264057	valid_1's l2: 0.0031054
[65]	training's l2: 0.00263192	valid_1's l2: 0.00310271
[66]	training's l2: 0.00262424	valid_1's l2: 0.00310241
[67]	training's l2: 0.00261627	valid_1's l2: 0.00310135
[68]	training's l2: 0.00260877	valid_1's l2: 0.00310105
[69]	training's l2: 0.00260006	valid_1's l2: 0.00309909
[70]	training's l2: 0.00259147	valid_1's l2: 0.00309732
[71]	training's l2: 0.00258418	valid_1's l2: 0.00309786
[72]	training's l2: 0.00257659	valid_1's l2: 0.0030979
[73]	training's l2: 0.00256943	valid_1's l2: 0.0030975
[74]	training's l2: 0.0025613	valid_1's l2: 0.00309673
[75]	training's l2: 0.00255404	valid_1's l2: 0.00309698
[76]	training's l2: 0.00254715	valid_1's l2: 0.00309675
[77]	training's l2: 0.00254052	valid_1's l2: 0.00309619
[78]	training's l2: 0.00253295	valid_1's l2: 0.00309567
[79]	training's l2: 0.00252609	valid_1's l2: 0.00309633
[80]	training's l2: 0.00251932	valid_1's l2: 0.00309528
[81]	training's l2: 0.00251192	valid_1's l2: 0.00309555
[82]	training's l2: 0.00250484	valid_1's l2: 0.0030956
[83]	training's l2: 0.00249818	valid_1's l2: 0.0030947
[84]	training's l2: 0.00249169	valid_1's l2: 0.00309449
[85]	training's l2: 0.00248461	valid_1's l2: 0.00309402
[86]	training's l2: 0.00247753	valid_1's l2: 0.0030927
[87]	training's l2: 0.00246991	valid_1's l2: 0.00309195
[88]	training's l2: 0.00246299	valid_1's l2: 0.0030915
[89]	training's l2: 0.00245597	valid_1's l2: 0.00309092
[90]	training's l2: 0.00244867	valid_1's l2: 0.00308955
[91]	training's l2: 0.00244211	valid_1's l2: 0.00308947
[92]	training's l2: 0.00243548	valid_1's l2: 0.00308827
[93]	training's l2: 0.00242891	valid_1's l2: 0.00308746
[94]	training's l2: 0.00242202	valid_1's l2: 0.00308768
[95]	training's l2: 0.0024159	valid_1's l2: 0.00308737
[96]	training's l2: 0.00240943	valid_1's l2: 0.00308674
[97]	training's l2: 0.00240329	valid_1's l2: 0.00308548
[98]	training's l2: 0.0023961	valid_1's l2: 0.00308399
[99]	training's l2: 0.00238947	valid_1's l2: 0.00308285
[100]	training's l2: 0.00238323	valid_1's l2: 0.00308234
[101]	training's l2: 0.00237742	valid_1's l2: 0.00308219
[102]	training's l2: 0.00237146	valid_1's l2: 0.00308212
[103]	training's l2: 0.0023653	valid_1's l2: 0.00308138
[104]	training's l2: 0.00235871	valid_1's l2: 0.00308113
[105]	training's l2: 0.00235207	valid_1's l2: 0.0030808
[106]	training's l2: 0.00234598	valid_1's l2: 0.00307994
[107]	training's l2: 0.00234008	valid_1's l2: 0.00307983
[108]	training's l2: 0.00233349	valid_1's l2: 0.00307882
[109]	training's l2: 0.00232748	valid_1's l2: 0.00307868
[110]	training's l2: 0.00232145	valid_1's l2: 0.00307807
[111]	training's l2: 0.00231545	valid_1's l2: 0.00307724
[112]	training's l2: 0.00230986	valid_1's l2: 0.00307751
[113]	training's l2: 0.00230399	valid_1's l2: 0.00307835
[114]	training's l2: 0.0022982	valid_1's l2: 0.00307684
[115]	training's l2: 0.00229288	valid_1's l2: 0.00307666
[116]	training's l2: 0.00228732	valid_1's l2: 0.00307587
[117]	training's l2: 0.00228141	valid_1's l2: 0.00307402
[118]	training's l2: 0.00227568	valid_1's l2: 0.00307455
[119]	training's l2: 0.00226993	valid_1's l2: 0.00307427
Did not meet early stopping. Best iteration is:
[119]	training's l2: 0.00226993	valid_1's l2: 0.00307427
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181242 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00395929	valid_1's l2: 0.0036796
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00386227	valid_1's l2: 0.00361648
[3]	training's l2: 0.00377571	valid_1's l2: 0.00356229
[4]	training's l2: 0.00369528	valid_1's l2: 0.00351623
[5]	training's l2: 0.00362628	valid_1's l2: 0.00347608
[6]	training's l2: 0.00356312	valid_1's l2: 0.00344298
[7]	training's l2: 0.00350416	valid_1's l2: 0.00341103
[8]	training's l2: 0.0034522	valid_1's l2: 0.00338516
[9]	training's l2: 0.00340233	valid_1's l2: 0.00336237
[10]	training's l2: 0.00335729	valid_1's l2: 0.00333992
[11]	training's l2: 0.00331234	valid_1's l2: 0.00332044
[12]	training's l2: 0.00327121	valid_1's l2: 0.00330417
[13]	training's l2: 0.00323493	valid_1's l2: 0.00328989
[14]	training's l2: 0.00320168	valid_1's l2: 0.00327677
[15]	training's l2: 0.00316917	valid_1's l2: 0.00326168
[16]	training's l2: 0.00314145	valid_1's l2: 0.0032505
[17]	training's l2: 0.00311502	valid_1's l2: 0.00324201
[18]	training's l2: 0.00308994	valid_1's l2: 0.00322839
[19]	training's l2: 0.00306588	valid_1's l2: 0.0032215
[20]	training's l2: 0.0030431	valid_1's l2: 0.00321289
[21]	training's l2: 0.00301966	valid_1's l2: 0.00320484
[22]	training's l2: 0.00299864	valid_1's l2: 0.00319909
[23]	training's l2: 0.00297764	valid_1's l2: 0.00319266
[24]	training's l2: 0.00295879	valid_1's l2: 0.00318809
[25]	training's l2: 0.00294011	valid_1's l2: 0.00318423
[26]	training's l2: 0.00292368	valid_1's l2: 0.00318183
[27]	training's l2: 0.00290719	valid_1's l2: 0.00317814
[28]	training's l2: 0.00289235	valid_1's l2: 0.00317417
[29]	training's l2: 0.00287514	valid_1's l2: 0.00317081
[30]	training's l2: 0.00286179	valid_1's l2: 0.00316732
[31]	training's l2: 0.00284691	valid_1's l2: 0.00316422
[32]	training's l2: 0.00283261	valid_1's l2: 0.00315887
[33]	training's l2: 0.00281791	valid_1's l2: 0.00315234
[34]	training's l2: 0.0028048	valid_1's l2: 0.00315051
[35]	training's l2: 0.00279232	valid_1's l2: 0.00314799
[36]	training's l2: 0.00277942	valid_1's l2: 0.00314523
[37]	training's l2: 0.00276638	valid_1's l2: 0.00314242
[38]	training's l2: 0.00275393	valid_1's l2: 0.00313766
[39]	training's l2: 0.00274156	valid_1's l2: 0.0031356
[40]	training's l2: 0.00272924	valid_1's l2: 0.00313257
[41]	training's l2: 0.00271898	valid_1's l2: 0.00313082
[42]	training's l2: 0.00270725	valid_1's l2: 0.00312807
[43]	training's l2: 0.00269676	valid_1's l2: 0.00312695
[44]	training's l2: 0.00268539	valid_1's l2: 0.00312535
[45]	training's l2: 0.00267511	valid_1's l2: 0.00312357
[46]	training's l2: 0.00266453	valid_1's l2: 0.00312315
[47]	training's l2: 0.00265354	valid_1's l2: 0.00312264
[48]	training's l2: 0.00264093	valid_1's l2: 0.00311761
[49]	training's l2: 0.00262988	valid_1's l2: 0.00311408
[50]	training's l2: 0.00261916	valid_1's l2: 0.00311172
[51]	training's l2: 0.00260911	valid_1's l2: 0.00310949
[52]	training's l2: 0.00259929	valid_1's l2: 0.00310745
[53]	training's l2: 0.00258988	valid_1's l2: 0.00310725
[54]	training's l2: 0.00257871	valid_1's l2: 0.003102
[55]	training's l2: 0.00256886	valid_1's l2: 0.00310119
[56]	training's l2: 0.00255972	valid_1's l2: 0.00309912
[57]	training's l2: 0.0025508	valid_1's l2: 0.00309825
[58]	training's l2: 0.0025419	valid_1's l2: 0.00309858
[59]	training's l2: 0.00253167	valid_1's l2: 0.00309628
[60]	training's l2: 0.00252199	valid_1's l2: 0.00309524
[61]	training's l2: 0.00251333	valid_1's l2: 0.00309438
[62]	training's l2: 0.00250517	valid_1's l2: 0.00309308
[63]	training's l2: 0.00249617	valid_1's l2: 0.0030927
[64]	training's l2: 0.00248728	valid_1's l2: 0.0030921
[65]	training's l2: 0.00247884	valid_1's l2: 0.00309076
[66]	training's l2: 0.00247077	valid_1's l2: 0.00309023
[67]	training's l2: 0.00246262	valid_1's l2: 0.00308913
[68]	training's l2: 0.00245411	valid_1's l2: 0.00308867
[69]	training's l2: 0.00244613	valid_1's l2: 0.00308776
[70]	training's l2: 0.00243726	valid_1's l2: 0.00308647
[71]	training's l2: 0.00242975	valid_1's l2: 0.00308705
[72]	training's l2: 0.0024211	valid_1's l2: 0.00308672
[73]	training's l2: 0.00241337	valid_1's l2: 0.00308653
[74]	training's l2: 0.00240519	valid_1's l2: 0.0030856
[75]	training's l2: 0.00239741	valid_1's l2: 0.00308423
[76]	training's l2: 0.00238985	valid_1's l2: 0.00308337
[77]	training's l2: 0.00238172	valid_1's l2: 0.00308244
[78]	training's l2: 0.00237411	valid_1's l2: 0.00308241
[79]	training's l2: 0.0023673	valid_1's l2: 0.00308252
[80]	training's l2: 0.00235932	valid_1's l2: 0.00308284
[81]	training's l2: 0.00235106	valid_1's l2: 0.00308049
[82]	training's l2: 0.00234376	valid_1's l2: 0.00308167
[83]	training's l2: 0.00233603	valid_1's l2: 0.00307986
[84]	training's l2: 0.00232922	valid_1's l2: 0.00307906
[85]	training's l2: 0.00232217	valid_1's l2: 0.00307865
[86]	training's l2: 0.0023154	valid_1's l2: 0.00307725
[87]	training's l2: 0.00230794	valid_1's l2: 0.00307659
[88]	training's l2: 0.00230035	valid_1's l2: 0.00307602
[89]	training's l2: 0.00229326	valid_1's l2: 0.00307531
[90]	training's l2: 0.00228691	valid_1's l2: 0.00307673
[91]	training's l2: 0.00228046	valid_1's l2: 0.00307608
[92]	training's l2: 0.00227295	valid_1's l2: 0.00307481
[93]	training's l2: 0.00226616	valid_1's l2: 0.00307427
[94]	training's l2: 0.00226015	valid_1's l2: 0.00307396
[95]	training's l2: 0.00225394	valid_1's l2: 0.0030742
[96]	training's l2: 0.00224675	valid_1's l2: 0.00307444
[97]	training's l2: 0.00223959	valid_1's l2: 0.00307401
[98]	training's l2: 0.00223286	valid_1's l2: 0.00307391
[99]	training's l2: 0.00222612	valid_1's l2: 0.00307334
[100]	training's l2: 0.00221968	valid_1's l2: 0.00307268
[101]	training's l2: 0.00221397	valid_1's l2: 0.00307255
[102]	training's l2: 0.00220736	valid_1's l2: 0.00307259
[103]	training's l2: 0.00220134	valid_1's l2: 0.003071
[104]	training's l2: 0.00219482	valid_1's l2: 0.00307033
[105]	training's l2: 0.00218873	valid_1's l2: 0.00307002
[106]	training's l2: 0.00218295	valid_1's l2: 0.00306968
[107]	training's l2: 0.00217756	valid_1's l2: 0.00306917
[108]	training's l2: 0.00217108	valid_1's l2: 0.00306863
[109]	training's l2: 0.00216553	valid_1's l2: 0.00306922
[110]	training's l2: 0.00216032	valid_1's l2: 0.0030689
[111]	training's l2: 0.0021543	valid_1's l2: 0.00306895
[112]	training's l2: 0.00214871	valid_1's l2: 0.00306914
[113]	training's l2: 0.00214329	valid_1's l2: 0.00306924
[114]	training's l2: 0.00213826	valid_1's l2: 0.00306963
[115]	training's l2: 0.00213191	valid_1's l2: 0.00307022
[116]	training's l2: 0.00212544	valid_1's l2: 0.00307082
[117]	training's l2: 0.00211998	valid_1's l2: 0.00307166
[118]	training's l2: 0.00211369	valid_1's l2: 0.00307131
[119]	training's l2: 0.00210837	valid_1's l2: 0.00307129
[120]	training's l2: 0.002102	valid_1's l2: 0.00307007
[121]	training's l2: 0.00209672	valid_1's l2: 0.0030706
[122]	training's l2: 0.0020915	valid_1's l2: 0.00307027
[123]	training's l2: 0.00208532	valid_1's l2: 0.00307
[124]	training's l2: 0.00208002	valid_1's l2: 0.00307072
[125]	training's l2: 0.00207395	valid_1's l2: 0.00307051
[126]	training's l2: 0.00206803	valid_1's l2: 0.00307025
[127]	training's l2: 0.00206344	valid_1's l2: 0.00307053
[128]	training's l2: 0.00205829	valid_1's l2: 0.00306997
[129]	training's l2: 0.00205335	valid_1's l2: 0.00307068
[130]	training's l2: 0.00204763	valid_1's l2: 0.00307002
[131]	training's l2: 0.00204284	valid_1's l2: 0.00307068
[132]	training's l2: 0.00203774	valid_1's l2: 0.00307091
[133]	training's l2: 0.00203296	valid_1's l2: 0.00307119
[134]	training's l2: 0.00202716	valid_1's l2: 0.00307115
[135]	training's l2: 0.00202241	valid_1's l2: 0.00307187
[136]	training's l2: 0.00201705	valid_1's l2: 0.00307216
[137]	training's l2: 0.00201151	valid_1's l2: 0.00307277
[138]	training's l2: 0.00200521	valid_1's l2: 0.003073
Early stopping, best iteration is:
[108]	training's l2: 0.00217108	valid_1's l2: 0.00306863
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185857 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00394328	valid_1's l2: 0.00366154
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00383451	valid_1's l2: 0.00358396
[3]	training's l2: 0.00374417	valid_1's l2: 0.0035233
[4]	training's l2: 0.00366469	valid_1's l2: 0.00347692
[5]	training's l2: 0.0035901	valid_1's l2: 0.00342886
[6]	training's l2: 0.00352776	valid_1's l2: 0.00339336
[7]	training's l2: 0.00347052	valid_1's l2: 0.00335967
[8]	training's l2: 0.00341509	valid_1's l2: 0.00332989
[9]	training's l2: 0.00337126	valid_1's l2: 0.00330611
[10]	training's l2: 0.00332603	valid_1's l2: 0.00328523
[11]	training's l2: 0.00328805	valid_1's l2: 0.00326828
[12]	training's l2: 0.00324983	valid_1's l2: 0.00325359
[13]	training's l2: 0.00321849	valid_1's l2: 0.00323953
[14]	training's l2: 0.00318494	valid_1's l2: 0.00322453
[15]	training's l2: 0.00315357	valid_1's l2: 0.00321089
[16]	training's l2: 0.00312729	valid_1's l2: 0.00320235
[17]	training's l2: 0.00310084	valid_1's l2: 0.00319255
[18]	training's l2: 0.00307577	valid_1's l2: 0.00318366
[19]	training's l2: 0.00305297	valid_1's l2: 0.00317884
[20]	training's l2: 0.00303192	valid_1's l2: 0.00317324
[21]	training's l2: 0.00300828	valid_1's l2: 0.00316519
[22]	training's l2: 0.00298901	valid_1's l2: 0.00315936
[23]	training's l2: 0.00296714	valid_1's l2: 0.00315423
[24]	training's l2: 0.00294912	valid_1's l2: 0.00315027
[25]	training's l2: 0.00293254	valid_1's l2: 0.0031461
[26]	training's l2: 0.00291472	valid_1's l2: 0.00314192
[27]	training's l2: 0.00289731	valid_1's l2: 0.00313767
[28]	training's l2: 0.00288193	valid_1's l2: 0.00313475
[29]	training's l2: 0.00286731	valid_1's l2: 0.00313155
[30]	training's l2: 0.00285322	valid_1's l2: 0.00313149
[31]	training's l2: 0.00283658	valid_1's l2: 0.00312795
[32]	training's l2: 0.00282204	valid_1's l2: 0.00312817
[33]	training's l2: 0.00280826	valid_1's l2: 0.00312531
[34]	training's l2: 0.00279528	valid_1's l2: 0.00312436
[35]	training's l2: 0.00278165	valid_1's l2: 0.0031226
[36]	training's l2: 0.0027686	valid_1's l2: 0.00312185
[37]	training's l2: 0.00275567	valid_1's l2: 0.00312033
[38]	training's l2: 0.00274172	valid_1's l2: 0.00311734
[39]	training's l2: 0.00273001	valid_1's l2: 0.00311555
[40]	training's l2: 0.0027147	valid_1's l2: 0.00311158
[41]	training's l2: 0.00270132	valid_1's l2: 0.00310879
[42]	training's l2: 0.00268869	valid_1's l2: 0.0031087
[43]	training's l2: 0.00267625	valid_1's l2: 0.00310784
[44]	training's l2: 0.00266296	valid_1's l2: 0.00310442
[45]	training's l2: 0.00265123	valid_1's l2: 0.00310322
[46]	training's l2: 0.00263939	valid_1's l2: 0.00310301
[47]	training's l2: 0.00262635	valid_1's l2: 0.00309902
[48]	training's l2: 0.00261403	valid_1's l2: 0.00309463
[49]	training's l2: 0.00260255	valid_1's l2: 0.0030917
[50]	training's l2: 0.00259107	valid_1's l2: 0.00308873
[51]	training's l2: 0.00258098	valid_1's l2: 0.00308888
[52]	training's l2: 0.00256976	valid_1's l2: 0.00308637
[53]	training's l2: 0.00255841	valid_1's l2: 0.0030842
[54]	training's l2: 0.00254702	valid_1's l2: 0.00308084
[55]	training's l2: 0.00253727	valid_1's l2: 0.00307932
[56]	training's l2: 0.00252688	valid_1's l2: 0.00307915
[57]	training's l2: 0.00251726	valid_1's l2: 0.00307638
[58]	training's l2: 0.00250768	valid_1's l2: 0.00307668
[59]	training's l2: 0.00249757	valid_1's l2: 0.0030743
[60]	training's l2: 0.0024874	valid_1's l2: 0.00307544
[61]	training's l2: 0.00247711	valid_1's l2: 0.00307462
[62]	training's l2: 0.00246709	valid_1's l2: 0.00307312
[63]	training's l2: 0.00245703	valid_1's l2: 0.0030728
[64]	training's l2: 0.0024474	valid_1's l2: 0.00307342
[65]	training's l2: 0.00243793	valid_1's l2: 0.0030737
[66]	training's l2: 0.00242761	valid_1's l2: 0.00307262
[67]	training's l2: 0.00241871	valid_1's l2: 0.00307388
[68]	training's l2: 0.00240977	valid_1's l2: 0.00307338
[69]	training's l2: 0.00240043	valid_1's l2: 0.00307425
[70]	training's l2: 0.00239161	valid_1's l2: 0.0030738
[71]	training's l2: 0.00238294	valid_1's l2: 0.00307331
[72]	training's l2: 0.00237398	valid_1's l2: 0.00307141
[73]	training's l2: 0.00236453	valid_1's l2: 0.00307103
[74]	training's l2: 0.00235528	valid_1's l2: 0.00307171
[75]	training's l2: 0.00234664	valid_1's l2: 0.00307228
[76]	training's l2: 0.00233924	valid_1's l2: 0.00307186
[77]	training's l2: 0.00233119	valid_1's l2: 0.00306939
[78]	training's l2: 0.00232113	valid_1's l2: 0.00306916
[79]	training's l2: 0.00231272	valid_1's l2: 0.00306802
[80]	training's l2: 0.00230573	valid_1's l2: 0.00306781
[81]	training's l2: 0.00229892	valid_1's l2: 0.0030674
[82]	training's l2: 0.00229085	valid_1's l2: 0.00306806
[83]	training's l2: 0.00228301	valid_1's l2: 0.00306791
[84]	training's l2: 0.00227496	valid_1's l2: 0.00306914
[85]	training's l2: 0.00226712	valid_1's l2: 0.00306988
[86]	training's l2: 0.00225852	valid_1's l2: 0.00307047
[87]	training's l2: 0.00225036	valid_1's l2: 0.003069
[88]	training's l2: 0.00224179	valid_1's l2: 0.0030689
[89]	training's l2: 0.00223472	valid_1's l2: 0.00306976
[90]	training's l2: 0.00222722	valid_1's l2: 0.00307116
[91]	training's l2: 0.0022197	valid_1's l2: 0.00307023
[92]	training's l2: 0.00221211	valid_1's l2: 0.00307122
[93]	training's l2: 0.00220464	valid_1's l2: 0.00307106
[94]	training's l2: 0.00219576	valid_1's l2: 0.00306833
[95]	training's l2: 0.00218784	valid_1's l2: 0.00306794
[96]	training's l2: 0.00218073	valid_1's l2: 0.00306862
[97]	training's l2: 0.00217307	valid_1's l2: 0.00306835
[98]	training's l2: 0.00216524	valid_1's l2: 0.00306791
[99]	training's l2: 0.00215795	valid_1's l2: 0.00306774
Did not meet early stopping. Best iteration is:
[99]	training's l2: 0.00215795	valid_1's l2: 0.00306774
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176006 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00398418	valid_1's l2: 0.00369492
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00390997	valid_1's l2: 0.0036472
[3]	training's l2: 0.00384101	valid_1's l2: 0.00360321
[4]	training's l2: 0.00377976	valid_1's l2: 0.00356648
[5]	training's l2: 0.00372019	valid_1's l2: 0.00353143
[6]	training's l2: 0.0036692	valid_1's l2: 0.00350105
[7]	training's l2: 0.00361862	valid_1's l2: 0.00347394
[8]	training's l2: 0.00357206	valid_1's l2: 0.00344716
[9]	training's l2: 0.00352765	valid_1's l2: 0.00342355
[10]	training's l2: 0.00348669	valid_1's l2: 0.00340505
[11]	training's l2: 0.00344739	valid_1's l2: 0.00338734
[12]	training's l2: 0.00340892	valid_1's l2: 0.00337125
[13]	training's l2: 0.00337555	valid_1's l2: 0.00335727
[14]	training's l2: 0.00334133	valid_1's l2: 0.00334014
[15]	training's l2: 0.00331182	valid_1's l2: 0.00332688
[16]	training's l2: 0.00328368	valid_1's l2: 0.00331472
[17]	training's l2: 0.00325765	valid_1's l2: 0.00330415
[18]	training's l2: 0.00323164	valid_1's l2: 0.00329216
[19]	training's l2: 0.00320764	valid_1's l2: 0.00328182
[20]	training's l2: 0.00318358	valid_1's l2: 0.00327176
[21]	training's l2: 0.00316255	valid_1's l2: 0.0032614
[22]	training's l2: 0.00314059	valid_1's l2: 0.0032526
[23]	training's l2: 0.00312192	valid_1's l2: 0.00324601
[24]	training's l2: 0.00310415	valid_1's l2: 0.00323914
[25]	training's l2: 0.00308587	valid_1's l2: 0.00323095
[26]	training's l2: 0.00306929	valid_1's l2: 0.00322432
[27]	training's l2: 0.00305196	valid_1's l2: 0.00321729
[28]	training's l2: 0.00303603	valid_1's l2: 0.00321252
[29]	training's l2: 0.00301992	valid_1's l2: 0.00320671
[30]	training's l2: 0.00300638	valid_1's l2: 0.00320415
[31]	training's l2: 0.00299194	valid_1's l2: 0.0032013
[32]	training's l2: 0.00297817	valid_1's l2: 0.0031954
[33]	training's l2: 0.0029651	valid_1's l2: 0.00319214
[34]	training's l2: 0.00295174	valid_1's l2: 0.00318706
[35]	training's l2: 0.00294014	valid_1's l2: 0.00318502
[36]	training's l2: 0.00292698	valid_1's l2: 0.0031815
[37]	training's l2: 0.0029153	valid_1's l2: 0.0031798
[38]	training's l2: 0.00290498	valid_1's l2: 0.00317743
[39]	training's l2: 0.0028935	valid_1's l2: 0.00317475
[40]	training's l2: 0.00288254	valid_1's l2: 0.00317204
[41]	training's l2: 0.0028727	valid_1's l2: 0.00316815
[42]	training's l2: 0.00286267	valid_1's l2: 0.00316673
[43]	training's l2: 0.00285232	valid_1's l2: 0.00316508
[44]	training's l2: 0.00284331	valid_1's l2: 0.00316316
[45]	training's l2: 0.0028331	valid_1's l2: 0.00315982
[46]	training's l2: 0.00282292	valid_1's l2: 0.00315584
[47]	training's l2: 0.00281344	valid_1's l2: 0.00315411
[48]	training's l2: 0.00280403	valid_1's l2: 0.00315112
[49]	training's l2: 0.00279539	valid_1's l2: 0.00314907
[50]	training's l2: 0.00278604	valid_1's l2: 0.00314684
[51]	training's l2: 0.00277763	valid_1's l2: 0.00314444
[52]	training's l2: 0.00276873	valid_1's l2: 0.00314263
[53]	training's l2: 0.00275961	valid_1's l2: 0.00314108
[54]	training's l2: 0.0027514	valid_1's l2: 0.00313772
[55]	training's l2: 0.00274308	valid_1's l2: 0.00313551
[56]	training's l2: 0.00273462	valid_1's l2: 0.00313451
[57]	training's l2: 0.00272624	valid_1's l2: 0.00313235
[58]	training's l2: 0.00271886	valid_1's l2: 0.00313118
[59]	training's l2: 0.00271145	valid_1's l2: 0.00312973
[60]	training's l2: 0.00270358	valid_1's l2: 0.0031283
[61]	training's l2: 0.00269584	valid_1's l2: 0.00312718
[62]	training's l2: 0.00268874	valid_1's l2: 0.00312611
[63]	training's l2: 0.00268094	valid_1's l2: 0.00312409
[64]	training's l2: 0.0026728	valid_1's l2: 0.00312214
[65]	training's l2: 0.0026657	valid_1's l2: 0.00312176
[66]	training's l2: 0.00265873	valid_1's l2: 0.00312116
[67]	training's l2: 0.00265002	valid_1's l2: 0.00311803
[68]	training's l2: 0.0026419	valid_1's l2: 0.00311515
[69]	training's l2: 0.00263504	valid_1's l2: 0.00311345
[70]	training's l2: 0.00262822	valid_1's l2: 0.00311325
[71]	training's l2: 0.0026214	valid_1's l2: 0.00311286
[72]	training's l2: 0.00261502	valid_1's l2: 0.00311342
[73]	training's l2: 0.00260806	valid_1's l2: 0.00311177
[74]	training's l2: 0.00260187	valid_1's l2: 0.00311079
[75]	training's l2: 0.00259413	valid_1's l2: 0.00310856
[76]	training's l2: 0.00258787	valid_1's l2: 0.00310847
[77]	training's l2: 0.00257997	valid_1's l2: 0.003105
[78]	training's l2: 0.00257347	valid_1's l2: 0.0031035
[79]	training's l2: 0.00256634	valid_1's l2: 0.00310032
[80]	training's l2: 0.00255947	valid_1's l2: 0.00309984
[81]	training's l2: 0.00255191	valid_1's l2: 0.00309799
[82]	training's l2: 0.00254473	valid_1's l2: 0.00309741
[83]	training's l2: 0.00253897	valid_1's l2: 0.00309633
[84]	training's l2: 0.00253267	valid_1's l2: 0.00309529
[85]	training's l2: 0.00252703	valid_1's l2: 0.00309563
[86]	training's l2: 0.00252133	valid_1's l2: 0.00309521
[87]	training's l2: 0.00251437	valid_1's l2: 0.00309269
[88]	training's l2: 0.00250857	valid_1's l2: 0.00309221
[89]	training's l2: 0.00250202	valid_1's l2: 0.00309193
[90]	training's l2: 0.00249562	valid_1's l2: 0.00309075
[91]	training's l2: 0.00249006	valid_1's l2: 0.00309087
[92]	training's l2: 0.00248389	valid_1's l2: 0.00308961
[93]	training's l2: 0.00247829	valid_1's l2: 0.003089
[94]	training's l2: 0.0024726	valid_1's l2: 0.00308908
[95]	training's l2: 0.00246663	valid_1's l2: 0.00308937
[96]	training's l2: 0.00246085	valid_1's l2: 0.00308871
[97]	training's l2: 0.00245572	valid_1's l2: 0.00308802
[98]	training's l2: 0.00245057	valid_1's l2: 0.00308688
[99]	training's l2: 0.00244487	valid_1's l2: 0.00308688
[100]	training's l2: 0.00243942	valid_1's l2: 0.00308689
[101]	training's l2: 0.00243389	valid_1's l2: 0.00308641
[102]	training's l2: 0.00242832	valid_1's l2: 0.00308696
[103]	training's l2: 0.00242282	valid_1's l2: 0.00308665
[104]	training's l2: 0.00241731	valid_1's l2: 0.00308577
[105]	training's l2: 0.0024118	valid_1's l2: 0.0030851
[106]	training's l2: 0.00240647	valid_1's l2: 0.00308449
[107]	training's l2: 0.00240156	valid_1's l2: 0.00308406
[108]	training's l2: 0.00239602	valid_1's l2: 0.00308348
[109]	training's l2: 0.00239124	valid_1's l2: 0.00308288
[110]	training's l2: 0.0023861	valid_1's l2: 0.00308281
[111]	training's l2: 0.00238088	valid_1's l2: 0.00308212
[112]	training's l2: 0.00237588	valid_1's l2: 0.00308204
[113]	training's l2: 0.00237076	valid_1's l2: 0.00308149
[114]	training's l2: 0.00236549	valid_1's l2: 0.00308126
[115]	training's l2: 0.00236031	valid_1's l2: 0.00308095
[116]	training's l2: 0.00235606	valid_1's l2: 0.00308069
[117]	training's l2: 0.00235149	valid_1's l2: 0.00308062
[118]	training's l2: 0.00234707	valid_1's l2: 0.00308011
[119]	training's l2: 0.00234246	valid_1's l2: 0.00308033
[120]	training's l2: 0.00233768	valid_1's l2: 0.00308022
[121]	training's l2: 0.0023322	valid_1's l2: 0.00307877
[122]	training's l2: 0.00232727	valid_1's l2: 0.00307792
[123]	training's l2: 0.00232223	valid_1's l2: 0.00307737
[124]	training's l2: 0.00231741	valid_1's l2: 0.00307728
[125]	training's l2: 0.00231249	valid_1's l2: 0.00307726
Did not meet early stopping. Best iteration is:
[125]	training's l2: 0.00231249	valid_1's l2: 0.00307726
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191018 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.002340
[1]	training's l2: 0.00394296	valid_1's l2: 0.00366245
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.00383506	valid_1's l2: 0.00358971
[3]	training's l2: 0.00374693	valid_1's l2: 0.00353523
[4]	training's l2: 0.00366459	valid_1's l2: 0.00348694
[5]	training's l2: 0.00358533	valid_1's l2: 0.00343821
[6]	training's l2: 0.00351896	valid_1's l2: 0.00340048
[7]	training's l2: 0.00346045	valid_1's l2: 0.00336967
[8]	training's l2: 0.00340525	valid_1's l2: 0.00334212
[9]	training's l2: 0.00335189	valid_1's l2: 0.00332058
[10]	training's l2: 0.00330768	valid_1's l2: 0.00329854
[11]	training's l2: 0.00326644	valid_1's l2: 0.00327924
[12]	training's l2: 0.00322886	valid_1's l2: 0.00326349
[13]	training's l2: 0.00319511	valid_1's l2: 0.00325507
[14]	training's l2: 0.00316553	valid_1's l2: 0.00324885
[15]	training's l2: 0.00313402	valid_1's l2: 0.0032337
[16]	training's l2: 0.00310656	valid_1's l2: 0.00322384
[17]	training's l2: 0.00308038	valid_1's l2: 0.00321409
[18]	training's l2: 0.00305704	valid_1's l2: 0.00320682
[19]	training's l2: 0.00303401	valid_1's l2: 0.00320347
[20]	training's l2: 0.00300898	valid_1's l2: 0.0031933
[21]	training's l2: 0.00298773	valid_1's l2: 0.00318931
[22]	training's l2: 0.00296906	valid_1's l2: 0.00318532
[23]	training's l2: 0.0029511	valid_1's l2: 0.00318188
[24]	training's l2: 0.00293039	valid_1's l2: 0.00317541
[25]	training's l2: 0.00291337	valid_1's l2: 0.00317348
[26]	training's l2: 0.00289578	valid_1's l2: 0.00316826
[27]	training's l2: 0.00288095	valid_1's l2: 0.00316397
[28]	training's l2: 0.00286415	valid_1's l2: 0.003161
[29]	training's l2: 0.00284863	valid_1's l2: 0.00315961
[30]	training's l2: 0.00283425	valid_1's l2: 0.00315845
[31]	training's l2: 0.00282087	valid_1's l2: 0.00315752
[32]	training's l2: 0.00280552	valid_1's l2: 0.00315382
[33]	training's l2: 0.00279235	valid_1's l2: 0.00314901
[34]	training's l2: 0.0027802	valid_1's l2: 0.00314647
[35]	training's l2: 0.00276576	valid_1's l2: 0.00314141
[36]	training's l2: 0.00275246	valid_1's l2: 0.00313914
[37]	training's l2: 0.00273717	valid_1's l2: 0.00313303
[38]	training's l2: 0.002724	valid_1's l2: 0.00313056
[39]	training's l2: 0.00271246	valid_1's l2: 0.00312867
[40]	training's l2: 0.00269886	valid_1's l2: 0.00312735
[41]	training's l2: 0.00268614	valid_1's l2: 0.00312424
[42]	training's l2: 0.00267398	valid_1's l2: 0.00312341
[43]	training's l2: 0.00266042	valid_1's l2: 0.0031182
[44]	training's l2: 0.00264929	valid_1's l2: 0.00311513
[45]	training's l2: 0.0026374	valid_1's l2: 0.00311131
[46]	training's l2: 0.00262596	valid_1's l2: 0.00311035
[47]	training's l2: 0.00261425	valid_1's l2: 0.00310744
[48]	training's l2: 0.00260395	valid_1's l2: 0.00310636
[49]	training's l2: 0.00259255	valid_1's l2: 0.00310442
[50]	training's l2: 0.00258277	valid_1's l2: 0.00310408
[51]	training's l2: 0.00257303	valid_1's l2: 0.00310511
[52]	training's l2: 0.00256273	valid_1's l2: 0.00310296
[53]	training's l2: 0.00255189	valid_1's l2: 0.00310159
[54]	training's l2: 0.0025421	valid_1's l2: 0.00310079
[55]	training's l2: 0.00253284	valid_1's l2: 0.00310035
[56]	training's l2: 0.0025238	valid_1's l2: 0.00309998
[57]	training's l2: 0.00251503	valid_1's l2: 0.00309839
[58]	training's l2: 0.00250525	valid_1's l2: 0.00309849
[59]	training's l2: 0.00249562	valid_1's l2: 0.00309825
[60]	training's l2: 0.00248646	valid_1's l2: 0.00309828
[61]	training's l2: 0.00247766	valid_1's l2: 0.00309706
[62]	training's l2: 0.00246904	valid_1's l2: 0.0030968
[63]	training's l2: 0.00246011	valid_1's l2: 0.00309638
[64]	training's l2: 0.00245172	valid_1's l2: 0.00309501
[65]	training's l2: 0.00244199	valid_1's l2: 0.00309516
[66]	training's l2: 0.00243245	valid_1's l2: 0.00309402
[67]	training's l2: 0.00242423	valid_1's l2: 0.00309469
[68]	training's l2: 0.00241574	valid_1's l2: 0.00309222
[69]	training's l2: 0.00240797	valid_1's l2: 0.00309162
[70]	training's l2: 0.00239921	valid_1's l2: 0.0030922
[71]	training's l2: 0.00239041	valid_1's l2: 0.00309067
[72]	training's l2: 0.00238153	valid_1's l2: 0.00308977
[73]	training's l2: 0.00237317	valid_1's l2: 0.00308966
[74]	training's l2: 0.00236469	valid_1's l2: 0.00308822
[75]	training's l2: 0.00235599	valid_1's l2: 0.00308872
[76]	training's l2: 0.00234841	valid_1's l2: 0.00308973
[77]	training's l2: 0.0023397	valid_1's l2: 0.00308897
[78]	training's l2: 0.00233143	valid_1's l2: 0.00308841
[79]	training's l2: 0.00232406	valid_1's l2: 0.00308947
[80]	training's l2: 0.00231584	valid_1's l2: 0.00308938
[81]	training's l2: 0.00230756	valid_1's l2: 0.00308933
[82]	training's l2: 0.00229992	valid_1's l2: 0.00308943
[83]	training's l2: 0.00229283	valid_1's l2: 0.00309004
[84]	training's l2: 0.00228566	valid_1's l2: 0.00309082
[85]	training's l2: 0.0022786	valid_1's l2: 0.00309044
[86]	training's l2: 0.00227128	valid_1's l2: 0.00309002
[87]	training's l2: 0.00226341	valid_1's l2: 0.00308874
[88]	training's l2: 0.00225603	valid_1's l2: 0.00308958
[89]	training's l2: 0.00224885	valid_1's l2: 0.0030916
[90]	training's l2: 0.00224126	valid_1's l2: 0.00309166
[91]	training's l2: 0.00223426	valid_1's l2: 0.0030912
[92]	training's l2: 0.0022276	valid_1's l2: 0.00309174
[93]	training's l2: 0.0022198	valid_1's l2: 0.00308951
[94]	training's l2: 0.00221333	valid_1's l2: 0.00308883
[95]	training's l2: 0.0022059	valid_1's l2: 0.00308979
[96]	training's l2: 0.00219874	valid_1's l2: 0.00309068
[97]	training's l2: 0.0021918	valid_1's l2: 0.00309028
[98]	training's l2: 0.00218466	valid_1's l2: 0.00309104
[99]	training's l2: 0.00217776	valid_1's l2: 0.00309061
[100]	training's l2: 0.00217011	valid_1's l2: 0.00309149
[101]	training's l2: 0.00216322	valid_1's l2: 0.00309076
[102]	training's l2: 0.00215626	valid_1's l2: 0.00308987
[103]	training's l2: 0.00214989	valid_1's l2: 0.00308977
[104]	training's l2: 0.00214302	valid_1's l2: 0.00309072
Early stopping, best iteration is:
[74]	training's l2: 0.00236469	valid_1's l2: 0.00308822
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.176319 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.697073	valid_1's l2: 0.699182
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.674685	valid_1's l2: 0.677392
[3]	training's l2: 0.653994	valid_1's l2: 0.657513
[4]	training's l2: 0.635576	valid_1's l2: 0.640336
[5]	training's l2: 0.618395	valid_1's l2: 0.623975
[6]	training's l2: 0.602941	valid_1's l2: 0.609517
[7]	training's l2: 0.589122	valid_1's l2: 0.596125
[8]	training's l2: 0.576033	valid_1's l2: 0.584439
[9]	training's l2: 0.564347	valid_1's l2: 0.573408
[10]	training's l2: 0.55332	valid_1's l2: 0.563665
[11]	training's l2: 0.543651	valid_1's l2: 0.554734
[12]	training's l2: 0.534546	valid_1's l2: 0.546737
[13]	training's l2: 0.526228	valid_1's l2: 0.540066
[14]	training's l2: 0.518315	valid_1's l2: 0.533851
[15]	training's l2: 0.510949	valid_1's l2: 0.527512
[16]	training's l2: 0.504051	valid_1's l2: 0.522256
[17]	training's l2: 0.497528	valid_1's l2: 0.51703
[18]	training's l2: 0.491865	valid_1's l2: 0.512077
[19]	training's l2: 0.486227	valid_1's l2: 0.508019
[20]	training's l2: 0.481238	valid_1's l2: 0.50428
[21]	training's l2: 0.476635	valid_1's l2: 0.501019
[22]	training's l2: 0.471794	valid_1's l2: 0.497305
[23]	training's l2: 0.467596	valid_1's l2: 0.494767
[24]	training's l2: 0.463052	valid_1's l2: 0.491852
[25]	training's l2: 0.458879	valid_1's l2: 0.488742
[26]	training's l2: 0.454932	valid_1's l2: 0.486127
[27]	training's l2: 0.451029	valid_1's l2: 0.483714
[28]	training's l2: 0.447228	valid_1's l2: 0.481207
[29]	training's l2: 0.444055	valid_1's l2: 0.479685
[30]	training's l2: 0.441048	valid_1's l2: 0.477619
[31]	training's l2: 0.438208	valid_1's l2: 0.476141
[32]	training's l2: 0.435366	valid_1's l2: 0.474413
[33]	training's l2: 0.432271	valid_1's l2: 0.47242
[34]	training's l2: 0.429034	valid_1's l2: 0.470262
[35]	training's l2: 0.426305	valid_1's l2: 0.46871
[36]	training's l2: 0.423512	valid_1's l2: 0.467131
[37]	training's l2: 0.420524	valid_1's l2: 0.46533
[38]	training's l2: 0.417917	valid_1's l2: 0.463901
[39]	training's l2: 0.4156	valid_1's l2: 0.462785
[40]	training's l2: 0.413365	valid_1's l2: 0.461817
[41]	training's l2: 0.411433	valid_1's l2: 0.461102
[42]	training's l2: 0.409448	valid_1's l2: 0.460428
[43]	training's l2: 0.406811	valid_1's l2: 0.459169
[44]	training's l2: 0.404569	valid_1's l2: 0.458377
[45]	training's l2: 0.402682	valid_1's l2: 0.457613
[46]	training's l2: 0.400688	valid_1's l2: 0.456577
[47]	training's l2: 0.398539	valid_1's l2: 0.455504
[48]	training's l2: 0.396744	valid_1's l2: 0.454722
[49]	training's l2: 0.395097	valid_1's l2: 0.454082
[50]	training's l2: 0.393463	valid_1's l2: 0.453594
[51]	training's l2: 0.391639	valid_1's l2: 0.452944
[52]	training's l2: 0.390048	valid_1's l2: 0.452549
[53]	training's l2: 0.388226	valid_1's l2: 0.45192
[54]	training's l2: 0.386562	valid_1's l2: 0.451607
[55]	training's l2: 0.384565	valid_1's l2: 0.450199
[56]	training's l2: 0.382197	valid_1's l2: 0.449042
[57]	training's l2: 0.379851	valid_1's l2: 0.447676
[58]	training's l2: 0.378279	valid_1's l2: 0.447346
[59]	training's l2: 0.376648	valid_1's l2: 0.446924
[60]	training's l2: 0.375103	valid_1's l2: 0.446468
[61]	training's l2: 0.373569	valid_1's l2: 0.446376
[62]	training's l2: 0.371899	valid_1's l2: 0.445433
[63]	training's l2: 0.370547	valid_1's l2: 0.44509
[64]	training's l2: 0.369255	valid_1's l2: 0.444902
[65]	training's l2: 0.367974	valid_1's l2: 0.444563
[66]	training's l2: 0.366175	valid_1's l2: 0.443923
[67]	training's l2: 0.364484	valid_1's l2: 0.443161
[68]	training's l2: 0.363189	valid_1's l2: 0.442883
[69]	training's l2: 0.361974	valid_1's l2: 0.442403
[70]	training's l2: 0.360297	valid_1's l2: 0.441678
[71]	training's l2: 0.35904	valid_1's l2: 0.441402
[72]	training's l2: 0.357591	valid_1's l2: 0.440777
[73]	training's l2: 0.356052	valid_1's l2: 0.440302
[74]	training's l2: 0.354732	valid_1's l2: 0.439985
[75]	training's l2: 0.353156	valid_1's l2: 0.439122
[76]	training's l2: 0.351966	valid_1's l2: 0.438995
[77]	training's l2: 0.350778	valid_1's l2: 0.438472
[78]	training's l2: 0.349619	valid_1's l2: 0.438293
[79]	training's l2: 0.348048	valid_1's l2: 0.437427
[80]	training's l2: 0.347017	valid_1's l2: 0.437435
[81]	training's l2: 0.345925	valid_1's l2: 0.437104
[82]	training's l2: 0.344504	valid_1's l2: 0.43616
[83]	training's l2: 0.343101	valid_1's l2: 0.435584
[84]	training's l2: 0.341974	valid_1's l2: 0.435328
[85]	training's l2: 0.340956	valid_1's l2: 0.435176
[86]	training's l2: 0.339736	valid_1's l2: 0.434942
[87]	training's l2: 0.338693	valid_1's l2: 0.434935
[88]	training's l2: 0.337605	valid_1's l2: 0.434496
[89]	training's l2: 0.336614	valid_1's l2: 0.434232
[90]	training's l2: 0.335731	valid_1's l2: 0.434099
[91]	training's l2: 0.334504	valid_1's l2: 0.433845
[92]	training's l2: 0.333383	valid_1's l2: 0.433473
[93]	training's l2: 0.332483	valid_1's l2: 0.433413
[94]	training's l2: 0.331551	valid_1's l2: 0.433534
[95]	training's l2: 0.330586	valid_1's l2: 0.433372
[96]	training's l2: 0.32967	valid_1's l2: 0.43339
[97]	training's l2: 0.328579	valid_1's l2: 0.433209
[98]	training's l2: 0.327615	valid_1's l2: 0.432933
[99]	training's l2: 0.326561	valid_1's l2: 0.432498
[100]	training's l2: 0.325512	valid_1's l2: 0.432382
[101]	training's l2: 0.324584	valid_1's l2: 0.432165
[102]	training's l2: 0.323649	valid_1's l2: 0.431814
[103]	training's l2: 0.322693	valid_1's l2: 0.431741
[104]	training's l2: 0.321802	valid_1's l2: 0.431559
[105]	training's l2: 0.320949	valid_1's l2: 0.431373
[106]	training's l2: 0.320016	valid_1's l2: 0.431262
[107]	training's l2: 0.319138	valid_1's l2: 0.431224
[108]	training's l2: 0.318213	valid_1's l2: 0.430948
[109]	training's l2: 0.317259	valid_1's l2: 0.430678
[110]	training's l2: 0.316422	valid_1's l2: 0.43059
[111]	training's l2: 0.315443	valid_1's l2: 0.430358
[112]	training's l2: 0.314639	valid_1's l2: 0.430233
[113]	training's l2: 0.313781	valid_1's l2: 0.430156
[114]	training's l2: 0.313042	valid_1's l2: 0.430036
[115]	training's l2: 0.312121	valid_1's l2: 0.429831
[116]	training's l2: 0.311034	valid_1's l2: 0.429264
[117]	training's l2: 0.310269	valid_1's l2: 0.429053
[118]	training's l2: 0.309319	valid_1's l2: 0.429202
[119]	training's l2: 0.308543	valid_1's l2: 0.429283
[120]	training's l2: 0.307768	valid_1's l2: 0.42925
[121]	training's l2: 0.306955	valid_1's l2: 0.429209
[122]	training's l2: 0.306196	valid_1's l2: 0.429153
[123]	training's l2: 0.305301	valid_1's l2: 0.429186
[124]	training's l2: 0.304421	valid_1's l2: 0.428815
[125]	training's l2: 0.303684	valid_1's l2: 0.428699
[126]	training's l2: 0.302914	valid_1's l2: 0.42874
[127]	training's l2: 0.302234	valid_1's l2: 0.428696
[128]	training's l2: 0.301399	valid_1's l2: 0.428701
[129]	training's l2: 0.300706	valid_1's l2: 0.42878
[130]	training's l2: 0.299888	valid_1's l2: 0.428541
[131]	training's l2: 0.299153	valid_1's l2: 0.42846
[132]	training's l2: 0.298356	valid_1's l2: 0.428512
[133]	training's l2: 0.297512	valid_1's l2: 0.428486
[134]	training's l2: 0.296823	valid_1's l2: 0.42856
[135]	training's l2: 0.296179	valid_1's l2: 0.428605
[136]	training's l2: 0.295382	valid_1's l2: 0.428221
[137]	training's l2: 0.294674	valid_1's l2: 0.428219
[138]	training's l2: 0.293943	valid_1's l2: 0.428058
[139]	training's l2: 0.293223	valid_1's l2: 0.427968
[140]	training's l2: 0.292558	valid_1's l2: 0.428018
[141]	training's l2: 0.291758	valid_1's l2: 0.427921
[142]	training's l2: 0.291102	valid_1's l2: 0.4279
[143]	training's l2: 0.290308	valid_1's l2: 0.427836
[144]	training's l2: 0.289646	valid_1's l2: 0.427917
[145]	training's l2: 0.288909	valid_1's l2: 0.427894
[146]	training's l2: 0.288264	valid_1's l2: 0.427858
[147]	training's l2: 0.287643	valid_1's l2: 0.427809
[148]	training's l2: 0.286984	valid_1's l2: 0.427769
[149]	training's l2: 0.286286	valid_1's l2: 0.427789
[150]	training's l2: 0.285539	valid_1's l2: 0.427834
[151]	training's l2: 0.284742	valid_1's l2: 0.427715
[152]	training's l2: 0.284101	valid_1's l2: 0.427645
[153]	training's l2: 0.283519	valid_1's l2: 0.427738
[154]	training's l2: 0.282862	valid_1's l2: 0.427711
[155]	training's l2: 0.282274	valid_1's l2: 0.427573
[156]	training's l2: 0.281621	valid_1's l2: 0.427391
[157]	training's l2: 0.280986	valid_1's l2: 0.427367
[158]	training's l2: 0.280383	valid_1's l2: 0.427283
[159]	training's l2: 0.279765	valid_1's l2: 0.427255
[160]	training's l2: 0.279048	valid_1's l2: 0.427194
[161]	training's l2: 0.278399	valid_1's l2: 0.427186
[162]	training's l2: 0.277743	valid_1's l2: 0.427101
[163]	training's l2: 0.277173	valid_1's l2: 0.42722
[164]	training's l2: 0.276585	valid_1's l2: 0.427071
[165]	training's l2: 0.276004	valid_1's l2: 0.427152
[166]	training's l2: 0.275378	valid_1's l2: 0.427177
[167]	training's l2: 0.274759	valid_1's l2: 0.427171
[168]	training's l2: 0.274132	valid_1's l2: 0.427169
[169]	training's l2: 0.273562	valid_1's l2: 0.427088
[170]	training's l2: 0.272919	valid_1's l2: 0.426839
[171]	training's l2: 0.272252	valid_1's l2: 0.426977
[172]	training's l2: 0.271616	valid_1's l2: 0.426932
[173]	training's l2: 0.271052	valid_1's l2: 0.426969
[174]	training's l2: 0.270359	valid_1's l2: 0.426888
[175]	training's l2: 0.269819	valid_1's l2: 0.426835
[176]	training's l2: 0.269299	valid_1's l2: 0.426699
[177]	training's l2: 0.268677	valid_1's l2: 0.42669
[178]	training's l2: 0.268128	valid_1's l2: 0.426642
[179]	training's l2: 0.267526	valid_1's l2: 0.426734
[180]	training's l2: 0.26698	valid_1's l2: 0.426635
[181]	training's l2: 0.266428	valid_1's l2: 0.426561
[182]	training's l2: 0.265796	valid_1's l2: 0.42664
[183]	training's l2: 0.265222	valid_1's l2: 0.42659
[184]	training's l2: 0.264543	valid_1's l2: 0.426453
[185]	training's l2: 0.264001	valid_1's l2: 0.426288
[186]	training's l2: 0.263369	valid_1's l2: 0.426326
[187]	training's l2: 0.262829	valid_1's l2: 0.426239
[188]	training's l2: 0.262237	valid_1's l2: 0.426137
[189]	training's l2: 0.2616	valid_1's l2: 0.426067
Did not meet early stopping. Best iteration is:
[189]	training's l2: 0.2616	valid_1's l2: 0.426067
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185819 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.690559	valid_1's l2: 0.692438
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.663571	valid_1's l2: 0.665369
[3]	training's l2: 0.639025	valid_1's l2: 0.64117
[4]	training's l2: 0.618229	valid_1's l2: 0.620578
[5]	training's l2: 0.599789	valid_1's l2: 0.602917
[6]	training's l2: 0.582342	valid_1's l2: 0.586931
[7]	training's l2: 0.5681	valid_1's l2: 0.573604
[8]	training's l2: 0.554722	valid_1's l2: 0.561716
[9]	training's l2: 0.54312	valid_1's l2: 0.551395
[10]	training's l2: 0.532608	valid_1's l2: 0.541903
[11]	training's l2: 0.523042	valid_1's l2: 0.533261
[12]	training's l2: 0.513857	valid_1's l2: 0.52658
[13]	training's l2: 0.50554	valid_1's l2: 0.519725
[14]	training's l2: 0.498577	valid_1's l2: 0.514038
[15]	training's l2: 0.491853	valid_1's l2: 0.508622
[16]	training's l2: 0.485723	valid_1's l2: 0.503751
[17]	training's l2: 0.480049	valid_1's l2: 0.499683
[18]	training's l2: 0.474403	valid_1's l2: 0.494767
[19]	training's l2: 0.469546	valid_1's l2: 0.4916
[20]	training's l2: 0.464492	valid_1's l2: 0.488499
[21]	training's l2: 0.459772	valid_1's l2: 0.485335
[22]	training's l2: 0.455874	valid_1's l2: 0.483156
[23]	training's l2: 0.451242	valid_1's l2: 0.479829
[24]	training's l2: 0.447496	valid_1's l2: 0.477857
[25]	training's l2: 0.444113	valid_1's l2: 0.475575
[26]	training's l2: 0.440788	valid_1's l2: 0.473383
[27]	training's l2: 0.437673	valid_1's l2: 0.471478
[28]	training's l2: 0.434426	valid_1's l2: 0.469766
[29]	training's l2: 0.431424	valid_1's l2: 0.467821
[30]	training's l2: 0.427743	valid_1's l2: 0.465602
[31]	training's l2: 0.424973	valid_1's l2: 0.464546
[32]	training's l2: 0.421748	valid_1's l2: 0.462549
[33]	training's l2: 0.418973	valid_1's l2: 0.461459
[34]	training's l2: 0.41684	valid_1's l2: 0.460383
[35]	training's l2: 0.41413	valid_1's l2: 0.458786
[36]	training's l2: 0.411417	valid_1's l2: 0.457768
[37]	training's l2: 0.408751	valid_1's l2: 0.456149
[38]	training's l2: 0.40669	valid_1's l2: 0.455563
[39]	training's l2: 0.404943	valid_1's l2: 0.455039
[40]	training's l2: 0.402204	valid_1's l2: 0.453115
[41]	training's l2: 0.40046	valid_1's l2: 0.452595
[42]	training's l2: 0.397317	valid_1's l2: 0.450746
[43]	training's l2: 0.395669	valid_1's l2: 0.450176
[44]	training's l2: 0.393606	valid_1's l2: 0.449199
[45]	training's l2: 0.392012	valid_1's l2: 0.448959
[46]	training's l2: 0.390273	valid_1's l2: 0.448462
[47]	training's l2: 0.388134	valid_1's l2: 0.447347
[48]	training's l2: 0.386481	valid_1's l2: 0.446754
[49]	training's l2: 0.384296	valid_1's l2: 0.445437
[50]	training's l2: 0.382478	valid_1's l2: 0.443704
[51]	training's l2: 0.380911	valid_1's l2: 0.443388
[52]	training's l2: 0.379309	valid_1's l2: 0.442796
[53]	training's l2: 0.377368	valid_1's l2: 0.442195
[54]	training's l2: 0.376069	valid_1's l2: 0.441745
[55]	training's l2: 0.374591	valid_1's l2: 0.441521
[56]	training's l2: 0.373032	valid_1's l2: 0.44084
[57]	training's l2: 0.370689	valid_1's l2: 0.439575
[58]	training's l2: 0.369373	valid_1's l2: 0.439477
[59]	training's l2: 0.36811	valid_1's l2: 0.43916
[60]	training's l2: 0.366747	valid_1's l2: 0.438832
[61]	training's l2: 0.365012	valid_1's l2: 0.43841
[62]	training's l2: 0.363048	valid_1's l2: 0.43711
[63]	training's l2: 0.361509	valid_1's l2: 0.436259
[64]	training's l2: 0.360258	valid_1's l2: 0.435872
[65]	training's l2: 0.358631	valid_1's l2: 0.435387
[66]	training's l2: 0.35751	valid_1's l2: 0.435074
[67]	training's l2: 0.356303	valid_1's l2: 0.434871
[68]	training's l2: 0.355015	valid_1's l2: 0.434534
[69]	training's l2: 0.353723	valid_1's l2: 0.434197
[70]	training's l2: 0.352225	valid_1's l2: 0.43329
[71]	training's l2: 0.351123	valid_1's l2: 0.433186
[72]	training's l2: 0.349791	valid_1's l2: 0.432685
[73]	training's l2: 0.348733	valid_1's l2: 0.432295
[74]	training's l2: 0.347182	valid_1's l2: 0.431581
[75]	training's l2: 0.345988	valid_1's l2: 0.431653
[76]	training's l2: 0.344929	valid_1's l2: 0.431612
[77]	training's l2: 0.34374	valid_1's l2: 0.431153
[78]	training's l2: 0.342703	valid_1's l2: 0.431155
[79]	training's l2: 0.341608	valid_1's l2: 0.430803
[80]	training's l2: 0.34068	valid_1's l2: 0.430705
[81]	training's l2: 0.339644	valid_1's l2: 0.430498
[82]	training's l2: 0.338599	valid_1's l2: 0.430358
[83]	training's l2: 0.337333	valid_1's l2: 0.429989
[84]	training's l2: 0.336384	valid_1's l2: 0.429931
[85]	training's l2: 0.335415	valid_1's l2: 0.42982
[86]	training's l2: 0.33424	valid_1's l2: 0.429345
[87]	training's l2: 0.333264	valid_1's l2: 0.429271
[88]	training's l2: 0.332373	valid_1's l2: 0.429122
[89]	training's l2: 0.331429	valid_1's l2: 0.429229
[90]	training's l2: 0.330371	valid_1's l2: 0.429095
[91]	training's l2: 0.329501	valid_1's l2: 0.429144
[92]	training's l2: 0.328449	valid_1's l2: 0.428814
[93]	training's l2: 0.327444	valid_1's l2: 0.428821
[94]	training's l2: 0.326624	valid_1's l2: 0.429074
[95]	training's l2: 0.32563	valid_1's l2: 0.428734
[96]	training's l2: 0.324708	valid_1's l2: 0.428637
[97]	training's l2: 0.323858	valid_1's l2: 0.428648
[98]	training's l2: 0.322872	valid_1's l2: 0.428351
[99]	training's l2: 0.321964	valid_1's l2: 0.428282
[100]	training's l2: 0.321111	valid_1's l2: 0.428468
[101]	training's l2: 0.320319	valid_1's l2: 0.428207
[102]	training's l2: 0.319569	valid_1's l2: 0.428268
[103]	training's l2: 0.318763	valid_1's l2: 0.428442
[104]	training's l2: 0.317987	valid_1's l2: 0.428498
[105]	training's l2: 0.316973	valid_1's l2: 0.427865
[106]	training's l2: 0.316276	valid_1's l2: 0.427847
[107]	training's l2: 0.315514	valid_1's l2: 0.427938
[108]	training's l2: 0.31479	valid_1's l2: 0.427981
[109]	training's l2: 0.314112	valid_1's l2: 0.427889
[110]	training's l2: 0.313301	valid_1's l2: 0.427914
[111]	training's l2: 0.312504	valid_1's l2: 0.427983
[112]	training's l2: 0.311593	valid_1's l2: 0.427956
[113]	training's l2: 0.310877	valid_1's l2: 0.4281
[114]	training's l2: 0.31011	valid_1's l2: 0.427893
[115]	training's l2: 0.309287	valid_1's l2: 0.427747
[116]	training's l2: 0.308588	valid_1's l2: 0.427724
[117]	training's l2: 0.307841	valid_1's l2: 0.427668
[118]	training's l2: 0.307147	valid_1's l2: 0.427489
[119]	training's l2: 0.306539	valid_1's l2: 0.427803
[120]	training's l2: 0.305867	valid_1's l2: 0.427827
[121]	training's l2: 0.305136	valid_1's l2: 0.42787
[122]	training's l2: 0.304355	valid_1's l2: 0.427818
[123]	training's l2: 0.303665	valid_1's l2: 0.427763
[124]	training's l2: 0.30272	valid_1's l2: 0.427804
[125]	training's l2: 0.301872	valid_1's l2: 0.427734
[126]	training's l2: 0.301106	valid_1's l2: 0.427666
[127]	training's l2: 0.300427	valid_1's l2: 0.42752
[128]	training's l2: 0.299732	valid_1's l2: 0.427558
[129]	training's l2: 0.299087	valid_1's l2: 0.427747
[130]	training's l2: 0.298392	valid_1's l2: 0.427858
[131]	training's l2: 0.29783	valid_1's l2: 0.427752
[132]	training's l2: 0.29701	valid_1's l2: 0.427239
[133]	training's l2: 0.296174	valid_1's l2: 0.427154
[134]	training's l2: 0.295559	valid_1's l2: 0.42708
[135]	training's l2: 0.2949	valid_1's l2: 0.427187
[136]	training's l2: 0.294183	valid_1's l2: 0.427199
[137]	training's l2: 0.293413	valid_1's l2: 0.427103
[138]	training's l2: 0.292839	valid_1's l2: 0.427261
[139]	training's l2: 0.292212	valid_1's l2: 0.427299
[140]	training's l2: 0.291677	valid_1's l2: 0.427318
[141]	training's l2: 0.29117	valid_1's l2: 0.427287
[142]	training's l2: 0.290548	valid_1's l2: 0.427324
[143]	training's l2: 0.289906	valid_1's l2: 0.427386
[144]	training's l2: 0.289132	valid_1's l2: 0.427366
[145]	training's l2: 0.28855	valid_1's l2: 0.427502
[146]	training's l2: 0.288002	valid_1's l2: 0.427622
[147]	training's l2: 0.287296	valid_1's l2: 0.427908
[148]	training's l2: 0.286623	valid_1's l2: 0.428
[149]	training's l2: 0.285984	valid_1's l2: 0.428061
[150]	training's l2: 0.285431	valid_1's l2: 0.428196
[151]	training's l2: 0.284711	valid_1's l2: 0.428312
[152]	training's l2: 0.284041	valid_1's l2: 0.428318
[153]	training's l2: 0.283358	valid_1's l2: 0.42819
[154]	training's l2: 0.282676	valid_1's l2: 0.428248
[155]	training's l2: 0.282027	valid_1's l2: 0.428312
[156]	training's l2: 0.281476	valid_1's l2: 0.428435
[157]	training's l2: 0.280851	valid_1's l2: 0.428339
[158]	training's l2: 0.280236	valid_1's l2: 0.428335
[159]	training's l2: 0.279636	valid_1's l2: 0.428126
[160]	training's l2: 0.278919	valid_1's l2: 0.42813
[161]	training's l2: 0.278273	valid_1's l2: 0.428051
[162]	training's l2: 0.277712	valid_1's l2: 0.428086
[163]	training's l2: 0.277132	valid_1's l2: 0.427892
[164]	training's l2: 0.276617	valid_1's l2: 0.427839
Early stopping, best iteration is:
[134]	training's l2: 0.295559	valid_1's l2: 0.42708
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183262 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.693383	valid_1's l2: 0.696508
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.667932	valid_1's l2: 0.672092
[3]	training's l2: 0.644655	valid_1's l2: 0.650186
[4]	training's l2: 0.623945	valid_1's l2: 0.631147
[5]	training's l2: 0.605458	valid_1's l2: 0.613808
[6]	training's l2: 0.589014	valid_1's l2: 0.598668
[7]	training's l2: 0.573479	valid_1's l2: 0.584957
[8]	training's l2: 0.559455	valid_1's l2: 0.572483
[9]	training's l2: 0.546461	valid_1's l2: 0.561405
[10]	training's l2: 0.534359	valid_1's l2: 0.55124
[11]	training's l2: 0.523801	valid_1's l2: 0.542908
[12]	training's l2: 0.513907	valid_1's l2: 0.535311
[13]	training's l2: 0.50499	valid_1's l2: 0.528625
[14]	training's l2: 0.496436	valid_1's l2: 0.522165
[15]	training's l2: 0.488639	valid_1's l2: 0.516362
[16]	training's l2: 0.481039	valid_1's l2: 0.511302
[17]	training's l2: 0.474303	valid_1's l2: 0.506207
[18]	training's l2: 0.468362	valid_1's l2: 0.502161
[19]	training's l2: 0.462285	valid_1's l2: 0.498121
[20]	training's l2: 0.456563	valid_1's l2: 0.494558
[21]	training's l2: 0.451031	valid_1's l2: 0.490894
[22]	training's l2: 0.446447	valid_1's l2: 0.488236
[23]	training's l2: 0.441539	valid_1's l2: 0.485242
[24]	training's l2: 0.436988	valid_1's l2: 0.482533
[25]	training's l2: 0.432719	valid_1's l2: 0.480112
[26]	training's l2: 0.428509	valid_1's l2: 0.477391
[27]	training's l2: 0.424403	valid_1's l2: 0.475201
[28]	training's l2: 0.420734	valid_1's l2: 0.473423
[29]	training's l2: 0.417294	valid_1's l2: 0.472332
[30]	training's l2: 0.414005	valid_1's l2: 0.470995
[31]	training's l2: 0.410607	valid_1's l2: 0.469351
[32]	training's l2: 0.407509	valid_1's l2: 0.468006
[33]	training's l2: 0.404127	valid_1's l2: 0.466234
[34]	training's l2: 0.400751	valid_1's l2: 0.464621
[35]	training's l2: 0.397643	valid_1's l2: 0.463538
[36]	training's l2: 0.394842	valid_1's l2: 0.462306
[37]	training's l2: 0.391853	valid_1's l2: 0.461013
[38]	training's l2: 0.389255	valid_1's l2: 0.459915
[39]	training's l2: 0.386072	valid_1's l2: 0.458371
[40]	training's l2: 0.383592	valid_1's l2: 0.457489
[41]	training's l2: 0.381086	valid_1's l2: 0.456429
[42]	training's l2: 0.377943	valid_1's l2: 0.454736
[43]	training's l2: 0.375525	valid_1's l2: 0.453901
[44]	training's l2: 0.373362	valid_1's l2: 0.453291
[45]	training's l2: 0.371337	valid_1's l2: 0.45319
[46]	training's l2: 0.368869	valid_1's l2: 0.451831
[47]	training's l2: 0.366252	valid_1's l2: 0.450484
[48]	training's l2: 0.36431	valid_1's l2: 0.449966
[49]	training's l2: 0.36226	valid_1's l2: 0.44963
[50]	training's l2: 0.360048	valid_1's l2: 0.448741
[51]	training's l2: 0.357209	valid_1's l2: 0.447269
[52]	training's l2: 0.355402	valid_1's l2: 0.447502
[53]	training's l2: 0.353294	valid_1's l2: 0.446679
[54]	training's l2: 0.350665	valid_1's l2: 0.445212
[55]	training's l2: 0.348871	valid_1's l2: 0.44496
[56]	training's l2: 0.346653	valid_1's l2: 0.443994
[57]	training's l2: 0.34506	valid_1's l2: 0.443781
[58]	training's l2: 0.342975	valid_1's l2: 0.442682
[59]	training's l2: 0.341022	valid_1's l2: 0.442004
[60]	training's l2: 0.339019	valid_1's l2: 0.441289
[61]	training's l2: 0.337377	valid_1's l2: 0.441052
[62]	training's l2: 0.335054	valid_1's l2: 0.439804
[63]	training's l2: 0.333386	valid_1's l2: 0.439754
[64]	training's l2: 0.331674	valid_1's l2: 0.439299
[65]	training's l2: 0.330185	valid_1's l2: 0.439117
[66]	training's l2: 0.328755	valid_1's l2: 0.438887
[67]	training's l2: 0.326786	valid_1's l2: 0.438111
[68]	training's l2: 0.325218	valid_1's l2: 0.437483
[69]	training's l2: 0.323484	valid_1's l2: 0.437247
[70]	training's l2: 0.321997	valid_1's l2: 0.436941
[71]	training's l2: 0.320595	valid_1's l2: 0.436828
[72]	training's l2: 0.31902	valid_1's l2: 0.436362
[73]	training's l2: 0.31754	valid_1's l2: 0.436168
[74]	training's l2: 0.316188	valid_1's l2: 0.435962
[75]	training's l2: 0.314231	valid_1's l2: 0.434768
[76]	training's l2: 0.312962	valid_1's l2: 0.4349
[77]	training's l2: 0.311489	valid_1's l2: 0.434831
[78]	training's l2: 0.310014	valid_1's l2: 0.434301
[79]	training's l2: 0.3086	valid_1's l2: 0.434126
[80]	training's l2: 0.307224	valid_1's l2: 0.433775
[81]	training's l2: 0.305981	valid_1's l2: 0.433783
[82]	training's l2: 0.304456	valid_1's l2: 0.433324
[83]	training's l2: 0.303155	valid_1's l2: 0.433086
[84]	training's l2: 0.301839	valid_1's l2: 0.43286
[85]	training's l2: 0.300318	valid_1's l2: 0.432079
[86]	training's l2: 0.299119	valid_1's l2: 0.432032
[87]	training's l2: 0.297947	valid_1's l2: 0.43212
[88]	training's l2: 0.296471	valid_1's l2: 0.431315
[89]	training's l2: 0.29528	valid_1's l2: 0.431344
[90]	training's l2: 0.294165	valid_1's l2: 0.431373
[91]	training's l2: 0.293074	valid_1's l2: 0.43136
[92]	training's l2: 0.291733	valid_1's l2: 0.430737
[93]	training's l2: 0.290477	valid_1's l2: 0.430551
[94]	training's l2: 0.289402	valid_1's l2: 0.430496
[95]	training's l2: 0.288278	valid_1's l2: 0.430564
[96]	training's l2: 0.287019	valid_1's l2: 0.430274
[97]	training's l2: 0.285944	valid_1's l2: 0.43031
[98]	training's l2: 0.284694	valid_1's l2: 0.430189
[99]	training's l2: 0.283611	valid_1's l2: 0.430367
[100]	training's l2: 0.282483	valid_1's l2: 0.430137
[101]	training's l2: 0.281253	valid_1's l2: 0.429944
[102]	training's l2: 0.280291	valid_1's l2: 0.429966
[103]	training's l2: 0.279266	valid_1's l2: 0.430133
[104]	training's l2: 0.278293	valid_1's l2: 0.430333
[105]	training's l2: 0.277162	valid_1's l2: 0.430226
[106]	training's l2: 0.276227	valid_1's l2: 0.4302
[107]	training's l2: 0.275166	valid_1's l2: 0.430118
[108]	training's l2: 0.274122	valid_1's l2: 0.43001
[109]	training's l2: 0.272984	valid_1's l2: 0.429979
[110]	training's l2: 0.271959	valid_1's l2: 0.429836
[111]	training's l2: 0.270909	valid_1's l2: 0.429879
[112]	training's l2: 0.269889	valid_1's l2: 0.429821
[113]	training's l2: 0.268913	valid_1's l2: 0.429699
[114]	training's l2: 0.267826	valid_1's l2: 0.429384
[115]	training's l2: 0.26683	valid_1's l2: 0.429389
[116]	training's l2: 0.265974	valid_1's l2: 0.429288
[117]	training's l2: 0.26505	valid_1's l2: 0.429193
[118]	training's l2: 0.263812	valid_1's l2: 0.428525
[119]	training's l2: 0.262976	valid_1's l2: 0.428514
[120]	training's l2: 0.26208	valid_1's l2: 0.428572
[121]	training's l2: 0.261184	valid_1's l2: 0.428697
[122]	training's l2: 0.260255	valid_1's l2: 0.42864
[123]	training's l2: 0.25937	valid_1's l2: 0.428493
[124]	training's l2: 0.258428	valid_1's l2: 0.428568
[125]	training's l2: 0.257581	valid_1's l2: 0.428509
[126]	training's l2: 0.2567	valid_1's l2: 0.428383
[127]	training's l2: 0.255875	valid_1's l2: 0.42837
[128]	training's l2: 0.254821	valid_1's l2: 0.428165
[129]	training's l2: 0.254057	valid_1's l2: 0.428322
[130]	training's l2: 0.25309	valid_1's l2: 0.427965
[131]	training's l2: 0.252173	valid_1's l2: 0.427836
[132]	training's l2: 0.251366	valid_1's l2: 0.427768
[133]	training's l2: 0.250468	valid_1's l2: 0.427754
[134]	training's l2: 0.249664	valid_1's l2: 0.42771
[135]	training's l2: 0.248871	valid_1's l2: 0.427756
[136]	training's l2: 0.248022	valid_1's l2: 0.427638
[137]	training's l2: 0.247155	valid_1's l2: 0.427654
[138]	training's l2: 0.246331	valid_1's l2: 0.427679
[139]	training's l2: 0.245482	valid_1's l2: 0.42771
[140]	training's l2: 0.244646	valid_1's l2: 0.427795
[141]	training's l2: 0.24388	valid_1's l2: 0.427711
[142]	training's l2: 0.243148	valid_1's l2: 0.427603
[143]	training's l2: 0.242284	valid_1's l2: 0.427392
[144]	training's l2: 0.241539	valid_1's l2: 0.427352
[145]	training's l2: 0.24079	valid_1's l2: 0.427391
[146]	training's l2: 0.239841	valid_1's l2: 0.426883
[147]	training's l2: 0.239066	valid_1's l2: 0.426867
[148]	training's l2: 0.23828	valid_1's l2: 0.426789
[149]	training's l2: 0.237597	valid_1's l2: 0.426801
[150]	training's l2: 0.236853	valid_1's l2: 0.426871
[151]	training's l2: 0.23602	valid_1's l2: 0.427068
[152]	training's l2: 0.235166	valid_1's l2: 0.427159
[153]	training's l2: 0.23445	valid_1's l2: 0.427071
[154]	training's l2: 0.233749	valid_1's l2: 0.427067
[155]	training's l2: 0.233042	valid_1's l2: 0.426998
[156]	training's l2: 0.232261	valid_1's l2: 0.427148
[157]	training's l2: 0.231621	valid_1's l2: 0.427129
[158]	training's l2: 0.230852	valid_1's l2: 0.427186
[159]	training's l2: 0.230069	valid_1's l2: 0.426722
[160]	training's l2: 0.229215	valid_1's l2: 0.426733
[161]	training's l2: 0.228479	valid_1's l2: 0.426758
[162]	training's l2: 0.22782	valid_1's l2: 0.426711
Did not meet early stopping. Best iteration is:
[162]	training's l2: 0.22782	valid_1's l2: 0.426711
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177226 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.680997	valid_1's l2: 0.685274
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.645766	valid_1's l2: 0.651959
[3]	training's l2: 0.616354	valid_1's l2: 0.625433
[4]	training's l2: 0.59042	valid_1's l2: 0.601998
[5]	training's l2: 0.568173	valid_1's l2: 0.583257
[6]	training's l2: 0.548921	valid_1's l2: 0.565898
[7]	training's l2: 0.531711	valid_1's l2: 0.551626
[8]	training's l2: 0.516545	valid_1's l2: 0.539643
[9]	training's l2: 0.503418	valid_1's l2: 0.529431
[10]	training's l2: 0.490867	valid_1's l2: 0.519592
[11]	training's l2: 0.480434	valid_1's l2: 0.512206
[12]	training's l2: 0.470457	valid_1's l2: 0.505165
[13]	training's l2: 0.461641	valid_1's l2: 0.498939
[14]	training's l2: 0.453465	valid_1's l2: 0.493356
[15]	training's l2: 0.445706	valid_1's l2: 0.488454
[16]	training's l2: 0.438438	valid_1's l2: 0.483153
[17]	training's l2: 0.432034	valid_1's l2: 0.479736
[18]	training's l2: 0.426091	valid_1's l2: 0.476836
[19]	training's l2: 0.420892	valid_1's l2: 0.474127
[20]	training's l2: 0.415368	valid_1's l2: 0.471046
[21]	training's l2: 0.409798	valid_1's l2: 0.467962
[22]	training's l2: 0.40497	valid_1's l2: 0.465642
[23]	training's l2: 0.400951	valid_1's l2: 0.464831
[24]	training's l2: 0.396507	valid_1's l2: 0.462957
[25]	training's l2: 0.392307	valid_1's l2: 0.46231
[26]	training's l2: 0.387774	valid_1's l2: 0.459872
[27]	training's l2: 0.383339	valid_1's l2: 0.457881
[28]	training's l2: 0.37976	valid_1's l2: 0.45692
[29]	training's l2: 0.375933	valid_1's l2: 0.455083
[30]	training's l2: 0.37193	valid_1's l2: 0.453235
[31]	training's l2: 0.368811	valid_1's l2: 0.452662
[32]	training's l2: 0.365468	valid_1's l2: 0.451549
[33]	training's l2: 0.362566	valid_1's l2: 0.450845
[34]	training's l2: 0.35892	valid_1's l2: 0.449306
[35]	training's l2: 0.35577	valid_1's l2: 0.447758
[36]	training's l2: 0.353122	valid_1's l2: 0.447339
[37]	training's l2: 0.349296	valid_1's l2: 0.445199
[38]	training's l2: 0.346464	valid_1's l2: 0.444394
[39]	training's l2: 0.343813	valid_1's l2: 0.443989
[40]	training's l2: 0.340698	valid_1's l2: 0.442097
[41]	training's l2: 0.337771	valid_1's l2: 0.441145
[42]	training's l2: 0.335399	valid_1's l2: 0.440701
[43]	training's l2: 0.332723	valid_1's l2: 0.439227
[44]	training's l2: 0.330452	valid_1's l2: 0.439193
[45]	training's l2: 0.327905	valid_1's l2: 0.437643
[46]	training's l2: 0.324741	valid_1's l2: 0.436124
[47]	training's l2: 0.322655	valid_1's l2: 0.435991
[48]	training's l2: 0.320602	valid_1's l2: 0.435676
[49]	training's l2: 0.31864	valid_1's l2: 0.435327
[50]	training's l2: 0.316104	valid_1's l2: 0.43395
[51]	training's l2: 0.314031	valid_1's l2: 0.433842
[52]	training's l2: 0.31214	valid_1's l2: 0.433946
[53]	training's l2: 0.310189	valid_1's l2: 0.434086
[54]	training's l2: 0.308291	valid_1's l2: 0.433668
[55]	training's l2: 0.306056	valid_1's l2: 0.432948
[56]	training's l2: 0.303814	valid_1's l2: 0.431777
[57]	training's l2: 0.301618	valid_1's l2: 0.430929
[58]	training's l2: 0.299677	valid_1's l2: 0.430751
[59]	training's l2: 0.297915	valid_1's l2: 0.430451
[60]	training's l2: 0.295954	valid_1's l2: 0.430097
[61]	training's l2: 0.294249	valid_1's l2: 0.429605
[62]	training's l2: 0.292472	valid_1's l2: 0.429238
[63]	training's l2: 0.290668	valid_1's l2: 0.429021
[64]	training's l2: 0.289061	valid_1's l2: 0.428747
[65]	training's l2: 0.287419	valid_1's l2: 0.428799
[66]	training's l2: 0.285809	valid_1's l2: 0.428788
[67]	training's l2: 0.284105	valid_1's l2: 0.428343
[68]	training's l2: 0.282599	valid_1's l2: 0.428472
[69]	training's l2: 0.281108	valid_1's l2: 0.428312
[70]	training's l2: 0.279361	valid_1's l2: 0.427766
[71]	training's l2: 0.277854	valid_1's l2: 0.427937
[72]	training's l2: 0.276287	valid_1's l2: 0.427806
[73]	training's l2: 0.274688	valid_1's l2: 0.427812
[74]	training's l2: 0.27323	valid_1's l2: 0.427878
[75]	training's l2: 0.271815	valid_1's l2: 0.427837
[76]	training's l2: 0.270488	valid_1's l2: 0.427703
[77]	training's l2: 0.268986	valid_1's l2: 0.427964
[78]	training's l2: 0.267599	valid_1's l2: 0.427859
[79]	training's l2: 0.26595	valid_1's l2: 0.427282
[80]	training's l2: 0.264719	valid_1's l2: 0.427593
[81]	training's l2: 0.263199	valid_1's l2: 0.427021
[82]	training's l2: 0.261916	valid_1's l2: 0.427005
[83]	training's l2: 0.26066	valid_1's l2: 0.426968
[84]	training's l2: 0.259285	valid_1's l2: 0.426884
[85]	training's l2: 0.257825	valid_1's l2: 0.426777
[86]	training's l2: 0.256331	valid_1's l2: 0.427203
[87]	training's l2: 0.255045	valid_1's l2: 0.427471
[88]	training's l2: 0.253706	valid_1's l2: 0.427349
[89]	training's l2: 0.252503	valid_1's l2: 0.42743
[90]	training's l2: 0.251221	valid_1's l2: 0.427301
[91]	training's l2: 0.250149	valid_1's l2: 0.427308
[92]	training's l2: 0.248979	valid_1's l2: 0.427211
[93]	training's l2: 0.247781	valid_1's l2: 0.426995
[94]	training's l2: 0.246567	valid_1's l2: 0.426835
[95]	training's l2: 0.245167	valid_1's l2: 0.426236
[96]	training's l2: 0.243927	valid_1's l2: 0.42626
[97]	training's l2: 0.242781	valid_1's l2: 0.426122
[98]	training's l2: 0.241616	valid_1's l2: 0.426059
[99]	training's l2: 0.240659	valid_1's l2: 0.426122
[100]	training's l2: 0.239424	valid_1's l2: 0.426142
[101]	training's l2: 0.238449	valid_1's l2: 0.426302
[102]	training's l2: 0.237222	valid_1's l2: 0.426276
[103]	training's l2: 0.236068	valid_1's l2: 0.426248
[104]	training's l2: 0.234954	valid_1's l2: 0.426337
[105]	training's l2: 0.233629	valid_1's l2: 0.426202
[106]	training's l2: 0.23269	valid_1's l2: 0.426055
[107]	training's l2: 0.231745	valid_1's l2: 0.426148
[108]	training's l2: 0.230622	valid_1's l2: 0.426122
[109]	training's l2: 0.229659	valid_1's l2: 0.426168
[110]	training's l2: 0.228673	valid_1's l2: 0.426202
[111]	training's l2: 0.227443	valid_1's l2: 0.426221
[112]	training's l2: 0.22644	valid_1's l2: 0.426127
[113]	training's l2: 0.225423	valid_1's l2: 0.426102
[114]	training's l2: 0.22422	valid_1's l2: 0.42622
[115]	training's l2: 0.223343	valid_1's l2: 0.426084
[116]	training's l2: 0.222461	valid_1's l2: 0.42607
[117]	training's l2: 0.221391	valid_1's l2: 0.426018
Did not meet early stopping. Best iteration is:
[117]	training's l2: 0.221391	valid_1's l2: 0.426018
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.173988 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.692411	valid_1's l2: 0.69465
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.666334	valid_1's l2: 0.66942
[3]	training's l2: 0.64266	valid_1's l2: 0.646856
[4]	training's l2: 0.621784	valid_1's l2: 0.627548
[5]	training's l2: 0.603186	valid_1's l2: 0.609705
[6]	training's l2: 0.58683	valid_1's l2: 0.59416
[7]	training's l2: 0.572274	valid_1's l2: 0.580567
[8]	training's l2: 0.558341	valid_1's l2: 0.568083
[9]	training's l2: 0.546142	valid_1's l2: 0.556938
[10]	training's l2: 0.535316	valid_1's l2: 0.54736
[11]	training's l2: 0.524952	valid_1's l2: 0.538338
[12]	training's l2: 0.516103	valid_1's l2: 0.530826
[13]	training's l2: 0.507087	valid_1's l2: 0.523556
[14]	training's l2: 0.499196	valid_1's l2: 0.517809
[15]	training's l2: 0.492227	valid_1's l2: 0.512409
[16]	training's l2: 0.48526	valid_1's l2: 0.507296
[17]	training's l2: 0.479173	valid_1's l2: 0.50246
[18]	training's l2: 0.473595	valid_1's l2: 0.498535
[19]	training's l2: 0.468266	valid_1's l2: 0.494734
[20]	training's l2: 0.462799	valid_1's l2: 0.490089
[21]	training's l2: 0.458097	valid_1's l2: 0.486886
[22]	training's l2: 0.453727	valid_1's l2: 0.484249
[23]	training's l2: 0.449471	valid_1's l2: 0.482016
[24]	training's l2: 0.445053	valid_1's l2: 0.479039
[25]	training's l2: 0.441397	valid_1's l2: 0.47689
[26]	training's l2: 0.437658	valid_1's l2: 0.474772
[27]	training's l2: 0.434203	valid_1's l2: 0.472831
[28]	training's l2: 0.430698	valid_1's l2: 0.470936
[29]	training's l2: 0.427777	valid_1's l2: 0.469414
[30]	training's l2: 0.424186	valid_1's l2: 0.467164
[31]	training's l2: 0.420971	valid_1's l2: 0.465306
[32]	training's l2: 0.418016	valid_1's l2: 0.464067
[33]	training's l2: 0.414869	valid_1's l2: 0.462657
[34]	training's l2: 0.412311	valid_1's l2: 0.461554
[35]	training's l2: 0.409444	valid_1's l2: 0.460118
[36]	training's l2: 0.406362	valid_1's l2: 0.45853
[37]	training's l2: 0.403796	valid_1's l2: 0.457412
[38]	training's l2: 0.401011	valid_1's l2: 0.456119
[39]	training's l2: 0.3988	valid_1's l2: 0.455461
[40]	training's l2: 0.396711	valid_1's l2: 0.454545
[41]	training's l2: 0.394152	valid_1's l2: 0.453271
[42]	training's l2: 0.391762	valid_1's l2: 0.452207
[43]	training's l2: 0.389808	valid_1's l2: 0.451447
[44]	training's l2: 0.387941	valid_1's l2: 0.450833
[45]	training's l2: 0.385951	valid_1's l2: 0.449848
[46]	training's l2: 0.382852	valid_1's l2: 0.448485
[47]	training's l2: 0.381055	valid_1's l2: 0.447893
[48]	training's l2: 0.379182	valid_1's l2: 0.447341
[49]	training's l2: 0.376969	valid_1's l2: 0.446423
[50]	training's l2: 0.375349	valid_1's l2: 0.446023
[51]	training's l2: 0.37316	valid_1's l2: 0.445316
[52]	training's l2: 0.370921	valid_1's l2: 0.444172
[53]	training's l2: 0.369252	valid_1's l2: 0.443763
[54]	training's l2: 0.367764	valid_1's l2: 0.44325
[55]	training's l2: 0.366186	valid_1's l2: 0.442679
[56]	training's l2: 0.364406	valid_1's l2: 0.442068
[57]	training's l2: 0.362796	valid_1's l2: 0.441744
[58]	training's l2: 0.361339	valid_1's l2: 0.441392
[59]	training's l2: 0.359447	valid_1's l2: 0.440395
[60]	training's l2: 0.358082	valid_1's l2: 0.440043
[61]	training's l2: 0.356187	valid_1's l2: 0.439388
[62]	training's l2: 0.354601	valid_1's l2: 0.438943
[63]	training's l2: 0.353186	valid_1's l2: 0.438635
[64]	training's l2: 0.350862	valid_1's l2: 0.437178
[65]	training's l2: 0.349213	valid_1's l2: 0.436692
[66]	training's l2: 0.347884	valid_1's l2: 0.436636
[67]	training's l2: 0.34661	valid_1's l2: 0.436503
[68]	training's l2: 0.345287	valid_1's l2: 0.436504
[69]	training's l2: 0.34412	valid_1's l2: 0.43656
[70]	training's l2: 0.342688	valid_1's l2: 0.435936
[71]	training's l2: 0.341209	valid_1's l2: 0.435404
[72]	training's l2: 0.339869	valid_1's l2: 0.435137
[73]	training's l2: 0.338495	valid_1's l2: 0.435128
[74]	training's l2: 0.337133	valid_1's l2: 0.434646
[75]	training's l2: 0.335831	valid_1's l2: 0.434623
[76]	training's l2: 0.334524	valid_1's l2: 0.434448
[77]	training's l2: 0.333209	valid_1's l2: 0.433901
[78]	training's l2: 0.331967	valid_1's l2: 0.433766
[79]	training's l2: 0.33078	valid_1's l2: 0.433593
[80]	training's l2: 0.329627	valid_1's l2: 0.433686
[81]	training's l2: 0.328593	valid_1's l2: 0.433777
[82]	training's l2: 0.327013	valid_1's l2: 0.432845
[83]	training's l2: 0.325598	valid_1's l2: 0.432333
[84]	training's l2: 0.324432	valid_1's l2: 0.432344
[85]	training's l2: 0.32332	valid_1's l2: 0.43214
[86]	training's l2: 0.322071	valid_1's l2: 0.43173
[87]	training's l2: 0.321023	valid_1's l2: 0.43165
[88]	training's l2: 0.319972	valid_1's l2: 0.431553
[89]	training's l2: 0.318757	valid_1's l2: 0.431438
[90]	training's l2: 0.317741	valid_1's l2: 0.431711
[91]	training's l2: 0.316481	valid_1's l2: 0.431359
[92]	training's l2: 0.315448	valid_1's l2: 0.431353
[93]	training's l2: 0.314344	valid_1's l2: 0.431265
[94]	training's l2: 0.313369	valid_1's l2: 0.431313
[95]	training's l2: 0.312331	valid_1's l2: 0.431208
[96]	training's l2: 0.311317	valid_1's l2: 0.431333
[97]	training's l2: 0.310379	valid_1's l2: 0.431196
[98]	training's l2: 0.309441	valid_1's l2: 0.431241
[99]	training's l2: 0.308535	valid_1's l2: 0.431167
[100]	training's l2: 0.307558	valid_1's l2: 0.431058
[101]	training's l2: 0.306485	valid_1's l2: 0.430768
[102]	training's l2: 0.305434	valid_1's l2: 0.430784
[103]	training's l2: 0.304488	valid_1's l2: 0.430636
[104]	training's l2: 0.30348	valid_1's l2: 0.430185
[105]	training's l2: 0.302541	valid_1's l2: 0.430159
[106]	training's l2: 0.301688	valid_1's l2: 0.43014
[107]	training's l2: 0.300781	valid_1's l2: 0.430239
[108]	training's l2: 0.299852	valid_1's l2: 0.430183
[109]	training's l2: 0.298989	valid_1's l2: 0.430182
[110]	training's l2: 0.298122	valid_1's l2: 0.430312
[111]	training's l2: 0.297206	valid_1's l2: 0.430106
[112]	training's l2: 0.296148	valid_1's l2: 0.42985
[113]	training's l2: 0.295307	valid_1's l2: 0.429764
[114]	training's l2: 0.294336	valid_1's l2: 0.429467
[115]	training's l2: 0.293534	valid_1's l2: 0.429512
[116]	training's l2: 0.29268	valid_1's l2: 0.429459
[117]	training's l2: 0.291822	valid_1's l2: 0.429456
[118]	training's l2: 0.291002	valid_1's l2: 0.429475
[119]	training's l2: 0.290033	valid_1's l2: 0.429258
[120]	training's l2: 0.289199	valid_1's l2: 0.429274
[121]	training's l2: 0.288422	valid_1's l2: 0.429277
[122]	training's l2: 0.287679	valid_1's l2: 0.429351
[123]	training's l2: 0.28679	valid_1's l2: 0.429411
[124]	training's l2: 0.286015	valid_1's l2: 0.429447
[125]	training's l2: 0.285233	valid_1's l2: 0.429338
[126]	training's l2: 0.284494	valid_1's l2: 0.429307
[127]	training's l2: 0.283694	valid_1's l2: 0.429356
[128]	training's l2: 0.282942	valid_1's l2: 0.429177
[129]	training's l2: 0.282097	valid_1's l2: 0.42918
[130]	training's l2: 0.281295	valid_1's l2: 0.429188
[131]	training's l2: 0.280407	valid_1's l2: 0.428939
[132]	training's l2: 0.279407	valid_1's l2: 0.428431
[133]	training's l2: 0.278628	valid_1's l2: 0.428347
[134]	training's l2: 0.277815	valid_1's l2: 0.428393
[135]	training's l2: 0.276983	valid_1's l2: 0.428226
[136]	training's l2: 0.276205	valid_1's l2: 0.428205
[137]	training's l2: 0.275404	valid_1's l2: 0.428235
[138]	training's l2: 0.274639	valid_1's l2: 0.428109
[139]	training's l2: 0.273927	valid_1's l2: 0.428141
[140]	training's l2: 0.273192	valid_1's l2: 0.428075
[141]	training's l2: 0.272503	valid_1's l2: 0.42799
[142]	training's l2: 0.271769	valid_1's l2: 0.427855
[143]	training's l2: 0.271053	valid_1's l2: 0.4279
[144]	training's l2: 0.27036	valid_1's l2: 0.427834
[145]	training's l2: 0.269536	valid_1's l2: 0.427358
[146]	training's l2: 0.268868	valid_1's l2: 0.427199
[147]	training's l2: 0.268252	valid_1's l2: 0.427167
[148]	training's l2: 0.267477	valid_1's l2: 0.42714
[149]	training's l2: 0.266796	valid_1's l2: 0.427102
[150]	training's l2: 0.266095	valid_1's l2: 0.427125
[151]	training's l2: 0.265422	valid_1's l2: 0.42712
[152]	training's l2: 0.264754	valid_1's l2: 0.42718
[153]	training's l2: 0.264015	valid_1's l2: 0.427126
[154]	training's l2: 0.263356	valid_1's l2: 0.427252
[155]	training's l2: 0.262739	valid_1's l2: 0.427253
[156]	training's l2: 0.261914	valid_1's l2: 0.426855
[157]	training's l2: 0.261175	valid_1's l2: 0.42684
[158]	training's l2: 0.260582	valid_1's l2: 0.42666
[159]	training's l2: 0.260004	valid_1's l2: 0.426643
[160]	training's l2: 0.259351	valid_1's l2: 0.426609
[161]	training's l2: 0.258696	valid_1's l2: 0.426661
[162]	training's l2: 0.258056	valid_1's l2: 0.426667
[163]	training's l2: 0.257328	valid_1's l2: 0.426797
[164]	training's l2: 0.256686	valid_1's l2: 0.426925
[165]	training's l2: 0.256002	valid_1's l2: 0.426729
[166]	training's l2: 0.255298	valid_1's l2: 0.426693
[167]	training's l2: 0.254654	valid_1's l2: 0.426573
[168]	training's l2: 0.253913	valid_1's l2: 0.42651
[169]	training's l2: 0.253289	valid_1's l2: 0.426477
[170]	training's l2: 0.252687	valid_1's l2: 0.426483
[171]	training's l2: 0.252028	valid_1's l2: 0.426405
[172]	training's l2: 0.251527	valid_1's l2: 0.426303
[173]	training's l2: 0.250956	valid_1's l2: 0.426362
[174]	training's l2: 0.250407	valid_1's l2: 0.426373
[175]	training's l2: 0.24986	valid_1's l2: 0.426355
[176]	training's l2: 0.249301	valid_1's l2: 0.426418
[177]	training's l2: 0.248704	valid_1's l2: 0.426404
[178]	training's l2: 0.248025	valid_1's l2: 0.426325
[179]	training's l2: 0.247416	valid_1's l2: 0.426243
[180]	training's l2: 0.246897	valid_1's l2: 0.426354
[181]	training's l2: 0.246374	valid_1's l2: 0.426365
[182]	training's l2: 0.245818	valid_1's l2: 0.426232
[183]	training's l2: 0.245218	valid_1's l2: 0.426418
[184]	training's l2: 0.244561	valid_1's l2: 0.42643
[185]	training's l2: 0.244019	valid_1's l2: 0.426413
[186]	training's l2: 0.243463	valid_1's l2: 0.426431
[187]	training's l2: 0.242738	valid_1's l2: 0.426456
[188]	training's l2: 0.242167	valid_1's l2: 0.426441
[189]	training's l2: 0.241623	valid_1's l2: 0.426534
[190]	training's l2: 0.240937	valid_1's l2: 0.426483
[191]	training's l2: 0.240365	valid_1's l2: 0.426517
[192]	training's l2: 0.239729	valid_1's l2: 0.426472
Did not meet early stopping. Best iteration is:
[192]	training's l2: 0.239729	valid_1's l2: 0.426472
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177371 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.695977	valid_1's l2: 0.698596
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.672654	valid_1's l2: 0.676182
[3]	training's l2: 0.651112	valid_1's l2: 0.655783
[4]	training's l2: 0.63177	valid_1's l2: 0.637991
[5]	training's l2: 0.613814	valid_1's l2: 0.621409
[6]	training's l2: 0.597778	valid_1's l2: 0.606407
[7]	training's l2: 0.582832	valid_1's l2: 0.593312
[8]	training's l2: 0.56954	valid_1's l2: 0.581266
[9]	training's l2: 0.557198	valid_1's l2: 0.570372
[10]	training's l2: 0.54559	valid_1's l2: 0.560302
[11]	training's l2: 0.535267	valid_1's l2: 0.551463
[12]	training's l2: 0.525944	valid_1's l2: 0.543291
[13]	training's l2: 0.517286	valid_1's l2: 0.536917
[14]	training's l2: 0.508715	valid_1's l2: 0.530249
[15]	training's l2: 0.501031	valid_1's l2: 0.524013
[16]	training's l2: 0.493861	valid_1's l2: 0.518147
[17]	training's l2: 0.486913	valid_1's l2: 0.51337
[18]	training's l2: 0.480694	valid_1's l2: 0.509008
[19]	training's l2: 0.474763	valid_1's l2: 0.504754
[20]	training's l2: 0.469157	valid_1's l2: 0.500867
[21]	training's l2: 0.463721	valid_1's l2: 0.496826
[22]	training's l2: 0.458615	valid_1's l2: 0.493318
[23]	training's l2: 0.45383	valid_1's l2: 0.490503
[24]	training's l2: 0.44935	valid_1's l2: 0.487655
[25]	training's l2: 0.44493	valid_1's l2: 0.484811
[26]	training's l2: 0.440606	valid_1's l2: 0.481909
[27]	training's l2: 0.436394	valid_1's l2: 0.479762
[28]	training's l2: 0.432909	valid_1's l2: 0.477914
[29]	training's l2: 0.42937	valid_1's l2: 0.47649
[30]	training's l2: 0.425634	valid_1's l2: 0.474187
[31]	training's l2: 0.422017	valid_1's l2: 0.472394
[32]	training's l2: 0.419008	valid_1's l2: 0.470835
[33]	training's l2: 0.415732	valid_1's l2: 0.469145
[34]	training's l2: 0.412646	valid_1's l2: 0.467445
[35]	training's l2: 0.409565	valid_1's l2: 0.465808
[36]	training's l2: 0.406254	valid_1's l2: 0.464672
[37]	training's l2: 0.403661	valid_1's l2: 0.463852
[38]	training's l2: 0.400825	valid_1's l2: 0.46235
[39]	training's l2: 0.39832	valid_1's l2: 0.46159
[40]	training's l2: 0.395487	valid_1's l2: 0.460007
[41]	training's l2: 0.393146	valid_1's l2: 0.459079
[42]	training's l2: 0.390655	valid_1's l2: 0.457956
[43]	training's l2: 0.387905	valid_1's l2: 0.456323
[44]	training's l2: 0.385791	valid_1's l2: 0.455766
[45]	training's l2: 0.383468	valid_1's l2: 0.454778
[46]	training's l2: 0.381076	valid_1's l2: 0.453747
[47]	training's l2: 0.379062	valid_1's l2: 0.453343
[48]	training's l2: 0.376489	valid_1's l2: 0.452138
[49]	training's l2: 0.374646	valid_1's l2: 0.451579
[50]	training's l2: 0.37268	valid_1's l2: 0.450823
[51]	training's l2: 0.370932	valid_1's l2: 0.450433
[52]	training's l2: 0.3692	valid_1's l2: 0.450046
[53]	training's l2: 0.367113	valid_1's l2: 0.449309
[54]	training's l2: 0.364879	valid_1's l2: 0.448112
[55]	training's l2: 0.362855	valid_1's l2: 0.447142
[56]	training's l2: 0.360746	valid_1's l2: 0.446195
[57]	training's l2: 0.359251	valid_1's l2: 0.445932
[58]	training's l2: 0.356907	valid_1's l2: 0.444702
[59]	training's l2: 0.355076	valid_1's l2: 0.44411
[60]	training's l2: 0.353552	valid_1's l2: 0.443663
[61]	training's l2: 0.351816	valid_1's l2: 0.443228
[62]	training's l2: 0.349935	valid_1's l2: 0.442164
[63]	training's l2: 0.348334	valid_1's l2: 0.441848
[64]	training's l2: 0.34643	valid_1's l2: 0.441178
[65]	training's l2: 0.345028	valid_1's l2: 0.441039
[66]	training's l2: 0.343161	valid_1's l2: 0.440124
[67]	training's l2: 0.341782	valid_1's l2: 0.439968
[68]	training's l2: 0.339918	valid_1's l2: 0.438913
[69]	training's l2: 0.338637	valid_1's l2: 0.438775
[70]	training's l2: 0.337281	valid_1's l2: 0.438526
[71]	training's l2: 0.33559	valid_1's l2: 0.437939
[72]	training's l2: 0.334311	valid_1's l2: 0.437661
[73]	training's l2: 0.3329	valid_1's l2: 0.43711
[74]	training's l2: 0.331665	valid_1's l2: 0.437073
[75]	training's l2: 0.329645	valid_1's l2: 0.435802
[76]	training's l2: 0.328353	valid_1's l2: 0.435779
[77]	training's l2: 0.327152	valid_1's l2: 0.435913
[78]	training's l2: 0.325898	valid_1's l2: 0.435533
[79]	training's l2: 0.324094	valid_1's l2: 0.434354
[80]	training's l2: 0.322824	valid_1's l2: 0.434207
[81]	training's l2: 0.321372	valid_1's l2: 0.433774
[82]	training's l2: 0.320169	valid_1's l2: 0.433506
[83]	training's l2: 0.318913	valid_1's l2: 0.433288
[84]	training's l2: 0.317516	valid_1's l2: 0.432853
[85]	training's l2: 0.316324	valid_1's l2: 0.432786
[86]	training's l2: 0.315168	valid_1's l2: 0.432698
[87]	training's l2: 0.313828	valid_1's l2: 0.432143
[88]	training's l2: 0.312741	valid_1's l2: 0.432127
[89]	training's l2: 0.311454	valid_1's l2: 0.431815
[90]	training's l2: 0.310239	valid_1's l2: 0.431366
[91]	training's l2: 0.309111	valid_1's l2: 0.431136
[92]	training's l2: 0.308066	valid_1's l2: 0.431111
[93]	training's l2: 0.306998	valid_1's l2: 0.430919
[94]	training's l2: 0.305796	valid_1's l2: 0.430764
[95]	training's l2: 0.30447	valid_1's l2: 0.42991
[96]	training's l2: 0.303443	valid_1's l2: 0.42972
[97]	training's l2: 0.302264	valid_1's l2: 0.429507
[98]	training's l2: 0.301209	valid_1's l2: 0.429284
[99]	training's l2: 0.30008	valid_1's l2: 0.42906
[100]	training's l2: 0.298929	valid_1's l2: 0.428682
[101]	training's l2: 0.297961	valid_1's l2: 0.428582
[102]	training's l2: 0.296981	valid_1's l2: 0.428589
[103]	training's l2: 0.295952	valid_1's l2: 0.428555
[104]	training's l2: 0.29497	valid_1's l2: 0.428375
[105]	training's l2: 0.294049	valid_1's l2: 0.428426
[106]	training's l2: 0.292867	valid_1's l2: 0.428026
[107]	training's l2: 0.291826	valid_1's l2: 0.42778
[108]	training's l2: 0.290872	valid_1's l2: 0.427717
[109]	training's l2: 0.289925	valid_1's l2: 0.427601
[110]	training's l2: 0.288972	valid_1's l2: 0.427494
[111]	training's l2: 0.288054	valid_1's l2: 0.427473
[112]	training's l2: 0.28715	valid_1's l2: 0.427501
[113]	training's l2: 0.286007	valid_1's l2: 0.427103
[114]	training's l2: 0.284931	valid_1's l2: 0.426897
[115]	training's l2: 0.284068	valid_1's l2: 0.426773
[116]	training's l2: 0.283206	valid_1's l2: 0.42669
[117]	training's l2: 0.282379	valid_1's l2: 0.426703
[118]	training's l2: 0.281352	valid_1's l2: 0.426446
[119]	training's l2: 0.280504	valid_1's l2: 0.426316
[120]	training's l2: 0.279359	valid_1's l2: 0.425782
[121]	training's l2: 0.278503	valid_1's l2: 0.425727
[122]	training's l2: 0.277588	valid_1's l2: 0.42556
[123]	training's l2: 0.276763	valid_1's l2: 0.425431
[124]	training's l2: 0.275892	valid_1's l2: 0.425364
[125]	training's l2: 0.275102	valid_1's l2: 0.425294
[126]	training's l2: 0.274328	valid_1's l2: 0.425255
[127]	training's l2: 0.27335	valid_1's l2: 0.425047
[128]	training's l2: 0.27255	valid_1's l2: 0.425145
[129]	training's l2: 0.271723	valid_1's l2: 0.425155
[130]	training's l2: 0.270911	valid_1's l2: 0.425187
[131]	training's l2: 0.269962	valid_1's l2: 0.425085
[132]	training's l2: 0.269181	valid_1's l2: 0.424891
[133]	training's l2: 0.268285	valid_1's l2: 0.424599
[134]	training's l2: 0.26757	valid_1's l2: 0.424695
[135]	training's l2: 0.266716	valid_1's l2: 0.424625
[136]	training's l2: 0.266007	valid_1's l2: 0.424701
[137]	training's l2: 0.265333	valid_1's l2: 0.424642
[138]	training's l2: 0.264493	valid_1's l2: 0.424427
[139]	training's l2: 0.263841	valid_1's l2: 0.424463
[140]	training's l2: 0.263129	valid_1's l2: 0.424637
[141]	training's l2: 0.262375	valid_1's l2: 0.424619
[142]	training's l2: 0.261572	valid_1's l2: 0.424439
[143]	training's l2: 0.260823	valid_1's l2: 0.424373
[144]	training's l2: 0.260061	valid_1's l2: 0.424355
[145]	training's l2: 0.259368	valid_1's l2: 0.424344
[146]	training's l2: 0.258492	valid_1's l2: 0.42428
[147]	training's l2: 0.257759	valid_1's l2: 0.42425
[148]	training's l2: 0.257101	valid_1's l2: 0.42427
[149]	training's l2: 0.256431	valid_1's l2: 0.424251
[150]	training's l2: 0.255779	valid_1's l2: 0.424281
[151]	training's l2: 0.255087	valid_1's l2: 0.424215
[152]	training's l2: 0.254457	valid_1's l2: 0.424152
Did not meet early stopping. Best iteration is:
[152]	training's l2: 0.254457	valid_1's l2: 0.424152
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183227 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.689063	valid_1's l2: 0.691662
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.660627	valid_1's l2: 0.664338
[3]	training's l2: 0.634815	valid_1's l2: 0.639976
[4]	training's l2: 0.611845	valid_1's l2: 0.618131
[5]	training's l2: 0.592104	valid_1's l2: 0.599332
[6]	training's l2: 0.573731	valid_1's l2: 0.582768
[7]	training's l2: 0.558158	valid_1's l2: 0.569552
[8]	training's l2: 0.544608	valid_1's l2: 0.557854
[9]	training's l2: 0.532107	valid_1's l2: 0.547212
[10]	training's l2: 0.520855	valid_1's l2: 0.537926
[11]	training's l2: 0.509996	valid_1's l2: 0.528842
[12]	training's l2: 0.50075	valid_1's l2: 0.521641
[13]	training's l2: 0.49246	valid_1's l2: 0.514824
[14]	training's l2: 0.484462	valid_1's l2: 0.509249
[15]	training's l2: 0.47739	valid_1's l2: 0.504189
[16]	training's l2: 0.470211	valid_1's l2: 0.499209
[17]	training's l2: 0.464079	valid_1's l2: 0.494681
[18]	training's l2: 0.458038	valid_1's l2: 0.490522
[19]	training's l2: 0.45263	valid_1's l2: 0.487136
[20]	training's l2: 0.447199	valid_1's l2: 0.483084
[21]	training's l2: 0.44254	valid_1's l2: 0.480323
[22]	training's l2: 0.438092	valid_1's l2: 0.477803
[23]	training's l2: 0.433903	valid_1's l2: 0.475885
[24]	training's l2: 0.42957	valid_1's l2: 0.473383
[25]	training's l2: 0.425401	valid_1's l2: 0.47086
[26]	training's l2: 0.421824	valid_1's l2: 0.469104
[27]	training's l2: 0.41846	valid_1's l2: 0.467699
[28]	training's l2: 0.414993	valid_1's l2: 0.465881
[29]	training's l2: 0.411767	valid_1's l2: 0.464642
[30]	training's l2: 0.408182	valid_1's l2: 0.462982
[31]	training's l2: 0.404943	valid_1's l2: 0.461324
[32]	training's l2: 0.402063	valid_1's l2: 0.460299
[33]	training's l2: 0.398499	valid_1's l2: 0.458176
[34]	training's l2: 0.395576	valid_1's l2: 0.457442
[35]	training's l2: 0.393061	valid_1's l2: 0.456312
[36]	training's l2: 0.389677	valid_1's l2: 0.454873
[37]	training's l2: 0.387034	valid_1's l2: 0.454367
[38]	training's l2: 0.384489	valid_1's l2: 0.453722
[39]	training's l2: 0.382318	valid_1's l2: 0.453193
[40]	training's l2: 0.379744	valid_1's l2: 0.452292
[41]	training's l2: 0.377401	valid_1's l2: 0.451561
[42]	training's l2: 0.374762	valid_1's l2: 0.450577
[43]	training's l2: 0.372537	valid_1's l2: 0.449797
[44]	training's l2: 0.369459	valid_1's l2: 0.448318
[45]	training's l2: 0.367394	valid_1's l2: 0.447668
[46]	training's l2: 0.365151	valid_1's l2: 0.446785
[47]	training's l2: 0.363169	valid_1's l2: 0.446426
[48]	training's l2: 0.360666	valid_1's l2: 0.445449
[49]	training's l2: 0.358607	valid_1's l2: 0.444612
[50]	training's l2: 0.356843	valid_1's l2: 0.444213
[51]	training's l2: 0.354808	valid_1's l2: 0.443258
[52]	training's l2: 0.353084	valid_1's l2: 0.442942
[53]	training's l2: 0.350983	valid_1's l2: 0.442167
[54]	training's l2: 0.349167	valid_1's l2: 0.441725
[55]	training's l2: 0.347536	valid_1's l2: 0.441399
[56]	training's l2: 0.345317	valid_1's l2: 0.440452
[57]	training's l2: 0.343437	valid_1's l2: 0.439655
[58]	training's l2: 0.341845	valid_1's l2: 0.439352
[59]	training's l2: 0.339763	valid_1's l2: 0.438482
[60]	training's l2: 0.338018	valid_1's l2: 0.438036
[61]	training's l2: 0.335907	valid_1's l2: 0.437152
[62]	training's l2: 0.33429	valid_1's l2: 0.436817
[63]	training's l2: 0.332678	valid_1's l2: 0.436714
[64]	training's l2: 0.331022	valid_1's l2: 0.436342
[65]	training's l2: 0.32958	valid_1's l2: 0.435969
[66]	training's l2: 0.327755	valid_1's l2: 0.435129
[67]	training's l2: 0.326271	valid_1's l2: 0.434767
[68]	training's l2: 0.324867	valid_1's l2: 0.434852
[69]	training's l2: 0.323495	valid_1's l2: 0.434385
[70]	training's l2: 0.321927	valid_1's l2: 0.434263
[71]	training's l2: 0.320657	valid_1's l2: 0.434066
[72]	training's l2: 0.319107	valid_1's l2: 0.434023
[73]	training's l2: 0.317757	valid_1's l2: 0.43394
[74]	training's l2: 0.316338	valid_1's l2: 0.433113
[75]	training's l2: 0.314723	valid_1's l2: 0.432832
[76]	training's l2: 0.313444	valid_1's l2: 0.432863
[77]	training's l2: 0.312177	valid_1's l2: 0.432659
[78]	training's l2: 0.310851	valid_1's l2: 0.432782
[79]	training's l2: 0.309614	valid_1's l2: 0.432819
[80]	training's l2: 0.308365	valid_1's l2: 0.432912
[81]	training's l2: 0.307201	valid_1's l2: 0.433122
[82]	training's l2: 0.30578	valid_1's l2: 0.432883
[83]	training's l2: 0.304425	valid_1's l2: 0.432407
[84]	training's l2: 0.303256	valid_1's l2: 0.432351
[85]	training's l2: 0.301911	valid_1's l2: 0.432122
[86]	training's l2: 0.300815	valid_1's l2: 0.432099
[87]	training's l2: 0.299698	valid_1's l2: 0.431912
[88]	training's l2: 0.298667	valid_1's l2: 0.431707
[89]	training's l2: 0.297526	valid_1's l2: 0.431767
[90]	training's l2: 0.29638	valid_1's l2: 0.431815
[91]	training's l2: 0.295106	valid_1's l2: 0.431497
[92]	training's l2: 0.29377	valid_1's l2: 0.431152
[93]	training's l2: 0.292742	valid_1's l2: 0.431046
[94]	training's l2: 0.29162	valid_1's l2: 0.431064
[95]	training's l2: 0.290598	valid_1's l2: 0.431117
[96]	training's l2: 0.289369	valid_1's l2: 0.430809
[97]	training's l2: 0.288253	valid_1's l2: 0.430721
[98]	training's l2: 0.28729	valid_1's l2: 0.431048
[99]	training's l2: 0.286143	valid_1's l2: 0.430889
[100]	training's l2: 0.284802	valid_1's l2: 0.430331
[101]	training's l2: 0.283731	valid_1's l2: 0.43035
[102]	training's l2: 0.28284	valid_1's l2: 0.430519
[103]	training's l2: 0.281846	valid_1's l2: 0.430656
[104]	training's l2: 0.280855	valid_1's l2: 0.430565
[105]	training's l2: 0.279784	valid_1's l2: 0.430473
[106]	training's l2: 0.278855	valid_1's l2: 0.430417
[107]	training's l2: 0.277837	valid_1's l2: 0.430398
[108]	training's l2: 0.276872	valid_1's l2: 0.430338
[109]	training's l2: 0.275845	valid_1's l2: 0.430324
[110]	training's l2: 0.274975	valid_1's l2: 0.430236
[111]	training's l2: 0.274086	valid_1's l2: 0.430236
[112]	training's l2: 0.273184	valid_1's l2: 0.430125
[113]	training's l2: 0.27221	valid_1's l2: 0.430166
[114]	training's l2: 0.271295	valid_1's l2: 0.430203
[115]	training's l2: 0.270458	valid_1's l2: 0.430286
[116]	training's l2: 0.269628	valid_1's l2: 0.430254
[117]	training's l2: 0.268779	valid_1's l2: 0.430179
[118]	training's l2: 0.267937	valid_1's l2: 0.430158
[119]	training's l2: 0.266999	valid_1's l2: 0.430063
[120]	training's l2: 0.266113	valid_1's l2: 0.430187
[121]	training's l2: 0.265075	valid_1's l2: 0.429824
[122]	training's l2: 0.26412	valid_1's l2: 0.429699
[123]	training's l2: 0.263203	valid_1's l2: 0.42958
[124]	training's l2: 0.262235	valid_1's l2: 0.429832
[125]	training's l2: 0.26135	valid_1's l2: 0.429867
[126]	training's l2: 0.260459	valid_1's l2: 0.429884
[127]	training's l2: 0.259664	valid_1's l2: 0.429995
[128]	training's l2: 0.258744	valid_1's l2: 0.430157
[129]	training's l2: 0.257899	valid_1's l2: 0.430188
[130]	training's l2: 0.257128	valid_1's l2: 0.430254
[131]	training's l2: 0.256278	valid_1's l2: 0.430156
[132]	training's l2: 0.255457	valid_1's l2: 0.430232
[133]	training's l2: 0.254668	valid_1's l2: 0.430058
[134]	training's l2: 0.253751	valid_1's l2: 0.429873
[135]	training's l2: 0.25293	valid_1's l2: 0.430087
[136]	training's l2: 0.252178	valid_1's l2: 0.430191
[137]	training's l2: 0.25143	valid_1's l2: 0.430213
[138]	training's l2: 0.250562	valid_1's l2: 0.430123
[139]	training's l2: 0.249746	valid_1's l2: 0.430129
[140]	training's l2: 0.24902	valid_1's l2: 0.43025
[141]	training's l2: 0.248145	valid_1's l2: 0.430296
[142]	training's l2: 0.247229	valid_1's l2: 0.4303
[143]	training's l2: 0.246435	valid_1's l2: 0.430421
[144]	training's l2: 0.245702	valid_1's l2: 0.430254
Did not meet early stopping. Best iteration is:
[144]	training's l2: 0.245702	valid_1's l2: 0.430254
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.192513 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.682779	valid_1's l2: 0.685371
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.64913	valid_1's l2: 0.652798
[3]	training's l2: 0.621282	valid_1's l2: 0.626206
[4]	training's l2: 0.596701	valid_1's l2: 0.602803
[5]	training's l2: 0.575568	valid_1's l2: 0.583293
[6]	training's l2: 0.557671	valid_1's l2: 0.567185
[7]	training's l2: 0.540954	valid_1's l2: 0.553331
[8]	training's l2: 0.527173	valid_1's l2: 0.541212
[9]	training's l2: 0.514003	valid_1's l2: 0.530463
[10]	training's l2: 0.5023	valid_1's l2: 0.521465
[11]	training's l2: 0.492397	valid_1's l2: 0.513696
[12]	training's l2: 0.483043	valid_1's l2: 0.507566
[13]	training's l2: 0.474718	valid_1's l2: 0.500944
[14]	training's l2: 0.466907	valid_1's l2: 0.495275
[15]	training's l2: 0.459944	valid_1's l2: 0.490459
[16]	training's l2: 0.453723	valid_1's l2: 0.486341
[17]	training's l2: 0.446982	valid_1's l2: 0.482183
[18]	training's l2: 0.440888	valid_1's l2: 0.478893
[19]	training's l2: 0.435874	valid_1's l2: 0.476173
[20]	training's l2: 0.430821	valid_1's l2: 0.47316
[21]	training's l2: 0.426302	valid_1's l2: 0.471271
[22]	training's l2: 0.421445	valid_1's l2: 0.468859
[23]	training's l2: 0.417241	valid_1's l2: 0.466514
[24]	training's l2: 0.413216	valid_1's l2: 0.464446
[25]	training's l2: 0.409753	valid_1's l2: 0.462942
[26]	training's l2: 0.405093	valid_1's l2: 0.460689
[27]	training's l2: 0.402009	valid_1's l2: 0.459951
[28]	training's l2: 0.398839	valid_1's l2: 0.458494
[29]	training's l2: 0.395885	valid_1's l2: 0.457555
[30]	training's l2: 0.393122	valid_1's l2: 0.456615
[31]	training's l2: 0.390067	valid_1's l2: 0.455426
[32]	training's l2: 0.386893	valid_1's l2: 0.453564
[33]	training's l2: 0.383678	valid_1's l2: 0.452014
[34]	training's l2: 0.380974	valid_1's l2: 0.451679
[35]	training's l2: 0.378158	valid_1's l2: 0.450848
[36]	training's l2: 0.375393	valid_1's l2: 0.449752
[37]	training's l2: 0.372924	valid_1's l2: 0.449093
[38]	training's l2: 0.36929	valid_1's l2: 0.447192
[39]	training's l2: 0.36683	valid_1's l2: 0.446486
[40]	training's l2: 0.364719	valid_1's l2: 0.446174
[41]	training's l2: 0.362563	valid_1's l2: 0.4458
[42]	training's l2: 0.359187	valid_1's l2: 0.443946
[43]	training's l2: 0.357114	valid_1's l2: 0.443107
[44]	training's l2: 0.354648	valid_1's l2: 0.442118
[45]	training's l2: 0.352644	valid_1's l2: 0.442121
[46]	training's l2: 0.350793	valid_1's l2: 0.441908
[47]	training's l2: 0.348243	valid_1's l2: 0.440871
[48]	training's l2: 0.346286	valid_1's l2: 0.440439
[49]	training's l2: 0.344529	valid_1's l2: 0.440374
[50]	training's l2: 0.342545	valid_1's l2: 0.440077
[51]	training's l2: 0.339893	valid_1's l2: 0.438485
[52]	training's l2: 0.338231	valid_1's l2: 0.43842
[53]	training's l2: 0.336455	valid_1's l2: 0.438081
[54]	training's l2: 0.333971	valid_1's l2: 0.436733
[55]	training's l2: 0.332284	valid_1's l2: 0.436304
[56]	training's l2: 0.330607	valid_1's l2: 0.436329
[57]	training's l2: 0.328958	valid_1's l2: 0.435991
[58]	training's l2: 0.326985	valid_1's l2: 0.435061
[59]	training's l2: 0.324925	valid_1's l2: 0.434302
[60]	training's l2: 0.32292	valid_1's l2: 0.433462
[61]	training's l2: 0.321494	valid_1's l2: 0.433294
[62]	training's l2: 0.320037	valid_1's l2: 0.43299
[63]	training's l2: 0.318578	valid_1's l2: 0.433024
[64]	training's l2: 0.316771	valid_1's l2: 0.432379
[65]	training's l2: 0.315066	valid_1's l2: 0.432018
[66]	training's l2: 0.313719	valid_1's l2: 0.431966
[67]	training's l2: 0.312149	valid_1's l2: 0.431363
[68]	training's l2: 0.310676	valid_1's l2: 0.431149
[69]	training's l2: 0.309306	valid_1's l2: 0.430982
[70]	training's l2: 0.307939	valid_1's l2: 0.430898
[71]	training's l2: 0.306402	valid_1's l2: 0.430611
[72]	training's l2: 0.305116	valid_1's l2: 0.430247
[73]	training's l2: 0.303556	valid_1's l2: 0.429753
[74]	training's l2: 0.30238	valid_1's l2: 0.429767
[75]	training's l2: 0.301088	valid_1's l2: 0.429665
[76]	training's l2: 0.299736	valid_1's l2: 0.4292
[77]	training's l2: 0.298304	valid_1's l2: 0.429084
[78]	training's l2: 0.297043	valid_1's l2: 0.429157
[79]	training's l2: 0.295676	valid_1's l2: 0.428888
[80]	training's l2: 0.294482	valid_1's l2: 0.428785
[81]	training's l2: 0.293256	valid_1's l2: 0.428795
[82]	training's l2: 0.292133	valid_1's l2: 0.428894
[83]	training's l2: 0.291027	valid_1's l2: 0.429209
[84]	training's l2: 0.289834	valid_1's l2: 0.429073
[85]	training's l2: 0.288708	valid_1's l2: 0.429059
[86]	training's l2: 0.287634	valid_1's l2: 0.429171
[87]	training's l2: 0.286625	valid_1's l2: 0.429291
[88]	training's l2: 0.285498	valid_1's l2: 0.428989
[89]	training's l2: 0.284438	valid_1's l2: 0.429082
[90]	training's l2: 0.283165	valid_1's l2: 0.429101
[91]	training's l2: 0.28211	valid_1's l2: 0.429231
[92]	training's l2: 0.281048	valid_1's l2: 0.429202
[93]	training's l2: 0.279945	valid_1's l2: 0.429167
[94]	training's l2: 0.278809	valid_1's l2: 0.42915
[95]	training's l2: 0.277669	valid_1's l2: 0.429126
[96]	training's l2: 0.276673	valid_1's l2: 0.429203
[97]	training's l2: 0.275401	valid_1's l2: 0.428661
[98]	training's l2: 0.274358	valid_1's l2: 0.428549
[99]	training's l2: 0.273279	valid_1's l2: 0.428503
[100]	training's l2: 0.272332	valid_1's l2: 0.428368
[101]	training's l2: 0.271386	valid_1's l2: 0.428425
[102]	training's l2: 0.270454	valid_1's l2: 0.428397
[103]	training's l2: 0.269346	valid_1's l2: 0.428393
[104]	training's l2: 0.268195	valid_1's l2: 0.42801
[105]	training's l2: 0.26726	valid_1's l2: 0.428156
[106]	training's l2: 0.266393	valid_1's l2: 0.428166
[107]	training's l2: 0.265477	valid_1's l2: 0.428113
[108]	training's l2: 0.264457	valid_1's l2: 0.428003
[109]	training's l2: 0.26338	valid_1's l2: 0.427948
[110]	training's l2: 0.262417	valid_1's l2: 0.427906
[111]	training's l2: 0.261409	valid_1's l2: 0.428082
[112]	training's l2: 0.260652	valid_1's l2: 0.428197
[113]	training's l2: 0.25967	valid_1's l2: 0.427995
[114]	training's l2: 0.25867	valid_1's l2: 0.428233
[115]	training's l2: 0.257774	valid_1's l2: 0.428299
[116]	training's l2: 0.25698	valid_1's l2: 0.428269
[117]	training's l2: 0.2562	valid_1's l2: 0.428267
[118]	training's l2: 0.255112	valid_1's l2: 0.428483
[119]	training's l2: 0.254075	valid_1's l2: 0.428309
[120]	training's l2: 0.253048	valid_1's l2: 0.428055
[121]	training's l2: 0.252338	valid_1's l2: 0.428131
[122]	training's l2: 0.25139	valid_1's l2: 0.428212
[123]	training's l2: 0.250499	valid_1's l2: 0.428205
[124]	training's l2: 0.249609	valid_1's l2: 0.428179
[125]	training's l2: 0.248751	valid_1's l2: 0.428218
[126]	training's l2: 0.248017	valid_1's l2: 0.428328
[127]	training's l2: 0.247089	valid_1's l2: 0.42834
[128]	training's l2: 0.246265	valid_1's l2: 0.428431
[129]	training's l2: 0.24542	valid_1's l2: 0.428364
[130]	training's l2: 0.244628	valid_1's l2: 0.428158
[131]	training's l2: 0.243704	valid_1's l2: 0.428052
[132]	training's l2: 0.242907	valid_1's l2: 0.428161
[133]	training's l2: 0.242262	valid_1's l2: 0.427996
[134]	training's l2: 0.241345	valid_1's l2: 0.427926
[135]	training's l2: 0.240453	valid_1's l2: 0.427913
[136]	training's l2: 0.239521	valid_1's l2: 0.427716
[137]	training's l2: 0.238697	valid_1's l2: 0.427658
[138]	training's l2: 0.238039	valid_1's l2: 0.427568
[139]	training's l2: 0.237312	valid_1's l2: 0.427571
[140]	training's l2: 0.236641	valid_1's l2: 0.427462
[141]	training's l2: 0.235829	valid_1's l2: 0.427336
[142]	training's l2: 0.235176	valid_1's l2: 0.42736
[143]	training's l2: 0.234253	valid_1's l2: 0.42752
[144]	training's l2: 0.233513	valid_1's l2: 0.427572
[145]	training's l2: 0.232762	valid_1's l2: 0.427608
[146]	training's l2: 0.231956	valid_1's l2: 0.42764
[147]	training's l2: 0.231207	valid_1's l2: 0.427646
[148]	training's l2: 0.230401	valid_1's l2: 0.427644
[149]	training's l2: 0.22979	valid_1's l2: 0.427611
[150]	training's l2: 0.229248	valid_1's l2: 0.42748
Did not meet early stopping. Best iteration is:
[150]	training's l2: 0.229248	valid_1's l2: 0.42748
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.183825 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.685468	valid_1's l2: 0.687523
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.654505	valid_1's l2: 0.656546
[3]	training's l2: 0.628453	valid_1's l2: 0.631952
[4]	training's l2: 0.605746	valid_1's l2: 0.610048
[5]	training's l2: 0.585544	valid_1's l2: 0.590761
[6]	training's l2: 0.568751	valid_1's l2: 0.574293
[7]	training's l2: 0.554051	valid_1's l2: 0.560412
[8]	training's l2: 0.540693	valid_1's l2: 0.548841
[9]	training's l2: 0.529164	valid_1's l2: 0.538613
[10]	training's l2: 0.519143	valid_1's l2: 0.52953
[11]	training's l2: 0.509895	valid_1's l2: 0.522164
[12]	training's l2: 0.501752	valid_1's l2: 0.51579
[13]	training's l2: 0.493622	valid_1's l2: 0.508435
[14]	training's l2: 0.486874	valid_1's l2: 0.503606
[15]	training's l2: 0.479373	valid_1's l2: 0.498471
[16]	training's l2: 0.473475	valid_1's l2: 0.493824
[17]	training's l2: 0.467909	valid_1's l2: 0.490027
[18]	training's l2: 0.462874	valid_1's l2: 0.48668
[19]	training's l2: 0.458614	valid_1's l2: 0.483893
[20]	training's l2: 0.454017	valid_1's l2: 0.480737
[21]	training's l2: 0.449657	valid_1's l2: 0.477818
[22]	training's l2: 0.444422	valid_1's l2: 0.474357
[23]	training's l2: 0.440219	valid_1's l2: 0.471455
[24]	training's l2: 0.437079	valid_1's l2: 0.469832
[25]	training's l2: 0.433151	valid_1's l2: 0.467725
[26]	training's l2: 0.429893	valid_1's l2: 0.466049
[27]	training's l2: 0.427046	valid_1's l2: 0.464583
[28]	training's l2: 0.424568	valid_1's l2: 0.463079
[29]	training's l2: 0.421492	valid_1's l2: 0.462128
[30]	training's l2: 0.418685	valid_1's l2: 0.46058
[31]	training's l2: 0.416488	valid_1's l2: 0.459389
[32]	training's l2: 0.413322	valid_1's l2: 0.457238
[33]	training's l2: 0.410459	valid_1's l2: 0.455473
[34]	training's l2: 0.407983	valid_1's l2: 0.454408
[35]	training's l2: 0.405527	valid_1's l2: 0.452931
[36]	training's l2: 0.403717	valid_1's l2: 0.452532
[37]	training's l2: 0.401974	valid_1's l2: 0.451678
[38]	training's l2: 0.399744	valid_1's l2: 0.450335
[39]	training's l2: 0.397615	valid_1's l2: 0.449276
[40]	training's l2: 0.395719	valid_1's l2: 0.448418
[41]	training's l2: 0.393469	valid_1's l2: 0.447558
[42]	training's l2: 0.391843	valid_1's l2: 0.44709
[43]	training's l2: 0.389529	valid_1's l2: 0.445522
[44]	training's l2: 0.387632	valid_1's l2: 0.443789
[45]	training's l2: 0.385483	valid_1's l2: 0.442641
[46]	training's l2: 0.383882	valid_1's l2: 0.442187
[47]	training's l2: 0.382322	valid_1's l2: 0.441925
[48]	training's l2: 0.380175	valid_1's l2: 0.440662
[49]	training's l2: 0.37875	valid_1's l2: 0.440365
[50]	training's l2: 0.376815	valid_1's l2: 0.438929
[51]	training's l2: 0.37546	valid_1's l2: 0.438703
[52]	training's l2: 0.373935	valid_1's l2: 0.438388
[53]	training's l2: 0.372746	valid_1's l2: 0.438115
[54]	training's l2: 0.371169	valid_1's l2: 0.437298
[55]	training's l2: 0.369015	valid_1's l2: 0.436131
[56]	training's l2: 0.366842	valid_1's l2: 0.43474
[57]	training's l2: 0.365422	valid_1's l2: 0.434432
[58]	training's l2: 0.364301	valid_1's l2: 0.434411
[59]	training's l2: 0.363046	valid_1's l2: 0.434234
[60]	training's l2: 0.361939	valid_1's l2: 0.433917
[61]	training's l2: 0.360201	valid_1's l2: 0.433049
[62]	training's l2: 0.359118	valid_1's l2: 0.433116
[63]	training's l2: 0.35729	valid_1's l2: 0.431753
[64]	training's l2: 0.356196	valid_1's l2: 0.431269
[65]	training's l2: 0.354906	valid_1's l2: 0.430947
[66]	training's l2: 0.353674	valid_1's l2: 0.430818
[67]	training's l2: 0.352675	valid_1's l2: 0.430869
[68]	training's l2: 0.35158	valid_1's l2: 0.431054
[69]	training's l2: 0.350139	valid_1's l2: 0.43024
[70]	training's l2: 0.349187	valid_1's l2: 0.430307
[71]	training's l2: 0.347892	valid_1's l2: 0.429593
[72]	training's l2: 0.346721	valid_1's l2: 0.428979
[73]	training's l2: 0.345796	valid_1's l2: 0.42885
[74]	training's l2: 0.344842	valid_1's l2: 0.428747
[75]	training's l2: 0.343811	valid_1's l2: 0.428342
[76]	training's l2: 0.342865	valid_1's l2: 0.428129
[77]	training's l2: 0.34171	valid_1's l2: 0.427892
[78]	training's l2: 0.340296	valid_1's l2: 0.426886
[79]	training's l2: 0.339343	valid_1's l2: 0.426758
[80]	training's l2: 0.33839	valid_1's l2: 0.426944
[81]	training's l2: 0.337526	valid_1's l2: 0.426923
[82]	training's l2: 0.3366	valid_1's l2: 0.427
[83]	training's l2: 0.335747	valid_1's l2: 0.426727
[84]	training's l2: 0.334789	valid_1's l2: 0.426701
[85]	training's l2: 0.333545	valid_1's l2: 0.425843
[86]	training's l2: 0.332688	valid_1's l2: 0.425929
[87]	training's l2: 0.331913	valid_1's l2: 0.425932
[88]	training's l2: 0.330705	valid_1's l2: 0.425745
[89]	training's l2: 0.329793	valid_1's l2: 0.425788
[90]	training's l2: 0.329012	valid_1's l2: 0.425641
[91]	training's l2: 0.328207	valid_1's l2: 0.425549
[92]	training's l2: 0.327554	valid_1's l2: 0.425755
[93]	training's l2: 0.32673	valid_1's l2: 0.425554
[94]	training's l2: 0.32595	valid_1's l2: 0.425453
[95]	training's l2: 0.325057	valid_1's l2: 0.42546
[96]	training's l2: 0.324285	valid_1's l2: 0.425605
[97]	training's l2: 0.323484	valid_1's l2: 0.425571
[98]	training's l2: 0.322738	valid_1's l2: 0.425516
[99]	training's l2: 0.322027	valid_1's l2: 0.425562
[100]	training's l2: 0.321232	valid_1's l2: 0.425347
[101]	training's l2: 0.320441	valid_1's l2: 0.425275
[102]	training's l2: 0.319459	valid_1's l2: 0.425044
[103]	training's l2: 0.318767	valid_1's l2: 0.425104
Did not meet early stopping. Best iteration is:
[103]	training's l2: 0.318767	valid_1's l2: 0.425104
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.185931 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
[LightGBM] [Info] Start training from score 0.005978
[1]	training's l2: 0.690328	valid_1's l2: 0.691488
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.663164	valid_1's l2: 0.662988
[3]	training's l2: 0.640007	valid_1's l2: 0.63944
[4]	training's l2: 0.620278	valid_1's l2: 0.619507
[5]	training's l2: 0.60303	valid_1's l2: 0.602328
[6]	training's l2: 0.58781	valid_1's l2: 0.586805
[7]	training's l2: 0.573367	valid_1's l2: 0.572905
[8]	training's l2: 0.560815	valid_1's l2: 0.560953
[9]	training's l2: 0.549821	valid_1's l2: 0.550945
[10]	training's l2: 0.539917	valid_1's l2: 0.542712
[11]	training's l2: 0.531431	valid_1's l2: 0.535243
[12]	training's l2: 0.523093	valid_1's l2: 0.528071
[13]	training's l2: 0.515581	valid_1's l2: 0.521583
[14]	training's l2: 0.509024	valid_1's l2: 0.516265
[15]	training's l2: 0.502499	valid_1's l2: 0.510938
[16]	training's l2: 0.496485	valid_1's l2: 0.506246
[17]	training's l2: 0.491345	valid_1's l2: 0.502697
[18]	training's l2: 0.485637	valid_1's l2: 0.498121
[19]	training's l2: 0.481265	valid_1's l2: 0.494891
[20]	training's l2: 0.47624	valid_1's l2: 0.491116
[21]	training's l2: 0.47178	valid_1's l2: 0.487695
[22]	training's l2: 0.467908	valid_1's l2: 0.484782
[23]	training's l2: 0.464084	valid_1's l2: 0.48165
[24]	training's l2: 0.459291	valid_1's l2: 0.478188
[25]	training's l2: 0.456416	valid_1's l2: 0.476387
[26]	training's l2: 0.453251	valid_1's l2: 0.474472
[27]	training's l2: 0.45008	valid_1's l2: 0.472489
[28]	training's l2: 0.447058	valid_1's l2: 0.470952
[29]	training's l2: 0.444115	valid_1's l2: 0.469004
[30]	training's l2: 0.441666	valid_1's l2: 0.46777
[31]	training's l2: 0.439252	valid_1's l2: 0.466597
[32]	training's l2: 0.436523	valid_1's l2: 0.464766
[33]	training's l2: 0.434029	valid_1's l2: 0.463463
[34]	training's l2: 0.43209	valid_1's l2: 0.462607
[35]	training's l2: 0.430011	valid_1's l2: 0.461789
[36]	training's l2: 0.427888	valid_1's l2: 0.460904
[37]	training's l2: 0.426247	valid_1's l2: 0.46009
[38]	training's l2: 0.423534	valid_1's l2: 0.458299
[39]	training's l2: 0.421811	valid_1's l2: 0.457582
[40]	training's l2: 0.420342	valid_1's l2: 0.456919
[41]	training's l2: 0.418814	valid_1's l2: 0.456443
[42]	training's l2: 0.41683	valid_1's l2: 0.455398
[43]	training's l2: 0.414475	valid_1's l2: 0.45391
[44]	training's l2: 0.412879	valid_1's l2: 0.452694
[45]	training's l2: 0.411202	valid_1's l2: 0.451982
[46]	training's l2: 0.409282	valid_1's l2: 0.450433
[47]	training's l2: 0.407888	valid_1's l2: 0.449826
[48]	training's l2: 0.40619	valid_1's l2: 0.448827
[49]	training's l2: 0.40495	valid_1's l2: 0.448537
[50]	training's l2: 0.402153	valid_1's l2: 0.446732
[51]	training's l2: 0.40036	valid_1's l2: 0.445603
[52]	training's l2: 0.399156	valid_1's l2: 0.445117
[53]	training's l2: 0.397992	valid_1's l2: 0.444858
[54]	training's l2: 0.39622	valid_1's l2: 0.443619
[55]	training's l2: 0.395185	valid_1's l2: 0.443369
[56]	training's l2: 0.393845	valid_1's l2: 0.442965
[57]	training's l2: 0.392732	valid_1's l2: 0.442417
[58]	training's l2: 0.391659	valid_1's l2: 0.442146
[59]	training's l2: 0.390511	valid_1's l2: 0.442035
[60]	training's l2: 0.389491	valid_1's l2: 0.441663
[61]	training's l2: 0.388485	valid_1's l2: 0.441492
[62]	training's l2: 0.387499	valid_1's l2: 0.441382
[63]	training's l2: 0.386069	valid_1's l2: 0.44032
[64]	training's l2: 0.384588	valid_1's l2: 0.439785
[65]	training's l2: 0.383176	valid_1's l2: 0.439249
[66]	training's l2: 0.382109	valid_1's l2: 0.439055
[67]	training's l2: 0.381103	valid_1's l2: 0.438689
[68]	training's l2: 0.380241	valid_1's l2: 0.438571
[69]	training's l2: 0.378607	valid_1's l2: 0.437653
[70]	training's l2: 0.377773	valid_1's l2: 0.437364
[71]	training's l2: 0.376357	valid_1's l2: 0.43694
[72]	training's l2: 0.375524	valid_1's l2: 0.43681
[73]	training's l2: 0.37409	valid_1's l2: 0.435555
[74]	training's l2: 0.373152	valid_1's l2: 0.435227
[75]	training's l2: 0.371953	valid_1's l2: 0.434871
[76]	training's l2: 0.37114	valid_1's l2: 0.434591
[77]	training's l2: 0.370065	valid_1's l2: 0.43423
[78]	training's l2: 0.369183	valid_1's l2: 0.433997
[79]	training's l2: 0.368106	valid_1's l2: 0.433582
[80]	training's l2: 0.367229	valid_1's l2: 0.433321
[81]	training's l2: 0.366342	valid_1's l2: 0.432991
[82]	training's l2: 0.36538	valid_1's l2: 0.432822
[83]	training's l2: 0.364657	valid_1's l2: 0.432718
[84]	training's l2: 0.363703	valid_1's l2: 0.432377
[85]	training's l2: 0.362959	valid_1's l2: 0.43234
[86]	training's l2: 0.362063	valid_1's l2: 0.431955
[87]	training's l2: 0.361112	valid_1's l2: 0.431625
[88]	training's l2: 0.360386	valid_1's l2: 0.431458
[89]	training's l2: 0.359651	valid_1's l2: 0.431399
[90]	training's l2: 0.358858	valid_1's l2: 0.431265
[91]	training's l2: 0.358093	valid_1's l2: 0.431206
[92]	training's l2: 0.357408	valid_1's l2: 0.43126
[93]	training's l2: 0.356618	valid_1's l2: 0.431189
[94]	training's l2: 0.355898	valid_1's l2: 0.431141
[95]	training's l2: 0.355267	valid_1's l2: 0.431191
[96]	training's l2: 0.35455	valid_1's l2: 0.431125
[97]	training's l2: 0.353866	valid_1's l2: 0.431041
[98]	training's l2: 0.353159	valid_1's l2: 0.431152
[99]	training's l2: 0.352204	valid_1's l2: 0.430813
[100]	training's l2: 0.351588	valid_1's l2: 0.430698
[101]	training's l2: 0.351039	valid_1's l2: 0.4307
[102]	training's l2: 0.35038	valid_1's l2: 0.430313
[103]	training's l2: 0.349783	valid_1's l2: 0.430209
[104]	training's l2: 0.348918	valid_1's l2: 0.429927
[105]	training's l2: 0.348297	valid_1's l2: 0.429834
[106]	training's l2: 0.347674	valid_1's l2: 0.429862
[107]	training's l2: 0.347037	valid_1's l2: 0.429913
[108]	training's l2: 0.346463	valid_1's l2: 0.429554
[109]	training's l2: 0.345727	valid_1's l2: 0.42904
[110]	training's l2: 0.344972	valid_1's l2: 0.428884
[111]	training's l2: 0.344378	valid_1's l2: 0.428888
[112]	training's l2: 0.343753	valid_1's l2: 0.42901
[113]	training's l2: 0.34317	valid_1's l2: 0.429064
[114]	training's l2: 0.342369	valid_1's l2: 0.428961
[115]	training's l2: 0.341769	valid_1's l2: 0.428982
[116]	training's l2: 0.340897	valid_1's l2: 0.428916
[117]	training's l2: 0.340365	valid_1's l2: 0.428726
[118]	training's l2: 0.339822	valid_1's l2: 0.428533
[119]	training's l2: 0.339123	valid_1's l2: 0.42846
[120]	training's l2: 0.338593	valid_1's l2: 0.428394
[121]	training's l2: 0.337873	valid_1's l2: 0.428172
[122]	training's l2: 0.337286	valid_1's l2: 0.428148
[123]	training's l2: 0.33669	valid_1's l2: 0.428298
[124]	training's l2: 0.336165	valid_1's l2: 0.428298
[125]	training's l2: 0.335301	valid_1's l2: 0.427993
[126]	training's l2: 0.334797	valid_1's l2: 0.427997
[127]	training's l2: 0.334141	valid_1's l2: 0.427917
[128]	training's l2: 0.333389	valid_1's l2: 0.427779
[129]	training's l2: 0.332747	valid_1's l2: 0.427866
[130]	training's l2: 0.332133	valid_1's l2: 0.427859
[131]	training's l2: 0.331606	valid_1's l2: 0.427846
[132]	training's l2: 0.331053	valid_1's l2: 0.427857
[133]	training's l2: 0.330508	valid_1's l2: 0.427728
[134]	training's l2: 0.329953	valid_1's l2: 0.427692
[135]	training's l2: 0.329545	valid_1's l2: 0.427732
[136]	training's l2: 0.329016	valid_1's l2: 0.42766
[137]	training's l2: 0.32849	valid_1's l2: 0.427633
[138]	training's l2: 0.327983	valid_1's l2: 0.427763
[139]	training's l2: 0.327501	valid_1's l2: 0.427681
[140]	training's l2: 0.326909	valid_1's l2: 0.427794
[141]	training's l2: 0.326493	valid_1's l2: 0.427832
[142]	training's l2: 0.325932	valid_1's l2: 0.427837
[143]	training's l2: 0.325356	valid_1's l2: 0.427866
[144]	training's l2: 0.324792	valid_1's l2: 0.427666
[145]	training's l2: 0.324168	valid_1's l2: 0.427686
[146]	training's l2: 0.323661	valid_1's l2: 0.427237
[147]	training's l2: 0.323183	valid_1's l2: 0.427152
[148]	training's l2: 0.322512	valid_1's l2: 0.427106
[149]	training's l2: 0.322039	valid_1's l2: 0.427094
[150]	training's l2: 0.321498	valid_1's l2: 0.427008
[151]	training's l2: 0.320987	valid_1's l2: 0.426957
[152]	training's l2: 0.3205	valid_1's l2: 0.426971
[153]	training's l2: 0.319924	valid_1's l2: 0.426675
[154]	training's l2: 0.319401	valid_1's l2: 0.426663
[155]	training's l2: 0.318895	valid_1's l2: 0.426634
[156]	training's l2: 0.31834	valid_1's l2: 0.42662
[157]	training's l2: 0.31774	valid_1's l2: 0.426401
[158]	training's l2: 0.317051	valid_1's l2: 0.426488
[159]	training's l2: 0.316606	valid_1's l2: 0.426533
[160]	training's l2: 0.31608	valid_1's l2: 0.42654
[161]	training's l2: 0.315721	valid_1's l2: 0.426679
[162]	training's l2: 0.315271	valid_1's l2: 0.426808
[163]	training's l2: 0.314902	valid_1's l2: 0.426774
[164]	training's l2: 0.314533	valid_1's l2: 0.426808
[165]	training's l2: 0.314047	valid_1's l2: 0.426857
[166]	training's l2: 0.313537	valid_1's l2: 0.426958
[167]	training's l2: 0.313032	valid_1's l2: 0.427254
[168]	training's l2: 0.312498	valid_1's l2: 0.427133
[169]	training's l2: 0.311918	valid_1's l2: 0.427216
[170]	training's l2: 0.311455	valid_1's l2: 0.427081
[171]	training's l2: 0.310964	valid_1's l2: 0.427054
[172]	training's l2: 0.310543	valid_1's l2: 0.427012
[173]	training's l2: 0.310099	valid_1's l2: 0.426926
[174]	training's l2: 0.309757	valid_1's l2: 0.426905
[175]	training's l2: 0.309295	valid_1's l2: 0.426748
[176]	training's l2: 0.308788	valid_1's l2: 0.426914
[177]	training's l2: 0.308299	valid_1's l2: 0.426851
[178]	training's l2: 0.307811	valid_1's l2: 0.426868
[179]	training's l2: 0.307361	valid_1's l2: 0.426955
[180]	training's l2: 0.306866	valid_1's l2: 0.426899
[181]	training's l2: 0.306317	valid_1's l2: 0.426744
[182]	training's l2: 0.305844	valid_1's l2: 0.426701
[183]	training's l2: 0.305176	valid_1's l2: 0.426375
[184]	training's l2: 0.304726	valid_1's l2: 0.426356
[185]	training's l2: 0.304254	valid_1's l2: 0.426357
[186]	training's l2: 0.303762	valid_1's l2: 0.426332
[187]	training's l2: 0.303399	valid_1's l2: 0.426336
[188]	training's l2: 0.302894	valid_1's l2: 0.426355
[189]	training's l2: 0.302526	valid_1's l2: 0.426415
[190]	training's l2: 0.302105	valid_1's l2: 0.426365
[191]	training's l2: 0.301661	valid_1's l2: 0.426445
Did not meet early stopping. Best iteration is:
[191]	training's l2: 0.301661	valid_1's l2: 0.426445
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181681 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.73571	valid_1's l2: 0.716793
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.705596	valid_1's l2: 0.69178
[3]	training's l2: 0.680669	valid_1's l2: 0.670451
[4]	training's l2: 0.657304	valid_1's l2: 0.650888
[5]	training's l2: 0.636779	valid_1's l2: 0.63419
[6]	training's l2: 0.619388	valid_1's l2: 0.620403
[7]	training's l2: 0.60054	valid_1's l2: 0.605569
[8]	training's l2: 0.586031	valid_1's l2: 0.594836
[9]	training's l2: 0.572278	valid_1's l2: 0.583845
[10]	training's l2: 0.561575	valid_1's l2: 0.575757
[11]	training's l2: 0.551827	valid_1's l2: 0.569409
[12]	training's l2: 0.540836	valid_1's l2: 0.560629
[13]	training's l2: 0.531176	valid_1's l2: 0.553935
[14]	training's l2: 0.521952	valid_1's l2: 0.547142
[15]	training's l2: 0.514009	valid_1's l2: 0.541072
[16]	training's l2: 0.506506	valid_1's l2: 0.535846
[17]	training's l2: 0.49897	valid_1's l2: 0.531226
[18]	training's l2: 0.492736	valid_1's l2: 0.5268
[19]	training's l2: 0.486004	valid_1's l2: 0.522193
[20]	training's l2: 0.480854	valid_1's l2: 0.519007
[21]	training's l2: 0.475629	valid_1's l2: 0.515747
[22]	training's l2: 0.470838	valid_1's l2: 0.513256
[23]	training's l2: 0.465415	valid_1's l2: 0.509554
[24]	training's l2: 0.460463	valid_1's l2: 0.506759
[25]	training's l2: 0.456517	valid_1's l2: 0.504572
[26]	training's l2: 0.450691	valid_1's l2: 0.499889
[27]	training's l2: 0.447092	valid_1's l2: 0.497995
[28]	training's l2: 0.443745	valid_1's l2: 0.496313
[29]	training's l2: 0.439312	valid_1's l2: 0.493971
[30]	training's l2: 0.435798	valid_1's l2: 0.491886
[31]	training's l2: 0.43052	valid_1's l2: 0.487874
[32]	training's l2: 0.427524	valid_1's l2: 0.486777
[33]	training's l2: 0.424316	valid_1's l2: 0.485174
[34]	training's l2: 0.421499	valid_1's l2: 0.4837
[35]	training's l2: 0.417418	valid_1's l2: 0.480828
[36]	training's l2: 0.415131	valid_1's l2: 0.479909
[37]	training's l2: 0.412897	valid_1's l2: 0.478789
[38]	training's l2: 0.410381	valid_1's l2: 0.477753
[39]	training's l2: 0.407961	valid_1's l2: 0.47684
[40]	training's l2: 0.404657	valid_1's l2: 0.474868
[41]	training's l2: 0.401913	valid_1's l2: 0.473241
[42]	training's l2: 0.399713	valid_1's l2: 0.4726
[43]	training's l2: 0.397656	valid_1's l2: 0.471791
[44]	training's l2: 0.395646	valid_1's l2: 0.470777
[45]	training's l2: 0.392487	valid_1's l2: 0.469439
[46]	training's l2: 0.39038	valid_1's l2: 0.468534
[47]	training's l2: 0.387986	valid_1's l2: 0.467362
[48]	training's l2: 0.38628	valid_1's l2: 0.467021
[49]	training's l2: 0.384704	valid_1's l2: 0.466496
[50]	training's l2: 0.381979	valid_1's l2: 0.464596
[51]	training's l2: 0.380283	valid_1's l2: 0.464405
[52]	training's l2: 0.378506	valid_1's l2: 0.463913
[53]	training's l2: 0.376858	valid_1's l2: 0.463361
[54]	training's l2: 0.374773	valid_1's l2: 0.462343
[55]	training's l2: 0.372318	valid_1's l2: 0.46051
[56]	training's l2: 0.370276	valid_1's l2: 0.459864
[57]	training's l2: 0.368713	valid_1's l2: 0.459689
[58]	training's l2: 0.367248	valid_1's l2: 0.459397
[59]	training's l2: 0.365241	valid_1's l2: 0.458179
[60]	training's l2: 0.363603	valid_1's l2: 0.458082
[61]	training's l2: 0.361806	valid_1's l2: 0.457829
[62]	training's l2: 0.360214	valid_1's l2: 0.457561
[63]	training's l2: 0.358785	valid_1's l2: 0.457204
[64]	training's l2: 0.357401	valid_1's l2: 0.456956
[65]	training's l2: 0.355181	valid_1's l2: 0.455499
[66]	training's l2: 0.353807	valid_1's l2: 0.455542
[67]	training's l2: 0.352414	valid_1's l2: 0.455378
[68]	training's l2: 0.351161	valid_1's l2: 0.455274
[69]	training's l2: 0.349761	valid_1's l2: 0.45527
[70]	training's l2: 0.348101	valid_1's l2: 0.455324
[71]	training's l2: 0.346834	valid_1's l2: 0.455429
[72]	training's l2: 0.345322	valid_1's l2: 0.454918
[73]	training's l2: 0.34414	valid_1's l2: 0.454749
[74]	training's l2: 0.342545	valid_1's l2: 0.454714
[75]	training's l2: 0.341353	valid_1's l2: 0.454736
[76]	training's l2: 0.34002	valid_1's l2: 0.454475
[77]	training's l2: 0.338887	valid_1's l2: 0.454379
[78]	training's l2: 0.337339	valid_1's l2: 0.453979
[79]	training's l2: 0.335999	valid_1's l2: 0.453955
[80]	training's l2: 0.334692	valid_1's l2: 0.45383
[81]	training's l2: 0.333523	valid_1's l2: 0.45406
[82]	training's l2: 0.332326	valid_1's l2: 0.453571
[83]	training's l2: 0.331133	valid_1's l2: 0.453481
[84]	training's l2: 0.330027	valid_1's l2: 0.453111
[85]	training's l2: 0.328943	valid_1's l2: 0.453344
[86]	training's l2: 0.327717	valid_1's l2: 0.453395
[87]	training's l2: 0.326684	valid_1's l2: 0.453315
[88]	training's l2: 0.325212	valid_1's l2: 0.452843
[89]	training's l2: 0.324163	valid_1's l2: 0.452871
[90]	training's l2: 0.323124	valid_1's l2: 0.452695
[91]	training's l2: 0.322104	valid_1's l2: 0.452557
[92]	training's l2: 0.320938	valid_1's l2: 0.452211
[93]	training's l2: 0.319812	valid_1's l2: 0.452069
[94]	training's l2: 0.318708	valid_1's l2: 0.452059
[95]	training's l2: 0.317564	valid_1's l2: 0.451886
[96]	training's l2: 0.316413	valid_1's l2: 0.451865
[97]	training's l2: 0.315324	valid_1's l2: 0.451406
[98]	training's l2: 0.31429	valid_1's l2: 0.451404
[99]	training's l2: 0.313129	valid_1's l2: 0.451277
[100]	training's l2: 0.312147	valid_1's l2: 0.451251
[101]	training's l2: 0.311144	valid_1's l2: 0.451162
[102]	training's l2: 0.310085	valid_1's l2: 0.45124
[103]	training's l2: 0.308924	valid_1's l2: 0.451107
[104]	training's l2: 0.307906	valid_1's l2: 0.450932
[105]	training's l2: 0.306577	valid_1's l2: 0.450938
[106]	training's l2: 0.305321	valid_1's l2: 0.450183
[107]	training's l2: 0.304254	valid_1's l2: 0.450283
[108]	training's l2: 0.303367	valid_1's l2: 0.450306
[109]	training's l2: 0.302496	valid_1's l2: 0.45013
[110]	training's l2: 0.301459	valid_1's l2: 0.450248
[111]	training's l2: 0.300575	valid_1's l2: 0.450278
[112]	training's l2: 0.299635	valid_1's l2: 0.450323
[113]	training's l2: 0.298648	valid_1's l2: 0.450215
[114]	training's l2: 0.297774	valid_1's l2: 0.450187
[115]	training's l2: 0.296768	valid_1's l2: 0.450147
[116]	training's l2: 0.295703	valid_1's l2: 0.450025
[117]	training's l2: 0.294877	valid_1's l2: 0.450146
[118]	training's l2: 0.294113	valid_1's l2: 0.450271
[119]	training's l2: 0.293267	valid_1's l2: 0.450269
[120]	training's l2: 0.292306	valid_1's l2: 0.450179
[121]	training's l2: 0.291505	valid_1's l2: 0.450256
[122]	training's l2: 0.290735	valid_1's l2: 0.450157
[123]	training's l2: 0.289903	valid_1's l2: 0.450203
[124]	training's l2: 0.289118	valid_1's l2: 0.450138
[125]	training's l2: 0.288227	valid_1's l2: 0.450221
[126]	training's l2: 0.287529	valid_1's l2: 0.450244
[127]	training's l2: 0.286552	valid_1's l2: 0.450121
[128]	training's l2: 0.285483	valid_1's l2: 0.449877
[129]	training's l2: 0.28465	valid_1's l2: 0.450179
[130]	training's l2: 0.283588	valid_1's l2: 0.450031
[131]	training's l2: 0.282801	valid_1's l2: 0.450149
[132]	training's l2: 0.282077	valid_1's l2: 0.45022
[133]	training's l2: 0.281217	valid_1's l2: 0.450203
[134]	training's l2: 0.280497	valid_1's l2: 0.450289
[135]	training's l2: 0.279684	valid_1's l2: 0.450309
[136]	training's l2: 0.278763	valid_1's l2: 0.450166
[137]	training's l2: 0.277986	valid_1's l2: 0.450081
[138]	training's l2: 0.276945	valid_1's l2: 0.449396
[139]	training's l2: 0.276164	valid_1's l2: 0.449285
[140]	training's l2: 0.275403	valid_1's l2: 0.449334
[141]	training's l2: 0.274675	valid_1's l2: 0.449232
[142]	training's l2: 0.273885	valid_1's l2: 0.449259
[143]	training's l2: 0.273075	valid_1's l2: 0.449135
[144]	training's l2: 0.272248	valid_1's l2: 0.449119
[145]	training's l2: 0.271377	valid_1's l2: 0.449049
[146]	training's l2: 0.270683	valid_1's l2: 0.449129
[147]	training's l2: 0.269942	valid_1's l2: 0.449257
[148]	training's l2: 0.269125	valid_1's l2: 0.449259
[149]	training's l2: 0.268303	valid_1's l2: 0.449284
[150]	training's l2: 0.2676	valid_1's l2: 0.449256
[151]	training's l2: 0.26685	valid_1's l2: 0.449316
[152]	training's l2: 0.266137	valid_1's l2: 0.449377
[153]	training's l2: 0.265434	valid_1's l2: 0.449389
[154]	training's l2: 0.264747	valid_1's l2: 0.44927
[155]	training's l2: 0.264027	valid_1's l2: 0.449393
[156]	training's l2: 0.263301	valid_1's l2: 0.449259
[157]	training's l2: 0.262543	valid_1's l2: 0.449046
[158]	training's l2: 0.261747	valid_1's l2: 0.449164
[159]	training's l2: 0.261064	valid_1's l2: 0.449473
[160]	training's l2: 0.260407	valid_1's l2: 0.44947
[161]	training's l2: 0.259658	valid_1's l2: 0.449482
Did not meet early stopping. Best iteration is:
[161]	training's l2: 0.259658	valid_1's l2: 0.449482
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181960 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.741075	valid_1's l2: 0.721762
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.715983	valid_1's l2: 0.701139
[3]	training's l2: 0.692705	valid_1's l2: 0.681964
[4]	training's l2: 0.672514	valid_1's l2: 0.664954
[5]	training's l2: 0.653779	valid_1's l2: 0.64941
[6]	training's l2: 0.63703	valid_1's l2: 0.635883
[7]	training's l2: 0.621271	valid_1's l2: 0.623411
[8]	training's l2: 0.606523	valid_1's l2: 0.611692
[9]	training's l2: 0.593208	valid_1's l2: 0.602039
[10]	training's l2: 0.58092	valid_1's l2: 0.592261
[11]	training's l2: 0.570949	valid_1's l2: 0.585003
[12]	training's l2: 0.560686	valid_1's l2: 0.577884
[13]	training's l2: 0.551781	valid_1's l2: 0.570861
[14]	training's l2: 0.541621	valid_1's l2: 0.562836
[15]	training's l2: 0.533943	valid_1's l2: 0.557598
[16]	training's l2: 0.524767	valid_1's l2: 0.550979
[17]	training's l2: 0.517015	valid_1's l2: 0.545427
[18]	training's l2: 0.510189	valid_1's l2: 0.540836
[19]	training's l2: 0.504155	valid_1's l2: 0.536999
[20]	training's l2: 0.496527	valid_1's l2: 0.531481
[21]	training's l2: 0.491179	valid_1's l2: 0.528209
[22]	training's l2: 0.4859	valid_1's l2: 0.524924
[23]	training's l2: 0.480633	valid_1's l2: 0.521734
[24]	training's l2: 0.474342	valid_1's l2: 0.517105
[25]	training's l2: 0.470355	valid_1's l2: 0.515078
[26]	training's l2: 0.466017	valid_1's l2: 0.512529
[27]	training's l2: 0.462137	valid_1's l2: 0.510984
[28]	training's l2: 0.457151	valid_1's l2: 0.507638
[29]	training's l2: 0.452821	valid_1's l2: 0.504824
[30]	training's l2: 0.448907	valid_1's l2: 0.502599
[31]	training's l2: 0.445467	valid_1's l2: 0.500381
[32]	training's l2: 0.442255	valid_1's l2: 0.498838
[33]	training's l2: 0.439265	valid_1's l2: 0.497214
[34]	training's l2: 0.436015	valid_1's l2: 0.495798
[35]	training's l2: 0.433507	valid_1's l2: 0.494607
[36]	training's l2: 0.430777	valid_1's l2: 0.49295
[37]	training's l2: 0.427617	valid_1's l2: 0.491184
[38]	training's l2: 0.424288	valid_1's l2: 0.489651
[39]	training's l2: 0.419431	valid_1's l2: 0.485525
[40]	training's l2: 0.415186	valid_1's l2: 0.482311
[41]	training's l2: 0.412732	valid_1's l2: 0.48152
[42]	training's l2: 0.410584	valid_1's l2: 0.480863
[43]	training's l2: 0.407764	valid_1's l2: 0.479393
[44]	training's l2: 0.403966	valid_1's l2: 0.476626
[45]	training's l2: 0.401306	valid_1's l2: 0.475567
[46]	training's l2: 0.399148	valid_1's l2: 0.474642
[47]	training's l2: 0.397191	valid_1's l2: 0.473718
[48]	training's l2: 0.395122	valid_1's l2: 0.473148
[49]	training's l2: 0.393223	valid_1's l2: 0.472351
[50]	training's l2: 0.391448	valid_1's l2: 0.4718
[51]	training's l2: 0.388669	valid_1's l2: 0.47028
[52]	training's l2: 0.386405	valid_1's l2: 0.469034
[53]	training's l2: 0.384537	valid_1's l2: 0.468426
[54]	training's l2: 0.382853	valid_1's l2: 0.467918
[55]	training's l2: 0.38042	valid_1's l2: 0.466782
[56]	training's l2: 0.378861	valid_1's l2: 0.46623
[57]	training's l2: 0.377164	valid_1's l2: 0.465416
[58]	training's l2: 0.374456	valid_1's l2: 0.46403
[59]	training's l2: 0.372939	valid_1's l2: 0.463842
[60]	training's l2: 0.371335	valid_1's l2: 0.462971
[61]	training's l2: 0.369226	valid_1's l2: 0.462255
[62]	training's l2: 0.367469	valid_1's l2: 0.461463
[63]	training's l2: 0.365996	valid_1's l2: 0.461341
[64]	training's l2: 0.364492	valid_1's l2: 0.46103
[65]	training's l2: 0.362443	valid_1's l2: 0.459948
[66]	training's l2: 0.361079	valid_1's l2: 0.459826
[67]	training's l2: 0.359452	valid_1's l2: 0.459416
[68]	training's l2: 0.358087	valid_1's l2: 0.459222
[69]	training's l2: 0.356781	valid_1's l2: 0.458849
[70]	training's l2: 0.354964	valid_1's l2: 0.457867
[71]	training's l2: 0.353059	valid_1's l2: 0.456855
[72]	training's l2: 0.351802	valid_1's l2: 0.456759
[73]	training's l2: 0.350585	valid_1's l2: 0.456468
[74]	training's l2: 0.348934	valid_1's l2: 0.455993
[75]	training's l2: 0.347728	valid_1's l2: 0.455705
[76]	training's l2: 0.346515	valid_1's l2: 0.455485
[77]	training's l2: 0.34528	valid_1's l2: 0.455475
[78]	training's l2: 0.343585	valid_1's l2: 0.454644
[79]	training's l2: 0.342091	valid_1's l2: 0.454082
[80]	training's l2: 0.340731	valid_1's l2: 0.453705
[81]	training's l2: 0.33929	valid_1's l2: 0.453732
[82]	training's l2: 0.338035	valid_1's l2: 0.453218
[83]	training's l2: 0.336905	valid_1's l2: 0.453078
[84]	training's l2: 0.335652	valid_1's l2: 0.452991
[85]	training's l2: 0.334529	valid_1's l2: 0.452826
[86]	training's l2: 0.333381	valid_1's l2: 0.452721
[87]	training's l2: 0.331945	valid_1's l2: 0.452311
[88]	training's l2: 0.330799	valid_1's l2: 0.452154
[89]	training's l2: 0.329432	valid_1's l2: 0.452119
[90]	training's l2: 0.328109	valid_1's l2: 0.451915
[91]	training's l2: 0.327016	valid_1's l2: 0.451632
[92]	training's l2: 0.325771	valid_1's l2: 0.451335
[93]	training's l2: 0.324722	valid_1's l2: 0.451163
[94]	training's l2: 0.323667	valid_1's l2: 0.450925
[95]	training's l2: 0.322477	valid_1's l2: 0.450697
[96]	training's l2: 0.32118	valid_1's l2: 0.450511
[97]	training's l2: 0.320199	valid_1's l2: 0.4505
[98]	training's l2: 0.31913	valid_1's l2: 0.450486
[99]	training's l2: 0.317988	valid_1's l2: 0.450272
[100]	training's l2: 0.316886	valid_1's l2: 0.44985
[101]	training's l2: 0.31588	valid_1's l2: 0.449827
[102]	training's l2: 0.314658	valid_1's l2: 0.449641
[103]	training's l2: 0.313545	valid_1's l2: 0.449503
[104]	training's l2: 0.312303	valid_1's l2: 0.448926
[105]	training's l2: 0.31129	valid_1's l2: 0.448872
[106]	training's l2: 0.310141	valid_1's l2: 0.448889
[107]	training's l2: 0.309106	valid_1's l2: 0.448594
[108]	training's l2: 0.308153	valid_1's l2: 0.448435
[109]	training's l2: 0.307167	valid_1's l2: 0.448408
[110]	training's l2: 0.306217	valid_1's l2: 0.448382
[111]	training's l2: 0.305259	valid_1's l2: 0.448214
[112]	training's l2: 0.304358	valid_1's l2: 0.448178
[113]	training's l2: 0.303461	valid_1's l2: 0.448084
[114]	training's l2: 0.302464	valid_1's l2: 0.448026
[115]	training's l2: 0.301504	valid_1's l2: 0.447881
[116]	training's l2: 0.30053	valid_1's l2: 0.447656
[117]	training's l2: 0.299587	valid_1's l2: 0.447767
[118]	training's l2: 0.298533	valid_1's l2: 0.447437
[119]	training's l2: 0.297541	valid_1's l2: 0.44708
[120]	training's l2: 0.296246	valid_1's l2: 0.44605
[121]	training's l2: 0.295125	valid_1's l2: 0.445748
[122]	training's l2: 0.294202	valid_1's l2: 0.445949
[123]	training's l2: 0.293279	valid_1's l2: 0.445694
[124]	training's l2: 0.292199	valid_1's l2: 0.445119
[125]	training's l2: 0.291121	valid_1's l2: 0.444959
[126]	training's l2: 0.2902	valid_1's l2: 0.445024
[127]	training's l2: 0.289334	valid_1's l2: 0.445082
[128]	training's l2: 0.288359	valid_1's l2: 0.445061
[129]	training's l2: 0.28751	valid_1's l2: 0.444985
[130]	training's l2: 0.286664	valid_1's l2: 0.445035
[131]	training's l2: 0.285801	valid_1's l2: 0.444785
[132]	training's l2: 0.284789	valid_1's l2: 0.444652
[133]	training's l2: 0.283832	valid_1's l2: 0.444347
[134]	training's l2: 0.282899	valid_1's l2: 0.444249
[135]	training's l2: 0.28189	valid_1's l2: 0.443719
[136]	training's l2: 0.281071	valid_1's l2: 0.443601
[137]	training's l2: 0.280269	valid_1's l2: 0.443547
[138]	training's l2: 0.279363	valid_1's l2: 0.443341
[139]	training's l2: 0.278658	valid_1's l2: 0.443394
[140]	training's l2: 0.277908	valid_1's l2: 0.443269
[141]	training's l2: 0.277104	valid_1's l2: 0.443082
[142]	training's l2: 0.276377	valid_1's l2: 0.443104
[143]	training's l2: 0.275498	valid_1's l2: 0.443224
[144]	training's l2: 0.274734	valid_1's l2: 0.44312
[145]	training's l2: 0.273964	valid_1's l2: 0.44298
[146]	training's l2: 0.273218	valid_1's l2: 0.443014
[147]	training's l2: 0.272526	valid_1's l2: 0.443178
[148]	training's l2: 0.271789	valid_1's l2: 0.442971
[149]	training's l2: 0.271079	valid_1's l2: 0.443027
[150]	training's l2: 0.27029	valid_1's l2: 0.442916
[151]	training's l2: 0.269562	valid_1's l2: 0.442825
[152]	training's l2: 0.268773	valid_1's l2: 0.442869
[153]	training's l2: 0.268064	valid_1's l2: 0.442839
[154]	training's l2: 0.267288	valid_1's l2: 0.442689
[155]	training's l2: 0.266502	valid_1's l2: 0.442695
[156]	training's l2: 0.265679	valid_1's l2: 0.442725
[157]	training's l2: 0.265105	valid_1's l2: 0.442812
[158]	training's l2: 0.264411	valid_1's l2: 0.442727
[159]	training's l2: 0.26363	valid_1's l2: 0.442775
[160]	training's l2: 0.26299	valid_1's l2: 0.442866
[161]	training's l2: 0.262253	valid_1's l2: 0.44292
[162]	training's l2: 0.261566	valid_1's l2: 0.442739
[163]	training's l2: 0.260839	valid_1's l2: 0.442678
[164]	training's l2: 0.26014	valid_1's l2: 0.442731
[165]	training's l2: 0.259459	valid_1's l2: 0.442636
[166]	training's l2: 0.258797	valid_1's l2: 0.442656
[167]	training's l2: 0.258139	valid_1's l2: 0.44275
[168]	training's l2: 0.257415	valid_1's l2: 0.442801
[169]	training's l2: 0.256693	valid_1's l2: 0.442853
[170]	training's l2: 0.256027	valid_1's l2: 0.442702
[171]	training's l2: 0.25545	valid_1's l2: 0.442737
[172]	training's l2: 0.254755	valid_1's l2: 0.442847
[173]	training's l2: 0.254022	valid_1's l2: 0.442816
[174]	training's l2: 0.253388	valid_1's l2: 0.442877
[175]	training's l2: 0.252776	valid_1's l2: 0.442789
[176]	training's l2: 0.252151	valid_1's l2: 0.44275
[177]	training's l2: 0.251453	valid_1's l2: 0.442785
[178]	training's l2: 0.250862	valid_1's l2: 0.442797
[179]	training's l2: 0.250261	valid_1's l2: 0.442815
Did not meet early stopping. Best iteration is:
[179]	training's l2: 0.250261	valid_1's l2: 0.442815
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.177198 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.744203	valid_1's l2: 0.725136
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.721379	valid_1's l2: 0.706944
[3]	training's l2: 0.699925	valid_1's l2: 0.689224
[4]	training's l2: 0.681007	valid_1's l2: 0.67337
[5]	training's l2: 0.663202	valid_1's l2: 0.658708
[6]	training's l2: 0.647177	valid_1's l2: 0.646329
[7]	training's l2: 0.631209	valid_1's l2: 0.633431
[8]	training's l2: 0.616885	valid_1's l2: 0.622206
[9]	training's l2: 0.604366	valid_1's l2: 0.612965
[10]	training's l2: 0.5928	valid_1's l2: 0.603478
[11]	training's l2: 0.581534	valid_1's l2: 0.595325
[12]	training's l2: 0.570897	valid_1's l2: 0.58725
[13]	training's l2: 0.561197	valid_1's l2: 0.580309
[14]	training's l2: 0.552741	valid_1's l2: 0.574373
[15]	training's l2: 0.544261	valid_1's l2: 0.568417
[16]	training's l2: 0.535295	valid_1's l2: 0.561395
[17]	training's l2: 0.528269	valid_1's l2: 0.556654
[18]	training's l2: 0.520752	valid_1's l2: 0.550873
[19]	training's l2: 0.513568	valid_1's l2: 0.546073
[20]	training's l2: 0.506645	valid_1's l2: 0.541109
[21]	training's l2: 0.500639	valid_1's l2: 0.537381
[22]	training's l2: 0.494435	valid_1's l2: 0.532702
[23]	training's l2: 0.489234	valid_1's l2: 0.529826
[24]	training's l2: 0.484374	valid_1's l2: 0.526863
[25]	training's l2: 0.477657	valid_1's l2: 0.522079
[26]	training's l2: 0.472718	valid_1's l2: 0.519413
[27]	training's l2: 0.467422	valid_1's l2: 0.515438
[28]	training's l2: 0.462991	valid_1's l2: 0.512973
[29]	training's l2: 0.4591	valid_1's l2: 0.510688
[30]	training's l2: 0.455363	valid_1's l2: 0.508763
[31]	training's l2: 0.450717	valid_1's l2: 0.505456
[32]	training's l2: 0.447106	valid_1's l2: 0.503664
[33]	training's l2: 0.443495	valid_1's l2: 0.502081
[34]	training's l2: 0.439836	valid_1's l2: 0.500158
[35]	training's l2: 0.43666	valid_1's l2: 0.498318
[36]	training's l2: 0.433702	valid_1's l2: 0.496902
[37]	training's l2: 0.430679	valid_1's l2: 0.495274
[38]	training's l2: 0.427864	valid_1's l2: 0.49388
[39]	training's l2: 0.425045	valid_1's l2: 0.492593
[40]	training's l2: 0.422514	valid_1's l2: 0.491765
[41]	training's l2: 0.419432	valid_1's l2: 0.490328
[42]	training's l2: 0.416708	valid_1's l2: 0.489395
[43]	training's l2: 0.414282	valid_1's l2: 0.488464
[44]	training's l2: 0.41217	valid_1's l2: 0.487688
[45]	training's l2: 0.40949	valid_1's l2: 0.486599
[46]	training's l2: 0.407117	valid_1's l2: 0.485357
[47]	training's l2: 0.403405	valid_1's l2: 0.482505
[48]	training's l2: 0.401116	valid_1's l2: 0.481525
[49]	training's l2: 0.397591	valid_1's l2: 0.478907
[50]	training's l2: 0.395733	valid_1's l2: 0.478301
[51]	training's l2: 0.393482	valid_1's l2: 0.477347
[52]	training's l2: 0.391506	valid_1's l2: 0.476594
[53]	training's l2: 0.3895	valid_1's l2: 0.476115
[54]	training's l2: 0.386564	valid_1's l2: 0.474134
[55]	training's l2: 0.384797	valid_1's l2: 0.473655
[56]	training's l2: 0.382699	valid_1's l2: 0.472809
[57]	training's l2: 0.381104	valid_1's l2: 0.472637
[58]	training's l2: 0.379037	valid_1's l2: 0.471772
[59]	training's l2: 0.376863	valid_1's l2: 0.470618
[60]	training's l2: 0.374642	valid_1's l2: 0.469637
[61]	training's l2: 0.371894	valid_1's l2: 0.46788
[62]	training's l2: 0.370361	valid_1's l2: 0.467528
[63]	training's l2: 0.367744	valid_1's l2: 0.46625
[64]	training's l2: 0.366103	valid_1's l2: 0.465814
[65]	training's l2: 0.3646	valid_1's l2: 0.465495
[66]	training's l2: 0.363177	valid_1's l2: 0.465088
[67]	training's l2: 0.361163	valid_1's l2: 0.46392
[68]	training's l2: 0.359515	valid_1's l2: 0.463193
[69]	training's l2: 0.357996	valid_1's l2: 0.463089
[70]	training's l2: 0.356447	valid_1's l2: 0.462218
[71]	training's l2: 0.355066	valid_1's l2: 0.461963
[72]	training's l2: 0.353393	valid_1's l2: 0.461381
[73]	training's l2: 0.35199	valid_1's l2: 0.460935
[74]	training's l2: 0.35062	valid_1's l2: 0.460639
[75]	training's l2: 0.349271	valid_1's l2: 0.460541
[76]	training's l2: 0.347509	valid_1's l2: 0.460049
[77]	training's l2: 0.346276	valid_1's l2: 0.459809
[78]	training's l2: 0.344792	valid_1's l2: 0.459027
[79]	training's l2: 0.343029	valid_1's l2: 0.458264
[80]	training's l2: 0.341671	valid_1's l2: 0.458145
[81]	training's l2: 0.340468	valid_1's l2: 0.45826
[82]	training's l2: 0.339101	valid_1's l2: 0.457878
[83]	training's l2: 0.337819	valid_1's l2: 0.457847
[84]	training's l2: 0.336467	valid_1's l2: 0.457661
[85]	training's l2: 0.335292	valid_1's l2: 0.457687
[86]	training's l2: 0.333959	valid_1's l2: 0.457473
[87]	training's l2: 0.332524	valid_1's l2: 0.456873
[88]	training's l2: 0.331383	valid_1's l2: 0.456721
[89]	training's l2: 0.33023	valid_1's l2: 0.45654
[90]	training's l2: 0.329108	valid_1's l2: 0.456267
[91]	training's l2: 0.327784	valid_1's l2: 0.456165
[92]	training's l2: 0.326609	valid_1's l2: 0.45619
[93]	training's l2: 0.325168	valid_1's l2: 0.455471
[94]	training's l2: 0.323954	valid_1's l2: 0.455101
[95]	training's l2: 0.322859	valid_1's l2: 0.455033
[96]	training's l2: 0.32182	valid_1's l2: 0.455116
[97]	training's l2: 0.320388	valid_1's l2: 0.454488
[98]	training's l2: 0.318888	valid_1's l2: 0.453922
[99]	training's l2: 0.317828	valid_1's l2: 0.453882
[100]	training's l2: 0.316438	valid_1's l2: 0.453474
[101]	training's l2: 0.315269	valid_1's l2: 0.453109
[102]	training's l2: 0.314104	valid_1's l2: 0.452714
[103]	training's l2: 0.313073	valid_1's l2: 0.452592
[104]	training's l2: 0.311796	valid_1's l2: 0.45259
[105]	training's l2: 0.310498	valid_1's l2: 0.451742
[106]	training's l2: 0.309435	valid_1's l2: 0.451431
[107]	training's l2: 0.308313	valid_1's l2: 0.451395
[108]	training's l2: 0.307344	valid_1's l2: 0.45134
[109]	training's l2: 0.306393	valid_1's l2: 0.4515
[110]	training's l2: 0.305191	valid_1's l2: 0.451159
[111]	training's l2: 0.304179	valid_1's l2: 0.450818
[112]	training's l2: 0.303171	valid_1's l2: 0.450823
[113]	training's l2: 0.30208	valid_1's l2: 0.450732
[114]	training's l2: 0.301087	valid_1's l2: 0.45072
[115]	training's l2: 0.300054	valid_1's l2: 0.450715
[116]	training's l2: 0.299035	valid_1's l2: 0.450763
[117]	training's l2: 0.29813	valid_1's l2: 0.450795
[118]	training's l2: 0.297215	valid_1's l2: 0.450857
[119]	training's l2: 0.296237	valid_1's l2: 0.450989
[120]	training's l2: 0.295237	valid_1's l2: 0.451014
[121]	training's l2: 0.294321	valid_1's l2: 0.450733
[122]	training's l2: 0.293371	valid_1's l2: 0.45074
[123]	training's l2: 0.292172	valid_1's l2: 0.450206
[124]	training's l2: 0.291225	valid_1's l2: 0.450043
[125]	training's l2: 0.290369	valid_1's l2: 0.449895
[126]	training's l2: 0.289451	valid_1's l2: 0.449879
[127]	training's l2: 0.288446	valid_1's l2: 0.449776
[128]	training's l2: 0.287501	valid_1's l2: 0.449806
[129]	training's l2: 0.286633	valid_1's l2: 0.449867
[130]	training's l2: 0.285664	valid_1's l2: 0.449867
[131]	training's l2: 0.284706	valid_1's l2: 0.449777
[132]	training's l2: 0.283848	valid_1's l2: 0.449471
[133]	training's l2: 0.282969	valid_1's l2: 0.449484
[134]	training's l2: 0.282124	valid_1's l2: 0.449442
[135]	training's l2: 0.281138	valid_1's l2: 0.449204
[136]	training's l2: 0.280161	valid_1's l2: 0.448957
[137]	training's l2: 0.279241	valid_1's l2: 0.449015
[138]	training's l2: 0.278286	valid_1's l2: 0.449146
[139]	training's l2: 0.277452	valid_1's l2: 0.44885
[140]	training's l2: 0.276553	valid_1's l2: 0.449009
[141]	training's l2: 0.275685	valid_1's l2: 0.448796
[142]	training's l2: 0.274932	valid_1's l2: 0.448755
[143]	training's l2: 0.27394	valid_1's l2: 0.448767
[144]	training's l2: 0.273093	valid_1's l2: 0.448812
[145]	training's l2: 0.272265	valid_1's l2: 0.448839
[146]	training's l2: 0.271262	valid_1's l2: 0.4484
[147]	training's l2: 0.27043	valid_1's l2: 0.448178
[148]	training's l2: 0.269615	valid_1's l2: 0.448246
[149]	training's l2: 0.268685	valid_1's l2: 0.448233
[150]	training's l2: 0.267878	valid_1's l2: 0.448377
[151]	training's l2: 0.267048	valid_1's l2: 0.448342
[152]	training's l2: 0.266311	valid_1's l2: 0.448415
[153]	training's l2: 0.26563	valid_1's l2: 0.448406
[154]	training's l2: 0.26453	valid_1's l2: 0.447774
[155]	training's l2: 0.263853	valid_1's l2: 0.447787
[156]	training's l2: 0.263179	valid_1's l2: 0.447764
[157]	training's l2: 0.26243	valid_1's l2: 0.447864
[158]	training's l2: 0.261666	valid_1's l2: 0.447927
[159]	training's l2: 0.260844	valid_1's l2: 0.448014
[160]	training's l2: 0.259997	valid_1's l2: 0.447748
[161]	training's l2: 0.259205	valid_1's l2: 0.447754
[162]	training's l2: 0.258456	valid_1's l2: 0.447777
[163]	training's l2: 0.257745	valid_1's l2: 0.44785
[164]	training's l2: 0.257007	valid_1's l2: 0.447875
[165]	training's l2: 0.256269	valid_1's l2: 0.447771
[166]	training's l2: 0.255641	valid_1's l2: 0.447698
[167]	training's l2: 0.254901	valid_1's l2: 0.447605
[168]	training's l2: 0.254188	valid_1's l2: 0.447527
[169]	training's l2: 0.253517	valid_1's l2: 0.44763
[170]	training's l2: 0.252791	valid_1's l2: 0.447642
[171]	training's l2: 0.252119	valid_1's l2: 0.447736
[172]	training's l2: 0.251493	valid_1's l2: 0.447832
[173]	training's l2: 0.250824	valid_1's l2: 0.447736
[174]	training's l2: 0.250114	valid_1's l2: 0.447721
[175]	training's l2: 0.249445	valid_1's l2: 0.44773
[176]	training's l2: 0.248711	valid_1's l2: 0.447755
[177]	training's l2: 0.248065	valid_1's l2: 0.447735
[178]	training's l2: 0.247435	valid_1's l2: 0.44765
[179]	training's l2: 0.246839	valid_1's l2: 0.447698
[180]	training's l2: 0.246177	valid_1's l2: 0.447648
[181]	training's l2: 0.24555	valid_1's l2: 0.447932
[182]	training's l2: 0.244967	valid_1's l2: 0.447934
[183]	training's l2: 0.244373	valid_1's l2: 0.447903
Did not meet early stopping. Best iteration is:
[183]	training's l2: 0.244373	valid_1's l2: 0.447903
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.188235 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.746211	valid_1's l2: 0.725755
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.725323	valid_1's l2: 0.707665
[3]	training's l2: 0.705562	valid_1's l2: 0.691105
[4]	training's l2: 0.688062	valid_1's l2: 0.67648
[5]	training's l2: 0.671939	valid_1's l2: 0.663043
[6]	training's l2: 0.657407	valid_1's l2: 0.651051
[7]	training's l2: 0.643769	valid_1's l2: 0.640227
[8]	training's l2: 0.630278	valid_1's l2: 0.629071
[9]	training's l2: 0.618411	valid_1's l2: 0.619797
[10]	training's l2: 0.608762	valid_1's l2: 0.611805
[11]	training's l2: 0.598556	valid_1's l2: 0.603688
[12]	training's l2: 0.589077	valid_1's l2: 0.596482
[13]	training's l2: 0.58	valid_1's l2: 0.588622
[14]	training's l2: 0.572574	valid_1's l2: 0.583294
[15]	training's l2: 0.563937	valid_1's l2: 0.576449
[16]	training's l2: 0.555625	valid_1's l2: 0.569685
[17]	training's l2: 0.549367	valid_1's l2: 0.565313
[18]	training's l2: 0.542278	valid_1's l2: 0.560802
[19]	training's l2: 0.53638	valid_1's l2: 0.556528
[20]	training's l2: 0.529888	valid_1's l2: 0.551581
[21]	training's l2: 0.524631	valid_1's l2: 0.547852
[22]	training's l2: 0.5181	valid_1's l2: 0.542868
[23]	training's l2: 0.512607	valid_1's l2: 0.539109
[24]	training's l2: 0.507718	valid_1's l2: 0.535964
[25]	training's l2: 0.503372	valid_1's l2: 0.532772
[26]	training's l2: 0.499437	valid_1's l2: 0.530216
[27]	training's l2: 0.494415	valid_1's l2: 0.526534
[28]	training's l2: 0.490716	valid_1's l2: 0.523939
[29]	training's l2: 0.486717	valid_1's l2: 0.521213
[30]	training's l2: 0.482741	valid_1's l2: 0.518046
[31]	training's l2: 0.479173	valid_1's l2: 0.515603
[32]	training's l2: 0.475114	valid_1's l2: 0.512561
[33]	training's l2: 0.472071	valid_1's l2: 0.511072
[34]	training's l2: 0.46887	valid_1's l2: 0.509579
[35]	training's l2: 0.465343	valid_1's l2: 0.507167
[36]	training's l2: 0.462188	valid_1's l2: 0.504706
[37]	training's l2: 0.45949	valid_1's l2: 0.503335
[38]	training's l2: 0.456796	valid_1's l2: 0.501877
[39]	training's l2: 0.454602	valid_1's l2: 0.500645
[40]	training's l2: 0.452196	valid_1's l2: 0.499243
[41]	training's l2: 0.450106	valid_1's l2: 0.497847
[42]	training's l2: 0.446051	valid_1's l2: 0.494713
[43]	training's l2: 0.443895	valid_1's l2: 0.493929
[44]	training's l2: 0.43988	valid_1's l2: 0.490557
[45]	training's l2: 0.436215	valid_1's l2: 0.48799
[46]	training's l2: 0.434453	valid_1's l2: 0.486928
[47]	training's l2: 0.432265	valid_1's l2: 0.485925
[48]	training's l2: 0.430638	valid_1's l2: 0.485241
[49]	training's l2: 0.427465	valid_1's l2: 0.48284
[50]	training's l2: 0.425599	valid_1's l2: 0.481983
[51]	training's l2: 0.423795	valid_1's l2: 0.480993
[52]	training's l2: 0.422248	valid_1's l2: 0.480219
[53]	training's l2: 0.420508	valid_1's l2: 0.479516
[54]	training's l2: 0.418508	valid_1's l2: 0.478512
[55]	training's l2: 0.417125	valid_1's l2: 0.477956
[56]	training's l2: 0.415321	valid_1's l2: 0.476795
[57]	training's l2: 0.413856	valid_1's l2: 0.476063
[58]	training's l2: 0.412504	valid_1's l2: 0.475615
[59]	training's l2: 0.411345	valid_1's l2: 0.475001
[60]	training's l2: 0.409711	valid_1's l2: 0.474087
[61]	training's l2: 0.408312	valid_1's l2: 0.473405
[62]	training's l2: 0.406608	valid_1's l2: 0.472374
[63]	training's l2: 0.405137	valid_1's l2: 0.471885
[64]	training's l2: 0.403978	valid_1's l2: 0.471341
[65]	training's l2: 0.40264	valid_1's l2: 0.470741
[66]	training's l2: 0.401018	valid_1's l2: 0.469849
[67]	training's l2: 0.39952	valid_1's l2: 0.469115
[68]	training's l2: 0.397985	valid_1's l2: 0.468457
[69]	training's l2: 0.396848	valid_1's l2: 0.46818
[70]	training's l2: 0.395547	valid_1's l2: 0.467615
[71]	training's l2: 0.394406	valid_1's l2: 0.467258
[72]	training's l2: 0.393282	valid_1's l2: 0.466949
[73]	training's l2: 0.391568	valid_1's l2: 0.465892
[74]	training's l2: 0.390386	valid_1's l2: 0.465562
[75]	training's l2: 0.389173	valid_1's l2: 0.464946
[76]	training's l2: 0.388116	valid_1's l2: 0.464648
[77]	training's l2: 0.38657	valid_1's l2: 0.463619
[78]	training's l2: 0.385043	valid_1's l2: 0.462865
[79]	training's l2: 0.384036	valid_1's l2: 0.462615
[80]	training's l2: 0.382941	valid_1's l2: 0.462412
[81]	training's l2: 0.382028	valid_1's l2: 0.462286
[82]	training's l2: 0.381109	valid_1's l2: 0.462073
[83]	training's l2: 0.379635	valid_1's l2: 0.461367
[84]	training's l2: 0.378056	valid_1's l2: 0.46027
[85]	training's l2: 0.377073	valid_1's l2: 0.459883
[86]	training's l2: 0.37609	valid_1's l2: 0.459708
[87]	training's l2: 0.375118	valid_1's l2: 0.459601
[88]	training's l2: 0.374168	valid_1's l2: 0.459386
[89]	training's l2: 0.372776	valid_1's l2: 0.458929
[90]	training's l2: 0.371892	valid_1's l2: 0.458845
[91]	training's l2: 0.370947	valid_1's l2: 0.458746
[92]	training's l2: 0.370114	valid_1's l2: 0.458665
[93]	training's l2: 0.369037	valid_1's l2: 0.458703
[94]	training's l2: 0.367962	valid_1's l2: 0.458239
[95]	training's l2: 0.367065	valid_1's l2: 0.458005
[96]	training's l2: 0.366032	valid_1's l2: 0.457864
[97]	training's l2: 0.365079	valid_1's l2: 0.45751
[98]	training's l2: 0.364159	valid_1's l2: 0.457141
[99]	training's l2: 0.363233	valid_1's l2: 0.456889
[100]	training's l2: 0.362438	valid_1's l2: 0.45665
[101]	training's l2: 0.361386	valid_1's l2: 0.45638
[102]	training's l2: 0.360459	valid_1's l2: 0.456185
[103]	training's l2: 0.359375	valid_1's l2: 0.455872
[104]	training's l2: 0.358324	valid_1's l2: 0.455353
[105]	training's l2: 0.357537	valid_1's l2: 0.455375
[106]	training's l2: 0.356719	valid_1's l2: 0.455029
[107]	training's l2: 0.355722	valid_1's l2: 0.454374
[108]	training's l2: 0.354843	valid_1's l2: 0.454179
[109]	training's l2: 0.353688	valid_1's l2: 0.453765
[110]	training's l2: 0.352931	valid_1's l2: 0.453773
[111]	training's l2: 0.352186	valid_1's l2: 0.453525
[112]	training's l2: 0.351273	valid_1's l2: 0.453211
[113]	training's l2: 0.350363	valid_1's l2: 0.452881
[114]	training's l2: 0.349536	valid_1's l2: 0.452508
[115]	training's l2: 0.34873	valid_1's l2: 0.452588
[116]	training's l2: 0.347819	valid_1's l2: 0.452419
[117]	training's l2: 0.347009	valid_1's l2: 0.452299
[118]	training's l2: 0.346214	valid_1's l2: 0.452171
[119]	training's l2: 0.345133	valid_1's l2: 0.451786
[120]	training's l2: 0.344321	valid_1's l2: 0.451914
[121]	training's l2: 0.343646	valid_1's l2: 0.451853
[122]	training's l2: 0.342966	valid_1's l2: 0.451826
[123]	training's l2: 0.34226	valid_1's l2: 0.451797
[124]	training's l2: 0.341191	valid_1's l2: 0.451263
[125]	training's l2: 0.340097	valid_1's l2: 0.45107
[126]	training's l2: 0.338914	valid_1's l2: 0.450307
[127]	training's l2: 0.338155	valid_1's l2: 0.450241
[128]	training's l2: 0.337405	valid_1's l2: 0.450142
[129]	training's l2: 0.33665	valid_1's l2: 0.449832
[130]	training's l2: 0.335849	valid_1's l2: 0.449872
[131]	training's l2: 0.33507	valid_1's l2: 0.449611
[132]	training's l2: 0.334348	valid_1's l2: 0.449413
[133]	training's l2: 0.33347	valid_1's l2: 0.449081
[134]	training's l2: 0.332798	valid_1's l2: 0.448986
[135]	training's l2: 0.332101	valid_1's l2: 0.448963
[136]	training's l2: 0.331394	valid_1's l2: 0.448865
[137]	training's l2: 0.330744	valid_1's l2: 0.4489
[138]	training's l2: 0.329913	valid_1's l2: 0.448566
[139]	training's l2: 0.329231	valid_1's l2: 0.448577
[140]	training's l2: 0.328616	valid_1's l2: 0.448432
[141]	training's l2: 0.32774	valid_1's l2: 0.448059
[142]	training's l2: 0.326999	valid_1's l2: 0.448118
[143]	training's l2: 0.326359	valid_1's l2: 0.448203
[144]	training's l2: 0.325726	valid_1's l2: 0.448142
[145]	training's l2: 0.325089	valid_1's l2: 0.447962
[146]	training's l2: 0.324395	valid_1's l2: 0.447867
[147]	training's l2: 0.323729	valid_1's l2: 0.447828
[148]	training's l2: 0.323143	valid_1's l2: 0.447851
[149]	training's l2: 0.322161	valid_1's l2: 0.447237
[150]	training's l2: 0.321556	valid_1's l2: 0.447198
[151]	training's l2: 0.32084	valid_1's l2: 0.447169
[152]	training's l2: 0.320128	valid_1's l2: 0.44726
[153]	training's l2: 0.319528	valid_1's l2: 0.447088
[154]	training's l2: 0.318961	valid_1's l2: 0.447031
[155]	training's l2: 0.318217	valid_1's l2: 0.446987
[156]	training's l2: 0.31763	valid_1's l2: 0.446913
[157]	training's l2: 0.316871	valid_1's l2: 0.446578
[158]	training's l2: 0.316253	valid_1's l2: 0.446453
[159]	training's l2: 0.315612	valid_1's l2: 0.446351
[160]	training's l2: 0.315026	valid_1's l2: 0.446391
[161]	training's l2: 0.314503	valid_1's l2: 0.446515
[162]	training's l2: 0.313934	valid_1's l2: 0.446386
[163]	training's l2: 0.313444	valid_1's l2: 0.446468
[164]	training's l2: 0.312951	valid_1's l2: 0.446512
[165]	training's l2: 0.312277	valid_1's l2: 0.446434
[166]	training's l2: 0.311749	valid_1's l2: 0.446395
[167]	training's l2: 0.311135	valid_1's l2: 0.446377
[168]	training's l2: 0.310381	valid_1's l2: 0.446138
[169]	training's l2: 0.309726	valid_1's l2: 0.446069
[170]	training's l2: 0.309146	valid_1's l2: 0.446031
[171]	training's l2: 0.308596	valid_1's l2: 0.445896
[172]	training's l2: 0.308084	valid_1's l2: 0.44623
[173]	training's l2: 0.307542	valid_1's l2: 0.446259
[174]	training's l2: 0.306929	valid_1's l2: 0.446253
[175]	training's l2: 0.306403	valid_1's l2: 0.446347
[176]	training's l2: 0.30573	valid_1's l2: 0.446474
[177]	training's l2: 0.305093	valid_1's l2: 0.446511
[178]	training's l2: 0.30459	valid_1's l2: 0.446549
[179]	training's l2: 0.304073	valid_1's l2: 0.44648
[180]	training's l2: 0.303531	valid_1's l2: 0.446509
[181]	training's l2: 0.302912	valid_1's l2: 0.446396
[182]	training's l2: 0.3024	valid_1's l2: 0.446318
[183]	training's l2: 0.301937	valid_1's l2: 0.44637
[184]	training's l2: 0.301455	valid_1's l2: 0.44638
[185]	training's l2: 0.300996	valid_1's l2: 0.44632
[186]	training's l2: 0.300467	valid_1's l2: 0.446065
[187]	training's l2: 0.299931	valid_1's l2: 0.4461
[188]	training's l2: 0.29934	valid_1's l2: 0.44609
[189]	training's l2: 0.298663	valid_1's l2: 0.446187
[190]	training's l2: 0.298143	valid_1's l2: 0.446055
[191]	training's l2: 0.297623	valid_1's l2: 0.446048
[192]	training's l2: 0.297076	valid_1's l2: 0.446023
[193]	training's l2: 0.296486	valid_1's l2: 0.446106
[194]	training's l2: 0.296	valid_1's l2: 0.446121
[195]	training's l2: 0.29555	valid_1's l2: 0.446077
[196]	training's l2: 0.295029	valid_1's l2: 0.446129
[197]	training's l2: 0.294621	valid_1's l2: 0.446219
[198]	training's l2: 0.294143	valid_1's l2: 0.446338
[199]	training's l2: 0.293638	valid_1's l2: 0.446427
[200]	training's l2: 0.293138	valid_1's l2: 0.446323
Did not meet early stopping. Best iteration is:
[200]	training's l2: 0.293138	valid_1's l2: 0.446323
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182772 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.741741	valid_1's l2: 0.721651
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.716364	valid_1's l2: 0.700153
[3]	training's l2: 0.694638	valid_1's l2: 0.681214
[4]	training's l2: 0.674981	valid_1's l2: 0.6648
[5]	training's l2: 0.65764	valid_1's l2: 0.651376
[6]	training's l2: 0.642329	valid_1's l2: 0.638592
[7]	training's l2: 0.627559	valid_1's l2: 0.626918
[8]	training's l2: 0.61367	valid_1's l2: 0.61549
[9]	training's l2: 0.601142	valid_1's l2: 0.605421
[10]	training's l2: 0.58957	valid_1's l2: 0.595761
[11]	training's l2: 0.578104	valid_1's l2: 0.586723
[12]	training's l2: 0.568863	valid_1's l2: 0.57984
[13]	training's l2: 0.561165	valid_1's l2: 0.574273
[14]	training's l2: 0.55194	valid_1's l2: 0.566848
[15]	training's l2: 0.544078	valid_1's l2: 0.561183
[16]	training's l2: 0.535501	valid_1's l2: 0.553805
[17]	training's l2: 0.526611	valid_1's l2: 0.547532
[18]	training's l2: 0.520914	valid_1's l2: 0.543345
[19]	training's l2: 0.515747	valid_1's l2: 0.539471
[20]	training's l2: 0.510899	valid_1's l2: 0.536283
[21]	training's l2: 0.505139	valid_1's l2: 0.532257
[22]	training's l2: 0.498665	valid_1's l2: 0.527253
[23]	training's l2: 0.494715	valid_1's l2: 0.525073
[24]	training's l2: 0.489057	valid_1's l2: 0.521213
[25]	training's l2: 0.485392	valid_1's l2: 0.518828
[26]	training's l2: 0.480767	valid_1's l2: 0.515788
[27]	training's l2: 0.476909	valid_1's l2: 0.513417
[28]	training's l2: 0.472637	valid_1's l2: 0.510536
[29]	training's l2: 0.46909	valid_1's l2: 0.508418
[30]	training's l2: 0.465911	valid_1's l2: 0.506317
[31]	training's l2: 0.463528	valid_1's l2: 0.505416
[32]	training's l2: 0.461022	valid_1's l2: 0.504034
[33]	training's l2: 0.458254	valid_1's l2: 0.503077
[34]	training's l2: 0.453504	valid_1's l2: 0.499413
[35]	training's l2: 0.448801	valid_1's l2: 0.495578
[36]	training's l2: 0.446069	valid_1's l2: 0.493904
[37]	training's l2: 0.443884	valid_1's l2: 0.492649
[38]	training's l2: 0.440506	valid_1's l2: 0.489818
[39]	training's l2: 0.438573	valid_1's l2: 0.48899
[40]	training's l2: 0.436338	valid_1's l2: 0.487878
[41]	training's l2: 0.432517	valid_1's l2: 0.484942
[42]	training's l2: 0.430741	valid_1's l2: 0.483899
[43]	training's l2: 0.428715	valid_1's l2: 0.482643
[44]	training's l2: 0.426844	valid_1's l2: 0.481708
[45]	training's l2: 0.424326	valid_1's l2: 0.479981
[46]	training's l2: 0.422102	valid_1's l2: 0.478649
[47]	training's l2: 0.420325	valid_1's l2: 0.477946
[48]	training's l2: 0.418818	valid_1's l2: 0.477424
[49]	training's l2: 0.417428	valid_1's l2: 0.476675
[50]	training's l2: 0.414766	valid_1's l2: 0.474983
[51]	training's l2: 0.412928	valid_1's l2: 0.473998
[52]	training's l2: 0.411501	valid_1's l2: 0.473537
[53]	training's l2: 0.409729	valid_1's l2: 0.472567
[54]	training's l2: 0.408521	valid_1's l2: 0.47212
[55]	training's l2: 0.406989	valid_1's l2: 0.471394
[56]	training's l2: 0.405089	valid_1's l2: 0.470188
[57]	training's l2: 0.403743	valid_1's l2: 0.469805
[58]	training's l2: 0.401838	valid_1's l2: 0.468929
[59]	training's l2: 0.400122	valid_1's l2: 0.468357
[60]	training's l2: 0.398836	valid_1's l2: 0.468153
[61]	training's l2: 0.397019	valid_1's l2: 0.467232
[62]	training's l2: 0.395714	valid_1's l2: 0.466633
[63]	training's l2: 0.393246	valid_1's l2: 0.465259
[64]	training's l2: 0.39176	valid_1's l2: 0.464714
[65]	training's l2: 0.389694	valid_1's l2: 0.463443
[66]	training's l2: 0.388442	valid_1's l2: 0.46312
[67]	training's l2: 0.386641	valid_1's l2: 0.462237
[68]	training's l2: 0.385668	valid_1's l2: 0.461939
[69]	training's l2: 0.384044	valid_1's l2: 0.460758
[70]	training's l2: 0.38295	valid_1's l2: 0.460291
[71]	training's l2: 0.381855	valid_1's l2: 0.460167
[72]	training's l2: 0.380796	valid_1's l2: 0.45983
[73]	training's l2: 0.378951	valid_1's l2: 0.458679
[74]	training's l2: 0.377695	valid_1's l2: 0.45856
[75]	training's l2: 0.376705	valid_1's l2: 0.458595
[76]	training's l2: 0.375634	valid_1's l2: 0.45836
[77]	training's l2: 0.374468	valid_1's l2: 0.458096
[78]	training's l2: 0.373447	valid_1's l2: 0.45786
[79]	training's l2: 0.371811	valid_1's l2: 0.457251
[80]	training's l2: 0.370527	valid_1's l2: 0.456542
[81]	training's l2: 0.369329	valid_1's l2: 0.456311
[82]	training's l2: 0.368304	valid_1's l2: 0.456041
[83]	training's l2: 0.366823	valid_1's l2: 0.455518
[84]	training's l2: 0.365899	valid_1's l2: 0.455301
[85]	training's l2: 0.364894	valid_1's l2: 0.454766
[86]	training's l2: 0.364011	valid_1's l2: 0.454526
[87]	training's l2: 0.363163	valid_1's l2: 0.454342
[88]	training's l2: 0.362174	valid_1's l2: 0.454374
[89]	training's l2: 0.361055	valid_1's l2: 0.45414
[90]	training's l2: 0.359839	valid_1's l2: 0.453979
[91]	training's l2: 0.358843	valid_1's l2: 0.453754
[92]	training's l2: 0.35779	valid_1's l2: 0.453558
[93]	training's l2: 0.356812	valid_1's l2: 0.453271
[94]	training's l2: 0.355915	valid_1's l2: 0.45312
[95]	training's l2: 0.355018	valid_1's l2: 0.452654
[96]	training's l2: 0.35424	valid_1's l2: 0.452709
[97]	training's l2: 0.35326	valid_1's l2: 0.452424
[98]	training's l2: 0.352484	valid_1's l2: 0.45235
[99]	training's l2: 0.350967	valid_1's l2: 0.451306
[100]	training's l2: 0.350169	valid_1's l2: 0.45131
[101]	training's l2: 0.349223	valid_1's l2: 0.451129
[102]	training's l2: 0.348413	valid_1's l2: 0.451064
[103]	training's l2: 0.347395	valid_1's l2: 0.450767
[104]	training's l2: 0.346063	valid_1's l2: 0.450304
[105]	training's l2: 0.345178	valid_1's l2: 0.450104
[106]	training's l2: 0.344357	valid_1's l2: 0.450058
[107]	training's l2: 0.343421	valid_1's l2: 0.450075
[108]	training's l2: 0.342551	valid_1's l2: 0.449806
[109]	training's l2: 0.341752	valid_1's l2: 0.449719
[110]	training's l2: 0.340425	valid_1's l2: 0.449147
[111]	training's l2: 0.339644	valid_1's l2: 0.448713
[112]	training's l2: 0.338846	valid_1's l2: 0.448723
[113]	training's l2: 0.338114	valid_1's l2: 0.448495
[114]	training's l2: 0.337256	valid_1's l2: 0.448396
[115]	training's l2: 0.336399	valid_1's l2: 0.448141
[116]	training's l2: 0.335654	valid_1's l2: 0.448169
[117]	training's l2: 0.334856	valid_1's l2: 0.448098
[118]	training's l2: 0.333978	valid_1's l2: 0.448053
[119]	training's l2: 0.333219	valid_1's l2: 0.448059
[120]	training's l2: 0.332347	valid_1's l2: 0.44805
[121]	training's l2: 0.331511	valid_1's l2: 0.447813
[122]	training's l2: 0.330767	valid_1's l2: 0.447768
[123]	training's l2: 0.33	valid_1's l2: 0.447746
[124]	training's l2: 0.329252	valid_1's l2: 0.447712
[125]	training's l2: 0.328556	valid_1's l2: 0.447558
[126]	training's l2: 0.32788	valid_1's l2: 0.447702
[127]	training's l2: 0.327106	valid_1's l2: 0.447685
[128]	training's l2: 0.326361	valid_1's l2: 0.447746
[129]	training's l2: 0.325625	valid_1's l2: 0.447687
[130]	training's l2: 0.324816	valid_1's l2: 0.447507
[131]	training's l2: 0.324176	valid_1's l2: 0.447476
[132]	training's l2: 0.323352	valid_1's l2: 0.447357
[133]	training's l2: 0.322377	valid_1's l2: 0.446912
[134]	training's l2: 0.32142	valid_1's l2: 0.446694
[135]	training's l2: 0.320816	valid_1's l2: 0.446662
[136]	training's l2: 0.320101	valid_1's l2: 0.446624
[137]	training's l2: 0.319421	valid_1's l2: 0.446673
[138]	training's l2: 0.318593	valid_1's l2: 0.446577
[139]	training's l2: 0.317745	valid_1's l2: 0.446335
[140]	training's l2: 0.317159	valid_1's l2: 0.446143
[141]	training's l2: 0.316539	valid_1's l2: 0.446251
[142]	training's l2: 0.315875	valid_1's l2: 0.445953
[143]	training's l2: 0.315223	valid_1's l2: 0.445908
[144]	training's l2: 0.314188	valid_1's l2: 0.445352
[145]	training's l2: 0.313335	valid_1's l2: 0.444762
[146]	training's l2: 0.312698	valid_1's l2: 0.444632
[147]	training's l2: 0.312063	valid_1's l2: 0.444702
[148]	training's l2: 0.311459	valid_1's l2: 0.444642
[149]	training's l2: 0.31093	valid_1's l2: 0.444711
[150]	training's l2: 0.310135	valid_1's l2: 0.444429
[151]	training's l2: 0.309389	valid_1's l2: 0.444357
[152]	training's l2: 0.308758	valid_1's l2: 0.444195
[153]	training's l2: 0.308085	valid_1's l2: 0.444249
[154]	training's l2: 0.307486	valid_1's l2: 0.444331
[155]	training's l2: 0.306901	valid_1's l2: 0.444327
[156]	training's l2: 0.306296	valid_1's l2: 0.444302
[157]	training's l2: 0.305694	valid_1's l2: 0.444338
[158]	training's l2: 0.305148	valid_1's l2: 0.444306
[159]	training's l2: 0.304532	valid_1's l2: 0.444157
[160]	training's l2: 0.30398	valid_1's l2: 0.444064
[161]	training's l2: 0.303408	valid_1's l2: 0.443985
[162]	training's l2: 0.302787	valid_1's l2: 0.443949
[163]	training's l2: 0.302276	valid_1's l2: 0.443823
[164]	training's l2: 0.301711	valid_1's l2: 0.443668
[165]	training's l2: 0.301119	valid_1's l2: 0.443787
[166]	training's l2: 0.300561	valid_1's l2: 0.443686
[167]	training's l2: 0.299869	valid_1's l2: 0.443892
[168]	training's l2: 0.298986	valid_1's l2: 0.443448
[169]	training's l2: 0.298371	valid_1's l2: 0.443333
[170]	training's l2: 0.297722	valid_1's l2: 0.443179
[171]	training's l2: 0.297158	valid_1's l2: 0.44314
[172]	training's l2: 0.296584	valid_1's l2: 0.443123
[173]	training's l2: 0.295934	valid_1's l2: 0.443364
[174]	training's l2: 0.295389	valid_1's l2: 0.443656
[175]	training's l2: 0.294896	valid_1's l2: 0.443677
[176]	training's l2: 0.294295	valid_1's l2: 0.4439
[177]	training's l2: 0.293768	valid_1's l2: 0.443889
[178]	training's l2: 0.293209	valid_1's l2: 0.44404
[179]	training's l2: 0.292674	valid_1's l2: 0.444002
[180]	training's l2: 0.292118	valid_1's l2: 0.443909
[181]	training's l2: 0.291537	valid_1's l2: 0.443921
[182]	training's l2: 0.291	valid_1's l2: 0.44384
[183]	training's l2: 0.290468	valid_1's l2: 0.443739
[184]	training's l2: 0.289878	valid_1's l2: 0.443741
Did not meet early stopping. Best iteration is:
[184]	training's l2: 0.289878	valid_1's l2: 0.443741
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179368 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.733678	valid_1's l2: 0.715316
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.702963	valid_1's l2: 0.689842
[3]	training's l2: 0.675009	valid_1's l2: 0.667116
[4]	training's l2: 0.650916	valid_1's l2: 0.647282
[5]	training's l2: 0.630151	valid_1's l2: 0.630855
[6]	training's l2: 0.612339	valid_1's l2: 0.617139
[7]	training's l2: 0.593891	valid_1's l2: 0.602484
[8]	training's l2: 0.578284	valid_1's l2: 0.590353
[9]	training's l2: 0.56531	valid_1's l2: 0.58147
[10]	training's l2: 0.552614	valid_1's l2: 0.571495
[11]	training's l2: 0.540733	valid_1's l2: 0.563291
[12]	training's l2: 0.530903	valid_1's l2: 0.556396
[13]	training's l2: 0.521138	valid_1's l2: 0.549274
[14]	training's l2: 0.511248	valid_1's l2: 0.541703
[15]	training's l2: 0.502403	valid_1's l2: 0.535476
[16]	training's l2: 0.493871	valid_1's l2: 0.529338
[17]	training's l2: 0.486146	valid_1's l2: 0.524084
[18]	training's l2: 0.478031	valid_1's l2: 0.518938
[19]	training's l2: 0.471996	valid_1's l2: 0.514846
[20]	training's l2: 0.464974	valid_1's l2: 0.510536
[21]	training's l2: 0.460022	valid_1's l2: 0.508027
[22]	training's l2: 0.455018	valid_1's l2: 0.505388
[23]	training's l2: 0.45078	valid_1's l2: 0.503561
[24]	training's l2: 0.446236	valid_1's l2: 0.500953
[25]	training's l2: 0.440369	valid_1's l2: 0.496255
[26]	training's l2: 0.436993	valid_1's l2: 0.494923
[27]	training's l2: 0.432943	valid_1's l2: 0.492543
[28]	training's l2: 0.429249	valid_1's l2: 0.490914
[29]	training's l2: 0.425169	valid_1's l2: 0.488175
[30]	training's l2: 0.422227	valid_1's l2: 0.4871
[31]	training's l2: 0.41872	valid_1's l2: 0.485023
[32]	training's l2: 0.414126	valid_1's l2: 0.48282
[33]	training's l2: 0.410435	valid_1's l2: 0.481558
[34]	training's l2: 0.407545	valid_1's l2: 0.480435
[35]	training's l2: 0.402748	valid_1's l2: 0.477187
[36]	training's l2: 0.399814	valid_1's l2: 0.475834
[37]	training's l2: 0.397138	valid_1's l2: 0.475002
[38]	training's l2: 0.394617	valid_1's l2: 0.474204
[39]	training's l2: 0.39238	valid_1's l2: 0.47345
[40]	training's l2: 0.389573	valid_1's l2: 0.472568
[41]	training's l2: 0.387187	valid_1's l2: 0.471788
[42]	training's l2: 0.384339	valid_1's l2: 0.470075
[43]	training's l2: 0.381061	valid_1's l2: 0.468086
[44]	training's l2: 0.378521	valid_1's l2: 0.467398
[45]	training's l2: 0.375926	valid_1's l2: 0.466228
[46]	training's l2: 0.373894	valid_1's l2: 0.465437
[47]	training's l2: 0.371116	valid_1's l2: 0.464319
[48]	training's l2: 0.368954	valid_1's l2: 0.463423
[49]	training's l2: 0.36697	valid_1's l2: 0.462841
[50]	training's l2: 0.364737	valid_1's l2: 0.462252
[51]	training's l2: 0.361986	valid_1's l2: 0.461055
[52]	training's l2: 0.360184	valid_1's l2: 0.460631
[53]	training's l2: 0.358022	valid_1's l2: 0.459808
[54]	training's l2: 0.35621	valid_1's l2: 0.459405
[55]	training's l2: 0.35411	valid_1's l2: 0.458824
[56]	training's l2: 0.352448	valid_1's l2: 0.458354
[57]	training's l2: 0.350683	valid_1's l2: 0.458179
[58]	training's l2: 0.349126	valid_1's l2: 0.458069
[59]	training's l2: 0.34694	valid_1's l2: 0.45762
[60]	training's l2: 0.345184	valid_1's l2: 0.457459
[61]	training's l2: 0.343594	valid_1's l2: 0.457035
[62]	training's l2: 0.341863	valid_1's l2: 0.457095
[63]	training's l2: 0.340318	valid_1's l2: 0.456827
[64]	training's l2: 0.338733	valid_1's l2: 0.456916
[65]	training's l2: 0.336984	valid_1's l2: 0.456694
[66]	training's l2: 0.335492	valid_1's l2: 0.45664
[67]	training's l2: 0.334106	valid_1's l2: 0.456443
[68]	training's l2: 0.332592	valid_1's l2: 0.455924
[69]	training's l2: 0.330545	valid_1's l2: 0.454692
[70]	training's l2: 0.32888	valid_1's l2: 0.454615
[71]	training's l2: 0.32733	valid_1's l2: 0.454408
[72]	training's l2: 0.326023	valid_1's l2: 0.454188
[73]	training's l2: 0.324012	valid_1's l2: 0.453144
[74]	training's l2: 0.322499	valid_1's l2: 0.452633
[75]	training's l2: 0.321096	valid_1's l2: 0.452603
[76]	training's l2: 0.319379	valid_1's l2: 0.452306
[77]	training's l2: 0.317995	valid_1's l2: 0.452259
[78]	training's l2: 0.316303	valid_1's l2: 0.451848
[79]	training's l2: 0.314599	valid_1's l2: 0.451444
[80]	training's l2: 0.31328	valid_1's l2: 0.451391
[81]	training's l2: 0.311887	valid_1's l2: 0.450925
[82]	training's l2: 0.310489	valid_1's l2: 0.450817
[83]	training's l2: 0.308829	valid_1's l2: 0.450352
[84]	training's l2: 0.307522	valid_1's l2: 0.450241
[85]	training's l2: 0.306292	valid_1's l2: 0.450158
[86]	training's l2: 0.305037	valid_1's l2: 0.450102
[87]	training's l2: 0.30373	valid_1's l2: 0.450096
[88]	training's l2: 0.302501	valid_1's l2: 0.449695
[89]	training's l2: 0.301283	valid_1's l2: 0.44969
[90]	training's l2: 0.299983	valid_1's l2: 0.449336
[91]	training's l2: 0.298566	valid_1's l2: 0.449045
[92]	training's l2: 0.297288	valid_1's l2: 0.448945
[93]	training's l2: 0.296194	valid_1's l2: 0.448746
[94]	training's l2: 0.295073	valid_1's l2: 0.448765
[95]	training's l2: 0.293863	valid_1's l2: 0.448774
[96]	training's l2: 0.292618	valid_1's l2: 0.4484
[97]	training's l2: 0.291353	valid_1's l2: 0.448476
[98]	training's l2: 0.290209	valid_1's l2: 0.448543
[99]	training's l2: 0.289097	valid_1's l2: 0.448647
[100]	training's l2: 0.287946	valid_1's l2: 0.448542
[101]	training's l2: 0.286676	valid_1's l2: 0.448497
[102]	training's l2: 0.285425	valid_1's l2: 0.44835
[103]	training's l2: 0.28445	valid_1's l2: 0.448431
[104]	training's l2: 0.28338	valid_1's l2: 0.448493
[105]	training's l2: 0.2824	valid_1's l2: 0.448361
[106]	training's l2: 0.281196	valid_1's l2: 0.447921
[107]	training's l2: 0.280119	valid_1's l2: 0.447655
[108]	training's l2: 0.279136	valid_1's l2: 0.447527
[109]	training's l2: 0.278163	valid_1's l2: 0.447678
[110]	training's l2: 0.277196	valid_1's l2: 0.447587
[111]	training's l2: 0.276083	valid_1's l2: 0.447327
[112]	training's l2: 0.275045	valid_1's l2: 0.447312
[113]	training's l2: 0.274082	valid_1's l2: 0.447382
[114]	training's l2: 0.273103	valid_1's l2: 0.447255
[115]	training's l2: 0.272058	valid_1's l2: 0.447152
[116]	training's l2: 0.271124	valid_1's l2: 0.447049
[117]	training's l2: 0.270062	valid_1's l2: 0.446988
[118]	training's l2: 0.26911	valid_1's l2: 0.446937
[119]	training's l2: 0.268203	valid_1's l2: 0.446652
[120]	training's l2: 0.26729	valid_1's l2: 0.446876
[121]	training's l2: 0.266333	valid_1's l2: 0.446651
[122]	training's l2: 0.265458	valid_1's l2: 0.446974
[123]	training's l2: 0.264431	valid_1's l2: 0.447117
[124]	training's l2: 0.263508	valid_1's l2: 0.447138
[125]	training's l2: 0.262614	valid_1's l2: 0.447176
[126]	training's l2: 0.261732	valid_1's l2: 0.44723
[127]	training's l2: 0.260757	valid_1's l2: 0.447322
[128]	training's l2: 0.259997	valid_1's l2: 0.447474
[129]	training's l2: 0.259104	valid_1's l2: 0.447668
[130]	training's l2: 0.258058	valid_1's l2: 0.447862
[131]	training's l2: 0.256791	valid_1's l2: 0.447228
[132]	training's l2: 0.255889	valid_1's l2: 0.447165
[133]	training's l2: 0.255053	valid_1's l2: 0.447007
[134]	training's l2: 0.254205	valid_1's l2: 0.447022
[135]	training's l2: 0.253317	valid_1's l2: 0.44687
[136]	training's l2: 0.25254	valid_1's l2: 0.446765
[137]	training's l2: 0.2517	valid_1's l2: 0.446816
[138]	training's l2: 0.2509	valid_1's l2: 0.446799
[139]	training's l2: 0.250148	valid_1's l2: 0.446707
[140]	training's l2: 0.249448	valid_1's l2: 0.44676
[141]	training's l2: 0.24867	valid_1's l2: 0.446874
[142]	training's l2: 0.247908	valid_1's l2: 0.446788
[143]	training's l2: 0.247145	valid_1's l2: 0.446888
[144]	training's l2: 0.246473	valid_1's l2: 0.446811
[145]	training's l2: 0.245597	valid_1's l2: 0.446992
[146]	training's l2: 0.244838	valid_1's l2: 0.447004
[147]	training's l2: 0.244027	valid_1's l2: 0.446986
[148]	training's l2: 0.243237	valid_1's l2: 0.446892
[149]	training's l2: 0.242398	valid_1's l2: 0.446863
[150]	training's l2: 0.241621	valid_1's l2: 0.446759
[151]	training's l2: 0.240741	valid_1's l2: 0.446749
Early stopping, best iteration is:
[121]	training's l2: 0.266333	valid_1's l2: 0.446651
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.181652 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.741523	valid_1's l2: 0.722054
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.71689	valid_1's l2: 0.701746
[3]	training's l2: 0.693834	valid_1's l2: 0.682881
[4]	training's l2: 0.673469	valid_1's l2: 0.665799
[5]	training's l2: 0.655357	valid_1's l2: 0.651107
[6]	training's l2: 0.638969	valid_1's l2: 0.638042
[7]	training's l2: 0.623812	valid_1's l2: 0.626117
[8]	training's l2: 0.609377	valid_1's l2: 0.614423
[9]	training's l2: 0.5964	valid_1's l2: 0.604882
[10]	training's l2: 0.585824	valid_1's l2: 0.596943
[11]	training's l2: 0.574314	valid_1's l2: 0.587553
[12]	training's l2: 0.564387	valid_1's l2: 0.580326
[13]	training's l2: 0.554914	valid_1's l2: 0.573908
[14]	training's l2: 0.54542	valid_1's l2: 0.566395
[15]	training's l2: 0.53766	valid_1's l2: 0.561004
[16]	training's l2: 0.530433	valid_1's l2: 0.556315
[17]	training's l2: 0.523081	valid_1's l2: 0.551241
[18]	training's l2: 0.515493	valid_1's l2: 0.545825
[19]	training's l2: 0.508239	valid_1's l2: 0.540425
[20]	training's l2: 0.502328	valid_1's l2: 0.53678
[21]	training's l2: 0.49606	valid_1's l2: 0.532271
[22]	training's l2: 0.490698	valid_1's l2: 0.528144
[23]	training's l2: 0.484439	valid_1's l2: 0.523805
[24]	training's l2: 0.480112	valid_1's l2: 0.520906
[25]	training's l2: 0.475065	valid_1's l2: 0.517353
[26]	training's l2: 0.470369	valid_1's l2: 0.514447
[27]	training's l2: 0.465898	valid_1's l2: 0.512191
[28]	training's l2: 0.46223	valid_1's l2: 0.510194
[29]	training's l2: 0.458534	valid_1's l2: 0.508168
[30]	training's l2: 0.454563	valid_1's l2: 0.50579
[31]	training's l2: 0.450995	valid_1's l2: 0.503488
[32]	training's l2: 0.447031	valid_1's l2: 0.50098
[33]	training's l2: 0.444237	valid_1's l2: 0.499814
[34]	training's l2: 0.441438	valid_1's l2: 0.498163
[35]	training's l2: 0.43877	valid_1's l2: 0.497046
[36]	training's l2: 0.435448	valid_1's l2: 0.494689
[37]	training's l2: 0.432911	valid_1's l2: 0.493381
[38]	training's l2: 0.429565	valid_1's l2: 0.491373
[39]	training's l2: 0.426702	valid_1's l2: 0.489462
[40]	training's l2: 0.424196	valid_1's l2: 0.488262
[41]	training's l2: 0.421407	valid_1's l2: 0.4873
[42]	training's l2: 0.41852	valid_1's l2: 0.485304
[43]	training's l2: 0.415281	valid_1's l2: 0.483103
[44]	training's l2: 0.4113	valid_1's l2: 0.480221
[45]	training's l2: 0.408455	valid_1's l2: 0.478679
[46]	training's l2: 0.406233	valid_1's l2: 0.477608
[47]	training's l2: 0.404277	valid_1's l2: 0.477117
[48]	training's l2: 0.401385	valid_1's l2: 0.474956
[49]	training's l2: 0.399579	valid_1's l2: 0.474401
[50]	training's l2: 0.397728	valid_1's l2: 0.473553
[51]	training's l2: 0.39538	valid_1's l2: 0.472511
[52]	training's l2: 0.392781	valid_1's l2: 0.470659
[53]	training's l2: 0.391064	valid_1's l2: 0.470061
[54]	training's l2: 0.389355	valid_1's l2: 0.469595
[55]	training's l2: 0.387159	valid_1's l2: 0.468217
[56]	training's l2: 0.385325	valid_1's l2: 0.467221
[57]	training's l2: 0.383806	valid_1's l2: 0.466852
[58]	training's l2: 0.382209	valid_1's l2: 0.466448
[59]	training's l2: 0.380777	valid_1's l2: 0.466219
[60]	training's l2: 0.378953	valid_1's l2: 0.465245
[61]	training's l2: 0.377173	valid_1's l2: 0.46462
[62]	training's l2: 0.375605	valid_1's l2: 0.463995
[63]	training's l2: 0.37406	valid_1's l2: 0.463508
[64]	training's l2: 0.371893	valid_1's l2: 0.462419
[65]	training's l2: 0.370577	valid_1's l2: 0.461982
[66]	training's l2: 0.369027	valid_1's l2: 0.461706
[67]	training's l2: 0.367633	valid_1's l2: 0.46124
[68]	training's l2: 0.366393	valid_1's l2: 0.460969
[69]	training's l2: 0.364726	valid_1's l2: 0.460314
[70]	training's l2: 0.363274	valid_1's l2: 0.459901
[71]	training's l2: 0.362095	valid_1's l2: 0.45964
[72]	training's l2: 0.360813	valid_1's l2: 0.459127
[73]	training's l2: 0.359564	valid_1's l2: 0.458839
[74]	training's l2: 0.358429	valid_1's l2: 0.458415
[75]	training's l2: 0.356661	valid_1's l2: 0.457676
[76]	training's l2: 0.355108	valid_1's l2: 0.456984
[77]	training's l2: 0.353983	valid_1's l2: 0.456603
[78]	training's l2: 0.352841	valid_1's l2: 0.456627
[79]	training's l2: 0.351225	valid_1's l2: 0.45552
[80]	training's l2: 0.349701	valid_1's l2: 0.454898
[81]	training's l2: 0.348655	valid_1's l2: 0.454753
[82]	training's l2: 0.34756	valid_1's l2: 0.454681
[83]	training's l2: 0.346429	valid_1's l2: 0.454321
[84]	training's l2: 0.345285	valid_1's l2: 0.454301
[85]	training's l2: 0.344065	valid_1's l2: 0.453914
[86]	training's l2: 0.342991	valid_1's l2: 0.453818
[87]	training's l2: 0.341885	valid_1's l2: 0.453667
[88]	training's l2: 0.340768	valid_1's l2: 0.453644
[89]	training's l2: 0.339768	valid_1's l2: 0.453512
[90]	training's l2: 0.338407	valid_1's l2: 0.453179
[91]	training's l2: 0.337383	valid_1's l2: 0.45306
[92]	training's l2: 0.336252	valid_1's l2: 0.452921
[93]	training's l2: 0.335111	valid_1's l2: 0.452685
[94]	training's l2: 0.33405	valid_1's l2: 0.452474
[95]	training's l2: 0.333129	valid_1's l2: 0.452487
[96]	training's l2: 0.331914	valid_1's l2: 0.452478
[97]	training's l2: 0.330941	valid_1's l2: 0.452375
[98]	training's l2: 0.329927	valid_1's l2: 0.452467
[99]	training's l2: 0.32894	valid_1's l2: 0.452341
[100]	training's l2: 0.327983	valid_1's l2: 0.452209
[101]	training's l2: 0.326916	valid_1's l2: 0.452011
[102]	training's l2: 0.325714	valid_1's l2: 0.451594
[103]	training's l2: 0.324647	valid_1's l2: 0.451604
[104]	training's l2: 0.323596	valid_1's l2: 0.451385
[105]	training's l2: 0.322714	valid_1's l2: 0.451715
[106]	training's l2: 0.321789	valid_1's l2: 0.4517
[107]	training's l2: 0.320325	valid_1's l2: 0.450642
[108]	training's l2: 0.319206	valid_1's l2: 0.450533
[109]	training's l2: 0.318288	valid_1's l2: 0.45051
[110]	training's l2: 0.317427	valid_1's l2: 0.450532
[111]	training's l2: 0.316523	valid_1's l2: 0.450523
[112]	training's l2: 0.315093	valid_1's l2: 0.449382
[113]	training's l2: 0.31416	valid_1's l2: 0.449342
[114]	training's l2: 0.313178	valid_1's l2: 0.449185
[115]	training's l2: 0.312343	valid_1's l2: 0.44919
[116]	training's l2: 0.311469	valid_1's l2: 0.449217
[117]	training's l2: 0.310533	valid_1's l2: 0.449012
[118]	training's l2: 0.309576	valid_1's l2: 0.449012
[119]	training's l2: 0.308743	valid_1's l2: 0.448943
[120]	training's l2: 0.307941	valid_1's l2: 0.448774
[121]	training's l2: 0.307048	valid_1's l2: 0.448736
[122]	training's l2: 0.306266	valid_1's l2: 0.448803
[123]	training's l2: 0.305487	valid_1's l2: 0.44893
[124]	training's l2: 0.304589	valid_1's l2: 0.448837
[125]	training's l2: 0.303779	valid_1's l2: 0.449096
[126]	training's l2: 0.302678	valid_1's l2: 0.448641
[127]	training's l2: 0.301742	valid_1's l2: 0.448455
[128]	training's l2: 0.300984	valid_1's l2: 0.448517
[129]	training's l2: 0.300105	valid_1's l2: 0.448419
[130]	training's l2: 0.299159	valid_1's l2: 0.448205
[131]	training's l2: 0.298327	valid_1's l2: 0.448291
[132]	training's l2: 0.297435	valid_1's l2: 0.447979
[133]	training's l2: 0.296535	valid_1's l2: 0.447534
[134]	training's l2: 0.295575	valid_1's l2: 0.446996
[135]	training's l2: 0.29477	valid_1's l2: 0.447022
[136]	training's l2: 0.293814	valid_1's l2: 0.44703
[137]	training's l2: 0.292973	valid_1's l2: 0.447077
[138]	training's l2: 0.292224	valid_1's l2: 0.446983
[139]	training's l2: 0.291451	valid_1's l2: 0.44711
[140]	training's l2: 0.290444	valid_1's l2: 0.446665
[141]	training's l2: 0.289704	valid_1's l2: 0.446638
[142]	training's l2: 0.28893	valid_1's l2: 0.44663
[143]	training's l2: 0.288196	valid_1's l2: 0.446588
[144]	training's l2: 0.287453	valid_1's l2: 0.4468
[145]	training's l2: 0.286685	valid_1's l2: 0.446803
[146]	training's l2: 0.286015	valid_1's l2: 0.446798
[147]	training's l2: 0.285141	valid_1's l2: 0.446612
[148]	training's l2: 0.284352	valid_1's l2: 0.446551
[149]	training's l2: 0.283607	valid_1's l2: 0.446741
[150]	training's l2: 0.282844	valid_1's l2: 0.44653
[151]	training's l2: 0.281975	valid_1's l2: 0.446458
[152]	training's l2: 0.28135	valid_1's l2: 0.44638
[153]	training's l2: 0.280629	valid_1's l2: 0.446618
[154]	training's l2: 0.279885	valid_1's l2: 0.446428
[155]	training's l2: 0.279286	valid_1's l2: 0.446387
[156]	training's l2: 0.278606	valid_1's l2: 0.446229
[157]	training's l2: 0.277847	valid_1's l2: 0.446124
[158]	training's l2: 0.277255	valid_1's l2: 0.446146
[159]	training's l2: 0.27648	valid_1's l2: 0.445786
[160]	training's l2: 0.275764	valid_1's l2: 0.445662
[161]	training's l2: 0.275037	valid_1's l2: 0.445424
[162]	training's l2: 0.274342	valid_1's l2: 0.445454
[163]	training's l2: 0.273806	valid_1's l2: 0.445474
[164]	training's l2: 0.273144	valid_1's l2: 0.445434
[165]	training's l2: 0.272422	valid_1's l2: 0.445476
[166]	training's l2: 0.271806	valid_1's l2: 0.445462
[167]	training's l2: 0.271198	valid_1's l2: 0.445551
[168]	training's l2: 0.270542	valid_1's l2: 0.44551
[169]	training's l2: 0.269755	valid_1's l2: 0.445174
[170]	training's l2: 0.269041	valid_1's l2: 0.445171
[171]	training's l2: 0.268392	valid_1's l2: 0.445181
[172]	training's l2: 0.267747	valid_1's l2: 0.445178
[173]	training's l2: 0.267129	valid_1's l2: 0.445007
[174]	training's l2: 0.266537	valid_1's l2: 0.445006
[175]	training's l2: 0.265869	valid_1's l2: 0.445034
[176]	training's l2: 0.265244	valid_1's l2: 0.444967
[177]	training's l2: 0.264769	valid_1's l2: 0.444963
[178]	training's l2: 0.26414	valid_1's l2: 0.445008
[179]	training's l2: 0.263528	valid_1's l2: 0.445089
Did not meet early stopping. Best iteration is:
[179]	training's l2: 0.263528	valid_1's l2: 0.445089
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.179521 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.733994	valid_1's l2: 0.715661
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.702571	valid_1's l2: 0.689372
[3]	training's l2: 0.676255	valid_1's l2: 0.666687
[4]	training's l2: 0.652404	valid_1's l2: 0.647072
[5]	training's l2: 0.631767	valid_1's l2: 0.630823
[6]	training's l2: 0.611353	valid_1's l2: 0.614241
[7]	training's l2: 0.594205	valid_1's l2: 0.600367
[8]	training's l2: 0.578332	valid_1's l2: 0.588754
[9]	training's l2: 0.564736	valid_1's l2: 0.578067
[10]	training's l2: 0.552402	valid_1's l2: 0.569288
[11]	training's l2: 0.541923	valid_1's l2: 0.561807
[12]	training's l2: 0.529676	valid_1's l2: 0.553215
[13]	training's l2: 0.521049	valid_1's l2: 0.546781
[14]	training's l2: 0.511446	valid_1's l2: 0.538641
[15]	training's l2: 0.503952	valid_1's l2: 0.533817
[16]	training's l2: 0.494931	valid_1's l2: 0.528336
[17]	training's l2: 0.488053	valid_1's l2: 0.523742
[18]	training's l2: 0.481088	valid_1's l2: 0.519126
[19]	training's l2: 0.475138	valid_1's l2: 0.51509
[20]	training's l2: 0.468613	valid_1's l2: 0.510881
[21]	training's l2: 0.46278	valid_1's l2: 0.507376
[22]	training's l2: 0.457651	valid_1's l2: 0.505012
[23]	training's l2: 0.453436	valid_1's l2: 0.502853
[24]	training's l2: 0.448111	valid_1's l2: 0.499619
[25]	training's l2: 0.443823	valid_1's l2: 0.497344
[26]	training's l2: 0.439807	valid_1's l2: 0.495463
[27]	training's l2: 0.436341	valid_1's l2: 0.49391
[28]	training's l2: 0.432059	valid_1's l2: 0.491801
[29]	training's l2: 0.429119	valid_1's l2: 0.490688
[30]	training's l2: 0.426202	valid_1's l2: 0.489441
[31]	training's l2: 0.421839	valid_1's l2: 0.48652
[32]	training's l2: 0.418737	valid_1's l2: 0.4854
[33]	training's l2: 0.414274	valid_1's l2: 0.482136
[34]	training's l2: 0.411729	valid_1's l2: 0.481187
[35]	training's l2: 0.408462	valid_1's l2: 0.479241
[36]	training's l2: 0.403879	valid_1's l2: 0.476059
[37]	training's l2: 0.401014	valid_1's l2: 0.47444
[38]	training's l2: 0.398502	valid_1's l2: 0.473332
[39]	training's l2: 0.395962	valid_1's l2: 0.472666
[40]	training's l2: 0.393649	valid_1's l2: 0.47195
[41]	training's l2: 0.390278	valid_1's l2: 0.469504
[42]	training's l2: 0.388047	valid_1's l2: 0.467974
[43]	training's l2: 0.384896	valid_1's l2: 0.467018
[44]	training's l2: 0.382847	valid_1's l2: 0.466906
[45]	training's l2: 0.380809	valid_1's l2: 0.466559
[46]	training's l2: 0.378926	valid_1's l2: 0.466195
[47]	training's l2: 0.376014	valid_1's l2: 0.464945
[48]	training's l2: 0.374043	valid_1's l2: 0.464233
[49]	training's l2: 0.372204	valid_1's l2: 0.463901
[50]	training's l2: 0.369975	valid_1's l2: 0.462973
[51]	training's l2: 0.368389	valid_1's l2: 0.462628
[52]	training's l2: 0.36622	valid_1's l2: 0.462184
[53]	training's l2: 0.364446	valid_1's l2: 0.462065
[54]	training's l2: 0.362291	valid_1's l2: 0.461021
[55]	training's l2: 0.360604	valid_1's l2: 0.460944
[56]	training's l2: 0.359035	valid_1's l2: 0.460514
[57]	training's l2: 0.357104	valid_1's l2: 0.460112
[58]	training's l2: 0.354561	valid_1's l2: 0.458561
[59]	training's l2: 0.352946	valid_1's l2: 0.458227
[60]	training's l2: 0.351468	valid_1's l2: 0.45794
[61]	training's l2: 0.349379	valid_1's l2: 0.457601
[62]	training's l2: 0.347883	valid_1's l2: 0.457602
[63]	training's l2: 0.345808	valid_1's l2: 0.45615
[64]	training's l2: 0.344293	valid_1's l2: 0.456117
[65]	training's l2: 0.342421	valid_1's l2: 0.455616
[66]	training's l2: 0.340668	valid_1's l2: 0.455603
[67]	training's l2: 0.339288	valid_1's l2: 0.45563
[68]	training's l2: 0.337613	valid_1's l2: 0.455351
[69]	training's l2: 0.336221	valid_1's l2: 0.455287
[70]	training's l2: 0.334816	valid_1's l2: 0.455002
[71]	training's l2: 0.333149	valid_1's l2: 0.454106
[72]	training's l2: 0.331238	valid_1's l2: 0.45314
[73]	training's l2: 0.329913	valid_1's l2: 0.453317
[74]	training's l2: 0.328531	valid_1's l2: 0.453157
[75]	training's l2: 0.327275	valid_1's l2: 0.453202
[76]	training's l2: 0.325877	valid_1's l2: 0.45295
[77]	training's l2: 0.324651	valid_1's l2: 0.452896
[78]	training's l2: 0.323347	valid_1's l2: 0.452807
[79]	training's l2: 0.322135	valid_1's l2: 0.452994
[80]	training's l2: 0.320454	valid_1's l2: 0.452289
[81]	training's l2: 0.319091	valid_1's l2: 0.452086
[82]	training's l2: 0.317892	valid_1's l2: 0.452034
[83]	training's l2: 0.316611	valid_1's l2: 0.451959
[84]	training's l2: 0.315355	valid_1's l2: 0.452018
[85]	training's l2: 0.314079	valid_1's l2: 0.451721
[86]	training's l2: 0.312905	valid_1's l2: 0.452211
[87]	training's l2: 0.311714	valid_1's l2: 0.452261
[88]	training's l2: 0.31001	valid_1's l2: 0.451505
[89]	training's l2: 0.308782	valid_1's l2: 0.451675
[90]	training's l2: 0.307172	valid_1's l2: 0.451038
[91]	training's l2: 0.306046	valid_1's l2: 0.451149
[92]	training's l2: 0.304933	valid_1's l2: 0.451112
[93]	training's l2: 0.303776	valid_1's l2: 0.450941
[94]	training's l2: 0.302738	valid_1's l2: 0.450913
[95]	training's l2: 0.301624	valid_1's l2: 0.450999
[96]	training's l2: 0.300545	valid_1's l2: 0.451193
[97]	training's l2: 0.299275	valid_1's l2: 0.451258
[98]	training's l2: 0.298233	valid_1's l2: 0.451201
[99]	training's l2: 0.296596	valid_1's l2: 0.450264
[100]	training's l2: 0.295615	valid_1's l2: 0.450275
[101]	training's l2: 0.29445	valid_1's l2: 0.450276
[102]	training's l2: 0.293372	valid_1's l2: 0.450248
[103]	training's l2: 0.292411	valid_1's l2: 0.450383
[104]	training's l2: 0.291243	valid_1's l2: 0.450082
[105]	training's l2: 0.290287	valid_1's l2: 0.450097
[106]	training's l2: 0.289264	valid_1's l2: 0.450311
[107]	training's l2: 0.288307	valid_1's l2: 0.450291
[108]	training's l2: 0.28725	valid_1's l2: 0.450183
[109]	training's l2: 0.286197	valid_1's l2: 0.449929
[110]	training's l2: 0.284831	valid_1's l2: 0.449379
[111]	training's l2: 0.283583	valid_1's l2: 0.44901
[112]	training's l2: 0.282618	valid_1's l2: 0.449074
[113]	training's l2: 0.281658	valid_1's l2: 0.449075
[114]	training's l2: 0.280543	valid_1's l2: 0.448761
[115]	training's l2: 0.279517	valid_1's l2: 0.44905
[116]	training's l2: 0.278625	valid_1's l2: 0.448961
[117]	training's l2: 0.277669	valid_1's l2: 0.448833
[118]	training's l2: 0.276681	valid_1's l2: 0.448508
[119]	training's l2: 0.275711	valid_1's l2: 0.44835
[120]	training's l2: 0.274768	valid_1's l2: 0.448389
[121]	training's l2: 0.273854	valid_1's l2: 0.448518
[122]	training's l2: 0.272874	valid_1's l2: 0.448647
[123]	training's l2: 0.271855	valid_1's l2: 0.448484
[124]	training's l2: 0.270829	valid_1's l2: 0.44847
[125]	training's l2: 0.269936	valid_1's l2: 0.448474
[126]	training's l2: 0.269005	valid_1's l2: 0.448342
[127]	training's l2: 0.268034	valid_1's l2: 0.448354
[128]	training's l2: 0.267152	valid_1's l2: 0.448472
[129]	training's l2: 0.266139	valid_1's l2: 0.448595
[130]	training's l2: 0.265213	valid_1's l2: 0.448845
[131]	training's l2: 0.264323	valid_1's l2: 0.448767
[132]	training's l2: 0.263445	valid_1's l2: 0.448762
[133]	training's l2: 0.262627	valid_1's l2: 0.448643
[134]	training's l2: 0.261798	valid_1's l2: 0.448567
[135]	training's l2: 0.261007	valid_1's l2: 0.448836
[136]	training's l2: 0.26017	valid_1's l2: 0.448623
[137]	training's l2: 0.259353	valid_1's l2: 0.448728
[138]	training's l2: 0.25862	valid_1's l2: 0.448711
[139]	training's l2: 0.257712	valid_1's l2: 0.448906
[140]	training's l2: 0.256965	valid_1's l2: 0.448752
[141]	training's l2: 0.256092	valid_1's l2: 0.448627
[142]	training's l2: 0.2552	valid_1's l2: 0.448417
[143]	training's l2: 0.254522	valid_1's l2: 0.448351
[144]	training's l2: 0.253751	valid_1's l2: 0.448472
[145]	training's l2: 0.253014	valid_1's l2: 0.44862
[146]	training's l2: 0.252284	valid_1's l2: 0.44867
[147]	training's l2: 0.251473	valid_1's l2: 0.448374
[148]	training's l2: 0.250683	valid_1's l2: 0.448474
[149]	training's l2: 0.249889	valid_1's l2: 0.448381
[150]	training's l2: 0.249085	valid_1's l2: 0.448617
[151]	training's l2: 0.248332	valid_1's l2: 0.448533
[152]	training's l2: 0.247626	valid_1's l2: 0.448536
[153]	training's l2: 0.24685	valid_1's l2: 0.448488
[154]	training's l2: 0.246152	valid_1's l2: 0.448494
[155]	training's l2: 0.245408	valid_1's l2: 0.44856
[156]	training's l2: 0.244579	valid_1's l2: 0.448567
Early stopping, best iteration is:
[126]	training's l2: 0.269005	valid_1's l2: 0.448342
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.180123 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.740468	valid_1's l2: 0.721584
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.71469	valid_1's l2: 0.700328
[3]	training's l2: 0.690866	valid_1's l2: 0.680526
[4]	training's l2: 0.669962	valid_1's l2: 0.663344
[5]	training's l2: 0.650615	valid_1's l2: 0.647815
[6]	training's l2: 0.632369	valid_1's l2: 0.633229
[7]	training's l2: 0.615196	valid_1's l2: 0.620291
[8]	training's l2: 0.601337	valid_1's l2: 0.61003
[9]	training's l2: 0.58758	valid_1's l2: 0.599874
[10]	training's l2: 0.575824	valid_1's l2: 0.591191
[11]	training's l2: 0.56391	valid_1's l2: 0.581762
[12]	training's l2: 0.554278	valid_1's l2: 0.574677
[13]	training's l2: 0.544389	valid_1's l2: 0.567773
[14]	training's l2: 0.535747	valid_1's l2: 0.561682
[15]	training's l2: 0.526722	valid_1's l2: 0.555309
[16]	training's l2: 0.517487	valid_1's l2: 0.548338
[17]	training's l2: 0.507953	valid_1's l2: 0.541723
[18]	training's l2: 0.500978	valid_1's l2: 0.536696
[19]	training's l2: 0.493858	valid_1's l2: 0.531857
[20]	training's l2: 0.487009	valid_1's l2: 0.527451
[21]	training's l2: 0.479252	valid_1's l2: 0.521426
[22]	training's l2: 0.473824	valid_1's l2: 0.518369
[23]	training's l2: 0.468412	valid_1's l2: 0.515119
[24]	training's l2: 0.463489	valid_1's l2: 0.511996
[25]	training's l2: 0.457619	valid_1's l2: 0.508564
[26]	training's l2: 0.453307	valid_1's l2: 0.506374
[27]	training's l2: 0.448055	valid_1's l2: 0.502649
[28]	training's l2: 0.443532	valid_1's l2: 0.499884
[29]	training's l2: 0.439972	valid_1's l2: 0.498419
[30]	training's l2: 0.436451	valid_1's l2: 0.497097
[31]	training's l2: 0.43251	valid_1's l2: 0.494581
[32]	training's l2: 0.428241	valid_1's l2: 0.491868
[33]	training's l2: 0.423677	valid_1's l2: 0.488944
[34]	training's l2: 0.420468	valid_1's l2: 0.487747
[35]	training's l2: 0.417837	valid_1's l2: 0.486755
[36]	training's l2: 0.414778	valid_1's l2: 0.485393
[37]	training's l2: 0.411801	valid_1's l2: 0.483997
[38]	training's l2: 0.409071	valid_1's l2: 0.48285
[39]	training's l2: 0.4064	valid_1's l2: 0.481893
[40]	training's l2: 0.403125	valid_1's l2: 0.480352
[41]	training's l2: 0.399594	valid_1's l2: 0.478263
[42]	training's l2: 0.396749	valid_1's l2: 0.477551
[43]	training's l2: 0.393608	valid_1's l2: 0.475889
[44]	training's l2: 0.391244	valid_1's l2: 0.475171
[45]	training's l2: 0.387458	valid_1's l2: 0.472571
[46]	training's l2: 0.385375	valid_1's l2: 0.472135
[47]	training's l2: 0.383268	valid_1's l2: 0.471412
[48]	training's l2: 0.380722	valid_1's l2: 0.470328
[49]	training's l2: 0.378496	valid_1's l2: 0.469628
[50]	training's l2: 0.376438	valid_1's l2: 0.46905
[51]	training's l2: 0.373851	valid_1's l2: 0.468327
[52]	training's l2: 0.371558	valid_1's l2: 0.467854
[53]	training's l2: 0.369155	valid_1's l2: 0.466304
[54]	training's l2: 0.367445	valid_1's l2: 0.466007
[55]	training's l2: 0.365575	valid_1's l2: 0.46506
[56]	training's l2: 0.363489	valid_1's l2: 0.464404
[57]	training's l2: 0.361154	valid_1's l2: 0.463492
[58]	training's l2: 0.359321	valid_1's l2: 0.462905
[59]	training's l2: 0.357464	valid_1's l2: 0.462448
[60]	training's l2: 0.355744	valid_1's l2: 0.462018
[61]	training's l2: 0.353671	valid_1's l2: 0.460807
[62]	training's l2: 0.352098	valid_1's l2: 0.460529
[63]	training's l2: 0.350108	valid_1's l2: 0.459691
[64]	training's l2: 0.348509	valid_1's l2: 0.459404
[65]	training's l2: 0.346915	valid_1's l2: 0.459429
[66]	training's l2: 0.345268	valid_1's l2: 0.458937
[67]	training's l2: 0.343382	valid_1's l2: 0.458455
[68]	training's l2: 0.341675	valid_1's l2: 0.457574
[69]	training's l2: 0.340017	valid_1's l2: 0.457296
[70]	training's l2: 0.33845	valid_1's l2: 0.457245
[71]	training's l2: 0.336943	valid_1's l2: 0.457129
[72]	training's l2: 0.335063	valid_1's l2: 0.456262
[73]	training's l2: 0.333665	valid_1's l2: 0.456356
[74]	training's l2: 0.332314	valid_1's l2: 0.456132
[75]	training's l2: 0.330866	valid_1's l2: 0.455761
[76]	training's l2: 0.329508	valid_1's l2: 0.455426
[77]	training's l2: 0.328069	valid_1's l2: 0.455037
[78]	training's l2: 0.326692	valid_1's l2: 0.454918
[79]	training's l2: 0.325119	valid_1's l2: 0.454429
[80]	training's l2: 0.323719	valid_1's l2: 0.454571
[81]	training's l2: 0.322432	valid_1's l2: 0.454642
[82]	training's l2: 0.321212	valid_1's l2: 0.454438
[83]	training's l2: 0.31972	valid_1's l2: 0.45408
[84]	training's l2: 0.318246	valid_1's l2: 0.453884
[85]	training's l2: 0.317045	valid_1's l2: 0.4539
[86]	training's l2: 0.315779	valid_1's l2: 0.453799
[87]	training's l2: 0.314131	valid_1's l2: 0.453129
[88]	training's l2: 0.31297	valid_1's l2: 0.452962
[89]	training's l2: 0.311604	valid_1's l2: 0.452674
[90]	training's l2: 0.310303	valid_1's l2: 0.452464
[91]	training's l2: 0.308772	valid_1's l2: 0.4522
[92]	training's l2: 0.307438	valid_1's l2: 0.452076
[93]	training's l2: 0.306244	valid_1's l2: 0.452305
[94]	training's l2: 0.304686	valid_1's l2: 0.45193
[95]	training's l2: 0.303263	valid_1's l2: 0.451654
[96]	training's l2: 0.301881	valid_1's l2: 0.451484
[97]	training's l2: 0.300764	valid_1's l2: 0.45155
[98]	training's l2: 0.299647	valid_1's l2: 0.451563
[99]	training's l2: 0.298453	valid_1's l2: 0.451272
[100]	training's l2: 0.297314	valid_1's l2: 0.451269
[101]	training's l2: 0.2962	valid_1's l2: 0.451479
[102]	training's l2: 0.295113	valid_1's l2: 0.451507
[103]	training's l2: 0.293528	valid_1's l2: 0.450982
[104]	training's l2: 0.292404	valid_1's l2: 0.450811
[105]	training's l2: 0.291202	valid_1's l2: 0.450655
[106]	training's l2: 0.289948	valid_1's l2: 0.450361
[107]	training's l2: 0.288785	valid_1's l2: 0.450373
[108]	training's l2: 0.287761	valid_1's l2: 0.450385
[109]	training's l2: 0.286679	valid_1's l2: 0.450282
[110]	training's l2: 0.285609	valid_1's l2: 0.450171
[111]	training's l2: 0.284569	valid_1's l2: 0.450217
[112]	training's l2: 0.283554	valid_1's l2: 0.450294
[113]	training's l2: 0.282558	valid_1's l2: 0.450365
[114]	training's l2: 0.281024	valid_1's l2: 0.449601
[115]	training's l2: 0.280056	valid_1's l2: 0.449698
[116]	training's l2: 0.279096	valid_1's l2: 0.449823
[117]	training's l2: 0.277866	valid_1's l2: 0.449469
[118]	training's l2: 0.276883	valid_1's l2: 0.449337
[119]	training's l2: 0.275894	valid_1's l2: 0.449207
[120]	training's l2: 0.274905	valid_1's l2: 0.449062
[121]	training's l2: 0.273959	valid_1's l2: 0.448931
[122]	training's l2: 0.27299	valid_1's l2: 0.448853
[123]	training's l2: 0.27201	valid_1's l2: 0.448821
[124]	training's l2: 0.27055	valid_1's l2: 0.448083
[125]	training's l2: 0.269659	valid_1's l2: 0.447968
[126]	training's l2: 0.268411	valid_1's l2: 0.447144
[127]	training's l2: 0.267626	valid_1's l2: 0.447208
[128]	training's l2: 0.266601	valid_1's l2: 0.447396
[129]	training's l2: 0.265613	valid_1's l2: 0.447471
[130]	training's l2: 0.264635	valid_1's l2: 0.447279
[131]	training's l2: 0.26371	valid_1's l2: 0.447193
[132]	training's l2: 0.262803	valid_1's l2: 0.447135
[133]	training's l2: 0.261863	valid_1's l2: 0.447171
[134]	training's l2: 0.261033	valid_1's l2: 0.447196
[135]	training's l2: 0.260007	valid_1's l2: 0.44711
[136]	training's l2: 0.259088	valid_1's l2: 0.447128
[137]	training's l2: 0.25826	valid_1's l2: 0.447223
[138]	training's l2: 0.25732	valid_1's l2: 0.44723
[139]	training's l2: 0.256502	valid_1's l2: 0.446921
[140]	training's l2: 0.25568	valid_1's l2: 0.446966
[141]	training's l2: 0.254822	valid_1's l2: 0.446875
[142]	training's l2: 0.253994	valid_1's l2: 0.447044
[143]	training's l2: 0.253215	valid_1's l2: 0.44687
[144]	training's l2: 0.252341	valid_1's l2: 0.447079
[145]	training's l2: 0.251452	valid_1's l2: 0.446587
[146]	training's l2: 0.250661	valid_1's l2: 0.446586
[147]	training's l2: 0.249755	valid_1's l2: 0.446475
[148]	training's l2: 0.248954	valid_1's l2: 0.446577
[149]	training's l2: 0.248174	valid_1's l2: 0.446578
[150]	training's l2: 0.247398	valid_1's l2: 0.446637
[151]	training's l2: 0.246625	valid_1's l2: 0.446634
[152]	training's l2: 0.245818	valid_1's l2: 0.446729
[153]	training's l2: 0.245081	valid_1's l2: 0.446835
[154]	training's l2: 0.244282	valid_1's l2: 0.446855
[155]	training's l2: 0.243552	valid_1's l2: 0.446821
[156]	training's l2: 0.242876	valid_1's l2: 0.446877
[157]	training's l2: 0.242069	valid_1's l2: 0.44681
[158]	training's l2: 0.241394	valid_1's l2: 0.446812
[159]	training's l2: 0.240711	valid_1's l2: 0.446706
[160]	training's l2: 0.239912	valid_1's l2: 0.44681
[161]	training's l2: 0.239168	valid_1's l2: 0.446805
Did not meet early stopping. Best iteration is:
[161]	training's l2: 0.239168	valid_1's l2: 0.446805
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.182228 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 139397
[LightGBM] [Info] Number of data points in the train set: 44675, number of used features: 651
[LightGBM] [Info] Start training from score -0.016464
[1]	training's l2: 0.729418	valid_1's l2: 0.711052
Training until validation scores don't improve for 30 rounds
[2]	training's l2: 0.694647	valid_1's l2: 0.681984
[3]	training's l2: 0.666724	valid_1's l2: 0.658709
[4]	training's l2: 0.64133	valid_1's l2: 0.637699
[5]	training's l2: 0.619347	valid_1's l2: 0.62018
[6]	training's l2: 0.59931	valid_1's l2: 0.603478
[7]	training's l2: 0.582134	valid_1's l2: 0.589314
[8]	training's l2: 0.56768	valid_1's l2: 0.578737
[9]	training's l2: 0.555867	valid_1's l2: 0.569826
[10]	training's l2: 0.541201	valid_1's l2: 0.558489
[11]	training's l2: 0.531925	valid_1's l2: 0.551394
[12]	training's l2: 0.522572	valid_1's l2: 0.545114
[13]	training's l2: 0.512034	valid_1's l2: 0.538044
[14]	training's l2: 0.504036	valid_1's l2: 0.531943
[15]	training's l2: 0.494511	valid_1's l2: 0.525091
[16]	training's l2: 0.48798	valid_1's l2: 0.520209
[17]	training's l2: 0.479913	valid_1's l2: 0.514859
[18]	training's l2: 0.47474	valid_1's l2: 0.511717
[19]	training's l2: 0.469755	valid_1's l2: 0.509169
[20]	training's l2: 0.462669	valid_1's l2: 0.504075
[21]	training's l2: 0.458306	valid_1's l2: 0.501551
[22]	training's l2: 0.453986	valid_1's l2: 0.499192
[23]	training's l2: 0.449875	valid_1's l2: 0.496182
[24]	training's l2: 0.445932	valid_1's l2: 0.494586
[25]	training's l2: 0.442539	valid_1's l2: 0.492419
[26]	training's l2: 0.435715	valid_1's l2: 0.487184
[27]	training's l2: 0.429677	valid_1's l2: 0.482552
[28]	training's l2: 0.425885	valid_1's l2: 0.480928
[29]	training's l2: 0.422335	valid_1's l2: 0.479025
[30]	training's l2: 0.419943	valid_1's l2: 0.478163
[31]	training's l2: 0.417266	valid_1's l2: 0.476949
[32]	training's l2: 0.413556	valid_1's l2: 0.474812
[33]	training's l2: 0.410791	valid_1's l2: 0.473486
[34]	training's l2: 0.408574	valid_1's l2: 0.473117
[35]	training's l2: 0.40617	valid_1's l2: 0.472109
[36]	training's l2: 0.402986	valid_1's l2: 0.470434
[37]	training's l2: 0.40029	valid_1's l2: 0.469429
[38]	training's l2: 0.39806	valid_1's l2: 0.468987
[39]	training's l2: 0.39476	valid_1's l2: 0.468212
[40]	training's l2: 0.392777	valid_1's l2: 0.467531
[41]	training's l2: 0.389435	valid_1's l2: 0.465422
[42]	training's l2: 0.387575	valid_1's l2: 0.46478
[43]	training's l2: 0.384756	valid_1's l2: 0.463069
[44]	training's l2: 0.382996	valid_1's l2: 0.462827
[45]	training's l2: 0.381114	valid_1's l2: 0.462161
[46]	training's l2: 0.378497	valid_1's l2: 0.460355
[47]	training's l2: 0.375863	valid_1's l2: 0.459094
[48]	training's l2: 0.374256	valid_1's l2: 0.459088
[49]	training's l2: 0.371765	valid_1's l2: 0.458875
[50]	training's l2: 0.370158	valid_1's l2: 0.458796
[51]	training's l2: 0.368623	valid_1's l2: 0.458305
[52]	training's l2: 0.366995	valid_1's l2: 0.458262
[53]	training's l2: 0.364892	valid_1's l2: 0.457302
[54]	training's l2: 0.363237	valid_1's l2: 0.456711
[55]	training's l2: 0.361536	valid_1's l2: 0.45692
[56]	training's l2: 0.359765	valid_1's l2: 0.456163
[57]	training's l2: 0.35813	valid_1's l2: 0.455841
[58]	training's l2: 0.355925	valid_1's l2: 0.454281
[59]	training's l2: 0.354315	valid_1's l2: 0.454276
[60]	training's l2: 0.352806	valid_1's l2: 0.45397
[61]	training's l2: 0.350685	valid_1's l2: 0.4538
[62]	training's l2: 0.349009	valid_1's l2: 0.453075
[63]	training's l2: 0.347642	valid_1's l2: 0.452665
[64]	training's l2: 0.345994	valid_1's l2: 0.452289
[65]	training's l2: 0.344607	valid_1's l2: 0.452417
[66]	training's l2: 0.342583	valid_1's l2: 0.451121
[67]	training's l2: 0.34115	valid_1's l2: 0.450988
[68]	training's l2: 0.339919	valid_1's l2: 0.4508
[69]	training's l2: 0.338605	valid_1's l2: 0.450821
[70]	training's l2: 0.337194	valid_1's l2: 0.450661
[71]	training's l2: 0.335803	valid_1's l2: 0.450113
[72]	training's l2: 0.334381	valid_1's l2: 0.450068
[73]	training's l2: 0.333159	valid_1's l2: 0.449831
[74]	training's l2: 0.331518	valid_1's l2: 0.449689
[75]	training's l2: 0.330141	valid_1's l2: 0.44959
[76]	training's l2: 0.328912	valid_1's l2: 0.449573
[77]	training's l2: 0.327022	valid_1's l2: 0.448543
[78]	training's l2: 0.325517	valid_1's l2: 0.447834
[79]	training's l2: 0.324383	valid_1's l2: 0.447764
[80]	training's l2: 0.323204	valid_1's l2: 0.447872
[81]	training's l2: 0.32198	valid_1's l2: 0.447554
[82]	training's l2: 0.320792	valid_1's l2: 0.447102
[83]	training's l2: 0.319595	valid_1's l2: 0.446949
[84]	training's l2: 0.318302	valid_1's l2: 0.446836
[85]	training's l2: 0.317122	valid_1's l2: 0.446752
[86]	training's l2: 0.315148	valid_1's l2: 0.445066
[87]	training's l2: 0.314026	valid_1's l2: 0.445125
[88]	training's l2: 0.313055	valid_1's l2: 0.445043
[89]	training's l2: 0.311789	valid_1's l2: 0.444557
[90]	training's l2: 0.310763	valid_1's l2: 0.444452
[91]	training's l2: 0.309757	valid_1's l2: 0.444456
[92]	training's l2: 0.308589	valid_1's l2: 0.444562
[93]	training's l2: 0.307551	valid_1's l2: 0.444815
[94]	training's l2: 0.306599	valid_1's l2: 0.444548
[95]	training's l2: 0.30545	valid_1's l2: 0.444347
[96]	training's l2: 0.304373	valid_1's l2: 0.444363
[97]	training's l2: 0.303146	valid_1's l2: 0.444504
[98]	training's l2: 0.30208	valid_1's l2: 0.444279
[99]	training's l2: 0.300984	valid_1's l2: 0.444171
[100]	training's l2: 0.299796	valid_1's l2: 0.443989
[101]	training's l2: 0.298788	valid_1's l2: 0.443924
[102]	training's l2: 0.297716	valid_1's l2: 0.443901
[103]	training's l2: 0.296426	valid_1's l2: 0.443421
[104]	training's l2: 0.295454	valid_1's l2: 0.443624
[105]	training's l2: 0.294657	valid_1's l2: 0.443507
[106]	training's l2: 0.29353	valid_1's l2: 0.443454
[107]	training's l2: 0.29256	valid_1's l2: 0.443522
[108]	training's l2: 0.291598	valid_1's l2: 0.443447
[109]	training's l2: 0.290708	valid_1's l2: 0.443376
[110]	training's l2: 0.289602	valid_1's l2: 0.443532
[111]	training's l2: 0.288372	valid_1's l2: 0.4433
[112]	training's l2: 0.287465	valid_1's l2: 0.443456
[113]	training's l2: 0.286671	valid_1's l2: 0.443482
[114]	training's l2: 0.285777	valid_1's l2: 0.443667
[115]	training's l2: 0.28485	valid_1's l2: 0.442955
[116]	training's l2: 0.28389	valid_1's l2: 0.44273
[117]	training's l2: 0.283068	valid_1's l2: 0.442701
[118]	training's l2: 0.282168	valid_1's l2: 0.442821
[119]	training's l2: 0.281351	valid_1's l2: 0.442899
[120]	training's l2: 0.280433	valid_1's l2: 0.442855
[121]	training's l2: 0.279452	valid_1's l2: 0.442922
[122]	training's l2: 0.278614	valid_1's l2: 0.442886
[123]	training's l2: 0.277758	valid_1's l2: 0.442943
[124]	training's l2: 0.276724	valid_1's l2: 0.442879
[125]	training's l2: 0.275944	valid_1's l2: 0.442682
[126]	training's l2: 0.275114	valid_1's l2: 0.442618
[127]	training's l2: 0.274241	valid_1's l2: 0.4429
[128]	training's l2: 0.273353	valid_1's l2: 0.44306
[129]	training's l2: 0.272594	valid_1's l2: 0.443078
[130]	training's l2: 0.271826	valid_1's l2: 0.443233
[131]	training's l2: 0.270997	valid_1's l2: 0.443285
[132]	training's l2: 0.270068	valid_1's l2: 0.443661
[133]	training's l2: 0.269348	valid_1's l2: 0.443621
[134]	training's l2: 0.268515	valid_1's l2: 0.443646
[135]	training's l2: 0.2676	valid_1's l2: 0.443346
[136]	training's l2: 0.266879	valid_1's l2: 0.443182
[137]	training's l2: 0.266154	valid_1's l2: 0.443354
[138]	training's l2: 0.265366	valid_1's l2: 0.443398
[139]	training's l2: 0.264532	valid_1's l2: 0.443431
[140]	training's l2: 0.263744	valid_1's l2: 0.443326
[141]	training's l2: 0.262942	valid_1's l2: 0.44344
[142]	training's l2: 0.262181	valid_1's l2: 0.443476
[143]	training's l2: 0.261458	valid_1's l2: 0.443255
[144]	training's l2: 0.260819	valid_1's l2: 0.443181
[145]	training's l2: 0.260009	valid_1's l2: 0.443187
[146]	training's l2: 0.259146	valid_1's l2: 0.443027
[147]	training's l2: 0.258299	valid_1's l2: 0.443137
[148]	training's l2: 0.257598	valid_1's l2: 0.443282
[149]	training's l2: 0.256875	valid_1's l2: 0.443184
[150]	training's l2: 0.256198	valid_1's l2: 0.443245
[151]	training's l2: 0.255524	valid_1's l2: 0.443134
[152]	training's l2: 0.254857	valid_1's l2: 0.443147
[153]	training's l2: 0.253993	valid_1's l2: 0.442984
[154]	training's l2: 0.253206	valid_1's l2: 0.443017
[155]	training's l2: 0.252537	valid_1's l2: 0.443109
[156]	training's l2: 0.251745	valid_1's l2: 0.443128
Early stopping, best iteration is:
[126]	training's l2: 0.275114	valid_1's l2: 0.442618
pace score: 1.397119727413377
pace_regression score: 0.016096836510973864
before_pace_regression score: 0.02671271598661657
after_pace_regression score: 0.04365435567634935
pace_conv score: 0.05456735264392349
first_up3 score: 0.6389945627589009
last_up3 score: 0.6659048609594618
