standard_time.pickle download finish Gilgamesh
up_average.pickle download finish Gilgamesh
up_pace_regressin.pickle download finish Gilgamesh
up_kind_ave_data.pickle download finish Gilgamesh
money_class_true_skill_data.pickle download finish Gilgamesh
race_ave_true_skill.pickle download finish Gilgamesh
race_money_data.pickle download finish Gilgamesh
time_index_data.pickle download finish Gilgamesh
train_time_data.pickle download finish Gilgamesh
train_ave_data.pickle download finish Gilgamesh
train_ave_key_data.pickle download finish Gilgamesh
race_info_data.pickle download finish Gilgamesh
jockey_analyze_data.pickle download finish Gilgamesh
jockey_id_data.pickle download finish Gilgamesh
jockey_year_rank_data.pickle download finish Gilgamesh
trainer_analyze_data.pickle download finish Gilgamesh
race_trainer_id_data.pickle download finish Gilgamesh
race_rank_data.pickle download finish Gilgamesh
next_race_data.pickle download finish Gilgamesh
foot_used.pickle download finish Gilgamesh
wrap_data.pickle download finish Gilgamesh
race_data.pickle download finish Gilgamesh
horce_data_storage.pickle download finish Gilgamesh
baba_index_data.pickle download finish Gilgamesh
parent_id_data.pickle download finish Gilgamesh
race_day.pickle download finish Gilgamesh
horce_sex_data.pickle download finish Gilgamesh
race_jockey_id_data.pickle download finish Gilgamesh
true_skill_data.pickle download finish Gilgamesh
waku_three_rate_data.pickle download finish Gilgamesh
corner_horce_body.pickle download finish Gilgamesh
omega_index_data.pickle download finish Gilgamesh
jockey_judgment_data.pickle download finish Gilgamesh
jockey_judgment_rate_data.pickle download finish Gilgamesh
trainer_judgment_data.pickle download finish Gilgamesh
trainer_judgment_rate_data.pickle download finish Gilgamesh
first_passing_true_skill_data.pickle download finish Gilgamesh
last_passing_true_skill_data.pickle download finish Gilgamesh
start rank:4
start rank:1
start rank:2
start rank:3
start rank:5
1-instance.pickle download finish Gilgamesh
2-instance.pickle download finish Gilgamesh
3-instance.pickle download finish Gilgamesh
4-instance.pickle download finish Gilgamesh
5-instance.pickle download finish Gilgamesh
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.272348 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 27.706	valid_1's l2: 26.8499
[20]	training's l2: 24.7132	valid_1's l2: 23.9305
[30]	training's l2: 23.2126	valid_1's l2: 22.5266
[40]	training's l2: 22.393	valid_1's l2: 21.8108
[50]	training's l2: 21.873	valid_1's l2: 21.4029
[60]	training's l2: 21.5036	valid_1's l2: 21.1518
[70]	training's l2: 21.2154	valid_1's l2: 20.9796
[80]	training's l2: 20.9785	valid_1's l2: 20.8578
[90]	training's l2: 20.7698	valid_1's l2: 20.765
[100]	training's l2: 20.5882	valid_1's l2: 20.6998
[110]	training's l2: 20.4195	valid_1's l2: 20.6473
[120]	training's l2: 20.2629	valid_1's l2: 20.6049
[130]	training's l2: 20.113	valid_1's l2: 20.5693
[140]	training's l2: 19.9717	valid_1's l2: 20.5434
[150]	training's l2: 19.8345	valid_1's l2: 20.5165
[160]	training's l2: 19.7039	valid_1's l2: 20.4969
[170]	training's l2: 19.5734	valid_1's l2: 20.4765
[180]	training's l2: 19.4463	valid_1's l2: 20.4563
[190]	training's l2: 19.3245	valid_1's l2: 20.4399
[200]	training's l2: 19.205	valid_1's l2: 20.4266
[210]	training's l2: 19.0872	valid_1's l2: 20.4132
[220]	training's l2: 18.9704	valid_1's l2: 20.3987
[230]	training's l2: 18.8572	valid_1's l2: 20.387
[240]	training's l2: 18.7459	valid_1's l2: 20.3787
[250]	training's l2: 18.637	valid_1's l2: 20.3702
[260]	training's l2: 18.5289	valid_1's l2: 20.3601
[270]	training's l2: 18.4239	valid_1's l2: 20.3571
[280]	training's l2: 18.3183	valid_1's l2: 20.3489
[290]	training's l2: 18.2161	valid_1's l2: 20.3432
[300]	training's l2: 18.1154	valid_1's l2: 20.3391
[310]	training's l2: 18.0185	valid_1's l2: 20.3341
[320]	training's l2: 17.9251	valid_1's l2: 20.3319
[330]	training's l2: 17.8317	valid_1's l2: 20.3324
[340]	training's l2: 17.7366	valid_1's l2: 20.3282
[350]	training's l2: 17.649	valid_1's l2: 20.3256
[360]	training's l2: 17.5582	valid_1's l2: 20.3212
[370]	training's l2: 17.4704	valid_1's l2: 20.3181
[380]	training's l2: 17.3831	valid_1's l2: 20.3186
[390]	training's l2: 17.3001	valid_1's l2: 20.3181
[400]	training's l2: 17.215	valid_1's l2: 20.3177
[410]	training's l2: 17.1369	valid_1's l2: 20.3154
[420]	training's l2: 17.054	valid_1's l2: 20.3156
[430]	training's l2: 16.9775	valid_1's l2: 20.3151
[440]	training's l2: 16.8948	valid_1's l2: 20.3112
[450]	training's l2: 16.8167	valid_1's l2: 20.3119
[460]	training's l2: 16.7436	valid_1's l2: 20.3139
[470]	training's l2: 16.6654	valid_1's l2: 20.3145
Early stopping, best iteration is:
[440]	training's l2: 16.8948	valid_1's l2: 20.3112
score1: 3.8423577318010875
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.244714 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 27.9749	valid_1's l2: 27.0773
[20]	training's l2: 25.0434	valid_1's l2: 24.1754
[30]	training's l2: 23.5548	valid_1's l2: 22.7345
[40]	training's l2: 22.7393	valid_1's l2: 21.9736
[50]	training's l2: 22.2345	valid_1's l2: 21.5421
[60]	training's l2: 21.8948	valid_1's l2: 21.2784
[70]	training's l2: 21.6361	valid_1's l2: 21.0912
[80]	training's l2: 21.4318	valid_1's l2: 20.957
[90]	training's l2: 21.2605	valid_1's l2: 20.8591
[100]	training's l2: 21.1085	valid_1's l2: 20.7808
[110]	training's l2: 20.976	valid_1's l2: 20.7237
[120]	training's l2: 20.8553	valid_1's l2: 20.6764
[130]	training's l2: 20.7391	valid_1's l2: 20.6307
[140]	training's l2: 20.6327	valid_1's l2: 20.5963
[150]	training's l2: 20.5283	valid_1's l2: 20.5643
[160]	training's l2: 20.4303	valid_1's l2: 20.5435
[170]	training's l2: 20.335	valid_1's l2: 20.5211
[180]	training's l2: 20.2393	valid_1's l2: 20.5011
[190]	training's l2: 20.1472	valid_1's l2: 20.4799
[200]	training's l2: 20.0599	valid_1's l2: 20.4623
[210]	training's l2: 19.9709	valid_1's l2: 20.4459
[220]	training's l2: 19.886	valid_1's l2: 20.4295
[230]	training's l2: 19.803	valid_1's l2: 20.4168
[240]	training's l2: 19.7229	valid_1's l2: 20.4034
[250]	training's l2: 19.6441	valid_1's l2: 20.3963
[260]	training's l2: 19.5654	valid_1's l2: 20.3889
[270]	training's l2: 19.4899	valid_1's l2: 20.383
[280]	training's l2: 19.4144	valid_1's l2: 20.3727
[290]	training's l2: 19.3399	valid_1's l2: 20.3644
[300]	training's l2: 19.2663	valid_1's l2: 20.3582
[310]	training's l2: 19.1956	valid_1's l2: 20.3497
[320]	training's l2: 19.1233	valid_1's l2: 20.3428
[330]	training's l2: 19.0531	valid_1's l2: 20.3402
[340]	training's l2: 18.9853	valid_1's l2: 20.3358
[350]	training's l2: 18.9179	valid_1's l2: 20.3324
[360]	training's l2: 18.8535	valid_1's l2: 20.3281
[370]	training's l2: 18.7907	valid_1's l2: 20.3253
[380]	training's l2: 18.7269	valid_1's l2: 20.3219
[390]	training's l2: 18.6652	valid_1's l2: 20.3209
[400]	training's l2: 18.6004	valid_1's l2: 20.3191
[410]	training's l2: 18.5393	valid_1's l2: 20.3188
[420]	training's l2: 18.477	valid_1's l2: 20.3183
[430]	training's l2: 18.4158	valid_1's l2: 20.3157
[440]	training's l2: 18.357	valid_1's l2: 20.3149
[450]	training's l2: 18.2974	valid_1's l2: 20.313
[460]	training's l2: 18.2415	valid_1's l2: 20.3115
[470]	training's l2: 18.1882	valid_1's l2: 20.3084
[480]	training's l2: 18.134	valid_1's l2: 20.3068
[490]	training's l2: 18.0791	valid_1's l2: 20.3069
[500]	training's l2: 18.0216	valid_1's l2: 20.3053
[510]	training's l2: 17.9688	valid_1's l2: 20.3039
[520]	training's l2: 17.9131	valid_1's l2: 20.3019
[530]	training's l2: 17.8602	valid_1's l2: 20.3034
[540]	training's l2: 17.8083	valid_1's l2: 20.3024
[550]	training's l2: 17.7542	valid_1's l2: 20.3024
[560]	training's l2: 17.699	valid_1's l2: 20.3011
[570]	training's l2: 17.6518	valid_1's l2: 20.3011
[580]	training's l2: 17.6014	valid_1's l2: 20.3029
Early stopping, best iteration is:
[555]	training's l2: 17.7272	valid_1's l2: 20.3003
score1: 3.847291415409513
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238409 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 29.7072	valid_1's l2: 28.8232
[20]	training's l2: 26.9597	valid_1's l2: 26.1089
[30]	training's l2: 25.1984	valid_1's l2: 24.3864
[40]	training's l2: 24.0422	valid_1's l2: 23.2776
[50]	training's l2: 23.2602	valid_1's l2: 22.5493
[60]	training's l2: 22.72	valid_1's l2: 22.0653
[70]	training's l2: 22.32	valid_1's l2: 21.7245
[80]	training's l2: 22.0156	valid_1's l2: 21.4802
[90]	training's l2: 21.7683	valid_1's l2: 21.3
[100]	training's l2: 21.562	valid_1's l2: 21.1541
[110]	training's l2: 21.3863	valid_1's l2: 21.043
[120]	training's l2: 21.2333	valid_1's l2: 20.9526
[130]	training's l2: 21.0948	valid_1's l2: 20.8768
[140]	training's l2: 20.9688	valid_1's l2: 20.8124
[150]	training's l2: 20.8508	valid_1's l2: 20.761
[160]	training's l2: 20.7437	valid_1's l2: 20.7167
[170]	training's l2: 20.6414	valid_1's l2: 20.6758
[180]	training's l2: 20.5474	valid_1's l2: 20.6443
[190]	training's l2: 20.4565	valid_1's l2: 20.6176
[200]	training's l2: 20.3679	valid_1's l2: 20.5929
[210]	training's l2: 20.2818	valid_1's l2: 20.5663
[220]	training's l2: 20.1993	valid_1's l2: 20.5454
[230]	training's l2: 20.1181	valid_1's l2: 20.5259
[240]	training's l2: 20.0404	valid_1's l2: 20.5099
[250]	training's l2: 19.964	valid_1's l2: 20.4955
[260]	training's l2: 19.8886	valid_1's l2: 20.4808
[270]	training's l2: 19.8139	valid_1's l2: 20.4674
[280]	training's l2: 19.7394	valid_1's l2: 20.4522
[290]	training's l2: 19.6658	valid_1's l2: 20.4395
[300]	training's l2: 19.5954	valid_1's l2: 20.4287
[310]	training's l2: 19.5243	valid_1's l2: 20.4181
[320]	training's l2: 19.4555	valid_1's l2: 20.4054
[330]	training's l2: 19.3887	valid_1's l2: 20.3958
[340]	training's l2: 19.3208	valid_1's l2: 20.3871
[350]	training's l2: 19.254	valid_1's l2: 20.3767
[360]	training's l2: 19.188	valid_1's l2: 20.3703
[370]	training's l2: 19.1229	valid_1's l2: 20.3624
[380]	training's l2: 19.0594	valid_1's l2: 20.3558
[390]	training's l2: 18.9976	valid_1's l2: 20.3511
[400]	training's l2: 18.9345	valid_1's l2: 20.3428
[410]	training's l2: 18.8733	valid_1's l2: 20.3361
[420]	training's l2: 18.8108	valid_1's l2: 20.3318
[430]	training's l2: 18.75	valid_1's l2: 20.3271
[440]	training's l2: 18.6899	valid_1's l2: 20.3234
[450]	training's l2: 18.6299	valid_1's l2: 20.3161
[460]	training's l2: 18.5714	valid_1's l2: 20.3131
[470]	training's l2: 18.5135	valid_1's l2: 20.3082
[480]	training's l2: 18.4562	valid_1's l2: 20.306
[490]	training's l2: 18.3995	valid_1's l2: 20.3029
[500]	training's l2: 18.3422	valid_1's l2: 20.3008
[510]	training's l2: 18.2869	valid_1's l2: 20.2967
[520]	training's l2: 18.2321	valid_1's l2: 20.2949
[530]	training's l2: 18.1804	valid_1's l2: 20.2956
[540]	training's l2: 18.1262	valid_1's l2: 20.2935
[550]	training's l2: 18.0718	valid_1's l2: 20.2902
[560]	training's l2: 18.0171	valid_1's l2: 20.2878
[570]	training's l2: 17.9671	valid_1's l2: 20.2857
[580]	training's l2: 17.916	valid_1's l2: 20.2859
[590]	training's l2: 17.8671	valid_1's l2: 20.2852
[600]	training's l2: 17.8185	valid_1's l2: 20.2852
[610]	training's l2: 17.769	valid_1's l2: 20.2837
[620]	training's l2: 17.7186	valid_1's l2: 20.283
[630]	training's l2: 17.67	valid_1's l2: 20.2829
[640]	training's l2: 17.6237	valid_1's l2: 20.2833
[650]	training's l2: 17.5774	valid_1's l2: 20.2834
Early stopping, best iteration is:
[623]	training's l2: 17.7047	valid_1's l2: 20.2824
score1: 3.8429557619364667
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.247358 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 28.2983	valid_1's l2: 27.3871
[20]	training's l2: 25.3948	valid_1's l2: 24.4971
[30]	training's l2: 23.8652	valid_1's l2: 22.9938
[40]	training's l2: 23.0133	valid_1's l2: 22.1823
[50]	training's l2: 22.48	valid_1's l2: 21.6952
[60]	training's l2: 22.1261	valid_1's l2: 21.3965
[70]	training's l2: 21.8666	valid_1's l2: 21.1959
[80]	training's l2: 21.6645	valid_1's l2: 21.0447
[90]	training's l2: 21.4971	valid_1's l2: 20.9344
[100]	training's l2: 21.355	valid_1's l2: 20.8506
[110]	training's l2: 21.2271	valid_1's l2: 20.7813
[120]	training's l2: 21.1154	valid_1's l2: 20.7258
[130]	training's l2: 21.0108	valid_1's l2: 20.6779
[140]	training's l2: 20.9138	valid_1's l2: 20.6404
[150]	training's l2: 20.8232	valid_1's l2: 20.6083
[160]	training's l2: 20.7375	valid_1's l2: 20.5805
[170]	training's l2: 20.6552	valid_1's l2: 20.5565
[180]	training's l2: 20.5768	valid_1's l2: 20.5319
[190]	training's l2: 20.5001	valid_1's l2: 20.5121
[200]	training's l2: 20.423	valid_1's l2: 20.4921
[210]	training's l2: 20.3494	valid_1's l2: 20.4768
[220]	training's l2: 20.2769	valid_1's l2: 20.4605
[230]	training's l2: 20.2068	valid_1's l2: 20.4476
[240]	training's l2: 20.1397	valid_1's l2: 20.4366
[250]	training's l2: 20.0733	valid_1's l2: 20.4242
[260]	training's l2: 20.0073	valid_1's l2: 20.4159
[270]	training's l2: 19.9413	valid_1's l2: 20.4047
[280]	training's l2: 19.8761	valid_1's l2: 20.3928
[290]	training's l2: 19.8111	valid_1's l2: 20.3841
[300]	training's l2: 19.7496	valid_1's l2: 20.3773
[310]	training's l2: 19.6894	valid_1's l2: 20.3689
[320]	training's l2: 19.6273	valid_1's l2: 20.3604
[330]	training's l2: 19.5688	valid_1's l2: 20.3565
[340]	training's l2: 19.5105	valid_1's l2: 20.3501
[350]	training's l2: 19.4539	valid_1's l2: 20.3446
[360]	training's l2: 19.3967	valid_1's l2: 20.3389
[370]	training's l2: 19.3412	valid_1's l2: 20.3349
[380]	training's l2: 19.2887	valid_1's l2: 20.3309
[390]	training's l2: 19.2343	valid_1's l2: 20.3244
[400]	training's l2: 19.1801	valid_1's l2: 20.3199
[410]	training's l2: 19.1293	valid_1's l2: 20.3175
[420]	training's l2: 19.077	valid_1's l2: 20.3136
[430]	training's l2: 19.027	valid_1's l2: 20.3105
[440]	training's l2: 18.9757	valid_1's l2: 20.3089
[450]	training's l2: 18.9274	valid_1's l2: 20.3061
[460]	training's l2: 18.8788	valid_1's l2: 20.3061
[470]	training's l2: 18.8321	valid_1's l2: 20.3042
[480]	training's l2: 18.783	valid_1's l2: 20.3005
[490]	training's l2: 18.7359	valid_1's l2: 20.2976
[500]	training's l2: 18.6887	valid_1's l2: 20.2948
[510]	training's l2: 18.6422	valid_1's l2: 20.2926
[520]	training's l2: 18.5975	valid_1's l2: 20.291
[530]	training's l2: 18.5517	valid_1's l2: 20.2914
[540]	training's l2: 18.5069	valid_1's l2: 20.289
[550]	training's l2: 18.4635	valid_1's l2: 20.2876
[560]	training's l2: 18.4179	valid_1's l2: 20.2856
[570]	training's l2: 18.3776	valid_1's l2: 20.2839
[580]	training's l2: 18.3329	valid_1's l2: 20.2819
[590]	training's l2: 18.2889	valid_1's l2: 20.2806
[600]	training's l2: 18.2449	valid_1's l2: 20.2798
[610]	training's l2: 18.2014	valid_1's l2: 20.2767
[620]	training's l2: 18.1586	valid_1's l2: 20.276
[630]	training's l2: 18.118	valid_1's l2: 20.2762
[640]	training's l2: 18.0771	valid_1's l2: 20.2739
[650]	training's l2: 18.0342	valid_1's l2: 20.2723
[660]	training's l2: 17.994	valid_1's l2: 20.2715
[670]	training's l2: 17.9511	valid_1's l2: 20.2727
[680]	training's l2: 17.9102	valid_1's l2: 20.2719
[690]	training's l2: 17.8697	valid_1's l2: 20.2719
Early stopping, best iteration is:
[662]	training's l2: 17.9863	valid_1's l2: 20.2715
score1: 3.8446993040518436
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.247250 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.1119	valid_1's l2: 30.1948
[20]	training's l2: 28.9239	valid_1's l2: 28.0024
[30]	training's l2: 27.2987	valid_1's l2: 26.3724
[40]	training's l2: 26.0833	valid_1's l2: 25.1588
[50]	training's l2: 25.1644	valid_1's l2: 24.2478
[60]	training's l2: 24.4641	valid_1's l2: 23.555
[70]	training's l2: 23.9241	valid_1's l2: 23.0271
[80]	training's l2: 23.5051	valid_1's l2: 22.6203
[90]	training's l2: 23.1729	valid_1's l2: 22.3004
[100]	training's l2: 22.9002	valid_1's l2: 22.0441
[110]	training's l2: 22.679	valid_1's l2: 21.8402
[120]	training's l2: 22.4953	valid_1's l2: 21.676
[130]	training's l2: 22.3402	valid_1's l2: 21.5403
[140]	training's l2: 22.2034	valid_1's l2: 21.422
[150]	training's l2: 22.0833	valid_1's l2: 21.3218
[160]	training's l2: 21.9813	valid_1's l2: 21.2407
[170]	training's l2: 21.8879	valid_1's l2: 21.1675
[180]	training's l2: 21.8055	valid_1's l2: 21.1034
[190]	training's l2: 21.7308	valid_1's l2: 21.0485
[200]	training's l2: 21.663	valid_1's l2: 20.9997
[210]	training's l2: 21.5985	valid_1's l2: 20.9547
[220]	training's l2: 21.5393	valid_1's l2: 20.9152
[230]	training's l2: 21.4811	valid_1's l2: 20.8753
[240]	training's l2: 21.4275	valid_1's l2: 20.8426
[250]	training's l2: 21.3777	valid_1's l2: 20.8139
[260]	training's l2: 21.3302	valid_1's l2: 20.7864
[270]	training's l2: 21.2856	valid_1's l2: 20.7628
[280]	training's l2: 21.242	valid_1's l2: 20.7402
[290]	training's l2: 21.2007	valid_1's l2: 20.7192
[300]	training's l2: 21.1586	valid_1's l2: 20.6974
[310]	training's l2: 21.1198	valid_1's l2: 20.6778
[320]	training's l2: 21.082	valid_1's l2: 20.6603
[330]	training's l2: 21.0462	valid_1's l2: 20.6451
[340]	training's l2: 21.0104	valid_1's l2: 20.6302
[350]	training's l2: 20.976	valid_1's l2: 20.6168
[360]	training's l2: 20.9422	valid_1's l2: 20.6038
[370]	training's l2: 20.9093	valid_1's l2: 20.5927
[380]	training's l2: 20.8764	valid_1's l2: 20.5801
[390]	training's l2: 20.8453	valid_1's l2: 20.5699
[400]	training's l2: 20.8132	valid_1's l2: 20.5587
[410]	training's l2: 20.7829	valid_1's l2: 20.548
[420]	training's l2: 20.7524	valid_1's l2: 20.5383
[430]	training's l2: 20.7224	valid_1's l2: 20.5282
[440]	training's l2: 20.693	valid_1's l2: 20.5194
[450]	training's l2: 20.6639	valid_1's l2: 20.5088
[460]	training's l2: 20.6355	valid_1's l2: 20.5003
[470]	training's l2: 20.6064	valid_1's l2: 20.4915
[480]	training's l2: 20.5768	valid_1's l2: 20.4821
[490]	training's l2: 20.5493	valid_1's l2: 20.475
[500]	training's l2: 20.5214	valid_1's l2: 20.4678
[510]	training's l2: 20.4938	valid_1's l2: 20.4602
[520]	training's l2: 20.4662	valid_1's l2: 20.4531
[530]	training's l2: 20.4393	valid_1's l2: 20.447
[540]	training's l2: 20.4123	valid_1's l2: 20.4404
[550]	training's l2: 20.3856	valid_1's l2: 20.4338
[560]	training's l2: 20.3599	valid_1's l2: 20.4278
[570]	training's l2: 20.3343	valid_1's l2: 20.422
[580]	training's l2: 20.3091	valid_1's l2: 20.4174
[590]	training's l2: 20.2839	valid_1's l2: 20.4136
[600]	training's l2: 20.2586	valid_1's l2: 20.4085
[610]	training's l2: 20.234	valid_1's l2: 20.4045
[620]	training's l2: 20.2099	valid_1's l2: 20.3999
[630]	training's l2: 20.1859	valid_1's l2: 20.3944
[640]	training's l2: 20.1612	valid_1's l2: 20.3894
[650]	training's l2: 20.1373	valid_1's l2: 20.3854
[660]	training's l2: 20.1127	valid_1's l2: 20.3821
[670]	training's l2: 20.0892	valid_1's l2: 20.3782
[680]	training's l2: 20.0656	valid_1's l2: 20.3751
[690]	training's l2: 20.0421	valid_1's l2: 20.3726
[700]	training's l2: 20.0187	valid_1's l2: 20.369
[710]	training's l2: 19.9951	valid_1's l2: 20.3648
[720]	training's l2: 19.9722	valid_1's l2: 20.362
[730]	training's l2: 19.9487	valid_1's l2: 20.3581
[740]	training's l2: 19.9259	valid_1's l2: 20.3554
[750]	training's l2: 19.9034	valid_1's l2: 20.3521
[760]	training's l2: 19.881	valid_1's l2: 20.3495
[770]	training's l2: 19.8588	valid_1's l2: 20.3455
[780]	training's l2: 19.837	valid_1's l2: 20.3432
[790]	training's l2: 19.8145	valid_1's l2: 20.3404
[800]	training's l2: 19.7924	valid_1's l2: 20.3375
[810]	training's l2: 19.7703	valid_1's l2: 20.3348
[820]	training's l2: 19.7486	valid_1's l2: 20.3322
[830]	training's l2: 19.7264	valid_1's l2: 20.3296
[840]	training's l2: 19.7047	valid_1's l2: 20.3269
[850]	training's l2: 19.6836	valid_1's l2: 20.3249
[860]	training's l2: 19.6619	valid_1's l2: 20.3221
[870]	training's l2: 19.6418	valid_1's l2: 20.3198
[880]	training's l2: 19.6209	valid_1's l2: 20.318
[890]	training's l2: 19.5998	valid_1's l2: 20.3153
[900]	training's l2: 19.5796	valid_1's l2: 20.3143
[910]	training's l2: 19.5594	valid_1's l2: 20.3131
[920]	training's l2: 19.5396	valid_1's l2: 20.3115
[930]	training's l2: 19.519	valid_1's l2: 20.3089
[940]	training's l2: 19.4986	valid_1's l2: 20.3072
[950]	training's l2: 19.4798	valid_1's l2: 20.3062
[960]	training's l2: 19.4602	valid_1's l2: 20.3036
[970]	training's l2: 19.4404	valid_1's l2: 20.302
[980]	training's l2: 19.4207	valid_1's l2: 20.3003
[990]	training's l2: 19.4015	valid_1's l2: 20.2988
[1000]	training's l2: 19.3818	valid_1's l2: 20.2971
[1010]	training's l2: 19.3631	valid_1's l2: 20.2966
[1020]	training's l2: 19.3449	valid_1's l2: 20.2957
[1030]	training's l2: 19.3255	valid_1's l2: 20.2937
[1040]	training's l2: 19.3078	valid_1's l2: 20.2924
[1050]	training's l2: 19.29	valid_1's l2: 20.2921
[1060]	training's l2: 19.2708	valid_1's l2: 20.2904
[1070]	training's l2: 19.2521	valid_1's l2: 20.2897
[1080]	training's l2: 19.233	valid_1's l2: 20.2874
[1090]	training's l2: 19.2159	valid_1's l2: 20.2865
[1100]	training's l2: 19.1976	valid_1's l2: 20.2858
[1110]	training's l2: 19.1796	valid_1's l2: 20.2841
[1120]	training's l2: 19.1614	valid_1's l2: 20.283
[1130]	training's l2: 19.1441	valid_1's l2: 20.2813
[1140]	training's l2: 19.1262	valid_1's l2: 20.2811
[1150]	training's l2: 19.1082	valid_1's l2: 20.2807
[1160]	training's l2: 19.0902	valid_1's l2: 20.2798
[1170]	training's l2: 19.0733	valid_1's l2: 20.2801
[1180]	training's l2: 19.0559	valid_1's l2: 20.2787
[1190]	training's l2: 19.0379	valid_1's l2: 20.278
[1200]	training's l2: 19.0207	valid_1's l2: 20.2775
[1210]	training's l2: 19.0036	valid_1's l2: 20.2765
[1220]	training's l2: 18.9864	valid_1's l2: 20.2756
[1230]	training's l2: 18.969	valid_1's l2: 20.2752
[1240]	training's l2: 18.9509	valid_1's l2: 20.2746
[1250]	training's l2: 18.9346	valid_1's l2: 20.2741
[1260]	training's l2: 18.918	valid_1's l2: 20.273
[1270]	training's l2: 18.9013	valid_1's l2: 20.2724
[1280]	training's l2: 18.8847	valid_1's l2: 20.2721
[1290]	training's l2: 18.8679	valid_1's l2: 20.2714
[1300]	training's l2: 18.8514	valid_1's l2: 20.2703
[1310]	training's l2: 18.8353	valid_1's l2: 20.269
[1320]	training's l2: 18.818	valid_1's l2: 20.2682
[1330]	training's l2: 18.8013	valid_1's l2: 20.2678
[1340]	training's l2: 18.7845	valid_1's l2: 20.2677
[1350]	training's l2: 18.7682	valid_1's l2: 20.2671
[1360]	training's l2: 18.7523	valid_1's l2: 20.266
[1370]	training's l2: 18.7363	valid_1's l2: 20.2648
[1380]	training's l2: 18.7199	valid_1's l2: 20.2644
[1390]	training's l2: 18.7032	valid_1's l2: 20.2641
[1400]	training's l2: 18.6872	valid_1's l2: 20.2637
[1410]	training's l2: 18.6705	valid_1's l2: 20.2635
[1420]	training's l2: 18.6537	valid_1's l2: 20.2631
[1430]	training's l2: 18.6377	valid_1's l2: 20.2629
[1440]	training's l2: 18.6207	valid_1's l2: 20.2625
[1450]	training's l2: 18.6041	valid_1's l2: 20.2625
[1460]	training's l2: 18.5883	valid_1's l2: 20.2616
[1470]	training's l2: 18.5723	valid_1's l2: 20.2609
[1480]	training's l2: 18.5569	valid_1's l2: 20.2605
[1490]	training's l2: 18.541	valid_1's l2: 20.2604
[1500]	training's l2: 18.5253	valid_1's l2: 20.2598
[1510]	training's l2: 18.511	valid_1's l2: 20.2597
[1520]	training's l2: 18.4946	valid_1's l2: 20.2594
[1530]	training's l2: 18.4791	valid_1's l2: 20.2593
[1540]	training's l2: 18.4625	valid_1's l2: 20.259
[1550]	training's l2: 18.4461	valid_1's l2: 20.2585
[1560]	training's l2: 18.4306	valid_1's l2: 20.258
[1570]	training's l2: 18.4142	valid_1's l2: 20.2574
[1580]	training's l2: 18.3976	valid_1's l2: 20.2562
[1590]	training's l2: 18.3809	valid_1's l2: 20.2558
[1600]	training's l2: 18.3652	valid_1's l2: 20.255
[1610]	training's l2: 18.3505	valid_1's l2: 20.2551
[1620]	training's l2: 18.3355	valid_1's l2: 20.2543
[1630]	training's l2: 18.3204	valid_1's l2: 20.2533
[1640]	training's l2: 18.3045	valid_1's l2: 20.253
[1650]	training's l2: 18.2879	valid_1's l2: 20.2526
[1660]	training's l2: 18.2736	valid_1's l2: 20.2529
[1670]	training's l2: 18.2586	valid_1's l2: 20.2528
[1680]	training's l2: 18.2436	valid_1's l2: 20.2527
[1690]	training's l2: 18.2291	valid_1's l2: 20.2531
Early stopping, best iteration is:
[1666]	training's l2: 18.2647	valid_1's l2: 20.2522
score1: 3.839257356377774
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259777 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 28.2193	valid_1's l2: 27.3247
[20]	training's l2: 25.2785	valid_1's l2: 24.4149
[30]	training's l2: 23.7257	valid_1's l2: 22.9134
[40]	training's l2: 22.8588	valid_1's l2: 22.1074
[50]	training's l2: 22.3191	valid_1's l2: 21.6345
[60]	training's l2: 21.9517	valid_1's l2: 21.3415
[70]	training's l2: 21.6734	valid_1's l2: 21.1377
[80]	training's l2: 21.459	valid_1's l2: 20.9961
[90]	training's l2: 21.2746	valid_1's l2: 20.885
[100]	training's l2: 21.1169	valid_1's l2: 20.8057
[110]	training's l2: 20.9774	valid_1's l2: 20.7415
[120]	training's l2: 20.8479	valid_1's l2: 20.6871
[130]	training's l2: 20.7306	valid_1's l2: 20.6448
[140]	training's l2: 20.6197	valid_1's l2: 20.6045
[150]	training's l2: 20.5146	valid_1's l2: 20.5713
[160]	training's l2: 20.4131	valid_1's l2: 20.5458
[170]	training's l2: 20.3168	valid_1's l2: 20.5237
[180]	training's l2: 20.2236	valid_1's l2: 20.5028
[190]	training's l2: 20.1313	valid_1's l2: 20.4819
[200]	training's l2: 20.0406	valid_1's l2: 20.4617
[210]	training's l2: 19.9508	valid_1's l2: 20.4444
[220]	training's l2: 19.8652	valid_1's l2: 20.4286
[230]	training's l2: 19.7797	valid_1's l2: 20.4154
[240]	training's l2: 19.6978	valid_1's l2: 20.4031
[250]	training's l2: 19.6191	valid_1's l2: 20.392
[260]	training's l2: 19.5393	valid_1's l2: 20.3791
[270]	training's l2: 19.4611	valid_1's l2: 20.3687
[280]	training's l2: 19.3859	valid_1's l2: 20.359
[290]	training's l2: 19.309	valid_1's l2: 20.3518
[300]	training's l2: 19.235	valid_1's l2: 20.3439
[310]	training's l2: 19.1624	valid_1's l2: 20.3394
[320]	training's l2: 19.09	valid_1's l2: 20.3348
[330]	training's l2: 19.0192	valid_1's l2: 20.3306
[340]	training's l2: 18.9498	valid_1's l2: 20.3248
[350]	training's l2: 18.882	valid_1's l2: 20.3228
[360]	training's l2: 18.8132	valid_1's l2: 20.3177
[370]	training's l2: 18.7464	valid_1's l2: 20.3121
[380]	training's l2: 18.6818	valid_1's l2: 20.3092
[390]	training's l2: 18.6189	valid_1's l2: 20.3074
[400]	training's l2: 18.5531	valid_1's l2: 20.3062
[410]	training's l2: 18.4907	valid_1's l2: 20.3034
[420]	training's l2: 18.4282	valid_1's l2: 20.3011
[430]	training's l2: 18.37	valid_1's l2: 20.2981
[440]	training's l2: 18.3117	valid_1's l2: 20.2981
[450]	training's l2: 18.2518	valid_1's l2: 20.2954
[460]	training's l2: 18.1959	valid_1's l2: 20.2936
[470]	training's l2: 18.1382	valid_1's l2: 20.2919
[480]	training's l2: 18.0825	valid_1's l2: 20.2907
[490]	training's l2: 18.0264	valid_1's l2: 20.2924
[500]	training's l2: 17.9714	valid_1's l2: 20.2918
Early stopping, best iteration is:
[477]	training's l2: 18.0974	valid_1's l2: 20.2903
score1: 3.844868833292207
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.249846 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 29.5752	valid_1's l2: 28.6746
[20]	training's l2: 26.8152	valid_1's l2: 25.928
[30]	training's l2: 25.0864	valid_1's l2: 24.2272
[40]	training's l2: 23.9757	valid_1's l2: 23.1505
[50]	training's l2: 23.2412	valid_1's l2: 22.4582
[60]	training's l2: 22.7422	valid_1's l2: 21.9982
[70]	training's l2: 22.3756	valid_1's l2: 21.6756
[80]	training's l2: 22.0913	valid_1's l2: 21.4394
[90]	training's l2: 21.8674	valid_1's l2: 21.268
[100]	training's l2: 21.6795	valid_1's l2: 21.1315
[110]	training's l2: 21.5254	valid_1's l2: 21.0258
[120]	training's l2: 21.3886	valid_1's l2: 20.9395
[130]	training's l2: 21.2652	valid_1's l2: 20.8681
[140]	training's l2: 21.1546	valid_1's l2: 20.8101
[150]	training's l2: 21.0535	valid_1's l2: 20.7619
[160]	training's l2: 20.9592	valid_1's l2: 20.7189
[170]	training's l2: 20.8705	valid_1's l2: 20.6808
[180]	training's l2: 20.7868	valid_1's l2: 20.6483
[190]	training's l2: 20.7079	valid_1's l2: 20.6186
[200]	training's l2: 20.6301	valid_1's l2: 20.5924
[210]	training's l2: 20.5567	valid_1's l2: 20.5722
[220]	training's l2: 20.4854	valid_1's l2: 20.554
[230]	training's l2: 20.416	valid_1's l2: 20.5376
[240]	training's l2: 20.3467	valid_1's l2: 20.5166
[250]	training's l2: 20.2807	valid_1's l2: 20.502
[260]	training's l2: 20.2156	valid_1's l2: 20.4897
[270]	training's l2: 20.1516	valid_1's l2: 20.4771
[280]	training's l2: 20.0874	valid_1's l2: 20.4608
[290]	training's l2: 20.0248	valid_1's l2: 20.4495
[300]	training's l2: 19.9634	valid_1's l2: 20.4391
[310]	training's l2: 19.903	valid_1's l2: 20.4291
[320]	training's l2: 19.8435	valid_1's l2: 20.4196
[330]	training's l2: 19.7849	valid_1's l2: 20.41
[340]	training's l2: 19.7272	valid_1's l2: 20.4022
[350]	training's l2: 19.6718	valid_1's l2: 20.396
[360]	training's l2: 19.6152	valid_1's l2: 20.3871
[370]	training's l2: 19.5596	valid_1's l2: 20.381
[380]	training's l2: 19.5048	valid_1's l2: 20.3733
[390]	training's l2: 19.4507	valid_1's l2: 20.368
[400]	training's l2: 19.3961	valid_1's l2: 20.363
[410]	training's l2: 19.3433	valid_1's l2: 20.3566
[420]	training's l2: 19.2903	valid_1's l2: 20.3514
[430]	training's l2: 19.2394	valid_1's l2: 20.347
[440]	training's l2: 19.1884	valid_1's l2: 20.3424
[450]	training's l2: 19.1374	valid_1's l2: 20.3377
[460]	training's l2: 19.0884	valid_1's l2: 20.333
[470]	training's l2: 19.0383	valid_1's l2: 20.3271
[480]	training's l2: 18.9892	valid_1's l2: 20.3254
[490]	training's l2: 18.9423	valid_1's l2: 20.3218
[500]	training's l2: 18.8947	valid_1's l2: 20.318
[510]	training's l2: 18.8481	valid_1's l2: 20.3146
[520]	training's l2: 18.8033	valid_1's l2: 20.3124
[530]	training's l2: 18.7573	valid_1's l2: 20.3088
[540]	training's l2: 18.7104	valid_1's l2: 20.3054
[550]	training's l2: 18.6637	valid_1's l2: 20.3004
[560]	training's l2: 18.6196	valid_1's l2: 20.2976
[570]	training's l2: 18.5748	valid_1's l2: 20.2946
[580]	training's l2: 18.5311	valid_1's l2: 20.292
[590]	training's l2: 18.4885	valid_1's l2: 20.2905
[600]	training's l2: 18.4434	valid_1's l2: 20.2873
[610]	training's l2: 18.4046	valid_1's l2: 20.2854
[620]	training's l2: 18.3618	valid_1's l2: 20.2826
[630]	training's l2: 18.3208	valid_1's l2: 20.2821
[640]	training's l2: 18.2794	valid_1's l2: 20.2786
[650]	training's l2: 18.2384	valid_1's l2: 20.2771
[660]	training's l2: 18.1982	valid_1's l2: 20.2741
[670]	training's l2: 18.1579	valid_1's l2: 20.2727
[680]	training's l2: 18.1175	valid_1's l2: 20.2718
[690]	training's l2: 18.0772	valid_1's l2: 20.2719
[700]	training's l2: 18.0388	valid_1's l2: 20.2719
[710]	training's l2: 17.9983	valid_1's l2: 20.2726
[720]	training's l2: 17.9605	valid_1's l2: 20.2725
Early stopping, best iteration is:
[694]	training's l2: 18.0615	valid_1's l2: 20.2712
score1: 3.8396338863474933
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241720 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 27.7232	valid_1's l2: 26.7774
[20]	training's l2: 24.8987	valid_1's l2: 23.9482
[30]	training's l2: 23.5481	valid_1's l2: 22.6183
[40]	training's l2: 22.833	valid_1's l2: 21.9294
[50]	training's l2: 22.4083	valid_1's l2: 21.5441
[60]	training's l2: 22.1242	valid_1's l2: 21.2938
[70]	training's l2: 21.9157	valid_1's l2: 21.1257
[80]	training's l2: 21.7592	valid_1's l2: 21.0027
[90]	training's l2: 21.6292	valid_1's l2: 20.9133
[100]	training's l2: 21.5156	valid_1's l2: 20.8372
[110]	training's l2: 21.4156	valid_1's l2: 20.7781
[120]	training's l2: 21.3254	valid_1's l2: 20.7269
[130]	training's l2: 21.243	valid_1's l2: 20.6832
[140]	training's l2: 21.1679	valid_1's l2: 20.654
[150]	training's l2: 21.0959	valid_1's l2: 20.6232
[160]	training's l2: 21.0288	valid_1's l2: 20.5965
[170]	training's l2: 20.9611	valid_1's l2: 20.5674
[180]	training's l2: 20.8998	valid_1's l2: 20.5479
[190]	training's l2: 20.8371	valid_1's l2: 20.5237
[200]	training's l2: 20.7793	valid_1's l2: 20.5049
[210]	training's l2: 20.7222	valid_1's l2: 20.4896
[220]	training's l2: 20.6669	valid_1's l2: 20.4743
[230]	training's l2: 20.6138	valid_1's l2: 20.4598
[240]	training's l2: 20.5607	valid_1's l2: 20.448
[250]	training's l2: 20.5081	valid_1's l2: 20.4379
[260]	training's l2: 20.4558	valid_1's l2: 20.4291
[270]	training's l2: 20.406	valid_1's l2: 20.4186
[280]	training's l2: 20.3582	valid_1's l2: 20.4139
[290]	training's l2: 20.3092	valid_1's l2: 20.404
[300]	training's l2: 20.2618	valid_1's l2: 20.3977
[310]	training's l2: 20.2152	valid_1's l2: 20.3875
[320]	training's l2: 20.1697	valid_1's l2: 20.3823
[330]	training's l2: 20.1239	valid_1's l2: 20.3748
[340]	training's l2: 20.0795	valid_1's l2: 20.3683
[350]	training's l2: 20.0368	valid_1's l2: 20.3667
[360]	training's l2: 19.9955	valid_1's l2: 20.3628
[370]	training's l2: 19.9543	valid_1's l2: 20.3576
[380]	training's l2: 19.9136	valid_1's l2: 20.3554
[390]	training's l2: 19.876	valid_1's l2: 20.3515
[400]	training's l2: 19.8366	valid_1's l2: 20.3484
[410]	training's l2: 19.7985	valid_1's l2: 20.3457
[420]	training's l2: 19.7587	valid_1's l2: 20.3431
[430]	training's l2: 19.7221	valid_1's l2: 20.3405
[440]	training's l2: 19.6845	valid_1's l2: 20.3384
[450]	training's l2: 19.646	valid_1's l2: 20.3377
[460]	training's l2: 19.6104	valid_1's l2: 20.3344
[470]	training's l2: 19.5738	valid_1's l2: 20.3321
[480]	training's l2: 19.5388	valid_1's l2: 20.331
[490]	training's l2: 19.5039	valid_1's l2: 20.3297
[500]	training's l2: 19.4686	valid_1's l2: 20.3268
[510]	training's l2: 19.4355	valid_1's l2: 20.3276
[520]	training's l2: 19.4011	valid_1's l2: 20.3271
[530]	training's l2: 19.3663	valid_1's l2: 20.3253
[540]	training's l2: 19.3302	valid_1's l2: 20.3242
[550]	training's l2: 19.2954	valid_1's l2: 20.3231
[560]	training's l2: 19.2613	valid_1's l2: 20.3234
[570]	training's l2: 19.2268	valid_1's l2: 20.3217
[580]	training's l2: 19.1946	valid_1's l2: 20.3203
[590]	training's l2: 19.1617	valid_1's l2: 20.3205
[600]	training's l2: 19.1321	valid_1's l2: 20.321
[610]	training's l2: 19.1008	valid_1's l2: 20.3206
Early stopping, best iteration is:
[583]	training's l2: 19.1854	valid_1's l2: 20.3197
score1: 3.8503310640868054
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.244870 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 27.7091	valid_1's l2: 26.8575
[20]	training's l2: 24.7107	valid_1's l2: 23.9324
[30]	training's l2: 23.2024	valid_1's l2: 22.5246
[40]	training's l2: 22.3749	valid_1's l2: 21.8016
[50]	training's l2: 21.8524	valid_1's l2: 21.3893
[60]	training's l2: 21.4827	valid_1's l2: 21.1384
[70]	training's l2: 21.1978	valid_1's l2: 20.9637
[80]	training's l2: 20.9604	valid_1's l2: 20.8416
[90]	training's l2: 20.7504	valid_1's l2: 20.751
[100]	training's l2: 20.5666	valid_1's l2: 20.6809
[110]	training's l2: 20.3987	valid_1's l2: 20.624
[120]	training's l2: 20.2417	valid_1's l2: 20.5822
[130]	training's l2: 20.0957	valid_1's l2: 20.5488
[140]	training's l2: 19.9533	valid_1's l2: 20.5138
[150]	training's l2: 19.8173	valid_1's l2: 20.489
[160]	training's l2: 19.6848	valid_1's l2: 20.469
[170]	training's l2: 19.5546	valid_1's l2: 20.4475
[180]	training's l2: 19.4287	valid_1's l2: 20.4281
[190]	training's l2: 19.3053	valid_1's l2: 20.4109
[200]	training's l2: 19.1837	valid_1's l2: 20.3954
[210]	training's l2: 19.066	valid_1's l2: 20.3822
[220]	training's l2: 18.9515	valid_1's l2: 20.3731
[230]	training's l2: 18.8407	valid_1's l2: 20.3626
[240]	training's l2: 18.7314	valid_1's l2: 20.3568
[250]	training's l2: 18.6198	valid_1's l2: 20.3492
[260]	training's l2: 18.5139	valid_1's l2: 20.3431
[270]	training's l2: 18.4095	valid_1's l2: 20.3395
[280]	training's l2: 18.3054	valid_1's l2: 20.3324
[290]	training's l2: 18.203	valid_1's l2: 20.3249
[300]	training's l2: 18.1058	valid_1's l2: 20.321
[310]	training's l2: 18.0076	valid_1's l2: 20.3174
[320]	training's l2: 17.9132	valid_1's l2: 20.3136
[330]	training's l2: 17.8192	valid_1's l2: 20.3133
[340]	training's l2: 17.727	valid_1's l2: 20.3095
[350]	training's l2: 17.6358	valid_1's l2: 20.3069
[360]	training's l2: 17.5493	valid_1's l2: 20.3099
[370]	training's l2: 17.4615	valid_1's l2: 20.3063
[380]	training's l2: 17.3751	valid_1's l2: 20.3052
[390]	training's l2: 17.2894	valid_1's l2: 20.3038
[400]	training's l2: 17.2058	valid_1's l2: 20.2991
[410]	training's l2: 17.1272	valid_1's l2: 20.2974
[420]	training's l2: 17.0452	valid_1's l2: 20.2994
[430]	training's l2: 16.9691	valid_1's l2: 20.2969
[440]	training's l2: 16.8878	valid_1's l2: 20.2971
[450]	training's l2: 16.8095	valid_1's l2: 20.2998
[460]	training's l2: 16.7301	valid_1's l2: 20.2989
Early stopping, best iteration is:
[437]	training's l2: 16.9119	valid_1's l2: 20.2964
score1: 3.8445297673358803
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.233765 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.2397	valid_1's l2: 29.3461
[20]	training's l2: 27.6652	valid_1's l2: 26.7913
[30]	training's l2: 25.9149	valid_1's l2: 25.0626
[40]	training's l2: 24.7051	valid_1's l2: 23.8849
[50]	training's l2: 23.8537	valid_1's l2: 23.0726
[60]	training's l2: 23.2437	valid_1's l2: 22.5037
[70]	training's l2: 22.7985	valid_1's l2: 22.0984
[80]	training's l2: 22.4531	valid_1's l2: 21.798
[90]	training's l2: 22.1806	valid_1's l2: 21.5717
[100]	training's l2: 21.9591	valid_1's l2: 21.3991
[110]	training's l2: 21.774	valid_1's l2: 21.2635
[120]	training's l2: 21.6127	valid_1's l2: 21.1506
[130]	training's l2: 21.4728	valid_1's l2: 21.0585
[140]	training's l2: 21.3486	valid_1's l2: 20.9809
[150]	training's l2: 21.2371	valid_1's l2: 20.9154
[160]	training's l2: 21.1322	valid_1's l2: 20.858
[170]	training's l2: 21.0348	valid_1's l2: 20.8102
[180]	training's l2: 20.9442	valid_1's l2: 20.7702
[190]	training's l2: 20.8596	valid_1's l2: 20.7346
[200]	training's l2: 20.78	valid_1's l2: 20.7009
[210]	training's l2: 20.7039	valid_1's l2: 20.6701
[220]	training's l2: 20.6277	valid_1's l2: 20.6396
[230]	training's l2: 20.5577	valid_1's l2: 20.6182
[240]	training's l2: 20.4881	valid_1's l2: 20.5968
[250]	training's l2: 20.4211	valid_1's l2: 20.5786
[260]	training's l2: 20.3553	valid_1's l2: 20.5617
[270]	training's l2: 20.2923	valid_1's l2: 20.546
[280]	training's l2: 20.2297	valid_1's l2: 20.5313
[290]	training's l2: 20.1682	valid_1's l2: 20.5171
[300]	training's l2: 20.1074	valid_1's l2: 20.502
[310]	training's l2: 20.0484	valid_1's l2: 20.4897
[320]	training's l2: 19.99	valid_1's l2: 20.4775
[330]	training's l2: 19.9312	valid_1's l2: 20.4647
[340]	training's l2: 19.8735	valid_1's l2: 20.4544
[350]	training's l2: 19.8163	valid_1's l2: 20.444
[360]	training's l2: 19.7603	valid_1's l2: 20.4325
[370]	training's l2: 19.7059	valid_1's l2: 20.4225
[380]	training's l2: 19.6522	valid_1's l2: 20.4141
[390]	training's l2: 19.5997	valid_1's l2: 20.4057
[400]	training's l2: 19.547	valid_1's l2: 20.3967
[410]	training's l2: 19.4951	valid_1's l2: 20.3901
[420]	training's l2: 19.4434	valid_1's l2: 20.3824
[430]	training's l2: 19.3927	valid_1's l2: 20.3747
[440]	training's l2: 19.3422	valid_1's l2: 20.3672
[450]	training's l2: 19.2913	valid_1's l2: 20.3623
[460]	training's l2: 19.2428	valid_1's l2: 20.3556
[470]	training's l2: 19.1941	valid_1's l2: 20.3512
[480]	training's l2: 19.1456	valid_1's l2: 20.3454
[490]	training's l2: 19.0983	valid_1's l2: 20.3402
[500]	training's l2: 19.0506	valid_1's l2: 20.3341
[510]	training's l2: 19.0033	valid_1's l2: 20.3299
[520]	training's l2: 18.9558	valid_1's l2: 20.3252
[530]	training's l2: 18.9093	valid_1's l2: 20.3236
[540]	training's l2: 18.8626	valid_1's l2: 20.3185
[550]	training's l2: 18.8165	valid_1's l2: 20.315
[560]	training's l2: 18.771	valid_1's l2: 20.3131
[570]	training's l2: 18.7273	valid_1's l2: 20.3112
[580]	training's l2: 18.683	valid_1's l2: 20.3081
[590]	training's l2: 18.6393	valid_1's l2: 20.3067
[600]	training's l2: 18.5952	valid_1's l2: 20.3045
[610]	training's l2: 18.5526	valid_1's l2: 20.3016
[620]	training's l2: 18.5101	valid_1's l2: 20.2997
[630]	training's l2: 18.4689	valid_1's l2: 20.2995
[640]	training's l2: 18.4274	valid_1's l2: 20.2992
[650]	training's l2: 18.3855	valid_1's l2: 20.2965
[660]	training's l2: 18.3451	valid_1's l2: 20.2954
[670]	training's l2: 18.3038	valid_1's l2: 20.2928
[680]	training's l2: 18.264	valid_1's l2: 20.2905
[690]	training's l2: 18.2254	valid_1's l2: 20.288
[700]	training's l2: 18.1858	valid_1's l2: 20.2867
[710]	training's l2: 18.1461	valid_1's l2: 20.2858
[720]	training's l2: 18.1075	valid_1's l2: 20.2842
[730]	training's l2: 18.0692	valid_1's l2: 20.2845
[740]	training's l2: 18.0327	valid_1's l2: 20.2837
[750]	training's l2: 17.9944	valid_1's l2: 20.283
[760]	training's l2: 17.9578	valid_1's l2: 20.2822
[770]	training's l2: 17.9195	valid_1's l2: 20.2821
[780]	training's l2: 17.882	valid_1's l2: 20.2792
[790]	training's l2: 17.8452	valid_1's l2: 20.2786
[800]	training's l2: 17.8087	valid_1's l2: 20.2768
[810]	training's l2: 17.7724	valid_1's l2: 20.2768
[820]	training's l2: 17.738	valid_1's l2: 20.2765
[830]	training's l2: 17.7034	valid_1's l2: 20.2771
[840]	training's l2: 17.6686	valid_1's l2: 20.2757
[850]	training's l2: 17.6298	valid_1's l2: 20.2751
[860]	training's l2: 17.5939	valid_1's l2: 20.274
[870]	training's l2: 17.558	valid_1's l2: 20.273
[880]	training's l2: 17.5244	valid_1's l2: 20.2725
[890]	training's l2: 17.4893	valid_1's l2: 20.273
[900]	training's l2: 17.454	valid_1's l2: 20.2718
[910]	training's l2: 17.4181	valid_1's l2: 20.2709
[920]	training's l2: 17.3831	valid_1's l2: 20.2713
[930]	training's l2: 17.349	valid_1's l2: 20.2701
[940]	training's l2: 17.3151	valid_1's l2: 20.2694
[950]	training's l2: 17.2806	valid_1's l2: 20.2702
[960]	training's l2: 17.2472	valid_1's l2: 20.2703
Early stopping, best iteration is:
[936]	training's l2: 17.3277	valid_1's l2: 20.2691
score1: 3.838628273062121
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.245857 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.9988	valid_1's l2: 31.0957
[20]	training's l2: 30.2888	valid_1's l2: 29.3951
[30]	training's l2: 28.8844	valid_1's l2: 28.0002
[40]	training's l2: 27.7286	valid_1's l2: 26.8548
[50]	training's l2: 26.7711	valid_1's l2: 25.9107
[60]	training's l2: 25.9765	valid_1's l2: 25.1299
[70]	training's l2: 25.3132	valid_1's l2: 24.4812
[80]	training's l2: 24.7579	valid_1's l2: 23.9446
[90]	training's l2: 24.2906	valid_1's l2: 23.4959
[100]	training's l2: 23.8958	valid_1's l2: 23.1195
[110]	training's l2: 23.5594	valid_1's l2: 22.8037
[120]	training's l2: 23.2723	valid_1's l2: 22.5367
[130]	training's l2: 23.0282	valid_1's l2: 22.3133
[140]	training's l2: 22.8174	valid_1's l2: 22.1229
[150]	training's l2: 22.6309	valid_1's l2: 21.9586
[160]	training's l2: 22.4656	valid_1's l2: 21.8173
[170]	training's l2: 22.3178	valid_1's l2: 21.6926
[180]	training's l2: 22.1869	valid_1's l2: 21.584
[190]	training's l2: 22.0694	valid_1's l2: 21.4918
[200]	training's l2: 21.9605	valid_1's l2: 21.4075
[210]	training's l2: 21.8626	valid_1's l2: 21.3355
[220]	training's l2: 21.771	valid_1's l2: 21.2692
[230]	training's l2: 21.6844	valid_1's l2: 21.2083
[240]	training's l2: 21.6028	valid_1's l2: 21.1518
[250]	training's l2: 21.5291	valid_1's l2: 21.1024
[260]	training's l2: 21.4604	valid_1's l2: 21.0583
[270]	training's l2: 21.3948	valid_1's l2: 21.0174
[280]	training's l2: 21.3325	valid_1's l2: 20.9791
[290]	training's l2: 21.2745	valid_1's l2: 20.946
[300]	training's l2: 21.2194	valid_1's l2: 20.9145
[310]	training's l2: 21.1655	valid_1's l2: 20.8849
[320]	training's l2: 21.1114	valid_1's l2: 20.8548
[330]	training's l2: 21.0598	valid_1's l2: 20.8272
[340]	training's l2: 21.0105	valid_1's l2: 20.803
[350]	training's l2: 20.9627	valid_1's l2: 20.7808
[360]	training's l2: 20.9173	valid_1's l2: 20.7587
[370]	training's l2: 20.8735	valid_1's l2: 20.7401
[380]	training's l2: 20.8304	valid_1's l2: 20.7228
[390]	training's l2: 20.7883	valid_1's l2: 20.7059
[400]	training's l2: 20.7486	valid_1's l2: 20.6906
[410]	training's l2: 20.7083	valid_1's l2: 20.6752
[420]	training's l2: 20.67	valid_1's l2: 20.6607
[430]	training's l2: 20.6322	valid_1's l2: 20.6471
[440]	training's l2: 20.5948	valid_1's l2: 20.6341
[450]	training's l2: 20.558	valid_1's l2: 20.6219
[460]	training's l2: 20.5224	valid_1's l2: 20.6106
[470]	training's l2: 20.487	valid_1's l2: 20.5999
[480]	training's l2: 20.4517	valid_1's l2: 20.5883
[490]	training's l2: 20.4172	valid_1's l2: 20.5779
[500]	training's l2: 20.3834	valid_1's l2: 20.568
[510]	training's l2: 20.3496	valid_1's l2: 20.5593
[520]	training's l2: 20.317	valid_1's l2: 20.5508
[530]	training's l2: 20.2841	valid_1's l2: 20.5423
[540]	training's l2: 20.2522	valid_1's l2: 20.5346
[550]	training's l2: 20.2209	valid_1's l2: 20.5274
[560]	training's l2: 20.1895	valid_1's l2: 20.5203
[570]	training's l2: 20.1577	valid_1's l2: 20.5134
[580]	training's l2: 20.1272	valid_1's l2: 20.5054
[590]	training's l2: 20.0967	valid_1's l2: 20.4989
[600]	training's l2: 20.066	valid_1's l2: 20.4924
[610]	training's l2: 20.0356	valid_1's l2: 20.4861
[620]	training's l2: 20.0053	valid_1's l2: 20.4799
[630]	training's l2: 19.9748	valid_1's l2: 20.4731
[640]	training's l2: 19.9449	valid_1's l2: 20.4667
[650]	training's l2: 19.9154	valid_1's l2: 20.4609
[660]	training's l2: 19.886	valid_1's l2: 20.456
[670]	training's l2: 19.856	valid_1's l2: 20.4501
[680]	training's l2: 19.8276	valid_1's l2: 20.4446
[690]	training's l2: 19.7984	valid_1's l2: 20.4391
[700]	training's l2: 19.7695	valid_1's l2: 20.4338
[710]	training's l2: 19.7412	valid_1's l2: 20.429
[720]	training's l2: 19.7128	valid_1's l2: 20.4244
[730]	training's l2: 19.6849	valid_1's l2: 20.4201
[740]	training's l2: 19.6573	valid_1's l2: 20.4152
[750]	training's l2: 19.6296	valid_1's l2: 20.4102
[760]	training's l2: 19.6023	valid_1's l2: 20.4057
[770]	training's l2: 19.5748	valid_1's l2: 20.4014
[780]	training's l2: 19.5478	valid_1's l2: 20.3972
[790]	training's l2: 19.5208	valid_1's l2: 20.3933
[800]	training's l2: 19.4942	valid_1's l2: 20.39
[810]	training's l2: 19.4676	valid_1's l2: 20.386
[820]	training's l2: 19.4411	valid_1's l2: 20.3821
[830]	training's l2: 19.4143	valid_1's l2: 20.3789
[840]	training's l2: 19.3883	valid_1's l2: 20.3757
[850]	training's l2: 19.3625	valid_1's l2: 20.3728
[860]	training's l2: 19.3367	valid_1's l2: 20.3696
[870]	training's l2: 19.3113	valid_1's l2: 20.3663
[880]	training's l2: 19.2855	valid_1's l2: 20.3623
[890]	training's l2: 19.2598	valid_1's l2: 20.3583
[900]	training's l2: 19.2343	valid_1's l2: 20.355
[910]	training's l2: 19.209	valid_1's l2: 20.3524
[920]	training's l2: 19.1837	valid_1's l2: 20.3501
[930]	training's l2: 19.1588	valid_1's l2: 20.3477
[940]	training's l2: 19.1336	valid_1's l2: 20.3442
[950]	training's l2: 19.1091	valid_1's l2: 20.3409
[960]	training's l2: 19.0841	valid_1's l2: 20.3383
[970]	training's l2: 19.0594	valid_1's l2: 20.3353
[980]	training's l2: 19.0353	valid_1's l2: 20.3327
[990]	training's l2: 19.0111	valid_1's l2: 20.3303
[1000]	training's l2: 18.9867	valid_1's l2: 20.3279
[1010]	training's l2: 18.9625	valid_1's l2: 20.3247
[1020]	training's l2: 18.9385	valid_1's l2: 20.3231
[1030]	training's l2: 18.9144	valid_1's l2: 20.3202
[1040]	training's l2: 18.8906	valid_1's l2: 20.3182
[1050]	training's l2: 18.8673	valid_1's l2: 20.3162
[1060]	training's l2: 18.8437	valid_1's l2: 20.314
[1070]	training's l2: 18.8203	valid_1's l2: 20.3119
[1080]	training's l2: 18.7965	valid_1's l2: 20.3095
[1090]	training's l2: 18.7735	valid_1's l2: 20.3074
[1100]	training's l2: 18.7507	valid_1's l2: 20.3061
[1110]	training's l2: 18.7278	valid_1's l2: 20.3043
[1120]	training's l2: 18.7051	valid_1's l2: 20.3022
[1130]	training's l2: 18.6823	valid_1's l2: 20.3008
[1140]	training's l2: 18.66	valid_1's l2: 20.2986
[1150]	training's l2: 18.6368	valid_1's l2: 20.2966
[1160]	training's l2: 18.614	valid_1's l2: 20.2953
[1170]	training's l2: 18.5917	valid_1's l2: 20.2941
[1180]	training's l2: 18.5688	valid_1's l2: 20.2922
[1190]	training's l2: 18.5469	valid_1's l2: 20.29
[1200]	training's l2: 18.525	valid_1's l2: 20.2884
[1210]	training's l2: 18.5032	valid_1's l2: 20.2869
[1220]	training's l2: 18.4814	valid_1's l2: 20.2851
[1230]	training's l2: 18.4596	valid_1's l2: 20.2829
[1240]	training's l2: 18.4375	valid_1's l2: 20.2821
[1250]	training's l2: 18.4163	valid_1's l2: 20.2811
[1260]	training's l2: 18.3946	valid_1's l2: 20.2791
[1270]	training's l2: 18.3734	valid_1's l2: 20.2773
[1280]	training's l2: 18.3523	valid_1's l2: 20.2762
[1290]	training's l2: 18.3311	valid_1's l2: 20.2753
[1300]	training's l2: 18.3098	valid_1's l2: 20.2739
[1310]	training's l2: 18.2889	valid_1's l2: 20.273
[1320]	training's l2: 18.2681	valid_1's l2: 20.2714
[1330]	training's l2: 18.2467	valid_1's l2: 20.2701
[1340]	training's l2: 18.2261	valid_1's l2: 20.2697
[1350]	training's l2: 18.2055	valid_1's l2: 20.2685
[1360]	training's l2: 18.1849	valid_1's l2: 20.2682
[1370]	training's l2: 18.1646	valid_1's l2: 20.2679
[1380]	training's l2: 18.1447	valid_1's l2: 20.267
[1390]	training's l2: 18.1245	valid_1's l2: 20.266
[1400]	training's l2: 18.1042	valid_1's l2: 20.2652
[1410]	training's l2: 18.0844	valid_1's l2: 20.2644
[1420]	training's l2: 18.0645	valid_1's l2: 20.2629
[1430]	training's l2: 18.0454	valid_1's l2: 20.2623
[1440]	training's l2: 18.025	valid_1's l2: 20.2613
[1450]	training's l2: 18.0057	valid_1's l2: 20.2606
[1460]	training's l2: 17.986	valid_1's l2: 20.2596
[1470]	training's l2: 17.9666	valid_1's l2: 20.2586
[1480]	training's l2: 17.9472	valid_1's l2: 20.2581
[1490]	training's l2: 17.9279	valid_1's l2: 20.2569
[1500]	training's l2: 17.9085	valid_1's l2: 20.2567
[1510]	training's l2: 17.8894	valid_1's l2: 20.2557
[1520]	training's l2: 17.8703	valid_1's l2: 20.2552
[1530]	training's l2: 17.8515	valid_1's l2: 20.2543
[1540]	training's l2: 17.8334	valid_1's l2: 20.2533
[1550]	training's l2: 17.8141	valid_1's l2: 20.2529
[1560]	training's l2: 17.7962	valid_1's l2: 20.2528
[1570]	training's l2: 17.7778	valid_1's l2: 20.2521
[1580]	training's l2: 17.7595	valid_1's l2: 20.2521
[1590]	training's l2: 17.7414	valid_1's l2: 20.2523
[1600]	training's l2: 17.7231	valid_1's l2: 20.2521
[1610]	training's l2: 17.7044	valid_1's l2: 20.2517
[1620]	training's l2: 17.6856	valid_1's l2: 20.2501
[1630]	training's l2: 17.6672	valid_1's l2: 20.2495
[1640]	training's l2: 17.6491	valid_1's l2: 20.2497
[1650]	training's l2: 17.6307	valid_1's l2: 20.2498
[1660]	training's l2: 17.6129	valid_1's l2: 20.2493
[1670]	training's l2: 17.5954	valid_1's l2: 20.2484
[1680]	training's l2: 17.5774	valid_1's l2: 20.2483
[1690]	training's l2: 17.56	valid_1's l2: 20.2475
[1700]	training's l2: 17.541	valid_1's l2: 20.2468
[1710]	training's l2: 17.5226	valid_1's l2: 20.2472
[1720]	training's l2: 17.5043	valid_1's l2: 20.2475
Early stopping, best iteration is:
[1699]	training's l2: 17.543	valid_1's l2: 20.2468
score1: 3.8355619206992517
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254794 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.2056	valid_1's l2: 30.3062
[20]	training's l2: 29.0402	valid_1's l2: 28.1558
[30]	training's l2: 27.4035	valid_1's l2: 26.5344
[40]	training's l2: 26.1553	valid_1's l2: 25.3044
[50]	training's l2: 25.1937	valid_1's l2: 24.3668
[60]	training's l2: 24.4506	valid_1's l2: 23.6488
[70]	training's l2: 23.8674	valid_1's l2: 23.0917
[80]	training's l2: 23.4051	valid_1's l2: 22.6594
[90]	training's l2: 23.0376	valid_1's l2: 22.3207
[100]	training's l2: 22.7412	valid_1's l2: 22.0546
[110]	training's l2: 22.4914	valid_1's l2: 21.8383
[120]	training's l2: 22.2811	valid_1's l2: 21.6605
[130]	training's l2: 22.1024	valid_1's l2: 21.5177
[140]	training's l2: 21.9449	valid_1's l2: 21.3979
[150]	training's l2: 21.809	valid_1's l2: 21.298
[160]	training's l2: 21.6841	valid_1's l2: 21.2115
[170]	training's l2: 21.57	valid_1's l2: 21.1311
[180]	training's l2: 21.4681	valid_1's l2: 21.0653
[190]	training's l2: 21.3738	valid_1's l2: 21.0069
[200]	training's l2: 21.2882	valid_1's l2: 20.9576
[210]	training's l2: 21.2082	valid_1's l2: 20.9118
[220]	training's l2: 21.1285	valid_1's l2: 20.8682
[230]	training's l2: 21.0554	valid_1's l2: 20.8302
[240]	training's l2: 20.9847	valid_1's l2: 20.7961
[250]	training's l2: 20.918	valid_1's l2: 20.7653
[260]	training's l2: 20.8552	valid_1's l2: 20.7379
[270]	training's l2: 20.7952	valid_1's l2: 20.7137
[280]	training's l2: 20.7364	valid_1's l2: 20.6903
[290]	training's l2: 20.6806	valid_1's l2: 20.6687
[300]	training's l2: 20.6263	valid_1's l2: 20.6499
[310]	training's l2: 20.5717	valid_1's l2: 20.6308
[320]	training's l2: 20.5208	valid_1's l2: 20.6141
[330]	training's l2: 20.4703	valid_1's l2: 20.5979
[340]	training's l2: 20.4197	valid_1's l2: 20.5827
[350]	training's l2: 20.3707	valid_1's l2: 20.5691
[360]	training's l2: 20.3231	valid_1's l2: 20.5557
[370]	training's l2: 20.2762	valid_1's l2: 20.5438
[380]	training's l2: 20.23	valid_1's l2: 20.5328
[390]	training's l2: 20.1849	valid_1's l2: 20.5228
[400]	training's l2: 20.1394	valid_1's l2: 20.5118
[410]	training's l2: 20.0948	valid_1's l2: 20.5012
[420]	training's l2: 20.0508	valid_1's l2: 20.4905
[430]	training's l2: 20.0073	valid_1's l2: 20.4815
[440]	training's l2: 19.9641	valid_1's l2: 20.4724
[450]	training's l2: 19.9213	valid_1's l2: 20.4645
[460]	training's l2: 19.8788	valid_1's l2: 20.458
[470]	training's l2: 19.8373	valid_1's l2: 20.4499
[480]	training's l2: 19.7951	valid_1's l2: 20.4411
[490]	training's l2: 19.753	valid_1's l2: 20.4319
[500]	training's l2: 19.7124	valid_1's l2: 20.425
[510]	training's l2: 19.672	valid_1's l2: 20.4181
[520]	training's l2: 19.6316	valid_1's l2: 20.4107
[530]	training's l2: 19.5925	valid_1's l2: 20.4055
[540]	training's l2: 19.554	valid_1's l2: 20.4
[550]	training's l2: 19.5151	valid_1's l2: 20.3949
[560]	training's l2: 19.4767	valid_1's l2: 20.3891
[570]	training's l2: 19.4384	valid_1's l2: 20.3855
[580]	training's l2: 19.4	valid_1's l2: 20.3812
[590]	training's l2: 19.3623	valid_1's l2: 20.3762
[600]	training's l2: 19.3249	valid_1's l2: 20.371
[610]	training's l2: 19.2878	valid_1's l2: 20.3676
[620]	training's l2: 19.2515	valid_1's l2: 20.3647
[630]	training's l2: 19.2152	valid_1's l2: 20.3603
[640]	training's l2: 19.1786	valid_1's l2: 20.3582
[650]	training's l2: 19.1429	valid_1's l2: 20.3532
[660]	training's l2: 19.1071	valid_1's l2: 20.3488
[670]	training's l2: 19.0717	valid_1's l2: 20.3467
[680]	training's l2: 19.0362	valid_1's l2: 20.343
[690]	training's l2: 19	valid_1's l2: 20.339
[700]	training's l2: 18.965	valid_1's l2: 20.3352
[710]	training's l2: 18.9304	valid_1's l2: 20.3328
[720]	training's l2: 18.8962	valid_1's l2: 20.3299
[730]	training's l2: 18.8626	valid_1's l2: 20.3276
[740]	training's l2: 18.8285	valid_1's l2: 20.3248
[750]	training's l2: 18.7949	valid_1's l2: 20.3222
[760]	training's l2: 18.7608	valid_1's l2: 20.3189
[770]	training's l2: 18.7265	valid_1's l2: 20.3157
[780]	training's l2: 18.6938	valid_1's l2: 20.3139
[790]	training's l2: 18.6607	valid_1's l2: 20.3108
[800]	training's l2: 18.6283	valid_1's l2: 20.3081
[810]	training's l2: 18.5958	valid_1's l2: 20.3055
[820]	training's l2: 18.5631	valid_1's l2: 20.3024
[830]	training's l2: 18.5315	valid_1's l2: 20.2996
[840]	training's l2: 18.5009	valid_1's l2: 20.2979
[850]	training's l2: 18.469	valid_1's l2: 20.2964
[860]	training's l2: 18.4372	valid_1's l2: 20.2946
[870]	training's l2: 18.4064	valid_1's l2: 20.292
[880]	training's l2: 18.3753	valid_1's l2: 20.2906
[890]	training's l2: 18.3443	valid_1's l2: 20.2889
[900]	training's l2: 18.314	valid_1's l2: 20.2871
[910]	training's l2: 18.2832	valid_1's l2: 20.2854
[920]	training's l2: 18.2532	valid_1's l2: 20.2843
[930]	training's l2: 18.2236	valid_1's l2: 20.283
[940]	training's l2: 18.1947	valid_1's l2: 20.2814
[950]	training's l2: 18.1652	valid_1's l2: 20.2806
[960]	training's l2: 18.1361	valid_1's l2: 20.2802
[970]	training's l2: 18.1076	valid_1's l2: 20.2804
[980]	training's l2: 18.0791	valid_1's l2: 20.2793
[990]	training's l2: 18.0504	valid_1's l2: 20.2789
[1000]	training's l2: 18.022	valid_1's l2: 20.2775
[1010]	training's l2: 17.9936	valid_1's l2: 20.276
[1020]	training's l2: 17.9649	valid_1's l2: 20.275
[1030]	training's l2: 17.9364	valid_1's l2: 20.2741
[1040]	training's l2: 17.9086	valid_1's l2: 20.2735
[1050]	training's l2: 17.8801	valid_1's l2: 20.2719
[1060]	training's l2: 17.8526	valid_1's l2: 20.27
[1070]	training's l2: 17.8247	valid_1's l2: 20.2692
[1080]	training's l2: 17.7975	valid_1's l2: 20.2683
[1090]	training's l2: 17.7702	valid_1's l2: 20.2669
[1100]	training's l2: 17.7429	valid_1's l2: 20.2658
[1110]	training's l2: 17.7174	valid_1's l2: 20.266
[1120]	training's l2: 17.6891	valid_1's l2: 20.2645
[1130]	training's l2: 17.6629	valid_1's l2: 20.2641
[1140]	training's l2: 17.6359	valid_1's l2: 20.264
[1150]	training's l2: 17.6079	valid_1's l2: 20.2645
[1160]	training's l2: 17.5816	valid_1's l2: 20.2636
[1170]	training's l2: 17.5544	valid_1's l2: 20.2627
[1180]	training's l2: 17.5278	valid_1's l2: 20.2621
[1190]	training's l2: 17.5006	valid_1's l2: 20.2616
[1200]	training's l2: 17.475	valid_1's l2: 20.2605
[1210]	training's l2: 17.4493	valid_1's l2: 20.2601
[1220]	training's l2: 17.4232	valid_1's l2: 20.2599
[1230]	training's l2: 17.3971	valid_1's l2: 20.2597
[1240]	training's l2: 17.3712	valid_1's l2: 20.2592
[1250]	training's l2: 17.3463	valid_1's l2: 20.2587
[1260]	training's l2: 17.3218	valid_1's l2: 20.2579
[1270]	training's l2: 17.2959	valid_1's l2: 20.2577
[1280]	training's l2: 17.2709	valid_1's l2: 20.2582
Early stopping, best iteration is:
[1258]	training's l2: 17.3268	valid_1's l2: 20.2575
score1: 3.8366010091668112
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243466 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 32.0311	valid_1's l2: 31.1274
[20]	training's l2: 30.3413	valid_1's l2: 29.4477
[30]	training's l2: 28.9497	valid_1's l2: 28.0659
[40]	training's l2: 27.7987	valid_1's l2: 26.9262
[50]	training's l2: 26.8445	valid_1's l2: 25.9843
[60]	training's l2: 26.0502	valid_1's l2: 25.2035
[70]	training's l2: 25.3851	valid_1's l2: 24.5522
[80]	training's l2: 24.8261	valid_1's l2: 24.0119
[90]	training's l2: 24.3553	valid_1's l2: 23.5582
[100]	training's l2: 23.9551	valid_1's l2: 23.1776
[110]	training's l2: 23.6148	valid_1's l2: 22.8572
[120]	training's l2: 23.3242	valid_1's l2: 22.5879
[130]	training's l2: 23.0758	valid_1's l2: 22.3603
[140]	training's l2: 22.8612	valid_1's l2: 22.1666
[150]	training's l2: 22.6728	valid_1's l2: 21.9985
[160]	training's l2: 22.5052	valid_1's l2: 21.8536
[170]	training's l2: 22.3546	valid_1's l2: 21.7263
[180]	training's l2: 22.2219	valid_1's l2: 21.6159
[190]	training's l2: 22.1011	valid_1's l2: 21.519
[200]	training's l2: 21.9919	valid_1's l2: 21.4341
[210]	training's l2: 21.8915	valid_1's l2: 21.3586
[220]	training's l2: 21.8004	valid_1's l2: 21.2931
[230]	training's l2: 21.7123	valid_1's l2: 21.2308
[240]	training's l2: 21.6309	valid_1's l2: 21.1744
[250]	training's l2: 21.556	valid_1's l2: 21.1247
[260]	training's l2: 21.4858	valid_1's l2: 21.078
[270]	training's l2: 21.4187	valid_1's l2: 21.0351
[280]	training's l2: 21.3562	valid_1's l2: 20.9972
[290]	training's l2: 21.2966	valid_1's l2: 20.9624
[300]	training's l2: 21.2414	valid_1's l2: 20.9313
[310]	training's l2: 21.187	valid_1's l2: 20.9005
[320]	training's l2: 21.1343	valid_1's l2: 20.8712
[330]	training's l2: 21.0823	valid_1's l2: 20.8429
[340]	training's l2: 21.0322	valid_1's l2: 20.8173
[350]	training's l2: 20.9843	valid_1's l2: 20.7936
[360]	training's l2: 20.9381	valid_1's l2: 20.7722
[370]	training's l2: 20.8932	valid_1's l2: 20.7511
[380]	training's l2: 20.8501	valid_1's l2: 20.7328
[390]	training's l2: 20.8084	valid_1's l2: 20.7165
[400]	training's l2: 20.7672	valid_1's l2: 20.6989
[410]	training's l2: 20.7279	valid_1's l2: 20.6838
[420]	training's l2: 20.6898	valid_1's l2: 20.67
[430]	training's l2: 20.6516	valid_1's l2: 20.6567
[440]	training's l2: 20.6141	valid_1's l2: 20.6432
[450]	training's l2: 20.5774	valid_1's l2: 20.6303
[460]	training's l2: 20.5418	valid_1's l2: 20.6184
[470]	training's l2: 20.5064	valid_1's l2: 20.6084
[480]	training's l2: 20.4719	valid_1's l2: 20.5971
[490]	training's l2: 20.4376	valid_1's l2: 20.5872
[500]	training's l2: 20.4035	valid_1's l2: 20.5779
[510]	training's l2: 20.3704	valid_1's l2: 20.5695
[520]	training's l2: 20.3373	valid_1's l2: 20.5608
[530]	training's l2: 20.3044	valid_1's l2: 20.5526
[540]	training's l2: 20.2725	valid_1's l2: 20.5449
[550]	training's l2: 20.2404	valid_1's l2: 20.5362
[560]	training's l2: 20.209	valid_1's l2: 20.5283
[570]	training's l2: 20.1781	valid_1's l2: 20.5216
[580]	training's l2: 20.1471	valid_1's l2: 20.5148
[590]	training's l2: 20.1166	valid_1's l2: 20.5078
[600]	training's l2: 20.0864	valid_1's l2: 20.5013
[610]	training's l2: 20.0565	valid_1's l2: 20.4955
[620]	training's l2: 20.026	valid_1's l2: 20.4877
[630]	training's l2: 19.9968	valid_1's l2: 20.4829
[640]	training's l2: 19.967	valid_1's l2: 20.4766
[650]	training's l2: 19.9378	valid_1's l2: 20.4695
[660]	training's l2: 19.9085	valid_1's l2: 20.4636
[670]	training's l2: 19.8793	valid_1's l2: 20.4582
[680]	training's l2: 19.8505	valid_1's l2: 20.4528
[690]	training's l2: 19.8213	valid_1's l2: 20.4479
[700]	training's l2: 19.7923	valid_1's l2: 20.442
[710]	training's l2: 19.764	valid_1's l2: 20.4364
[720]	training's l2: 19.736	valid_1's l2: 20.432
[730]	training's l2: 19.7078	valid_1's l2: 20.4263
[740]	training's l2: 19.6802	valid_1's l2: 20.4221
[750]	training's l2: 19.653	valid_1's l2: 20.4171
[760]	training's l2: 19.6259	valid_1's l2: 20.4126
[770]	training's l2: 19.5987	valid_1's l2: 20.4083
[780]	training's l2: 19.5714	valid_1's l2: 20.4034
[790]	training's l2: 19.5448	valid_1's l2: 20.3996
[800]	training's l2: 19.5175	valid_1's l2: 20.395
[810]	training's l2: 19.4912	valid_1's l2: 20.391
[820]	training's l2: 19.4649	valid_1's l2: 20.3868
[830]	training's l2: 19.4391	valid_1's l2: 20.3841
[840]	training's l2: 19.413	valid_1's l2: 20.3797
[850]	training's l2: 19.3866	valid_1's l2: 20.3763
[860]	training's l2: 19.3608	valid_1's l2: 20.3731
[870]	training's l2: 19.3349	valid_1's l2: 20.37
[880]	training's l2: 19.3093	valid_1's l2: 20.3664
[890]	training's l2: 19.2845	valid_1's l2: 20.3627
[900]	training's l2: 19.2591	valid_1's l2: 20.3599
[910]	training's l2: 19.2343	valid_1's l2: 20.3577
[920]	training's l2: 19.2092	valid_1's l2: 20.3546
[930]	training's l2: 19.1842	valid_1's l2: 20.3516
[940]	training's l2: 19.1597	valid_1's l2: 20.3488
[950]	training's l2: 19.1346	valid_1's l2: 20.3472
[960]	training's l2: 19.11	valid_1's l2: 20.3447
[970]	training's l2: 19.0859	valid_1's l2: 20.3426
[980]	training's l2: 19.0613	valid_1's l2: 20.3398
[990]	training's l2: 19.0372	valid_1's l2: 20.3367
[1000]	training's l2: 19.0135	valid_1's l2: 20.3352
[1010]	training's l2: 18.9896	valid_1's l2: 20.3332
[1020]	training's l2: 18.9656	valid_1's l2: 20.3306
[1030]	training's l2: 18.9415	valid_1's l2: 20.3282
[1040]	training's l2: 18.9178	valid_1's l2: 20.3262
[1050]	training's l2: 18.8939	valid_1's l2: 20.3238
[1060]	training's l2: 18.8704	valid_1's l2: 20.3218
[1070]	training's l2: 18.8469	valid_1's l2: 20.3192
[1080]	training's l2: 18.8237	valid_1's l2: 20.3166
[1090]	training's l2: 18.8009	valid_1's l2: 20.315
[1100]	training's l2: 18.7777	valid_1's l2: 20.3127
[1110]	training's l2: 18.7547	valid_1's l2: 20.3107
[1120]	training's l2: 18.7315	valid_1's l2: 20.3091
[1130]	training's l2: 18.7093	valid_1's l2: 20.3068
[1140]	training's l2: 18.6867	valid_1's l2: 20.3059
[1150]	training's l2: 18.6641	valid_1's l2: 20.3045
[1160]	training's l2: 18.6418	valid_1's l2: 20.3028
[1170]	training's l2: 18.6192	valid_1's l2: 20.3017
[1180]	training's l2: 18.597	valid_1's l2: 20.3001
[1190]	training's l2: 18.5742	valid_1's l2: 20.2992
[1200]	training's l2: 18.5521	valid_1's l2: 20.2976
[1210]	training's l2: 18.5298	valid_1's l2: 20.2955
[1220]	training's l2: 18.5079	valid_1's l2: 20.2932
[1230]	training's l2: 18.486	valid_1's l2: 20.2921
[1240]	training's l2: 18.4645	valid_1's l2: 20.291
[1250]	training's l2: 18.4426	valid_1's l2: 20.2892
[1260]	training's l2: 18.4203	valid_1's l2: 20.2878
[1270]	training's l2: 18.3994	valid_1's l2: 20.2864
[1280]	training's l2: 18.3782	valid_1's l2: 20.2851
[1290]	training's l2: 18.3573	valid_1's l2: 20.2842
[1300]	training's l2: 18.3368	valid_1's l2: 20.2829
[1310]	training's l2: 18.3155	valid_1's l2: 20.2815
[1320]	training's l2: 18.2941	valid_1's l2: 20.2804
[1330]	training's l2: 18.2733	valid_1's l2: 20.2797
[1340]	training's l2: 18.2525	valid_1's l2: 20.279
[1350]	training's l2: 18.232	valid_1's l2: 20.2781
[1360]	training's l2: 18.2119	valid_1's l2: 20.2777
[1370]	training's l2: 18.192	valid_1's l2: 20.2768
[1380]	training's l2: 18.1721	valid_1's l2: 20.2759
[1390]	training's l2: 18.1522	valid_1's l2: 20.2756
[1400]	training's l2: 18.1324	valid_1's l2: 20.2748
[1410]	training's l2: 18.1127	valid_1's l2: 20.2742
[1420]	training's l2: 18.0927	valid_1's l2: 20.2731
[1430]	training's l2: 18.0729	valid_1's l2: 20.2727
[1440]	training's l2: 18.0535	valid_1's l2: 20.2725
[1450]	training's l2: 18.034	valid_1's l2: 20.2713
[1460]	training's l2: 18.0151	valid_1's l2: 20.2717
[1470]	training's l2: 17.9945	valid_1's l2: 20.2705
[1480]	training's l2: 17.975	valid_1's l2: 20.2701
[1490]	training's l2: 17.9564	valid_1's l2: 20.2695
[1500]	training's l2: 17.9376	valid_1's l2: 20.2691
[1510]	training's l2: 17.9195	valid_1's l2: 20.2685
[1520]	training's l2: 17.9009	valid_1's l2: 20.2673
[1530]	training's l2: 17.8822	valid_1's l2: 20.2661
[1540]	training's l2: 17.8631	valid_1's l2: 20.2656
[1550]	training's l2: 17.8449	valid_1's l2: 20.2651
[1560]	training's l2: 17.8253	valid_1's l2: 20.2646
[1570]	training's l2: 17.8075	valid_1's l2: 20.2639
[1580]	training's l2: 17.7891	valid_1's l2: 20.2638
[1590]	training's l2: 17.7712	valid_1's l2: 20.2639
[1600]	training's l2: 17.753	valid_1's l2: 20.263
[1610]	training's l2: 17.735	valid_1's l2: 20.2625
[1620]	training's l2: 17.7166	valid_1's l2: 20.2618
[1630]	training's l2: 17.6983	valid_1's l2: 20.2611
[1640]	training's l2: 17.6801	valid_1's l2: 20.2609
[1650]	training's l2: 17.6626	valid_1's l2: 20.2602
[1660]	training's l2: 17.6448	valid_1's l2: 20.2604
[1670]	training's l2: 17.6273	valid_1's l2: 20.2607
[1680]	training's l2: 17.6099	valid_1's l2: 20.2606
[1690]	training's l2: 17.5915	valid_1's l2: 20.2597
[1700]	training's l2: 17.5736	valid_1's l2: 20.2598
[1710]	training's l2: 17.5565	valid_1's l2: 20.2594
[1720]	training's l2: 17.539	valid_1's l2: 20.2588
[1730]	training's l2: 17.5213	valid_1's l2: 20.258
[1740]	training's l2: 17.5034	valid_1's l2: 20.2576
[1750]	training's l2: 17.4859	valid_1's l2: 20.2574
[1760]	training's l2: 17.4684	valid_1's l2: 20.257
[1770]	training's l2: 17.4511	valid_1's l2: 20.2568
[1780]	training's l2: 17.4327	valid_1's l2: 20.2558
[1790]	training's l2: 17.4156	valid_1's l2: 20.2556
[1800]	training's l2: 17.398	valid_1's l2: 20.2555
[1810]	training's l2: 17.381	valid_1's l2: 20.2545
[1820]	training's l2: 17.3638	valid_1's l2: 20.254
[1830]	training's l2: 17.3465	valid_1's l2: 20.2534
[1840]	training's l2: 17.3287	valid_1's l2: 20.2527
[1850]	training's l2: 17.3114	valid_1's l2: 20.2528
[1860]	training's l2: 17.2948	valid_1's l2: 20.253
[1870]	training's l2: 17.2778	valid_1's l2: 20.2527
Early stopping, best iteration is:
[1842]	training's l2: 17.3255	valid_1's l2: 20.2525
score1: 3.8374634349006227
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.239601 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.9004	valid_1's l2: 30.9972
[20]	training's l2: 30.1306	valid_1's l2: 29.2369
[30]	training's l2: 28.6911	valid_1's l2: 27.8089
[40]	training's l2: 27.517	valid_1's l2: 26.645
[50]	training's l2: 26.5536	valid_1's l2: 25.6931
[60]	training's l2: 25.7618	valid_1's l2: 24.9161
[70]	training's l2: 25.1051	valid_1's l2: 24.275
[80]	training's l2: 24.5605	valid_1's l2: 23.7488
[90]	training's l2: 24.1055	valid_1's l2: 23.3121
[100]	training's l2: 23.7228	valid_1's l2: 22.9505
[110]	training's l2: 23.4	valid_1's l2: 22.65
[120]	training's l2: 23.127	valid_1's l2: 22.3993
[130]	training's l2: 22.8947	valid_1's l2: 22.1875
[140]	training's l2: 22.6916	valid_1's l2: 22.006
[150]	training's l2: 22.5128	valid_1's l2: 21.8521
[160]	training's l2: 22.3548	valid_1's l2: 21.7179
[170]	training's l2: 22.2155	valid_1's l2: 21.6018
[180]	training's l2: 22.0897	valid_1's l2: 21.5013
[190]	training's l2: 21.9764	valid_1's l2: 21.413
[200]	training's l2: 21.8732	valid_1's l2: 21.3355
[210]	training's l2: 21.7785	valid_1's l2: 21.2682
[220]	training's l2: 21.6879	valid_1's l2: 21.2048
[230]	training's l2: 21.6053	valid_1's l2: 21.1478
[240]	training's l2: 21.5296	valid_1's l2: 21.0964
[250]	training's l2: 21.4585	valid_1's l2: 21.0506
[260]	training's l2: 21.3905	valid_1's l2: 21.0076
[270]	training's l2: 21.3269	valid_1's l2: 20.9695
[280]	training's l2: 21.268	valid_1's l2: 20.935
[290]	training's l2: 21.2105	valid_1's l2: 20.902
[300]	training's l2: 21.1545	valid_1's l2: 20.8701
[310]	training's l2: 21.1004	valid_1's l2: 20.8402
[320]	training's l2: 21.0481	valid_1's l2: 20.8135
[330]	training's l2: 20.9975	valid_1's l2: 20.7889
[340]	training's l2: 20.9491	valid_1's l2: 20.7663
[350]	training's l2: 20.9033	valid_1's l2: 20.7456
[360]	training's l2: 20.8593	valid_1's l2: 20.7271
[370]	training's l2: 20.8153	valid_1's l2: 20.7086
[380]	training's l2: 20.773	valid_1's l2: 20.6923
[390]	training's l2: 20.7323	valid_1's l2: 20.677
[400]	training's l2: 20.6922	valid_1's l2: 20.6619
[410]	training's l2: 20.6531	valid_1's l2: 20.6479
[420]	training's l2: 20.6144	valid_1's l2: 20.6335
[430]	training's l2: 20.5759	valid_1's l2: 20.6202
[440]	training's l2: 20.5388	valid_1's l2: 20.6077
[450]	training's l2: 20.5026	valid_1's l2: 20.5964
[460]	training's l2: 20.4656	valid_1's l2: 20.5844
[470]	training's l2: 20.4304	valid_1's l2: 20.5743
[480]	training's l2: 20.3951	valid_1's l2: 20.5634
[490]	training's l2: 20.3602	valid_1's l2: 20.5543
[500]	training's l2: 20.326	valid_1's l2: 20.5451
[510]	training's l2: 20.2924	valid_1's l2: 20.5362
[520]	training's l2: 20.2591	valid_1's l2: 20.5283
[530]	training's l2: 20.2272	valid_1's l2: 20.5199
[540]	training's l2: 20.1945	valid_1's l2: 20.5118
[550]	training's l2: 20.1618	valid_1's l2: 20.5032
[560]	training's l2: 20.1301	valid_1's l2: 20.4972
[570]	training's l2: 20.0981	valid_1's l2: 20.49
[580]	training's l2: 20.0672	valid_1's l2: 20.4839
[590]	training's l2: 20.0365	valid_1's l2: 20.4785
[600]	training's l2: 20.0057	valid_1's l2: 20.4721
[610]	training's l2: 19.9752	valid_1's l2: 20.4657
[620]	training's l2: 19.944	valid_1's l2: 20.4597
[630]	training's l2: 19.9127	valid_1's l2: 20.4524
[640]	training's l2: 19.8825	valid_1's l2: 20.4457
[650]	training's l2: 19.8527	valid_1's l2: 20.4411
[660]	training's l2: 19.8224	valid_1's l2: 20.4348
[670]	training's l2: 19.7927	valid_1's l2: 20.429
[680]	training's l2: 19.7631	valid_1's l2: 20.4235
[690]	training's l2: 19.7337	valid_1's l2: 20.4175
[700]	training's l2: 19.7045	valid_1's l2: 20.4129
[710]	training's l2: 19.6759	valid_1's l2: 20.4079
[720]	training's l2: 19.6475	valid_1's l2: 20.4033
[730]	training's l2: 19.6191	valid_1's l2: 20.398
[740]	training's l2: 19.5909	valid_1's l2: 20.3937
[750]	training's l2: 19.563	valid_1's l2: 20.3893
[760]	training's l2: 19.5352	valid_1's l2: 20.3851
[770]	training's l2: 19.5073	valid_1's l2: 20.3799
[780]	training's l2: 19.4796	valid_1's l2: 20.376
[790]	training's l2: 19.452	valid_1's l2: 20.3729
[800]	training's l2: 19.4249	valid_1's l2: 20.3682
[810]	training's l2: 19.3976	valid_1's l2: 20.3645
[820]	training's l2: 19.3711	valid_1's l2: 20.3616
[830]	training's l2: 19.3444	valid_1's l2: 20.3577
[840]	training's l2: 19.3178	valid_1's l2: 20.3551
[850]	training's l2: 19.2914	valid_1's l2: 20.3517
[860]	training's l2: 19.2647	valid_1's l2: 20.3489
[870]	training's l2: 19.2388	valid_1's l2: 20.3463
[880]	training's l2: 19.2129	valid_1's l2: 20.3437
[890]	training's l2: 19.1869	valid_1's l2: 20.3413
[900]	training's l2: 19.1615	valid_1's l2: 20.3391
[910]	training's l2: 19.1358	valid_1's l2: 20.3366
[920]	training's l2: 19.1097	valid_1's l2: 20.3338
[930]	training's l2: 19.0842	valid_1's l2: 20.3312
[940]	training's l2: 19.0589	valid_1's l2: 20.33
[950]	training's l2: 19.033	valid_1's l2: 20.3273
[960]	training's l2: 19.0081	valid_1's l2: 20.3258
[970]	training's l2: 18.9829	valid_1's l2: 20.3219
[980]	training's l2: 18.9576	valid_1's l2: 20.3196
[990]	training's l2: 18.9327	valid_1's l2: 20.3167
[1000]	training's l2: 18.9083	valid_1's l2: 20.3147
[1010]	training's l2: 18.8843	valid_1's l2: 20.3129
[1020]	training's l2: 18.86	valid_1's l2: 20.3112
[1030]	training's l2: 18.8354	valid_1's l2: 20.3094
[1040]	training's l2: 18.8119	valid_1's l2: 20.3076
[1050]	training's l2: 18.788	valid_1's l2: 20.3048
[1060]	training's l2: 18.7638	valid_1's l2: 20.302
[1070]	training's l2: 18.7399	valid_1's l2: 20.2993
[1080]	training's l2: 18.7157	valid_1's l2: 20.2968
[1090]	training's l2: 18.692	valid_1's l2: 20.2948
[1100]	training's l2: 18.6684	valid_1's l2: 20.2925
[1110]	training's l2: 18.645	valid_1's l2: 20.2912
[1120]	training's l2: 18.6217	valid_1's l2: 20.2896
[1130]	training's l2: 18.5989	valid_1's l2: 20.2885
[1140]	training's l2: 18.5759	valid_1's l2: 20.2871
[1150]	training's l2: 18.5534	valid_1's l2: 20.2861
[1160]	training's l2: 18.53	valid_1's l2: 20.2844
[1170]	training's l2: 18.5072	valid_1's l2: 20.283
[1180]	training's l2: 18.4841	valid_1's l2: 20.2816
[1190]	training's l2: 18.4616	valid_1's l2: 20.28
[1200]	training's l2: 18.4392	valid_1's l2: 20.2789
[1210]	training's l2: 18.416	valid_1's l2: 20.2775
[1220]	training's l2: 18.3936	valid_1's l2: 20.2766
[1230]	training's l2: 18.3719	valid_1's l2: 20.2758
[1240]	training's l2: 18.3501	valid_1's l2: 20.274
[1250]	training's l2: 18.329	valid_1's l2: 20.273
[1260]	training's l2: 18.3073	valid_1's l2: 20.2721
[1270]	training's l2: 18.2857	valid_1's l2: 20.2706
[1280]	training's l2: 18.2644	valid_1's l2: 20.27
[1290]	training's l2: 18.243	valid_1's l2: 20.2684
[1300]	training's l2: 18.2217	valid_1's l2: 20.2675
[1310]	training's l2: 18.2019	valid_1's l2: 20.2667
[1320]	training's l2: 18.1805	valid_1's l2: 20.2664
[1330]	training's l2: 18.1599	valid_1's l2: 20.2659
[1340]	training's l2: 18.1393	valid_1's l2: 20.2649
[1350]	training's l2: 18.1189	valid_1's l2: 20.2642
[1360]	training's l2: 18.0982	valid_1's l2: 20.2636
[1370]	training's l2: 18.0778	valid_1's l2: 20.2626
[1380]	training's l2: 18.0576	valid_1's l2: 20.2616
[1390]	training's l2: 18.0375	valid_1's l2: 20.2611
[1400]	training's l2: 18.0169	valid_1's l2: 20.2603
[1410]	training's l2: 17.9969	valid_1's l2: 20.2599
[1420]	training's l2: 17.9768	valid_1's l2: 20.2585
[1430]	training's l2: 17.9569	valid_1's l2: 20.2583
[1440]	training's l2: 17.9374	valid_1's l2: 20.2573
[1450]	training's l2: 17.9172	valid_1's l2: 20.2568
[1460]	training's l2: 17.8969	valid_1's l2: 20.2559
[1470]	training's l2: 17.8775	valid_1's l2: 20.2557
[1480]	training's l2: 17.8583	valid_1's l2: 20.255
[1490]	training's l2: 17.8391	valid_1's l2: 20.2555
[1500]	training's l2: 17.819	valid_1's l2: 20.2551
[1510]	training's l2: 17.7994	valid_1's l2: 20.2544
[1520]	training's l2: 17.7813	valid_1's l2: 20.2539
[1530]	training's l2: 17.7618	valid_1's l2: 20.2539
[1540]	training's l2: 17.7417	valid_1's l2: 20.2533
[1550]	training's l2: 17.7229	valid_1's l2: 20.252
[1560]	training's l2: 17.7045	valid_1's l2: 20.2525
[1570]	training's l2: 17.6862	valid_1's l2: 20.2523
[1580]	training's l2: 17.6678	valid_1's l2: 20.2521
Early stopping, best iteration is:
[1552]	training's l2: 17.7191	valid_1's l2: 20.2518
score1: 3.8357253180390884
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.252055 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 32.0459	valid_1's l2: 31.1488
[20]	training's l2: 30.3627	valid_1's l2: 29.4832
[30]	training's l2: 28.9692	valid_1's l2: 28.1077
[40]	training's l2: 27.813	valid_1's l2: 26.9697
[50]	training's l2: 26.8509	valid_1's l2: 26.0277
[60]	training's l2: 26.046	valid_1's l2: 25.2427
[70]	training's l2: 25.37	valid_1's l2: 24.587
[80]	training's l2: 24.7998	valid_1's l2: 24.039
[90]	training's l2: 24.3175	valid_1's l2: 23.5804
[100]	training's l2: 23.9069	valid_1's l2: 23.1959
[110]	training's l2: 23.5553	valid_1's l2: 22.8714
[120]	training's l2: 23.2529	valid_1's l2: 22.595
[130]	training's l2: 22.9931	valid_1's l2: 22.3621
[140]	training's l2: 22.7683	valid_1's l2: 22.1654
[150]	training's l2: 22.571	valid_1's l2: 21.9942
[160]	training's l2: 22.3943	valid_1's l2: 21.8465
[170]	training's l2: 22.2368	valid_1's l2: 21.7184
[180]	training's l2: 22.0963	valid_1's l2: 21.6081
[190]	training's l2: 21.9693	valid_1's l2: 21.5087
[200]	training's l2: 21.8524	valid_1's l2: 21.4228
[210]	training's l2: 21.7445	valid_1's l2: 21.3471
[220]	training's l2: 21.6446	valid_1's l2: 21.2779
[230]	training's l2: 21.5533	valid_1's l2: 21.2185
[240]	training's l2: 21.4653	valid_1's l2: 21.1613
[250]	training's l2: 21.3821	valid_1's l2: 21.1074
[260]	training's l2: 21.3057	valid_1's l2: 21.0609
[270]	training's l2: 21.2344	valid_1's l2: 21.0198
[280]	training's l2: 21.1655	valid_1's l2: 20.9808
[290]	training's l2: 21.1003	valid_1's l2: 20.945
[300]	training's l2: 21.0367	valid_1's l2: 20.9111
[310]	training's l2: 20.9768	valid_1's l2: 20.881
[320]	training's l2: 20.9199	valid_1's l2: 20.8532
[330]	training's l2: 20.8623	valid_1's l2: 20.8243
[340]	training's l2: 20.8056	valid_1's l2: 20.7961
[350]	training's l2: 20.7519	valid_1's l2: 20.7734
[360]	training's l2: 20.6993	valid_1's l2: 20.7515
[370]	training's l2: 20.6479	valid_1's l2: 20.7317
[380]	training's l2: 20.5992	valid_1's l2: 20.7132
[390]	training's l2: 20.5509	valid_1's l2: 20.6938
[400]	training's l2: 20.5052	valid_1's l2: 20.6783
[410]	training's l2: 20.4593	valid_1's l2: 20.6619
[420]	training's l2: 20.4157	valid_1's l2: 20.6485
[430]	training's l2: 20.3726	valid_1's l2: 20.6346
[440]	training's l2: 20.3309	valid_1's l2: 20.6219
[450]	training's l2: 20.2891	valid_1's l2: 20.6101
[460]	training's l2: 20.2484	valid_1's l2: 20.5987
[470]	training's l2: 20.2083	valid_1's l2: 20.5878
[480]	training's l2: 20.1684	valid_1's l2: 20.577
[490]	training's l2: 20.1295	valid_1's l2: 20.5674
[500]	training's l2: 20.091	valid_1's l2: 20.5582
[510]	training's l2: 20.0526	valid_1's l2: 20.5493
[520]	training's l2: 20.0142	valid_1's l2: 20.5389
[530]	training's l2: 19.9775	valid_1's l2: 20.5313
[540]	training's l2: 19.9406	valid_1's l2: 20.5234
[550]	training's l2: 19.9042	valid_1's l2: 20.5161
[560]	training's l2: 19.8684	valid_1's l2: 20.5087
[570]	training's l2: 19.8324	valid_1's l2: 20.5016
[580]	training's l2: 19.7972	valid_1's l2: 20.4943
[590]	training's l2: 19.7618	valid_1's l2: 20.4873
[600]	training's l2: 19.7265	valid_1's l2: 20.4808
[610]	training's l2: 19.6921	valid_1's l2: 20.4754
[620]	training's l2: 19.6568	valid_1's l2: 20.4683
[630]	training's l2: 19.623	valid_1's l2: 20.462
[640]	training's l2: 19.5885	valid_1's l2: 20.4556
[650]	training's l2: 19.5547	valid_1's l2: 20.4495
[660]	training's l2: 19.5203	valid_1's l2: 20.4439
[670]	training's l2: 19.4865	valid_1's l2: 20.4392
[680]	training's l2: 19.4527	valid_1's l2: 20.4328
[690]	training's l2: 19.4197	valid_1's l2: 20.4279
[700]	training's l2: 19.3871	valid_1's l2: 20.4226
[710]	training's l2: 19.3541	valid_1's l2: 20.4174
[720]	training's l2: 19.3216	valid_1's l2: 20.4125
[730]	training's l2: 19.2893	valid_1's l2: 20.4079
[740]	training's l2: 19.2571	valid_1's l2: 20.403
[750]	training's l2: 19.2255	valid_1's l2: 20.3989
[760]	training's l2: 19.194	valid_1's l2: 20.3941
[770]	training's l2: 19.1625	valid_1's l2: 20.3896
[780]	training's l2: 19.1313	valid_1's l2: 20.3855
[790]	training's l2: 19.1001	valid_1's l2: 20.3813
[800]	training's l2: 19.0693	valid_1's l2: 20.3772
[810]	training's l2: 19.0388	valid_1's l2: 20.373
[820]	training's l2: 19.0085	valid_1's l2: 20.3686
[830]	training's l2: 18.9779	valid_1's l2: 20.3647
[840]	training's l2: 18.9482	valid_1's l2: 20.3612
[850]	training's l2: 18.9182	valid_1's l2: 20.3569
[860]	training's l2: 18.8886	valid_1's l2: 20.3535
[870]	training's l2: 18.8592	valid_1's l2: 20.35
[880]	training's l2: 18.8299	valid_1's l2: 20.3471
[890]	training's l2: 18.8008	valid_1's l2: 20.344
[900]	training's l2: 18.7711	valid_1's l2: 20.3409
[910]	training's l2: 18.742	valid_1's l2: 20.3385
[920]	training's l2: 18.7134	valid_1's l2: 20.3364
[930]	training's l2: 18.6846	valid_1's l2: 20.3326
[940]	training's l2: 18.6556	valid_1's l2: 20.3304
[950]	training's l2: 18.6274	valid_1's l2: 20.3279
[960]	training's l2: 18.5989	valid_1's l2: 20.3253
[970]	training's l2: 18.5708	valid_1's l2: 20.3228
[980]	training's l2: 18.5431	valid_1's l2: 20.3201
[990]	training's l2: 18.5151	valid_1's l2: 20.3177
[1000]	training's l2: 18.4875	valid_1's l2: 20.3164
[1010]	training's l2: 18.4598	valid_1's l2: 20.3141
[1020]	training's l2: 18.4321	valid_1's l2: 20.3118
[1030]	training's l2: 18.4044	valid_1's l2: 20.3092
[1040]	training's l2: 18.3769	valid_1's l2: 20.3064
[1050]	training's l2: 18.3498	valid_1's l2: 20.3042
[1060]	training's l2: 18.3221	valid_1's l2: 20.3027
[1070]	training's l2: 18.2944	valid_1's l2: 20.3003
[1080]	training's l2: 18.2674	valid_1's l2: 20.2985
[1090]	training's l2: 18.2404	valid_1's l2: 20.2963
[1100]	training's l2: 18.2133	valid_1's l2: 20.2946
[1110]	training's l2: 18.1867	valid_1's l2: 20.2926
[1120]	training's l2: 18.1605	valid_1's l2: 20.2911
[1130]	training's l2: 18.1346	valid_1's l2: 20.289
[1140]	training's l2: 18.1079	valid_1's l2: 20.2872
[1150]	training's l2: 18.0815	valid_1's l2: 20.2851
[1160]	training's l2: 18.0553	valid_1's l2: 20.2833
[1170]	training's l2: 18.0296	valid_1's l2: 20.2824
[1180]	training's l2: 18.0041	valid_1's l2: 20.2808
[1190]	training's l2: 17.9793	valid_1's l2: 20.2788
[1200]	training's l2: 17.9533	valid_1's l2: 20.2768
[1210]	training's l2: 17.9274	valid_1's l2: 20.276
[1220]	training's l2: 17.9026	valid_1's l2: 20.2744
[1230]	training's l2: 17.8772	valid_1's l2: 20.2734
[1240]	training's l2: 17.8524	valid_1's l2: 20.2716
[1250]	training's l2: 17.8276	valid_1's l2: 20.271
[1260]	training's l2: 17.8027	valid_1's l2: 20.2699
[1270]	training's l2: 17.7787	valid_1's l2: 20.2689
[1280]	training's l2: 17.7539	valid_1's l2: 20.2679
[1290]	training's l2: 17.7289	valid_1's l2: 20.267
[1300]	training's l2: 17.705	valid_1's l2: 20.2664
[1310]	training's l2: 17.6816	valid_1's l2: 20.2655
[1320]	training's l2: 17.6577	valid_1's l2: 20.2654
[1330]	training's l2: 17.6337	valid_1's l2: 20.2643
[1340]	training's l2: 17.6104	valid_1's l2: 20.2633
[1350]	training's l2: 17.5864	valid_1's l2: 20.2625
[1360]	training's l2: 17.5624	valid_1's l2: 20.2616
[1370]	training's l2: 17.5396	valid_1's l2: 20.261
[1380]	training's l2: 17.5168	valid_1's l2: 20.2604
[1390]	training's l2: 17.4932	valid_1's l2: 20.2591
[1400]	training's l2: 17.4703	valid_1's l2: 20.258
[1410]	training's l2: 17.4477	valid_1's l2: 20.2576
[1420]	training's l2: 17.4252	valid_1's l2: 20.2569
[1430]	training's l2: 17.4028	valid_1's l2: 20.256
[1440]	training's l2: 17.3808	valid_1's l2: 20.2561
[1450]	training's l2: 17.3578	valid_1's l2: 20.2554
[1460]	training's l2: 17.335	valid_1's l2: 20.2545
[1470]	training's l2: 17.3127	valid_1's l2: 20.2541
[1480]	training's l2: 17.2908	valid_1's l2: 20.2537
[1490]	training's l2: 17.2684	valid_1's l2: 20.2527
[1500]	training's l2: 17.2465	valid_1's l2: 20.2522
[1510]	training's l2: 17.2246	valid_1's l2: 20.2508
[1520]	training's l2: 17.2025	valid_1's l2: 20.2494
[1530]	training's l2: 17.1801	valid_1's l2: 20.2484
[1540]	training's l2: 17.1578	valid_1's l2: 20.2477
[1550]	training's l2: 17.1368	valid_1's l2: 20.2471
[1560]	training's l2: 17.1151	valid_1's l2: 20.2465
[1570]	training's l2: 17.0935	valid_1's l2: 20.2456
[1580]	training's l2: 17.0721	valid_1's l2: 20.2455
[1590]	training's l2: 17.0512	valid_1's l2: 20.2445
[1600]	training's l2: 17.0302	valid_1's l2: 20.2442
[1610]	training's l2: 17.0084	valid_1's l2: 20.2436
[1620]	training's l2: 16.9869	valid_1's l2: 20.2433
[1630]	training's l2: 16.9661	valid_1's l2: 20.2427
[1640]	training's l2: 16.9463	valid_1's l2: 20.2423
[1650]	training's l2: 16.9258	valid_1's l2: 20.2428
[1660]	training's l2: 16.9054	valid_1's l2: 20.2419
[1670]	training's l2: 16.8842	valid_1's l2: 20.241
[1680]	training's l2: 16.8637	valid_1's l2: 20.2402
[1690]	training's l2: 16.8425	valid_1's l2: 20.24
[1700]	training's l2: 16.8219	valid_1's l2: 20.2405
[1710]	training's l2: 16.8015	valid_1's l2: 20.2399
[1720]	training's l2: 16.7794	valid_1's l2: 20.2392
[1730]	training's l2: 16.7591	valid_1's l2: 20.2381
[1740]	training's l2: 16.7383	valid_1's l2: 20.2377
[1750]	training's l2: 16.7168	valid_1's l2: 20.2379
[1760]	training's l2: 16.6961	valid_1's l2: 20.2365
[1770]	training's l2: 16.6759	valid_1's l2: 20.2362
[1780]	training's l2: 16.6556	valid_1's l2: 20.2361
[1790]	training's l2: 16.6344	valid_1's l2: 20.2358
[1800]	training's l2: 16.6148	valid_1's l2: 20.236
[1810]	training's l2: 16.5947	valid_1's l2: 20.2362
[1820]	training's l2: 16.5749	valid_1's l2: 20.2351
[1830]	training's l2: 16.5546	valid_1's l2: 20.2354
[1840]	training's l2: 16.5342	valid_1's l2: 20.2357
[1850]	training's l2: 16.5146	valid_1's l2: 20.2347
[1860]	training's l2: 16.4947	valid_1's l2: 20.2343
[1870]	training's l2: 16.4753	valid_1's l2: 20.2343
[1880]	training's l2: 16.455	valid_1's l2: 20.2348
[1890]	training's l2: 16.4354	valid_1's l2: 20.2344
Early stopping, best iteration is:
[1862]	training's l2: 16.4911	valid_1's l2: 20.2338
score1: 3.836422401831154
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.246712 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.0133	valid_1's l2: 30.0833
[20]	training's l2: 28.8127	valid_1's l2: 27.8564
[30]	training's l2: 27.2116	valid_1's l2: 26.2436
[40]	training's l2: 26.0366	valid_1's l2: 25.0573
[50]	training's l2: 25.1652	valid_1's l2: 24.1763
[60]	training's l2: 24.5121	valid_1's l2: 23.5163
[70]	training's l2: 24.0152	valid_1's l2: 23.0164
[80]	training's l2: 23.6323	valid_1's l2: 22.6317
[90]	training's l2: 23.328	valid_1's l2: 22.3308
[100]	training's l2: 23.0865	valid_1's l2: 22.0934
[110]	training's l2: 22.887	valid_1's l2: 21.8995
[120]	training's l2: 22.7239	valid_1's l2: 21.7403
[130]	training's l2: 22.5851	valid_1's l2: 21.6081
[140]	training's l2: 22.4722	valid_1's l2: 21.5025
[150]	training's l2: 22.3737	valid_1's l2: 21.411
[160]	training's l2: 22.289	valid_1's l2: 21.3325
[170]	training's l2: 22.2171	valid_1's l2: 21.2664
[180]	training's l2: 22.152	valid_1's l2: 21.2081
[190]	training's l2: 22.093	valid_1's l2: 21.157
[200]	training's l2: 22.034	valid_1's l2: 21.1072
[210]	training's l2: 21.9843	valid_1's l2: 21.0645
[220]	training's l2: 21.9367	valid_1's l2: 21.0273
[230]	training's l2: 21.8925	valid_1's l2: 20.9923
[240]	training's l2: 21.8522	valid_1's l2: 20.9615
[250]	training's l2: 21.8118	valid_1's l2: 20.9301
[260]	training's l2: 21.7753	valid_1's l2: 20.9036
[270]	training's l2: 21.7408	valid_1's l2: 20.8792
[280]	training's l2: 21.707	valid_1's l2: 20.8552
[290]	training's l2: 21.6761	valid_1's l2: 20.8322
[300]	training's l2: 21.6449	valid_1's l2: 20.81
[310]	training's l2: 21.6164	valid_1's l2: 20.7903
[320]	training's l2: 21.5894	valid_1's l2: 20.773
[330]	training's l2: 21.5619	valid_1's l2: 20.7542
[340]	training's l2: 21.5368	valid_1's l2: 20.7379
[350]	training's l2: 21.5121	valid_1's l2: 20.723
[360]	training's l2: 21.4886	valid_1's l2: 20.7092
[370]	training's l2: 21.4657	valid_1's l2: 20.6961
[380]	training's l2: 21.4432	valid_1's l2: 20.6832
[390]	training's l2: 21.4218	valid_1's l2: 20.6714
[400]	training's l2: 21.4002	valid_1's l2: 20.658
[410]	training's l2: 21.3795	valid_1's l2: 20.6471
[420]	training's l2: 21.3583	valid_1's l2: 20.6348
[430]	training's l2: 21.3381	valid_1's l2: 20.6227
[440]	training's l2: 21.3194	valid_1's l2: 20.6124
[450]	training's l2: 21.2994	valid_1's l2: 20.6007
[460]	training's l2: 21.2799	valid_1's l2: 20.5904
[470]	training's l2: 21.2616	valid_1's l2: 20.5819
[480]	training's l2: 21.2435	valid_1's l2: 20.5737
[490]	training's l2: 21.2252	valid_1's l2: 20.5648
[500]	training's l2: 21.2082	valid_1's l2: 20.5569
[510]	training's l2: 21.1911	valid_1's l2: 20.5481
[520]	training's l2: 21.1744	valid_1's l2: 20.5408
[530]	training's l2: 21.1568	valid_1's l2: 20.5329
[540]	training's l2: 21.1406	valid_1's l2: 20.5254
[550]	training's l2: 21.1238	valid_1's l2: 20.5164
[560]	training's l2: 21.1075	valid_1's l2: 20.5086
[570]	training's l2: 21.0923	valid_1's l2: 20.5028
[580]	training's l2: 21.0769	valid_1's l2: 20.4967
[590]	training's l2: 21.0619	valid_1's l2: 20.4914
[600]	training's l2: 21.0466	valid_1's l2: 20.4851
[610]	training's l2: 21.0318	valid_1's l2: 20.4796
[620]	training's l2: 21.0173	valid_1's l2: 20.4731
[630]	training's l2: 21.0028	valid_1's l2: 20.467
[640]	training's l2: 20.9885	valid_1's l2: 20.4619
[650]	training's l2: 20.9746	valid_1's l2: 20.4565
[660]	training's l2: 20.9614	valid_1's l2: 20.4512
[670]	training's l2: 20.9476	valid_1's l2: 20.4461
[680]	training's l2: 20.9335	valid_1's l2: 20.4403
[690]	training's l2: 20.9197	valid_1's l2: 20.4349
[700]	training's l2: 20.9066	valid_1's l2: 20.4309
[710]	training's l2: 20.893	valid_1's l2: 20.4255
[720]	training's l2: 20.8797	valid_1's l2: 20.4207
[730]	training's l2: 20.8673	valid_1's l2: 20.4171
[740]	training's l2: 20.8541	valid_1's l2: 20.4123
[750]	training's l2: 20.8413	valid_1's l2: 20.4092
[760]	training's l2: 20.8289	valid_1's l2: 20.4061
[770]	training's l2: 20.816	valid_1's l2: 20.4022
[780]	training's l2: 20.8032	valid_1's l2: 20.3978
[790]	training's l2: 20.7906	valid_1's l2: 20.3943
[800]	training's l2: 20.7784	valid_1's l2: 20.39
[810]	training's l2: 20.7661	valid_1's l2: 20.3865
[820]	training's l2: 20.7542	valid_1's l2: 20.3836
[830]	training's l2: 20.7423	valid_1's l2: 20.3802
[840]	training's l2: 20.7304	valid_1's l2: 20.3774
[850]	training's l2: 20.7188	valid_1's l2: 20.3754
[860]	training's l2: 20.7065	valid_1's l2: 20.3724
[870]	training's l2: 20.6954	valid_1's l2: 20.3703
[880]	training's l2: 20.6835	valid_1's l2: 20.3682
[890]	training's l2: 20.6719	valid_1's l2: 20.3649
[900]	training's l2: 20.6608	valid_1's l2: 20.3622
[910]	training's l2: 20.6498	valid_1's l2: 20.3602
[920]	training's l2: 20.6391	valid_1's l2: 20.3586
[930]	training's l2: 20.6288	valid_1's l2: 20.3568
[940]	training's l2: 20.618	valid_1's l2: 20.3545
[950]	training's l2: 20.607	valid_1's l2: 20.3527
[960]	training's l2: 20.5959	valid_1's l2: 20.3497
[970]	training's l2: 20.5853	valid_1's l2: 20.3479
[980]	training's l2: 20.5745	valid_1's l2: 20.3459
[990]	training's l2: 20.5637	valid_1's l2: 20.3442
[1000]	training's l2: 20.554	valid_1's l2: 20.3427
[1010]	training's l2: 20.5436	valid_1's l2: 20.341
[1020]	training's l2: 20.5328	valid_1's l2: 20.3388
[1030]	training's l2: 20.5226	valid_1's l2: 20.337
[1040]	training's l2: 20.5123	valid_1's l2: 20.3348
[1050]	training's l2: 20.5025	valid_1's l2: 20.3332
[1060]	training's l2: 20.4925	valid_1's l2: 20.3319
[1070]	training's l2: 20.4824	valid_1's l2: 20.3302
[1080]	training's l2: 20.4725	valid_1's l2: 20.3294
[1090]	training's l2: 20.4624	valid_1's l2: 20.3277
[1100]	training's l2: 20.4531	valid_1's l2: 20.3256
[1110]	training's l2: 20.4439	valid_1's l2: 20.3242
[1120]	training's l2: 20.435	valid_1's l2: 20.3239
[1130]	training's l2: 20.4253	valid_1's l2: 20.3228
[1140]	training's l2: 20.4157	valid_1's l2: 20.3218
[1150]	training's l2: 20.4064	valid_1's l2: 20.3206
[1160]	training's l2: 20.397	valid_1's l2: 20.3198
[1170]	training's l2: 20.387	valid_1's l2: 20.3176
[1180]	training's l2: 20.3773	valid_1's l2: 20.3162
[1190]	training's l2: 20.3678	valid_1's l2: 20.3143
[1200]	training's l2: 20.3589	valid_1's l2: 20.3136
[1210]	training's l2: 20.3496	valid_1's l2: 20.3128
[1220]	training's l2: 20.3408	valid_1's l2: 20.3114
[1230]	training's l2: 20.3314	valid_1's l2: 20.3099
[1240]	training's l2: 20.3222	valid_1's l2: 20.3087
[1250]	training's l2: 20.3135	valid_1's l2: 20.3075
[1260]	training's l2: 20.3052	valid_1's l2: 20.3071
[1270]	training's l2: 20.2963	valid_1's l2: 20.3062
[1280]	training's l2: 20.2872	valid_1's l2: 20.3047
[1290]	training's l2: 20.2784	valid_1's l2: 20.3041
[1300]	training's l2: 20.2692	valid_1's l2: 20.3036
[1310]	training's l2: 20.2607	valid_1's l2: 20.3031
[1320]	training's l2: 20.2522	valid_1's l2: 20.302
[1330]	training's l2: 20.2433	valid_1's l2: 20.3013
[1340]	training's l2: 20.2343	valid_1's l2: 20.3001
[1350]	training's l2: 20.2252	valid_1's l2: 20.2992
[1360]	training's l2: 20.2163	valid_1's l2: 20.2988
[1370]	training's l2: 20.2074	valid_1's l2: 20.2981
[1380]	training's l2: 20.1989	valid_1's l2: 20.2976
[1390]	training's l2: 20.1906	valid_1's l2: 20.2972
[1400]	training's l2: 20.1823	valid_1's l2: 20.2968
[1410]	training's l2: 20.1736	valid_1's l2: 20.2962
[1420]	training's l2: 20.1651	valid_1's l2: 20.2957
[1430]	training's l2: 20.1562	valid_1's l2: 20.2945
[1440]	training's l2: 20.1476	valid_1's l2: 20.294
[1450]	training's l2: 20.1393	valid_1's l2: 20.2931
[1460]	training's l2: 20.13	valid_1's l2: 20.292
[1470]	training's l2: 20.1218	valid_1's l2: 20.2914
[1480]	training's l2: 20.1136	valid_1's l2: 20.2907
[1490]	training's l2: 20.105	valid_1's l2: 20.2899
[1500]	training's l2: 20.0971	valid_1's l2: 20.2886
[1510]	training's l2: 20.0888	valid_1's l2: 20.2881
[1520]	training's l2: 20.0807	valid_1's l2: 20.2871
[1530]	training's l2: 20.0718	valid_1's l2: 20.2859
[1540]	training's l2: 20.0631	valid_1's l2: 20.2859
[1550]	training's l2: 20.0542	valid_1's l2: 20.2856
[1560]	training's l2: 20.0465	valid_1's l2: 20.2851
[1570]	training's l2: 20.0381	valid_1's l2: 20.2846
[1580]	training's l2: 20.0296	valid_1's l2: 20.2839
[1590]	training's l2: 20.0213	valid_1's l2: 20.2836
[1600]	training's l2: 20.013	valid_1's l2: 20.2829
[1610]	training's l2: 20.0057	valid_1's l2: 20.2826
[1620]	training's l2: 19.9976	valid_1's l2: 20.2818
[1630]	training's l2: 19.989	valid_1's l2: 20.2814
[1640]	training's l2: 19.9811	valid_1's l2: 20.2812
[1650]	training's l2: 19.9735	valid_1's l2: 20.2807
[1660]	training's l2: 19.9661	valid_1's l2: 20.2799
[1670]	training's l2: 19.9576	valid_1's l2: 20.2786
[1680]	training's l2: 19.9492	valid_1's l2: 20.2776
[1690]	training's l2: 19.9416	valid_1's l2: 20.2765
[1700]	training's l2: 19.9331	valid_1's l2: 20.2754
[1710]	training's l2: 19.925	valid_1's l2: 20.2755
[1720]	training's l2: 19.917	valid_1's l2: 20.2749
[1730]	training's l2: 19.9088	valid_1's l2: 20.2743
[1740]	training's l2: 19.9017	valid_1's l2: 20.2741
[1750]	training's l2: 19.8936	valid_1's l2: 20.2734
[1760]	training's l2: 19.8861	valid_1's l2: 20.2728
[1770]	training's l2: 19.878	valid_1's l2: 20.2727
[1780]	training's l2: 19.8701	valid_1's l2: 20.272
[1790]	training's l2: 19.8613	valid_1's l2: 20.2711
[1800]	training's l2: 19.8538	valid_1's l2: 20.27
[1810]	training's l2: 19.8464	valid_1's l2: 20.2693
[1820]	training's l2: 19.8389	valid_1's l2: 20.2692
[1830]	training's l2: 19.8308	valid_1's l2: 20.269
[1840]	training's l2: 19.8224	valid_1's l2: 20.2678
[1850]	training's l2: 19.8151	valid_1's l2: 20.2676
[1860]	training's l2: 19.8074	valid_1's l2: 20.267
[1870]	training's l2: 19.7988	valid_1's l2: 20.2657
[1880]	training's l2: 19.7917	valid_1's l2: 20.2647
[1890]	training's l2: 19.7832	valid_1's l2: 20.264
[1900]	training's l2: 19.775	valid_1's l2: 20.2634
[1910]	training's l2: 19.7676	valid_1's l2: 20.263
[1920]	training's l2: 19.76	valid_1's l2: 20.2625
[1930]	training's l2: 19.7527	valid_1's l2: 20.2622
[1940]	training's l2: 19.7455	valid_1's l2: 20.2619
[1950]	training's l2: 19.7382	valid_1's l2: 20.2618
[1960]	training's l2: 19.7301	valid_1's l2: 20.2611
[1970]	training's l2: 19.7227	valid_1's l2: 20.2607
[1980]	training's l2: 19.7152	valid_1's l2: 20.2603
[1990]	training's l2: 19.7069	valid_1's l2: 20.2599
[2000]	training's l2: 19.6995	valid_1's l2: 20.2594
[2010]	training's l2: 19.6925	valid_1's l2: 20.2581
[2020]	training's l2: 19.6846	valid_1's l2: 20.2578
[2030]	training's l2: 19.6768	valid_1's l2: 20.2573
[2040]	training's l2: 19.6695	valid_1's l2: 20.2567
[2050]	training's l2: 19.6626	valid_1's l2: 20.2563
[2060]	training's l2: 19.6552	valid_1's l2: 20.2563
[2070]	training's l2: 19.6481	valid_1's l2: 20.2559
[2080]	training's l2: 19.6403	valid_1's l2: 20.2552
[2090]	training's l2: 19.6322	valid_1's l2: 20.2549
[2100]	training's l2: 19.6249	valid_1's l2: 20.2542
[2110]	training's l2: 19.6181	valid_1's l2: 20.2537
[2120]	training's l2: 19.6105	valid_1's l2: 20.2529
[2130]	training's l2: 19.6024	valid_1's l2: 20.2527
[2140]	training's l2: 19.5949	valid_1's l2: 20.2522
[2150]	training's l2: 19.5874	valid_1's l2: 20.2518
[2160]	training's l2: 19.5803	valid_1's l2: 20.2516
[2170]	training's l2: 19.5725	valid_1's l2: 20.2514
[2180]	training's l2: 19.565	valid_1's l2: 20.251
[2190]	training's l2: 19.5577	valid_1's l2: 20.2507
[2200]	training's l2: 19.5508	valid_1's l2: 20.2511
[2210]	training's l2: 19.5426	valid_1's l2: 20.2505
[2220]	training's l2: 19.5349	valid_1's l2: 20.2503
[2230]	training's l2: 19.5275	valid_1's l2: 20.2497
[2240]	training's l2: 19.5197	valid_1's l2: 20.249
[2250]	training's l2: 19.5123	valid_1's l2: 20.2486
[2260]	training's l2: 19.5051	valid_1's l2: 20.2482
[2270]	training's l2: 19.4977	valid_1's l2: 20.2479
[2280]	training's l2: 19.4904	valid_1's l2: 20.2475
[2290]	training's l2: 19.4828	valid_1's l2: 20.2471
[2300]	training's l2: 19.4757	valid_1's l2: 20.2461
[2310]	training's l2: 19.4689	valid_1's l2: 20.246
[2320]	training's l2: 19.4621	valid_1's l2: 20.2462
[2330]	training's l2: 19.4542	valid_1's l2: 20.2457
[2340]	training's l2: 19.4469	valid_1's l2: 20.2454
[2350]	training's l2: 19.4391	valid_1's l2: 20.2455
[2360]	training's l2: 19.4327	valid_1's l2: 20.2455
[2370]	training's l2: 19.4256	valid_1's l2: 20.2453
[2380]	training's l2: 19.418	valid_1's l2: 20.2447
[2390]	training's l2: 19.4106	valid_1's l2: 20.2444
[2400]	training's l2: 19.4031	valid_1's l2: 20.2435
[2410]	training's l2: 19.396	valid_1's l2: 20.2442
[2420]	training's l2: 19.389	valid_1's l2: 20.2442
Early stopping, best iteration is:
[2399]	training's l2: 19.4037	valid_1's l2: 20.2435
score1: 3.8386152114164354
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.266893 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 28.8203	valid_1's l2: 27.9263
[20]	training's l2: 25.9152	valid_1's l2: 25.0541
[30]	training's l2: 24.2516	valid_1's l2: 23.439
[40]	training's l2: 23.2571	valid_1's l2: 22.4998
[50]	training's l2: 22.6344	valid_1's l2: 21.9394
[60]	training's l2: 22.2038	valid_1's l2: 21.573
[70]	training's l2: 21.8919	valid_1's l2: 21.3327
[80]	training's l2: 21.6398	valid_1's l2: 21.1485
[90]	training's l2: 21.4404	valid_1's l2: 21.0172
[100]	training's l2: 21.2724	valid_1's l2: 20.9166
[110]	training's l2: 21.1191	valid_1's l2: 20.8336
[120]	training's l2: 20.9833	valid_1's l2: 20.7687
[130]	training's l2: 20.86	valid_1's l2: 20.7145
[140]	training's l2: 20.7456	valid_1's l2: 20.6705
[150]	training's l2: 20.6381	valid_1's l2: 20.6342
[160]	training's l2: 20.5364	valid_1's l2: 20.6011
[170]	training's l2: 20.4382	valid_1's l2: 20.5752
[180]	training's l2: 20.3448	valid_1's l2: 20.553
[190]	training's l2: 20.2532	valid_1's l2: 20.5309
[200]	training's l2: 20.1652	valid_1's l2: 20.5145
[210]	training's l2: 20.0788	valid_1's l2: 20.4946
[220]	training's l2: 19.9938	valid_1's l2: 20.4781
[230]	training's l2: 19.9102	valid_1's l2: 20.4597
[240]	training's l2: 19.8292	valid_1's l2: 20.4456
[250]	training's l2: 19.7495	valid_1's l2: 20.433
[260]	training's l2: 19.6722	valid_1's l2: 20.4225
[270]	training's l2: 19.5948	valid_1's l2: 20.4085
[280]	training's l2: 19.5203	valid_1's l2: 20.3968
[290]	training's l2: 19.4456	valid_1's l2: 20.3868
[300]	training's l2: 19.3721	valid_1's l2: 20.3783
[310]	training's l2: 19.2994	valid_1's l2: 20.3717
[320]	training's l2: 19.2281	valid_1's l2: 20.3646
[330]	training's l2: 19.1591	valid_1's l2: 20.3597
[340]	training's l2: 19.0886	valid_1's l2: 20.3531
[350]	training's l2: 19.0214	valid_1's l2: 20.346
[360]	training's l2: 18.9538	valid_1's l2: 20.3414
[370]	training's l2: 18.8858	valid_1's l2: 20.3365
[380]	training's l2: 18.82	valid_1's l2: 20.3316
[390]	training's l2: 18.7544	valid_1's l2: 20.3257
[400]	training's l2: 18.6897	valid_1's l2: 20.3236
[410]	training's l2: 18.6282	valid_1's l2: 20.3211
[420]	training's l2: 18.5656	valid_1's l2: 20.3165
[430]	training's l2: 18.5058	valid_1's l2: 20.3136
[440]	training's l2: 18.4486	valid_1's l2: 20.3131
[450]	training's l2: 18.39	valid_1's l2: 20.3099
[460]	training's l2: 18.3327	valid_1's l2: 20.3074
[470]	training's l2: 18.2734	valid_1's l2: 20.3047
[480]	training's l2: 18.216	valid_1's l2: 20.3019
[490]	training's l2: 18.1606	valid_1's l2: 20.302
[500]	training's l2: 18.1056	valid_1's l2: 20.3012
[510]	training's l2: 18.0498	valid_1's l2: 20.298
[520]	training's l2: 17.9975	valid_1's l2: 20.2959
[530]	training's l2: 17.946	valid_1's l2: 20.2929
[540]	training's l2: 17.8914	valid_1's l2: 20.2921
[550]	training's l2: 17.8383	valid_1's l2: 20.2899
[560]	training's l2: 17.7872	valid_1's l2: 20.2892
[570]	training's l2: 17.7354	valid_1's l2: 20.2865
[580]	training's l2: 17.6815	valid_1's l2: 20.2863
[590]	training's l2: 17.6284	valid_1's l2: 20.2828
[600]	training's l2: 17.5778	valid_1's l2: 20.2837
[610]	training's l2: 17.5273	valid_1's l2: 20.283
Early stopping, best iteration is:
[589]	training's l2: 17.6339	valid_1's l2: 20.2824
score1: 3.8395968879285953
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.240843 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.4828	valid_1's l2: 30.5693
[20]	training's l2: 29.4794	valid_1's l2: 28.5659
[30]	training's l2: 27.9287	valid_1's l2: 27.0136
[40]	training's l2: 26.7218	valid_1's l2: 25.8081
[50]	training's l2: 25.7737	valid_1's l2: 24.8654
[60]	training's l2: 25.0264	valid_1's l2: 24.1267
[70]	training's l2: 24.4323	valid_1's l2: 23.5419
[80]	training's l2: 23.9564	valid_1's l2: 23.0796
[90]	training's l2: 23.5745	valid_1's l2: 22.7108
[100]	training's l2: 23.263	valid_1's l2: 22.4115
[110]	training's l2: 23.0038	valid_1's l2: 22.1669
[120]	training's l2: 22.7845	valid_1's l2: 21.9641
[130]	training's l2: 22.5984	valid_1's l2: 21.7938
[140]	training's l2: 22.4422	valid_1's l2: 21.6554
[150]	training's l2: 22.3051	valid_1's l2: 21.5384
[160]	training's l2: 22.1852	valid_1's l2: 21.4361
[170]	training's l2: 22.0747	valid_1's l2: 21.3452
[180]	training's l2: 21.9772	valid_1's l2: 21.2664
[190]	training's l2: 21.8887	valid_1's l2: 21.1978
[200]	training's l2: 21.8078	valid_1's l2: 21.1355
[210]	training's l2: 21.7344	valid_1's l2: 21.0825
[220]	training's l2: 21.6684	valid_1's l2: 21.034
[230]	training's l2: 21.6062	valid_1's l2: 20.9898
[240]	training's l2: 21.5473	valid_1's l2: 20.951
[250]	training's l2: 21.4929	valid_1's l2: 20.9156
[260]	training's l2: 21.4402	valid_1's l2: 20.8821
[270]	training's l2: 21.3893	valid_1's l2: 20.8508
[280]	training's l2: 21.3425	valid_1's l2: 20.8229
[290]	training's l2: 21.298	valid_1's l2: 20.7983
[300]	training's l2: 21.2538	valid_1's l2: 20.7738
[310]	training's l2: 21.2127	valid_1's l2: 20.753
[320]	training's l2: 21.1722	valid_1's l2: 20.7319
[330]	training's l2: 21.1338	valid_1's l2: 20.7129
[340]	training's l2: 21.0966	valid_1's l2: 20.6952
[350]	training's l2: 21.0605	valid_1's l2: 20.6789
[360]	training's l2: 21.0245	valid_1's l2: 20.6624
[370]	training's l2: 20.9903	valid_1's l2: 20.6472
[380]	training's l2: 20.957	valid_1's l2: 20.6334
[390]	training's l2: 20.924	valid_1's l2: 20.6195
[400]	training's l2: 20.892	valid_1's l2: 20.6075
[410]	training's l2: 20.8604	valid_1's l2: 20.5957
[420]	training's l2: 20.8294	valid_1's l2: 20.5843
[430]	training's l2: 20.7991	valid_1's l2: 20.574
[440]	training's l2: 20.7696	valid_1's l2: 20.5649
[450]	training's l2: 20.7409	valid_1's l2: 20.5561
[460]	training's l2: 20.7119	valid_1's l2: 20.5461
[470]	training's l2: 20.6838	valid_1's l2: 20.5385
[480]	training's l2: 20.6552	valid_1's l2: 20.5296
[490]	training's l2: 20.6274	valid_1's l2: 20.5221
[500]	training's l2: 20.6004	valid_1's l2: 20.5139
[510]	training's l2: 20.5732	valid_1's l2: 20.5057
[520]	training's l2: 20.5459	valid_1's l2: 20.4984
[530]	training's l2: 20.5189	valid_1's l2: 20.4908
[540]	training's l2: 20.4921	valid_1's l2: 20.4831
[550]	training's l2: 20.4644	valid_1's l2: 20.474
[560]	training's l2: 20.4382	valid_1's l2: 20.4675
[570]	training's l2: 20.4118	valid_1's l2: 20.461
[580]	training's l2: 20.3855	valid_1's l2: 20.4541
[590]	training's l2: 20.36	valid_1's l2: 20.4485
[600]	training's l2: 20.3345	valid_1's l2: 20.4423
[610]	training's l2: 20.3094	valid_1's l2: 20.4362
[620]	training's l2: 20.2841	valid_1's l2: 20.4316
[630]	training's l2: 20.2592	valid_1's l2: 20.4253
[640]	training's l2: 20.2345	valid_1's l2: 20.42
[650]	training's l2: 20.2105	valid_1's l2: 20.4143
[660]	training's l2: 20.1861	valid_1's l2: 20.4096
[670]	training's l2: 20.1623	valid_1's l2: 20.4044
[680]	training's l2: 20.1393	valid_1's l2: 20.3998
[690]	training's l2: 20.1158	valid_1's l2: 20.3962
[700]	training's l2: 20.0926	valid_1's l2: 20.3921
[710]	training's l2: 20.0695	valid_1's l2: 20.3875
[720]	training's l2: 20.0466	valid_1's l2: 20.3826
[730]	training's l2: 20.0239	valid_1's l2: 20.3792
[740]	training's l2: 20.0002	valid_1's l2: 20.3742
[750]	training's l2: 19.9784	valid_1's l2: 20.3719
[760]	training's l2: 19.9556	valid_1's l2: 20.3683
[770]	training's l2: 19.9337	valid_1's l2: 20.3649
[780]	training's l2: 19.9112	valid_1's l2: 20.361
[790]	training's l2: 19.8894	valid_1's l2: 20.358
[800]	training's l2: 19.8677	valid_1's l2: 20.3546
[810]	training's l2: 19.8462	valid_1's l2: 20.3525
[820]	training's l2: 19.8251	valid_1's l2: 20.3505
[830]	training's l2: 19.8034	valid_1's l2: 20.3471
[840]	training's l2: 19.7816	valid_1's l2: 20.344
[850]	training's l2: 19.7606	valid_1's l2: 20.3409
[860]	training's l2: 19.7392	valid_1's l2: 20.3387
[870]	training's l2: 19.7187	valid_1's l2: 20.3368
[880]	training's l2: 19.6978	valid_1's l2: 20.3342
[890]	training's l2: 19.6775	valid_1's l2: 20.3314
[900]	training's l2: 19.6566	valid_1's l2: 20.3297
[910]	training's l2: 19.6361	valid_1's l2: 20.3272
[920]	training's l2: 19.6157	valid_1's l2: 20.3254
[930]	training's l2: 19.5955	valid_1's l2: 20.3227
[940]	training's l2: 19.5754	valid_1's l2: 20.3203
[950]	training's l2: 19.5549	valid_1's l2: 20.3186
[960]	training's l2: 19.535	valid_1's l2: 20.3168
[970]	training's l2: 19.5149	valid_1's l2: 20.3138
[980]	training's l2: 19.4946	valid_1's l2: 20.311
[990]	training's l2: 19.4744	valid_1's l2: 20.3092
[1000]	training's l2: 19.4547	valid_1's l2: 20.3066
[1010]	training's l2: 19.4349	valid_1's l2: 20.3053
[1020]	training's l2: 19.4165	valid_1's l2: 20.3036
[1030]	training's l2: 19.397	valid_1's l2: 20.3023
[1040]	training's l2: 19.3781	valid_1's l2: 20.3016
[1050]	training's l2: 19.3597	valid_1's l2: 20.3
[1060]	training's l2: 19.3414	valid_1's l2: 20.2987
[1070]	training's l2: 19.3228	valid_1's l2: 20.2976
[1080]	training's l2: 19.3049	valid_1's l2: 20.2969
[1090]	training's l2: 19.2864	valid_1's l2: 20.2961
[1100]	training's l2: 19.2679	valid_1's l2: 20.2946
[1110]	training's l2: 19.2493	valid_1's l2: 20.2937
[1120]	training's l2: 19.2318	valid_1's l2: 20.2931
[1130]	training's l2: 19.2133	valid_1's l2: 20.2927
[1140]	training's l2: 19.1955	valid_1's l2: 20.2918
[1150]	training's l2: 19.1777	valid_1's l2: 20.2911
[1160]	training's l2: 19.1601	valid_1's l2: 20.2906
[1170]	training's l2: 19.142	valid_1's l2: 20.2894
[1180]	training's l2: 19.1241	valid_1's l2: 20.2884
[1190]	training's l2: 19.1065	valid_1's l2: 20.2869
[1200]	training's l2: 19.0893	valid_1's l2: 20.2851
[1210]	training's l2: 19.0722	valid_1's l2: 20.2845
[1220]	training's l2: 19.0553	valid_1's l2: 20.2835
[1230]	training's l2: 19.0363	valid_1's l2: 20.2821
[1240]	training's l2: 19.0195	valid_1's l2: 20.2812
[1250]	training's l2: 19.0025	valid_1's l2: 20.2795
[1260]	training's l2: 18.9867	valid_1's l2: 20.2781
[1270]	training's l2: 18.9693	valid_1's l2: 20.2774
[1280]	training's l2: 18.9518	valid_1's l2: 20.2771
[1290]	training's l2: 18.9347	valid_1's l2: 20.2762
[1300]	training's l2: 18.9176	valid_1's l2: 20.2759
[1310]	training's l2: 18.901	valid_1's l2: 20.2743
[1320]	training's l2: 18.8841	valid_1's l2: 20.2734
[1330]	training's l2: 18.8684	valid_1's l2: 20.2725
[1340]	training's l2: 18.8522	valid_1's l2: 20.2713
[1350]	training's l2: 18.8356	valid_1's l2: 20.2711
[1360]	training's l2: 18.8191	valid_1's l2: 20.2708
[1370]	training's l2: 18.8025	valid_1's l2: 20.2701
[1380]	training's l2: 18.786	valid_1's l2: 20.2695
[1390]	training's l2: 18.7711	valid_1's l2: 20.2686
[1400]	training's l2: 18.7549	valid_1's l2: 20.2679
[1410]	training's l2: 18.7385	valid_1's l2: 20.2672
[1420]	training's l2: 18.723	valid_1's l2: 20.2661
[1430]	training's l2: 18.7066	valid_1's l2: 20.266
[1440]	training's l2: 18.6901	valid_1's l2: 20.2649
[1450]	training's l2: 18.6748	valid_1's l2: 20.2642
[1460]	training's l2: 18.6585	valid_1's l2: 20.2635
[1470]	training's l2: 18.6438	valid_1's l2: 20.2628
[1480]	training's l2: 18.6281	valid_1's l2: 20.2624
[1490]	training's l2: 18.6122	valid_1's l2: 20.2614
[1500]	training's l2: 18.5967	valid_1's l2: 20.2609
[1510]	training's l2: 18.5815	valid_1's l2: 20.2602
[1520]	training's l2: 18.5659	valid_1's l2: 20.2596
[1530]	training's l2: 18.5516	valid_1's l2: 20.2591
[1540]	training's l2: 18.5366	valid_1's l2: 20.2586
[1550]	training's l2: 18.5221	valid_1's l2: 20.2579
[1560]	training's l2: 18.5068	valid_1's l2: 20.2571
[1570]	training's l2: 18.4911	valid_1's l2: 20.2571
[1580]	training's l2: 18.4771	valid_1's l2: 20.2566
[1590]	training's l2: 18.4609	valid_1's l2: 20.2558
[1600]	training's l2: 18.4461	valid_1's l2: 20.2555
[1610]	training's l2: 18.4306	valid_1's l2: 20.2547
[1620]	training's l2: 18.4163	valid_1's l2: 20.255
[1630]	training's l2: 18.4016	valid_1's l2: 20.2544
[1640]	training's l2: 18.3861	valid_1's l2: 20.2546
[1650]	training's l2: 18.3711	valid_1's l2: 20.254
[1660]	training's l2: 18.3565	valid_1's l2: 20.2543
[1670]	training's l2: 18.3412	valid_1's l2: 20.2546
[1680]	training's l2: 18.3261	valid_1's l2: 20.2539
Early stopping, best iteration is:
[1652]	training's l2: 18.3683	valid_1's l2: 20.2537
score1: 3.839255179796616
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290209 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.5833	valid_1's l2: 29.6923
[20]	training's l2: 28.1287	valid_1's l2: 27.2586
[30]	training's l2: 26.3912	valid_1's l2: 25.5497
[40]	training's l2: 25.1435	valid_1's l2: 24.3319
[50]	training's l2: 24.2372	valid_1's l2: 23.4597
[60]	training's l2: 23.5652	valid_1's l2: 22.8257
[70]	training's l2: 23.0613	valid_1's l2: 22.3651
[80]	training's l2: 22.6781	valid_1's l2: 22.0233
[90]	training's l2: 22.372	valid_1's l2: 21.7641
[100]	training's l2: 22.1222	valid_1's l2: 21.5632
[110]	training's l2: 21.912	valid_1's l2: 21.4021
[120]	training's l2: 21.7337	valid_1's l2: 21.2755
[130]	training's l2: 21.576	valid_1's l2: 21.1678
[140]	training's l2: 21.4357	valid_1's l2: 21.0759
[150]	training's l2: 21.3137	valid_1's l2: 21.0012
[160]	training's l2: 21.2009	valid_1's l2: 20.9359
[170]	training's l2: 21.097	valid_1's l2: 20.88
[180]	training's l2: 20.9969	valid_1's l2: 20.8265
[190]	training's l2: 20.9033	valid_1's l2: 20.7832
[200]	training's l2: 20.8146	valid_1's l2: 20.7448
[210]	training's l2: 20.7321	valid_1's l2: 20.7129
[220]	training's l2: 20.6545	valid_1's l2: 20.685
[230]	training's l2: 20.5787	valid_1's l2: 20.6578
[240]	training's l2: 20.5077	valid_1's l2: 20.6373
[250]	training's l2: 20.4365	valid_1's l2: 20.6148
[260]	training's l2: 20.3681	valid_1's l2: 20.5951
[270]	training's l2: 20.3015	valid_1's l2: 20.5769
[280]	training's l2: 20.2371	valid_1's l2: 20.5602
[290]	training's l2: 20.1738	valid_1's l2: 20.5443
[300]	training's l2: 20.1131	valid_1's l2: 20.5297
[310]	training's l2: 20.0513	valid_1's l2: 20.517
[320]	training's l2: 19.9924	valid_1's l2: 20.5056
[330]	training's l2: 19.9333	valid_1's l2: 20.4938
[340]	training's l2: 19.8755	valid_1's l2: 20.4817
[350]	training's l2: 19.8181	valid_1's l2: 20.4702
[360]	training's l2: 19.7606	valid_1's l2: 20.4602
[370]	training's l2: 19.7037	valid_1's l2: 20.4494
[380]	training's l2: 19.6456	valid_1's l2: 20.438
[390]	training's l2: 19.5912	valid_1's l2: 20.429
[400]	training's l2: 19.5359	valid_1's l2: 20.4184
[410]	training's l2: 19.4808	valid_1's l2: 20.4083
[420]	training's l2: 19.4284	valid_1's l2: 20.4012
[430]	training's l2: 19.3757	valid_1's l2: 20.3937
[440]	training's l2: 19.3236	valid_1's l2: 20.3864
[450]	training's l2: 19.2731	valid_1's l2: 20.3798
[460]	training's l2: 19.2227	valid_1's l2: 20.3743
[470]	training's l2: 19.172	valid_1's l2: 20.368
[480]	training's l2: 19.1221	valid_1's l2: 20.3629
[490]	training's l2: 19.0734	valid_1's l2: 20.3578
[500]	training's l2: 19.0237	valid_1's l2: 20.3515
[510]	training's l2: 18.9743	valid_1's l2: 20.3471
[520]	training's l2: 18.9256	valid_1's l2: 20.3447
[530]	training's l2: 18.8777	valid_1's l2: 20.3404
[540]	training's l2: 18.8301	valid_1's l2: 20.3361
[550]	training's l2: 18.7834	valid_1's l2: 20.3296
[560]	training's l2: 18.7359	valid_1's l2: 20.3256
[570]	training's l2: 18.6901	valid_1's l2: 20.3218
[580]	training's l2: 18.6438	valid_1's l2: 20.3163
[590]	training's l2: 18.5974	valid_1's l2: 20.3113
[600]	training's l2: 18.5518	valid_1's l2: 20.3069
[610]	training's l2: 18.5067	valid_1's l2: 20.3024
[620]	training's l2: 18.4621	valid_1's l2: 20.2974
[630]	training's l2: 18.4174	valid_1's l2: 20.294
[640]	training's l2: 18.3729	valid_1's l2: 20.2902
[650]	training's l2: 18.3294	valid_1's l2: 20.2861
[660]	training's l2: 18.2863	valid_1's l2: 20.2842
[670]	training's l2: 18.2443	valid_1's l2: 20.2821
[680]	training's l2: 18.2013	valid_1's l2: 20.2796
[690]	training's l2: 18.1593	valid_1's l2: 20.2778
[700]	training's l2: 18.1175	valid_1's l2: 20.2757
[710]	training's l2: 18.0784	valid_1's l2: 20.2724
[720]	training's l2: 18.0398	valid_1's l2: 20.2713
[730]	training's l2: 18.0009	valid_1's l2: 20.2716
[740]	training's l2: 17.9612	valid_1's l2: 20.2707
[750]	training's l2: 17.9239	valid_1's l2: 20.2693
[760]	training's l2: 17.8845	valid_1's l2: 20.2694
[770]	training's l2: 17.8456	valid_1's l2: 20.2681
[780]	training's l2: 17.8071	valid_1's l2: 20.2664
[790]	training's l2: 17.7695	valid_1's l2: 20.2648
[800]	training's l2: 17.7308	valid_1's l2: 20.2629
[810]	training's l2: 17.6922	valid_1's l2: 20.2633
[820]	training's l2: 17.6545	valid_1's l2: 20.2621
[830]	training's l2: 17.6174	valid_1's l2: 20.2615
[840]	training's l2: 17.5814	valid_1's l2: 20.2604
[850]	training's l2: 17.5448	valid_1's l2: 20.2597
[860]	training's l2: 17.5098	valid_1's l2: 20.2593
[870]	training's l2: 17.4732	valid_1's l2: 20.2584
[880]	training's l2: 17.4363	valid_1's l2: 20.2576
[890]	training's l2: 17.4002	valid_1's l2: 20.2576
[900]	training's l2: 17.3652	valid_1's l2: 20.2566
[910]	training's l2: 17.3281	valid_1's l2: 20.2551
[920]	training's l2: 17.2922	valid_1's l2: 20.2542
[930]	training's l2: 17.2557	valid_1's l2: 20.2537
[940]	training's l2: 17.2202	valid_1's l2: 20.2531
[950]	training's l2: 17.1847	valid_1's l2: 20.2527
[960]	training's l2: 17.15	valid_1's l2: 20.2517
[970]	training's l2: 17.1179	valid_1's l2: 20.2524
[980]	training's l2: 17.0819	valid_1's l2: 20.2509
[990]	training's l2: 17.0465	valid_1's l2: 20.2504
[1000]	training's l2: 17.0136	valid_1's l2: 20.2495
[1010]	training's l2: 16.9799	valid_1's l2: 20.2491
[1020]	training's l2: 16.946	valid_1's l2: 20.2488
[1030]	training's l2: 16.9133	valid_1's l2: 20.2496
[1040]	training's l2: 16.8801	valid_1's l2: 20.2496
Early stopping, best iteration is:
[1018]	training's l2: 16.9524	valid_1's l2: 20.2484
score1: 3.835901779348402
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242760 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.1907	valid_1's l2: 29.2835
[20]	training's l2: 27.6186	valid_1's l2: 26.716
[30]	training's l2: 25.8912	valid_1's l2: 24.9988
[40]	training's l2: 24.7126	valid_1's l2: 23.8373
[50]	training's l2: 23.8899	valid_1's l2: 23.0382
[60]	training's l2: 23.3073	valid_1's l2: 22.4814
[70]	training's l2: 22.8839	valid_1's l2: 22.0862
[80]	training's l2: 22.5566	valid_1's l2: 21.7908
[90]	training's l2: 22.3037	valid_1's l2: 21.5747
[100]	training's l2: 22.1003	valid_1's l2: 21.4063
[110]	training's l2: 21.9265	valid_1's l2: 21.2663
[120]	training's l2: 21.7796	valid_1's l2: 21.1575
[130]	training's l2: 21.6547	valid_1's l2: 21.0685
[140]	training's l2: 21.5449	valid_1's l2: 20.9938
[150]	training's l2: 21.4445	valid_1's l2: 20.9293
[160]	training's l2: 21.3538	valid_1's l2: 20.8754
[170]	training's l2: 21.2686	valid_1's l2: 20.8252
[180]	training's l2: 21.1907	valid_1's l2: 20.7841
[190]	training's l2: 21.1175	valid_1's l2: 20.7466
[200]	training's l2: 21.0494	valid_1's l2: 20.7161
[210]	training's l2: 20.9837	valid_1's l2: 20.6877
[220]	training's l2: 20.9214	valid_1's l2: 20.6616
[230]	training's l2: 20.8625	valid_1's l2: 20.6374
[240]	training's l2: 20.8037	valid_1's l2: 20.614
[250]	training's l2: 20.7468	valid_1's l2: 20.5955
[260]	training's l2: 20.6935	valid_1's l2: 20.5787
[270]	training's l2: 20.6401	valid_1's l2: 20.5624
[280]	training's l2: 20.5879	valid_1's l2: 20.5463
[290]	training's l2: 20.538	valid_1's l2: 20.5333
[300]	training's l2: 20.4896	valid_1's l2: 20.5211
[310]	training's l2: 20.4413	valid_1's l2: 20.51
[320]	training's l2: 20.3939	valid_1's l2: 20.4984
[330]	training's l2: 20.3462	valid_1's l2: 20.486
[340]	training's l2: 20.2982	valid_1's l2: 20.4741
[350]	training's l2: 20.2521	valid_1's l2: 20.4615
[360]	training's l2: 20.2064	valid_1's l2: 20.4519
[370]	training's l2: 20.1606	valid_1's l2: 20.4402
[380]	training's l2: 20.1159	valid_1's l2: 20.4317
[390]	training's l2: 20.0722	valid_1's l2: 20.4224
[400]	training's l2: 20.0297	valid_1's l2: 20.4151
[410]	training's l2: 19.9873	valid_1's l2: 20.4072
[420]	training's l2: 19.9463	valid_1's l2: 20.3998
[430]	training's l2: 19.9042	valid_1's l2: 20.392
[440]	training's l2: 19.8635	valid_1's l2: 20.3859
[450]	training's l2: 19.8227	valid_1's l2: 20.3783
[460]	training's l2: 19.7821	valid_1's l2: 20.3722
[470]	training's l2: 19.7424	valid_1's l2: 20.3675
[480]	training's l2: 19.7026	valid_1's l2: 20.3647
[490]	training's l2: 19.6634	valid_1's l2: 20.3584
[500]	training's l2: 19.6245	valid_1's l2: 20.3551
[510]	training's l2: 19.5856	valid_1's l2: 20.35
[520]	training's l2: 19.5477	valid_1's l2: 20.346
[530]	training's l2: 19.5105	valid_1's l2: 20.3423
[540]	training's l2: 19.4734	valid_1's l2: 20.3383
[550]	training's l2: 19.4354	valid_1's l2: 20.3355
[560]	training's l2: 19.3983	valid_1's l2: 20.3311
[570]	training's l2: 19.3629	valid_1's l2: 20.3283
[580]	training's l2: 19.3266	valid_1's l2: 20.324
[590]	training's l2: 19.2894	valid_1's l2: 20.3203
[600]	training's l2: 19.2545	valid_1's l2: 20.3163
[610]	training's l2: 19.2201	valid_1's l2: 20.3137
[620]	training's l2: 19.1851	valid_1's l2: 20.3112
[630]	training's l2: 19.1506	valid_1's l2: 20.3094
[640]	training's l2: 19.1171	valid_1's l2: 20.3069
[650]	training's l2: 19.0831	valid_1's l2: 20.3054
[660]	training's l2: 19.0503	valid_1's l2: 20.3043
[670]	training's l2: 19.0181	valid_1's l2: 20.3012
[680]	training's l2: 18.9862	valid_1's l2: 20.2997
[690]	training's l2: 18.9541	valid_1's l2: 20.2971
[700]	training's l2: 18.9218	valid_1's l2: 20.2951
[710]	training's l2: 18.8894	valid_1's l2: 20.2935
[720]	training's l2: 18.8567	valid_1's l2: 20.2905
[730]	training's l2: 18.8274	valid_1's l2: 20.2905
[740]	training's l2: 18.7963	valid_1's l2: 20.2889
[750]	training's l2: 18.7654	valid_1's l2: 20.2883
[760]	training's l2: 18.7345	valid_1's l2: 20.2862
[770]	training's l2: 18.7032	valid_1's l2: 20.2836
[780]	training's l2: 18.6722	valid_1's l2: 20.2818
[790]	training's l2: 18.6418	valid_1's l2: 20.281
[800]	training's l2: 18.6111	valid_1's l2: 20.2805
[810]	training's l2: 18.5807	valid_1's l2: 20.2803
[820]	training's l2: 18.5516	valid_1's l2: 20.278
[830]	training's l2: 18.5216	valid_1's l2: 20.2765
[840]	training's l2: 18.4932	valid_1's l2: 20.2756
[850]	training's l2: 18.4638	valid_1's l2: 20.2726
[860]	training's l2: 18.4341	valid_1's l2: 20.2707
[870]	training's l2: 18.4061	valid_1's l2: 20.2706
[880]	training's l2: 18.3773	valid_1's l2: 20.2693
[890]	training's l2: 18.348	valid_1's l2: 20.2685
[900]	training's l2: 18.3201	valid_1's l2: 20.2679
[910]	training's l2: 18.2912	valid_1's l2: 20.2669
[920]	training's l2: 18.2637	valid_1's l2: 20.2669
[930]	training's l2: 18.2367	valid_1's l2: 20.267
[940]	training's l2: 18.2086	valid_1's l2: 20.2658
[950]	training's l2: 18.181	valid_1's l2: 20.2649
[960]	training's l2: 18.1527	valid_1's l2: 20.2646
[970]	training's l2: 18.1249	valid_1's l2: 20.2637
[980]	training's l2: 18.098	valid_1's l2: 20.2631
[990]	training's l2: 18.0691	valid_1's l2: 20.2632
[1000]	training's l2: 18.0428	valid_1's l2: 20.2628
[1010]	training's l2: 18.0169	valid_1's l2: 20.2619
[1020]	training's l2: 17.989	valid_1's l2: 20.2613
[1030]	training's l2: 17.961	valid_1's l2: 20.2606
[1040]	training's l2: 17.9348	valid_1's l2: 20.2599
[1050]	training's l2: 17.9081	valid_1's l2: 20.2591
[1060]	training's l2: 17.8834	valid_1's l2: 20.2576
[1070]	training's l2: 17.8556	valid_1's l2: 20.2564
[1080]	training's l2: 17.8288	valid_1's l2: 20.2572
[1090]	training's l2: 17.8019	valid_1's l2: 20.2576
[1100]	training's l2: 17.7755	valid_1's l2: 20.2581
Early stopping, best iteration is:
[1070]	training's l2: 17.8556	valid_1's l2: 20.2564
score1: 3.8371106478019303
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.240640 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.4953	valid_1's l2: 30.5887
[20]	training's l2: 29.4882	valid_1's l2: 28.5919
[30]	training's l2: 27.9264	valid_1's l2: 27.0402
[40]	training's l2: 26.7025	valid_1's l2: 25.8285
[50]	training's l2: 25.7387	valid_1's l2: 24.8779
[60]	training's l2: 24.9721	valid_1's l2: 24.1293
[70]	training's l2: 24.3617	valid_1's l2: 23.5364
[80]	training's l2: 23.8679	valid_1's l2: 23.0632
[90]	training's l2: 23.4671	valid_1's l2: 22.6847
[100]	training's l2: 23.1418	valid_1's l2: 22.3821
[110]	training's l2: 22.8723	valid_1's l2: 22.1356
[120]	training's l2: 22.6433	valid_1's l2: 21.9299
[130]	training's l2: 22.4465	valid_1's l2: 21.7576
[140]	training's l2: 22.2779	valid_1's l2: 21.6157
[150]	training's l2: 22.1315	valid_1's l2: 21.4959
[160]	training's l2: 22.0013	valid_1's l2: 21.3933
[170]	training's l2: 21.8836	valid_1's l2: 21.3041
[180]	training's l2: 21.7753	valid_1's l2: 21.2249
[190]	training's l2: 21.6776	valid_1's l2: 21.1558
[200]	training's l2: 21.59	valid_1's l2: 21.0951
[210]	training's l2: 21.5067	valid_1's l2: 21.0389
[220]	training's l2: 21.4317	valid_1's l2: 20.9908
[230]	training's l2: 21.3621	valid_1's l2: 20.9482
[240]	training's l2: 21.295	valid_1's l2: 20.909
[250]	training's l2: 21.2296	valid_1's l2: 20.8721
[260]	training's l2: 21.1682	valid_1's l2: 20.839
[270]	training's l2: 21.1096	valid_1's l2: 20.8101
[280]	training's l2: 21.0541	valid_1's l2: 20.7831
[290]	training's l2: 20.9999	valid_1's l2: 20.7568
[300]	training's l2: 20.949	valid_1's l2: 20.7344
[310]	training's l2: 20.9008	valid_1's l2: 20.7134
[320]	training's l2: 20.8524	valid_1's l2: 20.692
[330]	training's l2: 20.8061	valid_1's l2: 20.6747
[340]	training's l2: 20.7607	valid_1's l2: 20.6576
[350]	training's l2: 20.7164	valid_1's l2: 20.6414
[360]	training's l2: 20.6741	valid_1's l2: 20.6263
[370]	training's l2: 20.6322	valid_1's l2: 20.6128
[380]	training's l2: 20.5903	valid_1's l2: 20.5983
[390]	training's l2: 20.5498	valid_1's l2: 20.5853
[400]	training's l2: 20.5103	valid_1's l2: 20.573
[410]	training's l2: 20.4713	valid_1's l2: 20.5609
[420]	training's l2: 20.4326	valid_1's l2: 20.5507
[430]	training's l2: 20.3949	valid_1's l2: 20.5411
[440]	training's l2: 20.3579	valid_1's l2: 20.532
[450]	training's l2: 20.3213	valid_1's l2: 20.5229
[460]	training's l2: 20.2849	valid_1's l2: 20.5148
[470]	training's l2: 20.2487	valid_1's l2: 20.5052
[480]	training's l2: 20.2132	valid_1's l2: 20.4975
[490]	training's l2: 20.1781	valid_1's l2: 20.4912
[500]	training's l2: 20.1431	valid_1's l2: 20.483
[510]	training's l2: 20.109	valid_1's l2: 20.4759
[520]	training's l2: 20.0739	valid_1's l2: 20.4681
[530]	training's l2: 20.0392	valid_1's l2: 20.4612
[540]	training's l2: 20.0047	valid_1's l2: 20.4551
[550]	training's l2: 19.9704	valid_1's l2: 20.4473
[560]	training's l2: 19.9373	valid_1's l2: 20.4404
[570]	training's l2: 19.9038	valid_1's l2: 20.4337
[580]	training's l2: 19.8711	valid_1's l2: 20.4271
[590]	training's l2: 19.8385	valid_1's l2: 20.4205
[600]	training's l2: 19.8063	valid_1's l2: 20.4141
[610]	training's l2: 19.775	valid_1's l2: 20.409
[620]	training's l2: 19.7428	valid_1's l2: 20.4033
[630]	training's l2: 19.7114	valid_1's l2: 20.3981
[640]	training's l2: 19.6805	valid_1's l2: 20.3925
[650]	training's l2: 19.6501	valid_1's l2: 20.3873
[660]	training's l2: 19.6189	valid_1's l2: 20.3812
[670]	training's l2: 19.5883	valid_1's l2: 20.3773
[680]	training's l2: 19.5583	valid_1's l2: 20.3726
[690]	training's l2: 19.5282	valid_1's l2: 20.3685
[700]	training's l2: 19.4981	valid_1's l2: 20.3636
[710]	training's l2: 19.469	valid_1's l2: 20.3586
[720]	training's l2: 19.439	valid_1's l2: 20.3546
[730]	training's l2: 19.41	valid_1's l2: 20.3515
[740]	training's l2: 19.381	valid_1's l2: 20.3493
[750]	training's l2: 19.3521	valid_1's l2: 20.3461
[760]	training's l2: 19.3233	valid_1's l2: 20.342
[770]	training's l2: 19.2949	valid_1's l2: 20.3384
[780]	training's l2: 19.2664	valid_1's l2: 20.335
[790]	training's l2: 19.2386	valid_1's l2: 20.3323
[800]	training's l2: 19.2108	valid_1's l2: 20.3289
[810]	training's l2: 19.1827	valid_1's l2: 20.3254
[820]	training's l2: 19.1544	valid_1's l2: 20.323
[830]	training's l2: 19.1264	valid_1's l2: 20.3195
[840]	training's l2: 19.0995	valid_1's l2: 20.3173
[850]	training's l2: 19.0722	valid_1's l2: 20.3142
[860]	training's l2: 19.0447	valid_1's l2: 20.3121
[870]	training's l2: 19.0175	valid_1's l2: 20.3093
[880]	training's l2: 18.9904	valid_1's l2: 20.3068
[890]	training's l2: 18.9634	valid_1's l2: 20.3049
[900]	training's l2: 18.9365	valid_1's l2: 20.3028
[910]	training's l2: 18.9098	valid_1's l2: 20.3006
[920]	training's l2: 18.8841	valid_1's l2: 20.2983
[930]	training's l2: 18.8582	valid_1's l2: 20.2966
[940]	training's l2: 18.8318	valid_1's l2: 20.2941
[950]	training's l2: 18.806	valid_1's l2: 20.2917
[960]	training's l2: 18.7801	valid_1's l2: 20.2894
[970]	training's l2: 18.754	valid_1's l2: 20.288
[980]	training's l2: 18.7276	valid_1's l2: 20.2861
[990]	training's l2: 18.7021	valid_1's l2: 20.2844
[1000]	training's l2: 18.6764	valid_1's l2: 20.2838
[1010]	training's l2: 18.6519	valid_1's l2: 20.2821
[1020]	training's l2: 18.6272	valid_1's l2: 20.2806
[1030]	training's l2: 18.6024	valid_1's l2: 20.2799
[1040]	training's l2: 18.5782	valid_1's l2: 20.2785
[1050]	training's l2: 18.5537	valid_1's l2: 20.2775
[1060]	training's l2: 18.5293	valid_1's l2: 20.276
[1070]	training's l2: 18.5055	valid_1's l2: 20.2749
[1080]	training's l2: 18.4815	valid_1's l2: 20.2733
[1090]	training's l2: 18.4587	valid_1's l2: 20.2736
[1100]	training's l2: 18.4358	valid_1's l2: 20.2725
[1110]	training's l2: 18.4119	valid_1's l2: 20.2712
[1120]	training's l2: 18.3892	valid_1's l2: 20.2705
[1130]	training's l2: 18.366	valid_1's l2: 20.2694
[1140]	training's l2: 18.3427	valid_1's l2: 20.2676
[1150]	training's l2: 18.3194	valid_1's l2: 20.2662
[1160]	training's l2: 18.297	valid_1's l2: 20.2656
[1170]	training's l2: 18.2746	valid_1's l2: 20.2655
[1180]	training's l2: 18.2523	valid_1's l2: 20.2642
[1190]	training's l2: 18.2299	valid_1's l2: 20.263
[1200]	training's l2: 18.2082	valid_1's l2: 20.2627
[1210]	training's l2: 18.1861	valid_1's l2: 20.262
[1220]	training's l2: 18.1636	valid_1's l2: 20.261
[1230]	training's l2: 18.1414	valid_1's l2: 20.26
[1240]	training's l2: 18.1193	valid_1's l2: 20.2601
[1250]	training's l2: 18.0977	valid_1's l2: 20.2596
[1260]	training's l2: 18.0762	valid_1's l2: 20.2588
[1270]	training's l2: 18.0549	valid_1's l2: 20.258
[1280]	training's l2: 18.0336	valid_1's l2: 20.2578
[1290]	training's l2: 18.0113	valid_1's l2: 20.2561
[1300]	training's l2: 17.9907	valid_1's l2: 20.2557
[1310]	training's l2: 17.9697	valid_1's l2: 20.2553
[1320]	training's l2: 17.9484	valid_1's l2: 20.2542
[1330]	training's l2: 17.9277	valid_1's l2: 20.2537
[1340]	training's l2: 17.9068	valid_1's l2: 20.2536
[1350]	training's l2: 17.8846	valid_1's l2: 20.2532
[1360]	training's l2: 17.865	valid_1's l2: 20.2532
[1370]	training's l2: 17.8438	valid_1's l2: 20.2521
[1380]	training's l2: 17.8227	valid_1's l2: 20.2523
[1390]	training's l2: 17.8024	valid_1's l2: 20.2512
[1400]	training's l2: 17.7813	valid_1's l2: 20.2518
[1410]	training's l2: 17.7602	valid_1's l2: 20.2512
[1420]	training's l2: 17.739	valid_1's l2: 20.2505
[1430]	training's l2: 17.7188	valid_1's l2: 20.2503
[1440]	training's l2: 17.6986	valid_1's l2: 20.2501
[1450]	training's l2: 17.6798	valid_1's l2: 20.2492
[1460]	training's l2: 17.659	valid_1's l2: 20.2491
[1470]	training's l2: 17.6378	valid_1's l2: 20.2478
[1480]	training's l2: 17.6184	valid_1's l2: 20.2474
[1490]	training's l2: 17.5982	valid_1's l2: 20.247
[1500]	training's l2: 17.5775	valid_1's l2: 20.2466
[1510]	training's l2: 17.5593	valid_1's l2: 20.246
[1520]	training's l2: 17.5397	valid_1's l2: 20.2455
[1530]	training's l2: 17.5197	valid_1's l2: 20.2448
[1540]	training's l2: 17.4997	valid_1's l2: 20.2445
[1550]	training's l2: 17.4782	valid_1's l2: 20.244
[1560]	training's l2: 17.4581	valid_1's l2: 20.2437
[1570]	training's l2: 17.4381	valid_1's l2: 20.2429
[1580]	training's l2: 17.4196	valid_1's l2: 20.2428
[1590]	training's l2: 17.4001	valid_1's l2: 20.2425
[1600]	training's l2: 17.3813	valid_1's l2: 20.242
[1610]	training's l2: 17.3616	valid_1's l2: 20.2421
[1620]	training's l2: 17.3416	valid_1's l2: 20.2416
[1630]	training's l2: 17.3229	valid_1's l2: 20.2422
[1640]	training's l2: 17.3021	valid_1's l2: 20.2427
Early stopping, best iteration is:
[1619]	training's l2: 17.3436	valid_1's l2: 20.2416
score1: 3.834511654300707
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254886 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.6079	valid_1's l2: 30.7021
[20]	training's l2: 29.6645	valid_1's l2: 28.7679
[30]	training's l2: 28.1334	valid_1's l2: 27.2458
[40]	training's l2: 26.9203	valid_1's l2: 26.0435
[50]	training's l2: 25.9558	valid_1's l2: 25.0923
[60]	training's l2: 25.1795	valid_1's l2: 24.3327
[70]	training's l2: 24.5542	valid_1's l2: 23.7247
[80]	training's l2: 24.0465	valid_1's l2: 23.235
[90]	training's l2: 23.6304	valid_1's l2: 22.8402
[100]	training's l2: 23.2892	valid_1's l2: 22.521
[110]	training's l2: 23.0076	valid_1's l2: 22.2609
[120]	training's l2: 22.7707	valid_1's l2: 22.0466
[130]	training's l2: 22.5638	valid_1's l2: 21.8645
[140]	training's l2: 22.3863	valid_1's l2: 21.7131
[150]	training's l2: 22.2318	valid_1's l2: 21.5843
[160]	training's l2: 22.0948	valid_1's l2: 21.4749
[170]	training's l2: 21.9749	valid_1's l2: 21.3806
[180]	training's l2: 21.8645	valid_1's l2: 21.2987
[190]	training's l2: 21.7624	valid_1's l2: 21.2236
[200]	training's l2: 21.6699	valid_1's l2: 21.1585
[210]	training's l2: 21.586	valid_1's l2: 21.1009
[220]	training's l2: 21.5074	valid_1's l2: 21.0492
[230]	training's l2: 21.4354	valid_1's l2: 21.0033
[240]	training's l2: 21.3681	valid_1's l2: 20.963
[250]	training's l2: 21.3037	valid_1's l2: 20.9242
[260]	training's l2: 21.2402	valid_1's l2: 20.8868
[270]	training's l2: 21.1809	valid_1's l2: 20.8546
[280]	training's l2: 21.1245	valid_1's l2: 20.8255
[290]	training's l2: 21.0696	valid_1's l2: 20.7987
[300]	training's l2: 21.0172	valid_1's l2: 20.7728
[310]	training's l2: 20.968	valid_1's l2: 20.7513
[320]	training's l2: 20.9197	valid_1's l2: 20.7302
[330]	training's l2: 20.8734	valid_1's l2: 20.7103
[340]	training's l2: 20.8285	valid_1's l2: 20.6919
[350]	training's l2: 20.7841	valid_1's l2: 20.674
[360]	training's l2: 20.7413	valid_1's l2: 20.6577
[370]	training's l2: 20.699	valid_1's l2: 20.6418
[380]	training's l2: 20.6579	valid_1's l2: 20.6272
[390]	training's l2: 20.6181	valid_1's l2: 20.6142
[400]	training's l2: 20.5789	valid_1's l2: 20.6023
[410]	training's l2: 20.54	valid_1's l2: 20.5892
[420]	training's l2: 20.5025	valid_1's l2: 20.5774
[430]	training's l2: 20.4647	valid_1's l2: 20.5662
[440]	training's l2: 20.4279	valid_1's l2: 20.5559
[450]	training's l2: 20.3918	valid_1's l2: 20.547
[460]	training's l2: 20.356	valid_1's l2: 20.5373
[470]	training's l2: 20.3209	valid_1's l2: 20.5292
[480]	training's l2: 20.2863	valid_1's l2: 20.5215
[490]	training's l2: 20.2509	valid_1's l2: 20.5128
[500]	training's l2: 20.2171	valid_1's l2: 20.5047
[510]	training's l2: 20.1835	valid_1's l2: 20.4976
[520]	training's l2: 20.1498	valid_1's l2: 20.4903
[530]	training's l2: 20.1164	valid_1's l2: 20.4835
[540]	training's l2: 20.0835	valid_1's l2: 20.476
[550]	training's l2: 20.0506	valid_1's l2: 20.4681
[560]	training's l2: 20.0179	valid_1's l2: 20.4617
[570]	training's l2: 19.9852	valid_1's l2: 20.455
[580]	training's l2: 19.9532	valid_1's l2: 20.4486
[590]	training's l2: 19.921	valid_1's l2: 20.4419
[600]	training's l2: 19.8888	valid_1's l2: 20.4348
[610]	training's l2: 19.8578	valid_1's l2: 20.4288
[620]	training's l2: 19.8266	valid_1's l2: 20.4234
[630]	training's l2: 19.7961	valid_1's l2: 20.418
[640]	training's l2: 19.7653	valid_1's l2: 20.412
[650]	training's l2: 19.7355	valid_1's l2: 20.4072
[660]	training's l2: 19.7054	valid_1's l2: 20.4029
[670]	training's l2: 19.675	valid_1's l2: 20.398
[680]	training's l2: 19.6454	valid_1's l2: 20.3932
[690]	training's l2: 19.6155	valid_1's l2: 20.3887
[700]	training's l2: 19.5862	valid_1's l2: 20.3846
[710]	training's l2: 19.5575	valid_1's l2: 20.3804
[720]	training's l2: 19.5284	valid_1's l2: 20.3767
[730]	training's l2: 19.4995	valid_1's l2: 20.3726
[740]	training's l2: 19.4715	valid_1's l2: 20.3686
[750]	training's l2: 19.4435	valid_1's l2: 20.3649
[760]	training's l2: 19.4153	valid_1's l2: 20.3613
[770]	training's l2: 19.3878	valid_1's l2: 20.3582
[780]	training's l2: 19.3603	valid_1's l2: 20.3547
[790]	training's l2: 19.3327	valid_1's l2: 20.3511
[800]	training's l2: 19.3051	valid_1's l2: 20.3484
[810]	training's l2: 19.2779	valid_1's l2: 20.3453
[820]	training's l2: 19.2507	valid_1's l2: 20.3423
[830]	training's l2: 19.224	valid_1's l2: 20.3396
[840]	training's l2: 19.1972	valid_1's l2: 20.3368
[850]	training's l2: 19.1706	valid_1's l2: 20.3344
[860]	training's l2: 19.1439	valid_1's l2: 20.3311
[870]	training's l2: 19.1166	valid_1's l2: 20.3297
[880]	training's l2: 19.0892	valid_1's l2: 20.3276
[890]	training's l2: 19.0633	valid_1's l2: 20.3257
[900]	training's l2: 19.0369	valid_1's l2: 20.3234
[910]	training's l2: 19.0111	valid_1's l2: 20.3203
[920]	training's l2: 18.9846	valid_1's l2: 20.3164
[930]	training's l2: 18.959	valid_1's l2: 20.3134
[940]	training's l2: 18.9336	valid_1's l2: 20.3115
[950]	training's l2: 18.9079	valid_1's l2: 20.3096
[960]	training's l2: 18.8831	valid_1's l2: 20.3073
[970]	training's l2: 18.8579	valid_1's l2: 20.3055
[980]	training's l2: 18.8333	valid_1's l2: 20.3031
[990]	training's l2: 18.8084	valid_1's l2: 20.3016
[1000]	training's l2: 18.7831	valid_1's l2: 20.2999
[1010]	training's l2: 18.7583	valid_1's l2: 20.2976
[1020]	training's l2: 18.7342	valid_1's l2: 20.2964
[1030]	training's l2: 18.7097	valid_1's l2: 20.2946
[1040]	training's l2: 18.6862	valid_1's l2: 20.2924
[1050]	training's l2: 18.6631	valid_1's l2: 20.2914
[1060]	training's l2: 18.6394	valid_1's l2: 20.2894
[1070]	training's l2: 18.6161	valid_1's l2: 20.2878
[1080]	training's l2: 18.593	valid_1's l2: 20.2867
[1090]	training's l2: 18.5705	valid_1's l2: 20.286
[1100]	training's l2: 18.5469	valid_1's l2: 20.285
[1110]	training's l2: 18.5229	valid_1's l2: 20.2838
[1120]	training's l2: 18.5006	valid_1's l2: 20.2823
[1130]	training's l2: 18.4782	valid_1's l2: 20.2812
[1140]	training's l2: 18.4556	valid_1's l2: 20.2802
[1150]	training's l2: 18.432	valid_1's l2: 20.2792
[1160]	training's l2: 18.409	valid_1's l2: 20.2783
[1170]	training's l2: 18.3864	valid_1's l2: 20.2761
[1180]	training's l2: 18.364	valid_1's l2: 20.2762
[1190]	training's l2: 18.3415	valid_1's l2: 20.2759
[1200]	training's l2: 18.3195	valid_1's l2: 20.2752
[1210]	training's l2: 18.2966	valid_1's l2: 20.2738
[1220]	training's l2: 18.2749	valid_1's l2: 20.2721
[1230]	training's l2: 18.254	valid_1's l2: 20.2716
[1240]	training's l2: 18.2325	valid_1's l2: 20.2705
[1250]	training's l2: 18.2108	valid_1's l2: 20.27
[1260]	training's l2: 18.1892	valid_1's l2: 20.2691
[1270]	training's l2: 18.1675	valid_1's l2: 20.2681
[1280]	training's l2: 18.1464	valid_1's l2: 20.2673
[1290]	training's l2: 18.1249	valid_1's l2: 20.2665
[1300]	training's l2: 18.104	valid_1's l2: 20.2654
[1310]	training's l2: 18.084	valid_1's l2: 20.2648
[1320]	training's l2: 18.0626	valid_1's l2: 20.2642
[1330]	training's l2: 18.0427	valid_1's l2: 20.2641
[1340]	training's l2: 18.022	valid_1's l2: 20.2643
[1350]	training's l2: 18.001	valid_1's l2: 20.2634
[1360]	training's l2: 17.9801	valid_1's l2: 20.2631
[1370]	training's l2: 17.9592	valid_1's l2: 20.262
[1380]	training's l2: 17.9384	valid_1's l2: 20.2613
[1390]	training's l2: 17.9186	valid_1's l2: 20.2611
[1400]	training's l2: 17.8988	valid_1's l2: 20.2612
[1410]	training's l2: 17.8786	valid_1's l2: 20.26
[1420]	training's l2: 17.8585	valid_1's l2: 20.2595
[1430]	training's l2: 17.8377	valid_1's l2: 20.2579
[1440]	training's l2: 17.8172	valid_1's l2: 20.2571
[1450]	training's l2: 17.7967	valid_1's l2: 20.2577
[1460]	training's l2: 17.7773	valid_1's l2: 20.2577
[1470]	training's l2: 17.7563	valid_1's l2: 20.2567
[1480]	training's l2: 17.736	valid_1's l2: 20.2561
[1490]	training's l2: 17.7158	valid_1's l2: 20.2556
[1500]	training's l2: 17.6952	valid_1's l2: 20.2554
[1510]	training's l2: 17.6748	valid_1's l2: 20.2546
[1520]	training's l2: 17.6552	valid_1's l2: 20.2545
[1530]	training's l2: 17.6352	valid_1's l2: 20.2536
[1540]	training's l2: 17.6176	valid_1's l2: 20.2527
[1550]	training's l2: 17.5979	valid_1's l2: 20.2513
[1560]	training's l2: 17.5788	valid_1's l2: 20.2501
[1570]	training's l2: 17.5594	valid_1's l2: 20.2494
[1580]	training's l2: 17.5404	valid_1's l2: 20.2497
[1590]	training's l2: 17.5212	valid_1's l2: 20.2486
[1600]	training's l2: 17.5018	valid_1's l2: 20.248
[1610]	training's l2: 17.4826	valid_1's l2: 20.2482
[1620]	training's l2: 17.4637	valid_1's l2: 20.2475
[1630]	training's l2: 17.4449	valid_1's l2: 20.2477
[1640]	training's l2: 17.4254	valid_1's l2: 20.2481
[1650]	training's l2: 17.4073	valid_1's l2: 20.2483
Early stopping, best iteration is:
[1621]	training's l2: 17.4618	valid_1's l2: 20.2475
score1: 3.8381689118182334
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256829 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 32.0688	valid_1's l2: 31.1658
[20]	training's l2: 30.4046	valid_1's l2: 29.5129
[30]	training's l2: 29.0261	valid_1's l2: 28.1461
[40]	training's l2: 27.8815	valid_1's l2: 27.0138
[50]	training's l2: 26.929	valid_1's l2: 26.0751
[60]	training's l2: 26.1326	valid_1's l2: 25.2916
[70]	training's l2: 25.4641	valid_1's l2: 24.6399
[80]	training's l2: 24.8995	valid_1's l2: 24.0939
[90]	training's l2: 24.422	valid_1's l2: 23.6353
[100]	training's l2: 24.0166	valid_1's l2: 23.2484
[110]	training's l2: 23.6688	valid_1's l2: 22.9215
[120]	training's l2: 23.3721	valid_1's l2: 22.648
[130]	training's l2: 23.1165	valid_1's l2: 22.4135
[140]	training's l2: 22.8954	valid_1's l2: 22.2142
[150]	training's l2: 22.7022	valid_1's l2: 22.0426
[160]	training's l2: 22.5307	valid_1's l2: 21.895
[170]	training's l2: 22.3762	valid_1's l2: 21.765
[180]	training's l2: 22.2383	valid_1's l2: 21.6513
[190]	training's l2: 22.115	valid_1's l2: 21.5519
[200]	training's l2: 22.002	valid_1's l2: 21.4646
[210]	training's l2: 21.8983	valid_1's l2: 21.386
[220]	training's l2: 21.8037	valid_1's l2: 21.3178
[230]	training's l2: 21.7158	valid_1's l2: 21.2564
[240]	training's l2: 21.6311	valid_1's l2: 21.1989
[250]	training's l2: 21.5523	valid_1's l2: 21.1452
[260]	training's l2: 21.4799	valid_1's l2: 21.0977
[270]	training's l2: 21.4121	valid_1's l2: 21.0556
[280]	training's l2: 21.3476	valid_1's l2: 21.0161
[290]	training's l2: 21.2849	valid_1's l2: 20.9785
[300]	training's l2: 21.2272	valid_1's l2: 20.9456
[310]	training's l2: 21.1716	valid_1's l2: 20.9139
[320]	training's l2: 21.1176	valid_1's l2: 20.8845
[330]	training's l2: 21.0648	valid_1's l2: 20.8563
[340]	training's l2: 21.0131	valid_1's l2: 20.8294
[350]	training's l2: 20.9638	valid_1's l2: 20.8056
[360]	training's l2: 20.9154	valid_1's l2: 20.784
[370]	training's l2: 20.8687	valid_1's l2: 20.7633
[380]	training's l2: 20.8241	valid_1's l2: 20.7439
[390]	training's l2: 20.78	valid_1's l2: 20.7253
[400]	training's l2: 20.7381	valid_1's l2: 20.7094
[410]	training's l2: 20.6969	valid_1's l2: 20.6931
[420]	training's l2: 20.6576	valid_1's l2: 20.6797
[430]	training's l2: 20.6178	valid_1's l2: 20.6643
[440]	training's l2: 20.5798	valid_1's l2: 20.6514
[450]	training's l2: 20.5422	valid_1's l2: 20.6394
[460]	training's l2: 20.5044	valid_1's l2: 20.6262
[470]	training's l2: 20.4681	valid_1's l2: 20.6152
[480]	training's l2: 20.4321	valid_1's l2: 20.6028
[490]	training's l2: 20.3967	valid_1's l2: 20.5917
[500]	training's l2: 20.3613	valid_1's l2: 20.5817
[510]	training's l2: 20.3274	valid_1's l2: 20.5726
[520]	training's l2: 20.2939	valid_1's l2: 20.5625
[530]	training's l2: 20.2602	valid_1's l2: 20.5535
[540]	training's l2: 20.2271	valid_1's l2: 20.5458
[550]	training's l2: 20.1938	valid_1's l2: 20.5369
[560]	training's l2: 20.1612	valid_1's l2: 20.5291
[570]	training's l2: 20.1296	valid_1's l2: 20.5214
[580]	training's l2: 20.0981	valid_1's l2: 20.514
[590]	training's l2: 20.0663	valid_1's l2: 20.5073
[600]	training's l2: 20.0347	valid_1's l2: 20.4997
[610]	training's l2: 20.0039	valid_1's l2: 20.4935
[620]	training's l2: 19.973	valid_1's l2: 20.487
[630]	training's l2: 19.9428	valid_1's l2: 20.4813
[640]	training's l2: 19.9126	valid_1's l2: 20.4762
[650]	training's l2: 19.8824	valid_1's l2: 20.4707
[660]	training's l2: 19.852	valid_1's l2: 20.4651
[670]	training's l2: 19.822	valid_1's l2: 20.4588
[680]	training's l2: 19.7922	valid_1's l2: 20.4535
[690]	training's l2: 19.7623	valid_1's l2: 20.4477
[700]	training's l2: 19.7325	valid_1's l2: 20.4421
[710]	training's l2: 19.7031	valid_1's l2: 20.4374
[720]	training's l2: 19.6742	valid_1's l2: 20.433
[730]	training's l2: 19.6453	valid_1's l2: 20.428
[740]	training's l2: 19.6165	valid_1's l2: 20.4231
[750]	training's l2: 19.5881	valid_1's l2: 20.4174
[760]	training's l2: 19.5597	valid_1's l2: 20.4129
[770]	training's l2: 19.5313	valid_1's l2: 20.4077
[780]	training's l2: 19.5035	valid_1's l2: 20.4033
[790]	training's l2: 19.4759	valid_1's l2: 20.3985
[800]	training's l2: 19.449	valid_1's l2: 20.3946
[810]	training's l2: 19.4218	valid_1's l2: 20.3903
[820]	training's l2: 19.3942	valid_1's l2: 20.3852
[830]	training's l2: 19.3667	valid_1's l2: 20.3809
[840]	training's l2: 19.34	valid_1's l2: 20.3773
[850]	training's l2: 19.3133	valid_1's l2: 20.3737
[860]	training's l2: 19.2869	valid_1's l2: 20.3713
[870]	training's l2: 19.2604	valid_1's l2: 20.3681
[880]	training's l2: 19.2342	valid_1's l2: 20.3655
[890]	training's l2: 19.2082	valid_1's l2: 20.3623
[900]	training's l2: 19.1821	valid_1's l2: 20.3592
[910]	training's l2: 19.1562	valid_1's l2: 20.3565
[920]	training's l2: 19.1303	valid_1's l2: 20.3538
[930]	training's l2: 19.105	valid_1's l2: 20.3503
[940]	training's l2: 19.0795	valid_1's l2: 20.348
[950]	training's l2: 19.0548	valid_1's l2: 20.3453
[960]	training's l2: 19.0292	valid_1's l2: 20.343
[970]	training's l2: 19.0044	valid_1's l2: 20.3412
[980]	training's l2: 18.9794	valid_1's l2: 20.3396
[990]	training's l2: 18.9548	valid_1's l2: 20.3381
[1000]	training's l2: 18.93	valid_1's l2: 20.3355
[1010]	training's l2: 18.9054	valid_1's l2: 20.3334
[1020]	training's l2: 18.8802	valid_1's l2: 20.3314
[1030]	training's l2: 18.8556	valid_1's l2: 20.329
[1040]	training's l2: 18.8315	valid_1's l2: 20.3268
[1050]	training's l2: 18.8075	valid_1's l2: 20.3242
[1060]	training's l2: 18.7837	valid_1's l2: 20.322
[1070]	training's l2: 18.7591	valid_1's l2: 20.3203
[1080]	training's l2: 18.7351	valid_1's l2: 20.3185
[1090]	training's l2: 18.7109	valid_1's l2: 20.3171
[1100]	training's l2: 18.6871	valid_1's l2: 20.3152
[1110]	training's l2: 18.6631	valid_1's l2: 20.3133
[1120]	training's l2: 18.6389	valid_1's l2: 20.3124
[1130]	training's l2: 18.6155	valid_1's l2: 20.3104
[1140]	training's l2: 18.5924	valid_1's l2: 20.3083
[1150]	training's l2: 18.5689	valid_1's l2: 20.3064
[1160]	training's l2: 18.546	valid_1's l2: 20.3051
[1170]	training's l2: 18.5234	valid_1's l2: 20.3043
[1180]	training's l2: 18.5002	valid_1's l2: 20.3025
[1190]	training's l2: 18.4771	valid_1's l2: 20.3008
[1200]	training's l2: 18.455	valid_1's l2: 20.2997
[1210]	training's l2: 18.4326	valid_1's l2: 20.2985
[1220]	training's l2: 18.4104	valid_1's l2: 20.2962
[1230]	training's l2: 18.3874	valid_1's l2: 20.2946
[1240]	training's l2: 18.3656	valid_1's l2: 20.293
[1250]	training's l2: 18.3424	valid_1's l2: 20.2906
[1260]	training's l2: 18.3205	valid_1's l2: 20.2903
[1270]	training's l2: 18.2983	valid_1's l2: 20.2889
[1280]	training's l2: 18.2762	valid_1's l2: 20.2868
[1290]	training's l2: 18.2538	valid_1's l2: 20.285
[1300]	training's l2: 18.2325	valid_1's l2: 20.2841
[1310]	training's l2: 18.2108	valid_1's l2: 20.2835
[1320]	training's l2: 18.1889	valid_1's l2: 20.2825
[1330]	training's l2: 18.1674	valid_1's l2: 20.2814
[1340]	training's l2: 18.1454	valid_1's l2: 20.2802
[1350]	training's l2: 18.1248	valid_1's l2: 20.2792
[1360]	training's l2: 18.1041	valid_1's l2: 20.278
[1370]	training's l2: 18.0826	valid_1's l2: 20.2772
[1380]	training's l2: 18.0614	valid_1's l2: 20.2761
[1390]	training's l2: 18.0411	valid_1's l2: 20.2752
[1400]	training's l2: 18.0214	valid_1's l2: 20.274
[1410]	training's l2: 18.0011	valid_1's l2: 20.2729
[1420]	training's l2: 17.9808	valid_1's l2: 20.2725
[1430]	training's l2: 17.9602	valid_1's l2: 20.2716
[1440]	training's l2: 17.9395	valid_1's l2: 20.2697
[1450]	training's l2: 17.9189	valid_1's l2: 20.2691
[1460]	training's l2: 17.8996	valid_1's l2: 20.2688
[1470]	training's l2: 17.8792	valid_1's l2: 20.2678
[1480]	training's l2: 17.8596	valid_1's l2: 20.2668
[1490]	training's l2: 17.8399	valid_1's l2: 20.2662
[1500]	training's l2: 17.821	valid_1's l2: 20.2658
[1510]	training's l2: 17.8015	valid_1's l2: 20.265
[1520]	training's l2: 17.7812	valid_1's l2: 20.2646
[1530]	training's l2: 17.7623	valid_1's l2: 20.2634
[1540]	training's l2: 17.7424	valid_1's l2: 20.2629
[1550]	training's l2: 17.7236	valid_1's l2: 20.2624
[1560]	training's l2: 17.7044	valid_1's l2: 20.261
[1570]	training's l2: 17.6851	valid_1's l2: 20.2598
[1580]	training's l2: 17.6663	valid_1's l2: 20.2593
[1590]	training's l2: 17.6467	valid_1's l2: 20.2585
[1600]	training's l2: 17.6282	valid_1's l2: 20.2578
[1610]	training's l2: 17.6089	valid_1's l2: 20.2571
[1620]	training's l2: 17.5901	valid_1's l2: 20.2569
[1630]	training's l2: 17.5713	valid_1's l2: 20.2556
[1640]	training's l2: 17.5524	valid_1's l2: 20.2555
[1650]	training's l2: 17.533	valid_1's l2: 20.2553
[1660]	training's l2: 17.5135	valid_1's l2: 20.2551
[1670]	training's l2: 17.495	valid_1's l2: 20.2544
[1680]	training's l2: 17.4769	valid_1's l2: 20.254
[1690]	training's l2: 17.4578	valid_1's l2: 20.2529
[1700]	training's l2: 17.439	valid_1's l2: 20.2527
[1710]	training's l2: 17.4212	valid_1's l2: 20.2521
[1720]	training's l2: 17.4029	valid_1's l2: 20.2526
[1730]	training's l2: 17.3851	valid_1's l2: 20.2525
[1740]	training's l2: 17.3667	valid_1's l2: 20.2519
[1750]	training's l2: 17.3481	valid_1's l2: 20.2522
[1760]	training's l2: 17.3294	valid_1's l2: 20.2524
[1770]	training's l2: 17.3118	valid_1's l2: 20.2518
Early stopping, best iteration is:
[1743]	training's l2: 17.3614	valid_1's l2: 20.2517
score1: 3.835254714854493
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289462 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.3772	valid_1's l2: 30.4807
[20]	training's l2: 29.3003	valid_1's l2: 28.4211
[30]	training's l2: 27.6972	valid_1's l2: 26.8345
[40]	training's l2: 26.4529	valid_1's l2: 25.6086
[50]	training's l2: 25.4797	valid_1's l2: 24.6584
[60]	training's l2: 24.7122	valid_1's l2: 23.9187
[70]	training's l2: 24.1025	valid_1's l2: 23.3367
[80]	training's l2: 23.6125	valid_1's l2: 22.8743
[90]	training's l2: 23.2171	valid_1's l2: 22.5093
[100]	training's l2: 22.8965	valid_1's l2: 22.2201
[110]	training's l2: 22.6296	valid_1's l2: 21.9841
[120]	training's l2: 22.4022	valid_1's l2: 21.7919
[130]	training's l2: 22.2083	valid_1's l2: 21.6324
[140]	training's l2: 22.0383	valid_1's l2: 21.4988
[150]	training's l2: 21.8881	valid_1's l2: 21.3868
[160]	training's l2: 21.7573	valid_1's l2: 21.2924
[170]	training's l2: 21.6367	valid_1's l2: 21.2101
[180]	training's l2: 21.5243	valid_1's l2: 21.1345
[190]	training's l2: 21.4238	valid_1's l2: 21.0713
[200]	training's l2: 21.3307	valid_1's l2: 21.0155
[210]	training's l2: 21.2433	valid_1's l2: 20.9618
[220]	training's l2: 21.1634	valid_1's l2: 20.9171
[230]	training's l2: 21.0879	valid_1's l2: 20.8765
[240]	training's l2: 21.0133	valid_1's l2: 20.8379
[250]	training's l2: 20.9415	valid_1's l2: 20.8036
[260]	training's l2: 20.8732	valid_1's l2: 20.7736
[270]	training's l2: 20.8082	valid_1's l2: 20.7467
[280]	training's l2: 20.747	valid_1's l2: 20.7223
[290]	training's l2: 20.6884	valid_1's l2: 20.6998
[300]	training's l2: 20.63	valid_1's l2: 20.6785
[310]	training's l2: 20.5748	valid_1's l2: 20.6579
[320]	training's l2: 20.5212	valid_1's l2: 20.6402
[330]	training's l2: 20.4685	valid_1's l2: 20.6238
[340]	training's l2: 20.4168	valid_1's l2: 20.6078
[350]	training's l2: 20.3658	valid_1's l2: 20.5927
[360]	training's l2: 20.3162	valid_1's l2: 20.5778
[370]	training's l2: 20.2685	valid_1's l2: 20.564
[380]	training's l2: 20.2205	valid_1's l2: 20.5521
[390]	training's l2: 20.173	valid_1's l2: 20.5407
[400]	training's l2: 20.1265	valid_1's l2: 20.5299
[410]	training's l2: 20.081	valid_1's l2: 20.5193
[420]	training's l2: 20.0364	valid_1's l2: 20.5103
[430]	training's l2: 19.9918	valid_1's l2: 20.5006
[440]	training's l2: 19.9481	valid_1's l2: 20.4917
[450]	training's l2: 19.9052	valid_1's l2: 20.4832
[460]	training's l2: 19.862	valid_1's l2: 20.4762
[470]	training's l2: 19.8193	valid_1's l2: 20.468
[480]	training's l2: 19.777	valid_1's l2: 20.4618
[490]	training's l2: 19.734	valid_1's l2: 20.4548
[500]	training's l2: 19.6916	valid_1's l2: 20.4462
[510]	training's l2: 19.6497	valid_1's l2: 20.4404
[520]	training's l2: 19.6086	valid_1's l2: 20.4327
[530]	training's l2: 19.568	valid_1's l2: 20.426
[540]	training's l2: 19.5274	valid_1's l2: 20.4182
[550]	training's l2: 19.4874	valid_1's l2: 20.4118
[560]	training's l2: 19.448	valid_1's l2: 20.4055
[570]	training's l2: 19.408	valid_1's l2: 20.3993
[580]	training's l2: 19.3685	valid_1's l2: 20.393
[590]	training's l2: 19.3301	valid_1's l2: 20.387
[600]	training's l2: 19.2911	valid_1's l2: 20.3817
[610]	training's l2: 19.2526	valid_1's l2: 20.3764
[620]	training's l2: 19.2152	valid_1's l2: 20.3719
[630]	training's l2: 19.178	valid_1's l2: 20.3676
[640]	training's l2: 19.1406	valid_1's l2: 20.3648
[650]	training's l2: 19.1033	valid_1's l2: 20.3608
[660]	training's l2: 19.0661	valid_1's l2: 20.3573
[670]	training's l2: 19.0294	valid_1's l2: 20.3527
[680]	training's l2: 18.9936	valid_1's l2: 20.3491
[690]	training's l2: 18.9575	valid_1's l2: 20.3459
[700]	training's l2: 18.9221	valid_1's l2: 20.342
[710]	training's l2: 18.8861	valid_1's l2: 20.3393
[720]	training's l2: 18.8511	valid_1's l2: 20.3368
[730]	training's l2: 18.8153	valid_1's l2: 20.3339
[740]	training's l2: 18.7803	valid_1's l2: 20.3313
[750]	training's l2: 18.7454	valid_1's l2: 20.3276
[760]	training's l2: 18.7104	valid_1's l2: 20.3244
[770]	training's l2: 18.6758	valid_1's l2: 20.3214
[780]	training's l2: 18.6422	valid_1's l2: 20.3181
[790]	training's l2: 18.6086	valid_1's l2: 20.3167
[800]	training's l2: 18.5757	valid_1's l2: 20.3142
[810]	training's l2: 18.541	valid_1's l2: 20.311
[820]	training's l2: 18.5072	valid_1's l2: 20.3083
[830]	training's l2: 18.4734	valid_1's l2: 20.3048
[840]	training's l2: 18.4414	valid_1's l2: 20.302
[850]	training's l2: 18.4077	valid_1's l2: 20.3001
[860]	training's l2: 18.3749	valid_1's l2: 20.2979
[870]	training's l2: 18.3429	valid_1's l2: 20.2955
[880]	training's l2: 18.3102	valid_1's l2: 20.2934
[890]	training's l2: 18.2783	valid_1's l2: 20.2921
[900]	training's l2: 18.2468	valid_1's l2: 20.2907
[910]	training's l2: 18.216	valid_1's l2: 20.2895
[920]	training's l2: 18.1849	valid_1's l2: 20.2879
[930]	training's l2: 18.1541	valid_1's l2: 20.2864
[940]	training's l2: 18.1226	valid_1's l2: 20.2845
[950]	training's l2: 18.0924	valid_1's l2: 20.283
[960]	training's l2: 18.0611	valid_1's l2: 20.2808
[970]	training's l2: 18.0303	valid_1's l2: 20.279
[980]	training's l2: 18.0011	valid_1's l2: 20.2786
[990]	training's l2: 17.972	valid_1's l2: 20.2787
[1000]	training's l2: 17.9428	valid_1's l2: 20.2776
[1010]	training's l2: 17.9131	valid_1's l2: 20.2775
[1020]	training's l2: 17.8835	valid_1's l2: 20.2766
[1030]	training's l2: 17.8556	valid_1's l2: 20.276
[1040]	training's l2: 17.8273	valid_1's l2: 20.2749
[1050]	training's l2: 17.7989	valid_1's l2: 20.274
[1060]	training's l2: 17.771	valid_1's l2: 20.2732
[1070]	training's l2: 17.7421	valid_1's l2: 20.2715
[1080]	training's l2: 17.7137	valid_1's l2: 20.2704
[1090]	training's l2: 17.6846	valid_1's l2: 20.2695
[1100]	training's l2: 17.6563	valid_1's l2: 20.2685
[1110]	training's l2: 17.629	valid_1's l2: 20.267
[1120]	training's l2: 17.6017	valid_1's l2: 20.2664
[1130]	training's l2: 17.5748	valid_1's l2: 20.2652
[1140]	training's l2: 17.547	valid_1's l2: 20.2652
[1150]	training's l2: 17.5192	valid_1's l2: 20.2646
[1160]	training's l2: 17.4925	valid_1's l2: 20.2641
[1170]	training's l2: 17.4648	valid_1's l2: 20.2628
[1180]	training's l2: 17.4375	valid_1's l2: 20.2622
[1190]	training's l2: 17.4109	valid_1's l2: 20.2608
[1200]	training's l2: 17.3849	valid_1's l2: 20.26
[1210]	training's l2: 17.358	valid_1's l2: 20.2592
[1220]	training's l2: 17.3315	valid_1's l2: 20.2587
[1230]	training's l2: 17.3073	valid_1's l2: 20.2587
[1240]	training's l2: 17.2808	valid_1's l2: 20.2572
[1250]	training's l2: 17.2546	valid_1's l2: 20.2566
[1260]	training's l2: 17.2292	valid_1's l2: 20.2559
[1270]	training's l2: 17.2019	valid_1's l2: 20.2559
[1280]	training's l2: 17.1782	valid_1's l2: 20.2554
[1290]	training's l2: 17.1502	valid_1's l2: 20.2542
[1300]	training's l2: 17.1254	valid_1's l2: 20.2536
[1310]	training's l2: 17.1002	valid_1's l2: 20.2535
[1320]	training's l2: 17.0749	valid_1's l2: 20.2538
[1330]	training's l2: 17.0507	valid_1's l2: 20.2535
[1340]	training's l2: 17.0252	valid_1's l2: 20.2525
[1350]	training's l2: 17.0007	valid_1's l2: 20.2523
[1360]	training's l2: 16.9764	valid_1's l2: 20.2517
[1370]	training's l2: 16.9512	valid_1's l2: 20.252
[1380]	training's l2: 16.9278	valid_1's l2: 20.251
[1390]	training's l2: 16.9021	valid_1's l2: 20.2513
[1400]	training's l2: 16.8779	valid_1's l2: 20.2519
[1410]	training's l2: 16.8536	valid_1's l2: 20.2514
Early stopping, best iteration is:
[1380]	training's l2: 16.9278	valid_1's l2: 20.251
score1: 3.8368950397176715
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260103 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 32.1179	valid_1's l2: 31.2073
[20]	training's l2: 30.4909	valid_1's l2: 29.583
[30]	training's l2: 29.1405	valid_1's l2: 28.2364
[40]	training's l2: 28.0188	valid_1's l2: 27.1206
[50]	training's l2: 27.0822	valid_1's l2: 26.1906
[60]	training's l2: 26.2989	valid_1's l2: 25.4131
[70]	training's l2: 25.6395	valid_1's l2: 24.7623
[80]	training's l2: 25.0825	valid_1's l2: 24.2177
[90]	training's l2: 24.6128	valid_1's l2: 23.7581
[100]	training's l2: 24.2119	valid_1's l2: 23.3679
[110]	training's l2: 23.8699	valid_1's l2: 23.0398
[120]	training's l2: 23.5778	valid_1's l2: 22.7618
[130]	training's l2: 23.3275	valid_1's l2: 22.5246
[140]	training's l2: 23.1119	valid_1's l2: 22.322
[150]	training's l2: 22.9221	valid_1's l2: 22.1472
[160]	training's l2: 22.7518	valid_1's l2: 21.9929
[170]	training's l2: 22.6015	valid_1's l2: 21.8592
[180]	training's l2: 22.4682	valid_1's l2: 21.7424
[190]	training's l2: 22.3482	valid_1's l2: 21.6397
[200]	training's l2: 22.2394	valid_1's l2: 21.5489
[210]	training's l2: 22.1418	valid_1's l2: 21.4691
[220]	training's l2: 22.0525	valid_1's l2: 21.3972
[230]	training's l2: 21.9673	valid_1's l2: 21.3298
[240]	training's l2: 21.8898	valid_1's l2: 21.2707
[250]	training's l2: 21.8166	valid_1's l2: 21.2163
[260]	training's l2: 21.7496	valid_1's l2: 21.1672
[270]	training's l2: 21.6874	valid_1's l2: 21.1212
[280]	training's l2: 21.629	valid_1's l2: 21.0801
[290]	training's l2: 21.5737	valid_1's l2: 21.042
[300]	training's l2: 21.5212	valid_1's l2: 21.0068
[310]	training's l2: 21.4727	valid_1's l2: 20.9756
[320]	training's l2: 21.4247	valid_1's l2: 20.9459
[330]	training's l2: 21.3774	valid_1's l2: 20.9167
[340]	training's l2: 21.3327	valid_1's l2: 20.8908
[350]	training's l2: 21.29	valid_1's l2: 20.8663
[360]	training's l2: 21.2486	valid_1's l2: 20.8436
[370]	training's l2: 21.2087	valid_1's l2: 20.8224
[380]	training's l2: 21.1699	valid_1's l2: 20.8014
[390]	training's l2: 21.1326	valid_1's l2: 20.7828
[400]	training's l2: 21.0962	valid_1's l2: 20.7649
[410]	training's l2: 21.0617	valid_1's l2: 20.7478
[420]	training's l2: 21.0276	valid_1's l2: 20.7319
[430]	training's l2: 20.9948	valid_1's l2: 20.7177
[440]	training's l2: 20.9622	valid_1's l2: 20.7026
[450]	training's l2: 20.9306	valid_1's l2: 20.6891
[460]	training's l2: 20.8999	valid_1's l2: 20.677
[470]	training's l2: 20.869	valid_1's l2: 20.6645
[480]	training's l2: 20.8393	valid_1's l2: 20.6529
[490]	training's l2: 20.8093	valid_1's l2: 20.641
[500]	training's l2: 20.7801	valid_1's l2: 20.6289
[510]	training's l2: 20.7515	valid_1's l2: 20.6185
[520]	training's l2: 20.723	valid_1's l2: 20.6077
[530]	training's l2: 20.6955	valid_1's l2: 20.5985
[540]	training's l2: 20.6686	valid_1's l2: 20.5891
[550]	training's l2: 20.6419	valid_1's l2: 20.5807
[560]	training's l2: 20.6153	valid_1's l2: 20.5729
[570]	training's l2: 20.5896	valid_1's l2: 20.5658
[580]	training's l2: 20.5639	valid_1's l2: 20.5585
[590]	training's l2: 20.5387	valid_1's l2: 20.5519
[600]	training's l2: 20.5137	valid_1's l2: 20.5443
[610]	training's l2: 20.4888	valid_1's l2: 20.5382
[620]	training's l2: 20.4639	valid_1's l2: 20.5317
[630]	training's l2: 20.4399	valid_1's l2: 20.5263
[640]	training's l2: 20.4158	valid_1's l2: 20.52
[650]	training's l2: 20.3918	valid_1's l2: 20.5144
[660]	training's l2: 20.368	valid_1's l2: 20.5083
[670]	training's l2: 20.344	valid_1's l2: 20.5027
[680]	training's l2: 20.3206	valid_1's l2: 20.498
[690]	training's l2: 20.2965	valid_1's l2: 20.492
[700]	training's l2: 20.2728	valid_1's l2: 20.4865
[710]	training's l2: 20.2493	valid_1's l2: 20.4803
[720]	training's l2: 20.2263	valid_1's l2: 20.4748
[730]	training's l2: 20.203	valid_1's l2: 20.4689
[740]	training's l2: 20.1795	valid_1's l2: 20.4633
[750]	training's l2: 20.1564	valid_1's l2: 20.4579
[760]	training's l2: 20.1343	valid_1's l2: 20.4531
[770]	training's l2: 20.1117	valid_1's l2: 20.4485
[780]	training's l2: 20.0897	valid_1's l2: 20.4441
[790]	training's l2: 20.0675	valid_1's l2: 20.4392
[800]	training's l2: 20.0452	valid_1's l2: 20.4337
[810]	training's l2: 20.0231	valid_1's l2: 20.429
[820]	training's l2: 20.0013	valid_1's l2: 20.4244
[830]	training's l2: 19.9799	valid_1's l2: 20.42
[840]	training's l2: 19.9579	valid_1's l2: 20.4154
[850]	training's l2: 19.9363	valid_1's l2: 20.4109
[860]	training's l2: 19.9149	valid_1's l2: 20.4068
[870]	training's l2: 19.8939	valid_1's l2: 20.4037
[880]	training's l2: 19.8733	valid_1's l2: 20.4003
[890]	training's l2: 19.8522	valid_1's l2: 20.3966
[900]	training's l2: 19.8311	valid_1's l2: 20.3928
[910]	training's l2: 19.8101	valid_1's l2: 20.3892
[920]	training's l2: 19.7891	valid_1's l2: 20.3851
[930]	training's l2: 19.7689	valid_1's l2: 20.381
[940]	training's l2: 19.7484	valid_1's l2: 20.3776
[950]	training's l2: 19.7285	valid_1's l2: 20.3751
[960]	training's l2: 19.7085	valid_1's l2: 20.372
[970]	training's l2: 19.6886	valid_1's l2: 20.3695
[980]	training's l2: 19.6688	valid_1's l2: 20.3663
[990]	training's l2: 19.649	valid_1's l2: 20.3634
[1000]	training's l2: 19.6297	valid_1's l2: 20.3606
[1010]	training's l2: 19.6105	valid_1's l2: 20.3576
[1020]	training's l2: 19.5912	valid_1's l2: 20.3546
[1030]	training's l2: 19.572	valid_1's l2: 20.3524
[1040]	training's l2: 19.5525	valid_1's l2: 20.3498
[1050]	training's l2: 19.5335	valid_1's l2: 20.347
[1060]	training's l2: 19.5142	valid_1's l2: 20.344
[1070]	training's l2: 19.4951	valid_1's l2: 20.3421
[1080]	training's l2: 19.4768	valid_1's l2: 20.3401
[1090]	training's l2: 19.4579	valid_1's l2: 20.3384
[1100]	training's l2: 19.4388	valid_1's l2: 20.3362
[1110]	training's l2: 19.4199	valid_1's l2: 20.3337
[1120]	training's l2: 19.4015	valid_1's l2: 20.3324
[1130]	training's l2: 19.3826	valid_1's l2: 20.3302
[1140]	training's l2: 19.3642	valid_1's l2: 20.3285
[1150]	training's l2: 19.3457	valid_1's l2: 20.3262
[1160]	training's l2: 19.3275	valid_1's l2: 20.3239
[1170]	training's l2: 19.3093	valid_1's l2: 20.3226
[1180]	training's l2: 19.2912	valid_1's l2: 20.3203
[1190]	training's l2: 19.2731	valid_1's l2: 20.318
[1200]	training's l2: 19.2548	valid_1's l2: 20.3164
[1210]	training's l2: 19.2368	valid_1's l2: 20.3145
[1220]	training's l2: 19.2192	valid_1's l2: 20.3133
[1230]	training's l2: 19.2012	valid_1's l2: 20.312
[1240]	training's l2: 19.1835	valid_1's l2: 20.3102
[1250]	training's l2: 19.1658	valid_1's l2: 20.3086
[1260]	training's l2: 19.1479	valid_1's l2: 20.307
[1270]	training's l2: 19.1304	valid_1's l2: 20.3057
[1280]	training's l2: 19.1126	valid_1's l2: 20.3041
[1290]	training's l2: 19.0947	valid_1's l2: 20.3024
[1300]	training's l2: 19.0771	valid_1's l2: 20.3011
[1310]	training's l2: 19.0598	valid_1's l2: 20.2995
[1320]	training's l2: 19.0425	valid_1's l2: 20.2979
[1330]	training's l2: 19.0252	valid_1's l2: 20.2967
[1340]	training's l2: 19.0082	valid_1's l2: 20.2957
[1350]	training's l2: 18.9907	valid_1's l2: 20.2941
[1360]	training's l2: 18.9739	valid_1's l2: 20.2926
[1370]	training's l2: 18.9574	valid_1's l2: 20.2918
[1380]	training's l2: 18.9407	valid_1's l2: 20.2904
[1390]	training's l2: 18.9243	valid_1's l2: 20.29
[1400]	training's l2: 18.9076	valid_1's l2: 20.289
[1410]	training's l2: 18.8916	valid_1's l2: 20.2878
[1420]	training's l2: 18.8756	valid_1's l2: 20.2878
[1430]	training's l2: 18.8588	valid_1's l2: 20.2865
[1440]	training's l2: 18.8426	valid_1's l2: 20.2856
[1450]	training's l2: 18.8271	valid_1's l2: 20.2847
[1460]	training's l2: 18.8109	valid_1's l2: 20.2836
[1470]	training's l2: 18.7951	valid_1's l2: 20.2823
[1480]	training's l2: 18.7789	valid_1's l2: 20.2817
[1490]	training's l2: 18.7635	valid_1's l2: 20.281
[1500]	training's l2: 18.7478	valid_1's l2: 20.2804
[1510]	training's l2: 18.7319	valid_1's l2: 20.2798
[1520]	training's l2: 18.7158	valid_1's l2: 20.2787
[1530]	training's l2: 18.7005	valid_1's l2: 20.2782
[1540]	training's l2: 18.6841	valid_1's l2: 20.2775
[1550]	training's l2: 18.6681	valid_1's l2: 20.2769
[1560]	training's l2: 18.6523	valid_1's l2: 20.2765
[1570]	training's l2: 18.6367	valid_1's l2: 20.2756
[1580]	training's l2: 18.6213	valid_1's l2: 20.2746
[1590]	training's l2: 18.6055	valid_1's l2: 20.2743
[1600]	training's l2: 18.5908	valid_1's l2: 20.2737
[1610]	training's l2: 18.5758	valid_1's l2: 20.2733
[1620]	training's l2: 18.5599	valid_1's l2: 20.2719
[1630]	training's l2: 18.5449	valid_1's l2: 20.2711
[1640]	training's l2: 18.5297	valid_1's l2: 20.2702
[1650]	training's l2: 18.5153	valid_1's l2: 20.2692
[1660]	training's l2: 18.5003	valid_1's l2: 20.2682
[1670]	training's l2: 18.4851	valid_1's l2: 20.268
[1680]	training's l2: 18.4703	valid_1's l2: 20.2673
[1690]	training's l2: 18.4546	valid_1's l2: 20.2665
[1700]	training's l2: 18.4398	valid_1's l2: 20.2662
[1710]	training's l2: 18.4251	valid_1's l2: 20.2659
[1720]	training's l2: 18.4109	valid_1's l2: 20.2651
[1730]	training's l2: 18.3964	valid_1's l2: 20.2644
[1740]	training's l2: 18.3825	valid_1's l2: 20.2634
[1750]	training's l2: 18.3687	valid_1's l2: 20.2634
[1760]	training's l2: 18.3541	valid_1's l2: 20.2623
[1770]	training's l2: 18.3399	valid_1's l2: 20.262
[1780]	training's l2: 18.3249	valid_1's l2: 20.2613
[1790]	training's l2: 18.3097	valid_1's l2: 20.2614
[1800]	training's l2: 18.296	valid_1's l2: 20.2612
[1810]	training's l2: 18.2817	valid_1's l2: 20.2608
[1820]	training's l2: 18.2679	valid_1's l2: 20.2607
[1830]	training's l2: 18.2541	valid_1's l2: 20.2604
[1840]	training's l2: 18.2399	valid_1's l2: 20.2601
[1850]	training's l2: 18.2253	valid_1's l2: 20.2595
[1860]	training's l2: 18.2114	valid_1's l2: 20.2593
[1870]	training's l2: 18.1968	valid_1's l2: 20.259
[1880]	training's l2: 18.1824	valid_1's l2: 20.2591
[1890]	training's l2: 18.1683	valid_1's l2: 20.2586
[1900]	training's l2: 18.1547	valid_1's l2: 20.258
[1910]	training's l2: 18.141	valid_1's l2: 20.258
[1920]	training's l2: 18.1272	valid_1's l2: 20.2579
[1930]	training's l2: 18.1136	valid_1's l2: 20.2575
[1940]	training's l2: 18.0992	valid_1's l2: 20.2568
[1950]	training's l2: 18.085	valid_1's l2: 20.2564
[1960]	training's l2: 18.0713	valid_1's l2: 20.2558
[1970]	training's l2: 18.0571	valid_1's l2: 20.2552
[1980]	training's l2: 18.0441	valid_1's l2: 20.2545
[1990]	training's l2: 18.0304	valid_1's l2: 20.2542
[2000]	training's l2: 18.0164	valid_1's l2: 20.2534
[2010]	training's l2: 18.0034	valid_1's l2: 20.2534
[2020]	training's l2: 17.9893	valid_1's l2: 20.2526
[2030]	training's l2: 17.975	valid_1's l2: 20.2524
[2040]	training's l2: 17.9609	valid_1's l2: 20.2522
[2050]	training's l2: 17.947	valid_1's l2: 20.2521
[2060]	training's l2: 17.9339	valid_1's l2: 20.2513
[2070]	training's l2: 17.9204	valid_1's l2: 20.2512
[2080]	training's l2: 17.9069	valid_1's l2: 20.251
[2090]	training's l2: 17.8935	valid_1's l2: 20.2508
[2100]	training's l2: 17.8804	valid_1's l2: 20.2505
[2110]	training's l2: 17.8668	valid_1's l2: 20.2505
[2120]	training's l2: 17.8534	valid_1's l2: 20.2507
[2130]	training's l2: 17.8403	valid_1's l2: 20.2505
Early stopping, best iteration is:
[2101]	training's l2: 17.879	valid_1's l2: 20.2504
score1: 3.836282995206004
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243402 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.7844	valid_1's l2: 29.8959
[20]	training's l2: 28.4115	valid_1's l2: 27.5478
[30]	training's l2: 26.6935	valid_1's l2: 25.8572
[40]	training's l2: 25.4351	valid_1's l2: 24.6293
[50]	training's l2: 24.5008	valid_1's l2: 23.7315
[60]	training's l2: 23.7975	valid_1's l2: 23.0655
[70]	training's l2: 23.2596	valid_1's l2: 22.5693
[80]	training's l2: 22.8455	valid_1's l2: 22.1984
[90]	training's l2: 22.5184	valid_1's l2: 21.913
[100]	training's l2: 22.2493	valid_1's l2: 21.6902
[110]	training's l2: 22.0272	valid_1's l2: 21.5139
[120]	training's l2: 21.8334	valid_1's l2: 21.3691
[130]	training's l2: 21.6692	valid_1's l2: 21.2544
[140]	training's l2: 21.5214	valid_1's l2: 21.1552
[150]	training's l2: 21.3881	valid_1's l2: 21.0711
[160]	training's l2: 21.2705	valid_1's l2: 21.0017
[170]	training's l2: 21.1589	valid_1's l2: 20.9389
[180]	training's l2: 21.0585	valid_1's l2: 20.886
[190]	training's l2: 20.9626	valid_1's l2: 20.8378
[200]	training's l2: 20.8712	valid_1's l2: 20.7936
[210]	training's l2: 20.7849	valid_1's l2: 20.7561
[220]	training's l2: 20.7033	valid_1's l2: 20.7228
[230]	training's l2: 20.6252	valid_1's l2: 20.6935
[240]	training's l2: 20.5508	valid_1's l2: 20.6683
[250]	training's l2: 20.4808	valid_1's l2: 20.6453
[260]	training's l2: 20.4114	valid_1's l2: 20.6251
[270]	training's l2: 20.3441	valid_1's l2: 20.6053
[280]	training's l2: 20.2781	valid_1's l2: 20.5879
[290]	training's l2: 20.2138	valid_1's l2: 20.5699
[300]	training's l2: 20.1508	valid_1's l2: 20.5539
[310]	training's l2: 20.0899	valid_1's l2: 20.5398
[320]	training's l2: 20.0307	valid_1's l2: 20.5287
[330]	training's l2: 19.9721	valid_1's l2: 20.5166
[340]	training's l2: 19.914	valid_1's l2: 20.5046
[350]	training's l2: 19.8561	valid_1's l2: 20.493
[360]	training's l2: 19.7999	valid_1's l2: 20.4823
[370]	training's l2: 19.7444	valid_1's l2: 20.4726
[380]	training's l2: 19.6886	valid_1's l2: 20.4619
[390]	training's l2: 19.6329	valid_1's l2: 20.4524
[400]	training's l2: 19.5783	valid_1's l2: 20.4417
[410]	training's l2: 19.5249	valid_1's l2: 20.4323
[420]	training's l2: 19.4715	valid_1's l2: 20.4223
[430]	training's l2: 19.4184	valid_1's l2: 20.4134
[440]	training's l2: 19.3656	valid_1's l2: 20.4049
[450]	training's l2: 19.3144	valid_1's l2: 20.3975
[460]	training's l2: 19.2631	valid_1's l2: 20.39
[470]	training's l2: 19.213	valid_1's l2: 20.3823
[480]	training's l2: 19.163	valid_1's l2: 20.3747
[490]	training's l2: 19.1141	valid_1's l2: 20.3684
[500]	training's l2: 19.0658	valid_1's l2: 20.3646
[510]	training's l2: 19.0169	valid_1's l2: 20.359
[520]	training's l2: 18.9688	valid_1's l2: 20.356
[530]	training's l2: 18.922	valid_1's l2: 20.3516
[540]	training's l2: 18.8749	valid_1's l2: 20.347
[550]	training's l2: 18.827	valid_1's l2: 20.342
[560]	training's l2: 18.7801	valid_1's l2: 20.338
[570]	training's l2: 18.7339	valid_1's l2: 20.3335
[580]	training's l2: 18.6888	valid_1's l2: 20.3307
[590]	training's l2: 18.6437	valid_1's l2: 20.3267
[600]	training's l2: 18.5979	valid_1's l2: 20.3229
[610]	training's l2: 18.5533	valid_1's l2: 20.3192
[620]	training's l2: 18.5087	valid_1's l2: 20.3146
[630]	training's l2: 18.4652	valid_1's l2: 20.3106
[640]	training's l2: 18.4222	valid_1's l2: 20.3089
[650]	training's l2: 18.3784	valid_1's l2: 20.3069
[660]	training's l2: 18.3355	valid_1's l2: 20.3039
[670]	training's l2: 18.2926	valid_1's l2: 20.3002
[680]	training's l2: 18.2505	valid_1's l2: 20.2967
[690]	training's l2: 18.2086	valid_1's l2: 20.2937
[700]	training's l2: 18.1667	valid_1's l2: 20.2913
[710]	training's l2: 18.126	valid_1's l2: 20.29
[720]	training's l2: 18.0853	valid_1's l2: 20.289
[730]	training's l2: 18.0442	valid_1's l2: 20.2862
[740]	training's l2: 18.003	valid_1's l2: 20.2848
[750]	training's l2: 17.9639	valid_1's l2: 20.2827
[760]	training's l2: 17.9238	valid_1's l2: 20.2808
[770]	training's l2: 17.884	valid_1's l2: 20.2789
[780]	training's l2: 17.8437	valid_1's l2: 20.2777
[790]	training's l2: 17.8064	valid_1's l2: 20.2779
[800]	training's l2: 17.7691	valid_1's l2: 20.2756
[810]	training's l2: 17.7307	valid_1's l2: 20.2744
[820]	training's l2: 17.6937	valid_1's l2: 20.2739
[830]	training's l2: 17.6547	valid_1's l2: 20.2749
[840]	training's l2: 17.6165	valid_1's l2: 20.2733
[850]	training's l2: 17.5809	valid_1's l2: 20.273
[860]	training's l2: 17.5441	valid_1's l2: 20.2722
[870]	training's l2: 17.5077	valid_1's l2: 20.2716
[880]	training's l2: 17.4711	valid_1's l2: 20.2706
[890]	training's l2: 17.4366	valid_1's l2: 20.2699
[900]	training's l2: 17.4012	valid_1's l2: 20.2686
[910]	training's l2: 17.3652	valid_1's l2: 20.2668
[920]	training's l2: 17.3313	valid_1's l2: 20.2668
[930]	training's l2: 17.2969	valid_1's l2: 20.2654
[940]	training's l2: 17.2633	valid_1's l2: 20.2649
[950]	training's l2: 17.2297	valid_1's l2: 20.2647
[960]	training's l2: 17.196	valid_1's l2: 20.2626
[970]	training's l2: 17.1629	valid_1's l2: 20.2627
[980]	training's l2: 17.1301	valid_1's l2: 20.2614
[990]	training's l2: 17.0966	valid_1's l2: 20.2616
[1000]	training's l2: 17.0632	valid_1's l2: 20.2602
[1010]	training's l2: 17.0291	valid_1's l2: 20.2595
[1020]	training's l2: 16.9965	valid_1's l2: 20.2593
[1030]	training's l2: 16.9639	valid_1's l2: 20.2585
[1040]	training's l2: 16.9305	valid_1's l2: 20.2585
[1050]	training's l2: 16.8967	valid_1's l2: 20.2582
[1060]	training's l2: 16.8636	valid_1's l2: 20.2585
Early stopping, best iteration is:
[1037]	training's l2: 16.9406	valid_1's l2: 20.258
score1: 3.8368623707691794
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.250001 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.4035	valid_1's l2: 30.5045
[20]	training's l2: 29.3408	valid_1's l2: 28.4569
[30]	training's l2: 27.7474	valid_1's l2: 26.8783
[40]	training's l2: 26.5062	valid_1's l2: 25.6572
[50]	training's l2: 25.534	valid_1's l2: 24.706
[60]	training's l2: 24.7653	valid_1's l2: 23.9614
[70]	training's l2: 24.1545	valid_1's l2: 23.374
[80]	training's l2: 23.6637	valid_1's l2: 22.9118
[90]	training's l2: 23.2651	valid_1's l2: 22.5427
[100]	training's l2: 22.9418	valid_1's l2: 22.2491
[110]	training's l2: 22.6742	valid_1's l2: 22.0111
[120]	training's l2: 22.4456	valid_1's l2: 21.8157
[130]	training's l2: 22.2497	valid_1's l2: 21.6531
[140]	training's l2: 22.0796	valid_1's l2: 21.5166
[150]	training's l2: 21.9295	valid_1's l2: 21.4019
[160]	training's l2: 21.7976	valid_1's l2: 21.3056
[170]	training's l2: 21.6781	valid_1's l2: 21.2222
[180]	training's l2: 21.5667	valid_1's l2: 21.1452
[190]	training's l2: 21.4661	valid_1's l2: 21.0803
[200]	training's l2: 21.3739	valid_1's l2: 21.0247
[210]	training's l2: 21.288	valid_1's l2: 20.9731
[220]	training's l2: 21.2083	valid_1's l2: 20.9271
[230]	training's l2: 21.1345	valid_1's l2: 20.8868
[240]	training's l2: 21.0603	valid_1's l2: 20.8482
[250]	training's l2: 20.9902	valid_1's l2: 20.8131
[260]	training's l2: 20.9237	valid_1's l2: 20.7822
[270]	training's l2: 20.8589	valid_1's l2: 20.753
[280]	training's l2: 20.7974	valid_1's l2: 20.7268
[290]	training's l2: 20.7394	valid_1's l2: 20.7047
[300]	training's l2: 20.6819	valid_1's l2: 20.6838
[310]	training's l2: 20.6275	valid_1's l2: 20.6641
[320]	training's l2: 20.5749	valid_1's l2: 20.6474
[330]	training's l2: 20.5234	valid_1's l2: 20.6312
[340]	training's l2: 20.4717	valid_1's l2: 20.6135
[350]	training's l2: 20.422	valid_1's l2: 20.5982
[360]	training's l2: 20.3728	valid_1's l2: 20.5831
[370]	training's l2: 20.3247	valid_1's l2: 20.5686
[380]	training's l2: 20.2778	valid_1's l2: 20.5576
[390]	training's l2: 20.2316	valid_1's l2: 20.548
[400]	training's l2: 20.1865	valid_1's l2: 20.5375
[410]	training's l2: 20.1423	valid_1's l2: 20.5273
[420]	training's l2: 20.0986	valid_1's l2: 20.5174
[430]	training's l2: 20.0543	valid_1's l2: 20.508
[440]	training's l2: 20.011	valid_1's l2: 20.4987
[450]	training's l2: 19.9682	valid_1's l2: 20.4887
[460]	training's l2: 19.9257	valid_1's l2: 20.4804
[470]	training's l2: 19.884	valid_1's l2: 20.4733
[480]	training's l2: 19.842	valid_1's l2: 20.4655
[490]	training's l2: 19.8014	valid_1's l2: 20.4577
[500]	training's l2: 19.7603	valid_1's l2: 20.4504
[510]	training's l2: 19.7194	valid_1's l2: 20.4438
[520]	training's l2: 19.6785	valid_1's l2: 20.437
[530]	training's l2: 19.638	valid_1's l2: 20.4284
[540]	training's l2: 19.5986	valid_1's l2: 20.4205
[550]	training's l2: 19.5596	valid_1's l2: 20.4146
[560]	training's l2: 19.5205	valid_1's l2: 20.4072
[570]	training's l2: 19.4816	valid_1's l2: 20.4013
[580]	training's l2: 19.4443	valid_1's l2: 20.396
[590]	training's l2: 19.4068	valid_1's l2: 20.3916
[600]	training's l2: 19.3697	valid_1's l2: 20.3863
[610]	training's l2: 19.3332	valid_1's l2: 20.3821
[620]	training's l2: 19.2965	valid_1's l2: 20.3775
[630]	training's l2: 19.26	valid_1's l2: 20.3731
[640]	training's l2: 19.2238	valid_1's l2: 20.3703
[650]	training's l2: 19.1882	valid_1's l2: 20.3671
[660]	training's l2: 19.1523	valid_1's l2: 20.3638
[670]	training's l2: 19.1158	valid_1's l2: 20.3597
[680]	training's l2: 19.0797	valid_1's l2: 20.357
[690]	training's l2: 19.0448	valid_1's l2: 20.3539
[700]	training's l2: 19.0099	valid_1's l2: 20.3491
[710]	training's l2: 18.9749	valid_1's l2: 20.3452
[720]	training's l2: 18.941	valid_1's l2: 20.3422
[730]	training's l2: 18.9063	valid_1's l2: 20.3387
[740]	training's l2: 18.872	valid_1's l2: 20.3365
[750]	training's l2: 18.8383	valid_1's l2: 20.334
[760]	training's l2: 18.8049	valid_1's l2: 20.3306
[770]	training's l2: 18.7712	valid_1's l2: 20.3266
[780]	training's l2: 18.737	valid_1's l2: 20.3233
[790]	training's l2: 18.7039	valid_1's l2: 20.3198
[800]	training's l2: 18.6707	valid_1's l2: 20.3178
[810]	training's l2: 18.6382	valid_1's l2: 20.3148
[820]	training's l2: 18.6064	valid_1's l2: 20.3114
[830]	training's l2: 18.5732	valid_1's l2: 20.309
[840]	training's l2: 18.5409	valid_1's l2: 20.3066
[850]	training's l2: 18.5096	valid_1's l2: 20.3044
[860]	training's l2: 18.4788	valid_1's l2: 20.3025
[870]	training's l2: 18.4475	valid_1's l2: 20.3004
[880]	training's l2: 18.4162	valid_1's l2: 20.2985
[890]	training's l2: 18.3848	valid_1's l2: 20.2964
[900]	training's l2: 18.3538	valid_1's l2: 20.2939
[910]	training's l2: 18.3237	valid_1's l2: 20.2911
[920]	training's l2: 18.2921	valid_1's l2: 20.2893
[930]	training's l2: 18.2609	valid_1's l2: 20.2875
[940]	training's l2: 18.2302	valid_1's l2: 20.2848
[950]	training's l2: 18.2004	valid_1's l2: 20.2842
[960]	training's l2: 18.1696	valid_1's l2: 20.2831
[970]	training's l2: 18.1409	valid_1's l2: 20.2819
[980]	training's l2: 18.1111	valid_1's l2: 20.2806
[990]	training's l2: 18.0824	valid_1's l2: 20.2791
[1000]	training's l2: 18.0545	valid_1's l2: 20.2778
[1010]	training's l2: 18.025	valid_1's l2: 20.2754
[1020]	training's l2: 17.9965	valid_1's l2: 20.2734
[1030]	training's l2: 17.9685	valid_1's l2: 20.2726
[1040]	training's l2: 17.9409	valid_1's l2: 20.2729
[1050]	training's l2: 17.9141	valid_1's l2: 20.2716
[1060]	training's l2: 17.886	valid_1's l2: 20.2707
[1070]	training's l2: 17.8586	valid_1's l2: 20.2698
[1080]	training's l2: 17.8301	valid_1's l2: 20.268
[1090]	training's l2: 17.8041	valid_1's l2: 20.2677
[1100]	training's l2: 17.7765	valid_1's l2: 20.2666
[1110]	training's l2: 17.7493	valid_1's l2: 20.2662
[1120]	training's l2: 17.7207	valid_1's l2: 20.2646
[1130]	training's l2: 17.6944	valid_1's l2: 20.264
[1140]	training's l2: 17.6675	valid_1's l2: 20.2647
[1150]	training's l2: 17.641	valid_1's l2: 20.2635
[1160]	training's l2: 17.6144	valid_1's l2: 20.2632
[1170]	training's l2: 17.5877	valid_1's l2: 20.2619
[1180]	training's l2: 17.5624	valid_1's l2: 20.2619
[1190]	training's l2: 17.536	valid_1's l2: 20.2622
[1200]	training's l2: 17.51	valid_1's l2: 20.2625
[1210]	training's l2: 17.4832	valid_1's l2: 20.262
Early stopping, best iteration is:
[1186]	training's l2: 17.5468	valid_1's l2: 20.2616
score1: 3.8402323406607892
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.290162 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.0647	valid_1's l2: 30.1591
[20]	training's l2: 28.8358	valid_1's l2: 27.9393
[30]	training's l2: 27.1813	valid_1's l2: 26.2978
[40]	training's l2: 25.9406	valid_1's l2: 25.0713
[50]	training's l2: 25.0002	valid_1's l2: 24.1504
[60]	training's l2: 24.2818	valid_1's l2: 23.4535
[70]	training's l2: 23.7258	valid_1's l2: 22.9204
[80]	training's l2: 23.2934	valid_1's l2: 22.5129
[90]	training's l2: 22.9513	valid_1's l2: 22.1976
[100]	training's l2: 22.671	valid_1's l2: 21.9463
[110]	training's l2: 22.4393	valid_1's l2: 21.7435
[120]	training's l2: 22.2456	valid_1's l2: 21.5808
[130]	training's l2: 22.0809	valid_1's l2: 21.4483
[140]	training's l2: 21.9374	valid_1's l2: 21.3377
[150]	training's l2: 21.8063	valid_1's l2: 21.2388
[160]	training's l2: 21.6915	valid_1's l2: 21.1549
[170]	training's l2: 21.5887	valid_1's l2: 21.0839
[180]	training's l2: 21.4955	valid_1's l2: 21.0218
[190]	training's l2: 21.412	valid_1's l2: 20.9692
[200]	training's l2: 21.3324	valid_1's l2: 20.922
[210]	training's l2: 21.2562	valid_1's l2: 20.879
[220]	training's l2: 21.1845	valid_1's l2: 20.8402
[230]	training's l2: 21.118	valid_1's l2: 20.8063
[240]	training's l2: 21.0534	valid_1's l2: 20.7761
[250]	training's l2: 20.9924	valid_1's l2: 20.7467
[260]	training's l2: 20.9354	valid_1's l2: 20.7211
[270]	training's l2: 20.8806	valid_1's l2: 20.699
[280]	training's l2: 20.8262	valid_1's l2: 20.6763
[290]	training's l2: 20.7737	valid_1's l2: 20.6564
[300]	training's l2: 20.724	valid_1's l2: 20.6375
[310]	training's l2: 20.6741	valid_1's l2: 20.6205
[320]	training's l2: 20.6263	valid_1's l2: 20.6037
[330]	training's l2: 20.5794	valid_1's l2: 20.5889
[340]	training's l2: 20.5339	valid_1's l2: 20.5754
[350]	training's l2: 20.4892	valid_1's l2: 20.5639
[360]	training's l2: 20.4452	valid_1's l2: 20.5515
[370]	training's l2: 20.4018	valid_1's l2: 20.5398
[380]	training's l2: 20.3589	valid_1's l2: 20.5272
[390]	training's l2: 20.3163	valid_1's l2: 20.5162
[400]	training's l2: 20.2745	valid_1's l2: 20.5063
[410]	training's l2: 20.2342	valid_1's l2: 20.4967
[420]	training's l2: 20.1937	valid_1's l2: 20.489
[430]	training's l2: 20.1542	valid_1's l2: 20.4799
[440]	training's l2: 20.1133	valid_1's l2: 20.4695
[450]	training's l2: 20.0735	valid_1's l2: 20.4599
[460]	training's l2: 20.0335	valid_1's l2: 20.4523
[470]	training's l2: 19.9949	valid_1's l2: 20.4459
[480]	training's l2: 19.9568	valid_1's l2: 20.4382
[490]	training's l2: 19.919	valid_1's l2: 20.4316
[500]	training's l2: 19.8809	valid_1's l2: 20.4252
[510]	training's l2: 19.8441	valid_1's l2: 20.419
[520]	training's l2: 19.8068	valid_1's l2: 20.4121
[530]	training's l2: 19.7703	valid_1's l2: 20.4065
[540]	training's l2: 19.7346	valid_1's l2: 20.4018
[550]	training's l2: 19.6982	valid_1's l2: 20.3959
[560]	training's l2: 19.6629	valid_1's l2: 20.3898
[570]	training's l2: 19.628	valid_1's l2: 20.3853
[580]	training's l2: 19.5934	valid_1's l2: 20.381
[590]	training's l2: 19.5586	valid_1's l2: 20.3764
[600]	training's l2: 19.5243	valid_1's l2: 20.3712
[610]	training's l2: 19.4903	valid_1's l2: 20.367
[620]	training's l2: 19.4559	valid_1's l2: 20.3622
[630]	training's l2: 19.422	valid_1's l2: 20.3575
[640]	training's l2: 19.3886	valid_1's l2: 20.3541
[650]	training's l2: 19.355	valid_1's l2: 20.3505
[660]	training's l2: 19.3224	valid_1's l2: 20.3474
[670]	training's l2: 19.2895	valid_1's l2: 20.3434
[680]	training's l2: 19.2577	valid_1's l2: 20.3398
[690]	training's l2: 19.2248	valid_1's l2: 20.3369
[700]	training's l2: 19.1932	valid_1's l2: 20.3328
[710]	training's l2: 19.1601	valid_1's l2: 20.3304
[720]	training's l2: 19.1282	valid_1's l2: 20.3269
[730]	training's l2: 19.0975	valid_1's l2: 20.3253
[740]	training's l2: 19.0662	valid_1's l2: 20.3225
[750]	training's l2: 19.0353	valid_1's l2: 20.3199
[760]	training's l2: 19.0048	valid_1's l2: 20.3181
[770]	training's l2: 18.9738	valid_1's l2: 20.3151
[780]	training's l2: 18.9439	valid_1's l2: 20.3127
[790]	training's l2: 18.9147	valid_1's l2: 20.3096
[800]	training's l2: 18.8848	valid_1's l2: 20.3078
[810]	training's l2: 18.8552	valid_1's l2: 20.3063
[820]	training's l2: 18.8257	valid_1's l2: 20.3053
[830]	training's l2: 18.7971	valid_1's l2: 20.3039
[840]	training's l2: 18.7684	valid_1's l2: 20.3026
[850]	training's l2: 18.7396	valid_1's l2: 20.2999
[860]	training's l2: 18.7104	valid_1's l2: 20.2985
[870]	training's l2: 18.6823	valid_1's l2: 20.2959
[880]	training's l2: 18.6544	valid_1's l2: 20.2949
[890]	training's l2: 18.6267	valid_1's l2: 20.2928
[900]	training's l2: 18.599	valid_1's l2: 20.2919
[910]	training's l2: 18.5701	valid_1's l2: 20.2897
[920]	training's l2: 18.5425	valid_1's l2: 20.288
[930]	training's l2: 18.5152	valid_1's l2: 20.2875
[940]	training's l2: 18.4886	valid_1's l2: 20.2863
[950]	training's l2: 18.4617	valid_1's l2: 20.2865
[960]	training's l2: 18.4337	valid_1's l2: 20.2856
[970]	training's l2: 18.4073	valid_1's l2: 20.2842
[980]	training's l2: 18.381	valid_1's l2: 20.2822
[990]	training's l2: 18.3552	valid_1's l2: 20.281
[1000]	training's l2: 18.3282	valid_1's l2: 20.2797
[1010]	training's l2: 18.3027	valid_1's l2: 20.2792
[1020]	training's l2: 18.2768	valid_1's l2: 20.2782
[1030]	training's l2: 18.2509	valid_1's l2: 20.2781
[1040]	training's l2: 18.2255	valid_1's l2: 20.2776
[1050]	training's l2: 18.2002	valid_1's l2: 20.2765
[1060]	training's l2: 18.1758	valid_1's l2: 20.2759
[1070]	training's l2: 18.1507	valid_1's l2: 20.2752
[1080]	training's l2: 18.1254	valid_1's l2: 20.2746
[1090]	training's l2: 18.1	valid_1's l2: 20.2739
[1100]	training's l2: 18.0749	valid_1's l2: 20.2737
[1110]	training's l2: 18.0494	valid_1's l2: 20.2729
[1120]	training's l2: 18.0238	valid_1's l2: 20.2726
[1130]	training's l2: 17.9985	valid_1's l2: 20.272
[1140]	training's l2: 17.9752	valid_1's l2: 20.2717
[1150]	training's l2: 17.9516	valid_1's l2: 20.2714
[1160]	training's l2: 17.927	valid_1's l2: 20.2711
[1170]	training's l2: 17.9044	valid_1's l2: 20.2701
[1180]	training's l2: 17.8806	valid_1's l2: 20.2702
[1190]	training's l2: 17.8573	valid_1's l2: 20.2707
Early stopping, best iteration is:
[1167]	training's l2: 17.9112	valid_1's l2: 20.2698
score1: 3.8377421573276163
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251737 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.5684	valid_1's l2: 30.6758
[20]	training's l2: 29.5934	valid_1's l2: 28.7233
[30]	training's l2: 28.0345	valid_1's l2: 27.1863
[40]	training's l2: 26.7997	valid_1's l2: 25.9757
[50]	training's l2: 25.8148	valid_1's l2: 25.0164
[60]	training's l2: 25.0245	valid_1's l2: 24.2517
[70]	training's l2: 24.3868	valid_1's l2: 23.6431
[80]	training's l2: 23.8659	valid_1's l2: 23.1541
[90]	training's l2: 23.4377	valid_1's l2: 22.7584
[100]	training's l2: 23.0838	valid_1's l2: 22.4386
[110]	training's l2: 22.7898	valid_1's l2: 22.1772
[120]	training's l2: 22.5418	valid_1's l2: 21.9624
[130]	training's l2: 22.3266	valid_1's l2: 21.785
[140]	training's l2: 22.14	valid_1's l2: 21.6355
[150]	training's l2: 21.9765	valid_1's l2: 21.5089
[160]	training's l2: 21.8289	valid_1's l2: 21.401
[170]	training's l2: 21.6967	valid_1's l2: 21.3091
[180]	training's l2: 21.5773	valid_1's l2: 21.2299
[190]	training's l2: 21.4658	valid_1's l2: 21.1582
[200]	training's l2: 21.3621	valid_1's l2: 21.0918
[210]	training's l2: 21.2679	valid_1's l2: 21.038
[220]	training's l2: 21.1792	valid_1's l2: 20.9875
[230]	training's l2: 21.0958	valid_1's l2: 20.9409
[240]	training's l2: 21.0174	valid_1's l2: 20.9008
[250]	training's l2: 20.9438	valid_1's l2: 20.8637
[260]	training's l2: 20.8711	valid_1's l2: 20.8291
[270]	training's l2: 20.7997	valid_1's l2: 20.796
[280]	training's l2: 20.7318	valid_1's l2: 20.7671
[290]	training's l2: 20.6663	valid_1's l2: 20.741
[300]	training's l2: 20.6039	valid_1's l2: 20.7171
[310]	training's l2: 20.5436	valid_1's l2: 20.6943
[320]	training's l2: 20.4857	valid_1's l2: 20.6753
[330]	training's l2: 20.429	valid_1's l2: 20.6566
[340]	training's l2: 20.3739	valid_1's l2: 20.6388
[350]	training's l2: 20.3215	valid_1's l2: 20.624
[360]	training's l2: 20.2685	valid_1's l2: 20.6079
[370]	training's l2: 20.2175	valid_1's l2: 20.5957
[380]	training's l2: 20.1659	valid_1's l2: 20.5807
[390]	training's l2: 20.117	valid_1's l2: 20.5681
[400]	training's l2: 20.0679	valid_1's l2: 20.5558
[410]	training's l2: 20.02	valid_1's l2: 20.5448
[420]	training's l2: 19.9722	valid_1's l2: 20.5331
[430]	training's l2: 19.9257	valid_1's l2: 20.5247
[440]	training's l2: 19.8794	valid_1's l2: 20.516
[450]	training's l2: 19.8334	valid_1's l2: 20.5064
[460]	training's l2: 19.7886	valid_1's l2: 20.4974
[470]	training's l2: 19.7444	valid_1's l2: 20.489
[480]	training's l2: 19.6992	valid_1's l2: 20.4803
[490]	training's l2: 19.6546	valid_1's l2: 20.4715
[500]	training's l2: 19.611	valid_1's l2: 20.464
[510]	training's l2: 19.5674	valid_1's l2: 20.4562
[520]	training's l2: 19.5235	valid_1's l2: 20.4486
[530]	training's l2: 19.4809	valid_1's l2: 20.4413
[540]	training's l2: 19.4379	valid_1's l2: 20.4342
[550]	training's l2: 19.3963	valid_1's l2: 20.4292
[560]	training's l2: 19.355	valid_1's l2: 20.4231
[570]	training's l2: 19.3138	valid_1's l2: 20.4163
[580]	training's l2: 19.2724	valid_1's l2: 20.4098
[590]	training's l2: 19.2316	valid_1's l2: 20.4029
[600]	training's l2: 19.1915	valid_1's l2: 20.3971
[610]	training's l2: 19.1517	valid_1's l2: 20.3916
[620]	training's l2: 19.112	valid_1's l2: 20.3859
[630]	training's l2: 19.0722	valid_1's l2: 20.3818
[640]	training's l2: 19.0333	valid_1's l2: 20.3762
[650]	training's l2: 18.9945	valid_1's l2: 20.3718
[660]	training's l2: 18.9562	valid_1's l2: 20.3671
[670]	training's l2: 18.9184	valid_1's l2: 20.3633
[680]	training's l2: 18.8799	valid_1's l2: 20.3596
[690]	training's l2: 18.8424	valid_1's l2: 20.3558
[700]	training's l2: 18.8048	valid_1's l2: 20.352
[710]	training's l2: 18.767	valid_1's l2: 20.3491
[720]	training's l2: 18.7298	valid_1's l2: 20.3458
[730]	training's l2: 18.6933	valid_1's l2: 20.3439
[740]	training's l2: 18.6569	valid_1's l2: 20.3401
[750]	training's l2: 18.6211	valid_1's l2: 20.3381
[760]	training's l2: 18.5848	valid_1's l2: 20.3351
[770]	training's l2: 18.5489	valid_1's l2: 20.3318
[780]	training's l2: 18.5136	valid_1's l2: 20.3291
[790]	training's l2: 18.4782	valid_1's l2: 20.3258
[800]	training's l2: 18.4429	valid_1's l2: 20.3229
[810]	training's l2: 18.4076	valid_1's l2: 20.3204
[820]	training's l2: 18.3725	valid_1's l2: 20.3165
[830]	training's l2: 18.3377	valid_1's l2: 20.3143
[840]	training's l2: 18.3032	valid_1's l2: 20.3106
[850]	training's l2: 18.2687	valid_1's l2: 20.3076
[860]	training's l2: 18.2344	valid_1's l2: 20.3052
[870]	training's l2: 18.2002	valid_1's l2: 20.3025
[880]	training's l2: 18.1661	valid_1's l2: 20.3
[890]	training's l2: 18.1322	valid_1's l2: 20.2972
[900]	training's l2: 18.0987	valid_1's l2: 20.294
[910]	training's l2: 18.0655	valid_1's l2: 20.2921
[920]	training's l2: 18.0318	valid_1's l2: 20.2898
[930]	training's l2: 17.9988	valid_1's l2: 20.2877
[940]	training's l2: 17.966	valid_1's l2: 20.2853
[950]	training's l2: 17.9339	valid_1's l2: 20.2838
[960]	training's l2: 17.9016	valid_1's l2: 20.2816
[970]	training's l2: 17.8696	valid_1's l2: 20.2803
[980]	training's l2: 17.8381	valid_1's l2: 20.2794
[990]	training's l2: 17.8062	valid_1's l2: 20.2783
[1000]	training's l2: 17.7742	valid_1's l2: 20.277
[1010]	training's l2: 17.7428	valid_1's l2: 20.276
[1020]	training's l2: 17.7118	valid_1's l2: 20.275
[1030]	training's l2: 17.6807	valid_1's l2: 20.2738
[1040]	training's l2: 17.6498	valid_1's l2: 20.2731
[1050]	training's l2: 17.6194	valid_1's l2: 20.2717
[1060]	training's l2: 17.5896	valid_1's l2: 20.2706
[1070]	training's l2: 17.5593	valid_1's l2: 20.2699
[1080]	training's l2: 17.5296	valid_1's l2: 20.2691
[1090]	training's l2: 17.4996	valid_1's l2: 20.2689
[1100]	training's l2: 17.4707	valid_1's l2: 20.2693
[1110]	training's l2: 17.4411	valid_1's l2: 20.2678
[1120]	training's l2: 17.4121	valid_1's l2: 20.267
[1130]	training's l2: 17.3835	valid_1's l2: 20.2652
[1140]	training's l2: 17.3554	valid_1's l2: 20.2657
[1150]	training's l2: 17.3278	valid_1's l2: 20.264
[1160]	training's l2: 17.3	valid_1's l2: 20.2652
[1170]	training's l2: 17.2712	valid_1's l2: 20.263
[1180]	training's l2: 17.2417	valid_1's l2: 20.2621
[1190]	training's l2: 17.212	valid_1's l2: 20.2613
[1200]	training's l2: 17.1844	valid_1's l2: 20.2605
[1210]	training's l2: 17.1562	valid_1's l2: 20.2605
[1220]	training's l2: 17.1284	valid_1's l2: 20.2591
[1230]	training's l2: 17.1005	valid_1's l2: 20.2583
[1240]	training's l2: 17.0732	valid_1's l2: 20.2567
[1250]	training's l2: 17.0457	valid_1's l2: 20.2564
[1260]	training's l2: 17.0183	valid_1's l2: 20.256
[1270]	training's l2: 16.9907	valid_1's l2: 20.2551
[1280]	training's l2: 16.9644	valid_1's l2: 20.2547
[1290]	training's l2: 16.9371	valid_1's l2: 20.2547
[1300]	training's l2: 16.9101	valid_1's l2: 20.254
[1310]	training's l2: 16.8825	valid_1's l2: 20.2542
[1320]	training's l2: 16.856	valid_1's l2: 20.2544
[1330]	training's l2: 16.8295	valid_1's l2: 20.2546
[1340]	training's l2: 16.8046	valid_1's l2: 20.254
[1350]	training's l2: 16.7785	valid_1's l2: 20.2542
Early stopping, best iteration is:
[1324]	training's l2: 16.845	valid_1's l2: 20.2537
score1: 3.837840140871736
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.241288 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.7255	valid_1's l2: 30.8238
[20]	training's l2: 29.8472	valid_1's l2: 28.9589
[30]	training's l2: 28.3458	valid_1's l2: 27.4712
[40]	training's l2: 27.1413	valid_1's l2: 26.2821
[50]	training's l2: 26.1691	valid_1's l2: 25.3237
[60]	training's l2: 25.3797	valid_1's l2: 24.5516
[70]	training's l2: 24.7353	valid_1's l2: 23.9294
[80]	training's l2: 24.207	valid_1's l2: 23.4221
[90]	training's l2: 23.7692	valid_1's l2: 23.0072
[100]	training's l2: 23.4048	valid_1's l2: 22.6694
[110]	training's l2: 23.1017	valid_1's l2: 22.3911
[120]	training's l2: 22.847	valid_1's l2: 22.1612
[130]	training's l2: 22.6283	valid_1's l2: 21.9691
[140]	training's l2: 22.4362	valid_1's l2: 21.8049
[150]	training's l2: 22.2699	valid_1's l2: 21.667
[160]	training's l2: 22.1221	valid_1's l2: 21.5476
[170]	training's l2: 21.9906	valid_1's l2: 21.4459
[180]	training's l2: 21.8708	valid_1's l2: 21.3574
[190]	training's l2: 21.7638	valid_1's l2: 21.2804
[200]	training's l2: 21.6623	valid_1's l2: 21.2108
[210]	training's l2: 21.5687	valid_1's l2: 21.1465
[220]	training's l2: 21.4836	valid_1's l2: 21.0909
[230]	training's l2: 21.404	valid_1's l2: 21.0419
[240]	training's l2: 21.3293	valid_1's l2: 20.996
[250]	training's l2: 21.2592	valid_1's l2: 20.9551
[260]	training's l2: 21.1931	valid_1's l2: 20.9189
[270]	training's l2: 21.1296	valid_1's l2: 20.8849
[280]	training's l2: 21.0664	valid_1's l2: 20.8507
[290]	training's l2: 21.0058	valid_1's l2: 20.8202
[300]	training's l2: 20.9485	valid_1's l2: 20.7926
[310]	training's l2: 20.8935	valid_1's l2: 20.7672
[320]	training's l2: 20.8401	valid_1's l2: 20.7445
[330]	training's l2: 20.7897	valid_1's l2: 20.7256
[340]	training's l2: 20.7416	valid_1's l2: 20.7063
[350]	training's l2: 20.6936	valid_1's l2: 20.6884
[360]	training's l2: 20.6474	valid_1's l2: 20.6708
[370]	training's l2: 20.6023	valid_1's l2: 20.6556
[380]	training's l2: 20.5583	valid_1's l2: 20.6398
[390]	training's l2: 20.5145	valid_1's l2: 20.6254
[400]	training's l2: 20.4713	valid_1's l2: 20.6121
[410]	training's l2: 20.4285	valid_1's l2: 20.5987
[420]	training's l2: 20.3877	valid_1's l2: 20.5873
[430]	training's l2: 20.3467	valid_1's l2: 20.5746
[440]	training's l2: 20.3063	valid_1's l2: 20.5636
[450]	training's l2: 20.2668	valid_1's l2: 20.5523
[460]	training's l2: 20.2279	valid_1's l2: 20.5424
[470]	training's l2: 20.1899	valid_1's l2: 20.5329
[480]	training's l2: 20.1519	valid_1's l2: 20.524
[490]	training's l2: 20.1141	valid_1's l2: 20.5146
[500]	training's l2: 20.0771	valid_1's l2: 20.5065
[510]	training's l2: 20.0405	valid_1's l2: 20.5009
[520]	training's l2: 20.0036	valid_1's l2: 20.4928
[530]	training's l2: 19.9674	valid_1's l2: 20.4865
[540]	training's l2: 19.932	valid_1's l2: 20.479
[550]	training's l2: 19.8965	valid_1's l2: 20.4722
[560]	training's l2: 19.8612	valid_1's l2: 20.4654
[570]	training's l2: 19.8264	valid_1's l2: 20.458
[580]	training's l2: 19.7911	valid_1's l2: 20.4516
[590]	training's l2: 19.7562	valid_1's l2: 20.4463
[600]	training's l2: 19.7214	valid_1's l2: 20.4393
[610]	training's l2: 19.6876	valid_1's l2: 20.4333
[620]	training's l2: 19.6539	valid_1's l2: 20.4273
[630]	training's l2: 19.6204	valid_1's l2: 20.4213
[640]	training's l2: 19.5868	valid_1's l2: 20.4159
[650]	training's l2: 19.5536	valid_1's l2: 20.4112
[660]	training's l2: 19.5203	valid_1's l2: 20.405
[670]	training's l2: 19.4883	valid_1's l2: 20.4007
[680]	training's l2: 19.456	valid_1's l2: 20.396
[690]	training's l2: 19.424	valid_1's l2: 20.3915
[700]	training's l2: 19.3923	valid_1's l2: 20.387
[710]	training's l2: 19.3607	valid_1's l2: 20.3826
[720]	training's l2: 19.3299	valid_1's l2: 20.3787
[730]	training's l2: 19.2988	valid_1's l2: 20.3736
[740]	training's l2: 19.2679	valid_1's l2: 20.3696
[750]	training's l2: 19.2373	valid_1's l2: 20.3667
[760]	training's l2: 19.2065	valid_1's l2: 20.3631
[770]	training's l2: 19.1765	valid_1's l2: 20.36
[780]	training's l2: 19.1464	valid_1's l2: 20.3568
[790]	training's l2: 19.1161	valid_1's l2: 20.3533
[800]	training's l2: 19.0862	valid_1's l2: 20.3514
[810]	training's l2: 19.0564	valid_1's l2: 20.349
[820]	training's l2: 19.0271	valid_1's l2: 20.3458
[830]	training's l2: 18.9978	valid_1's l2: 20.3431
[840]	training's l2: 18.9688	valid_1's l2: 20.3396
[850]	training's l2: 18.9395	valid_1's l2: 20.3363
[860]	training's l2: 18.9111	valid_1's l2: 20.3344
[870]	training's l2: 18.8823	valid_1's l2: 20.3314
[880]	training's l2: 18.854	valid_1's l2: 20.3288
[890]	training's l2: 18.8254	valid_1's l2: 20.3276
[900]	training's l2: 18.7971	valid_1's l2: 20.3254
[910]	training's l2: 18.7681	valid_1's l2: 20.3221
[920]	training's l2: 18.7392	valid_1's l2: 20.3195
[930]	training's l2: 18.7108	valid_1's l2: 20.3171
[940]	training's l2: 18.6827	valid_1's l2: 20.3143
[950]	training's l2: 18.6552	valid_1's l2: 20.3119
[960]	training's l2: 18.627	valid_1's l2: 20.3087
[970]	training's l2: 18.599	valid_1's l2: 20.3066
[980]	training's l2: 18.5712	valid_1's l2: 20.3042
[990]	training's l2: 18.5441	valid_1's l2: 20.3025
[1000]	training's l2: 18.5169	valid_1's l2: 20.3005
[1010]	training's l2: 18.49	valid_1's l2: 20.299
[1020]	training's l2: 18.4638	valid_1's l2: 20.2976
[1030]	training's l2: 18.4366	valid_1's l2: 20.2948
[1040]	training's l2: 18.4107	valid_1's l2: 20.2936
[1050]	training's l2: 18.3848	valid_1's l2: 20.2922
[1060]	training's l2: 18.3589	valid_1's l2: 20.2903
[1070]	training's l2: 18.3331	valid_1's l2: 20.2887
[1080]	training's l2: 18.3073	valid_1's l2: 20.2873
[1090]	training's l2: 18.2812	valid_1's l2: 20.2856
[1100]	training's l2: 18.2561	valid_1's l2: 20.284
[1110]	training's l2: 18.2297	valid_1's l2: 20.2822
[1120]	training's l2: 18.2058	valid_1's l2: 20.2817
[1130]	training's l2: 18.1809	valid_1's l2: 20.2807
[1140]	training's l2: 18.1566	valid_1's l2: 20.2802
[1150]	training's l2: 18.1321	valid_1's l2: 20.2786
[1160]	training's l2: 18.1071	valid_1's l2: 20.2781
[1170]	training's l2: 18.0828	valid_1's l2: 20.2763
[1180]	training's l2: 18.0588	valid_1's l2: 20.2751
[1190]	training's l2: 18.034	valid_1's l2: 20.2733
[1200]	training's l2: 18.01	valid_1's l2: 20.2723
[1210]	training's l2: 17.9859	valid_1's l2: 20.2715
[1220]	training's l2: 17.9615	valid_1's l2: 20.2695
[1230]	training's l2: 17.9377	valid_1's l2: 20.2682
[1240]	training's l2: 17.9139	valid_1's l2: 20.2673
[1250]	training's l2: 17.8904	valid_1's l2: 20.2653
[1260]	training's l2: 17.8674	valid_1's l2: 20.2649
[1270]	training's l2: 17.8445	valid_1's l2: 20.264
[1280]	training's l2: 17.8225	valid_1's l2: 20.2638
[1290]	training's l2: 17.7996	valid_1's l2: 20.2637
[1300]	training's l2: 17.7776	valid_1's l2: 20.2627
[1310]	training's l2: 17.7543	valid_1's l2: 20.2608
[1320]	training's l2: 17.7316	valid_1's l2: 20.2601
[1330]	training's l2: 17.7091	valid_1's l2: 20.2596
[1340]	training's l2: 17.6872	valid_1's l2: 20.2591
[1350]	training's l2: 17.6651	valid_1's l2: 20.2592
[1360]	training's l2: 17.6425	valid_1's l2: 20.2573
[1370]	training's l2: 17.62	valid_1's l2: 20.2562
[1380]	training's l2: 17.5977	valid_1's l2: 20.2554
[1390]	training's l2: 17.5747	valid_1's l2: 20.255
[1400]	training's l2: 17.553	valid_1's l2: 20.2539
[1410]	training's l2: 17.5295	valid_1's l2: 20.2529
[1420]	training's l2: 17.5075	valid_1's l2: 20.2528
[1430]	training's l2: 17.4862	valid_1's l2: 20.2517
[1440]	training's l2: 17.4644	valid_1's l2: 20.2504
[1450]	training's l2: 17.4423	valid_1's l2: 20.2497
[1460]	training's l2: 17.4209	valid_1's l2: 20.2485
[1470]	training's l2: 17.4003	valid_1's l2: 20.2479
[1480]	training's l2: 17.3789	valid_1's l2: 20.2475
[1490]	training's l2: 17.3584	valid_1's l2: 20.2474
[1500]	training's l2: 17.3362	valid_1's l2: 20.2471
[1510]	training's l2: 17.3157	valid_1's l2: 20.2461
[1520]	training's l2: 17.295	valid_1's l2: 20.2465
[1530]	training's l2: 17.2741	valid_1's l2: 20.2458
[1540]	training's l2: 17.2544	valid_1's l2: 20.2459
[1550]	training's l2: 17.2338	valid_1's l2: 20.2451
[1560]	training's l2: 17.2134	valid_1's l2: 20.2445
[1570]	training's l2: 17.192	valid_1's l2: 20.2442
[1580]	training's l2: 17.1698	valid_1's l2: 20.2429
[1590]	training's l2: 17.1493	valid_1's l2: 20.2427
[1600]	training's l2: 17.1281	valid_1's l2: 20.2427
[1610]	training's l2: 17.1077	valid_1's l2: 20.2428
[1620]	training's l2: 17.0881	valid_1's l2: 20.2424
[1630]	training's l2: 17.0682	valid_1's l2: 20.242
[1640]	training's l2: 17.0471	valid_1's l2: 20.2411
[1650]	training's l2: 17.027	valid_1's l2: 20.2412
[1660]	training's l2: 17.0067	valid_1's l2: 20.2407
[1670]	training's l2: 16.9858	valid_1's l2: 20.2405
[1680]	training's l2: 16.9657	valid_1's l2: 20.2401
[1690]	training's l2: 16.9449	valid_1's l2: 20.2401
[1700]	training's l2: 16.9252	valid_1's l2: 20.241
[1710]	training's l2: 16.9049	valid_1's l2: 20.2406
Early stopping, best iteration is:
[1682]	training's l2: 16.9616	valid_1's l2: 20.2397
score1: 3.835457342748614
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262476 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.3948	valid_1's l2: 29.5069
[20]	training's l2: 27.8648	valid_1's l2: 27.005
[30]	training's l2: 26.1071	valid_1's l2: 25.2802
[40]	training's l2: 24.8675	valid_1's l2: 24.0802
[50]	training's l2: 23.9796	valid_1's l2: 23.2351
[60]	training's l2: 23.3299	valid_1's l2: 22.6331
[70]	training's l2: 22.8482	valid_1's l2: 22.1991
[80]	training's l2: 22.479	valid_1's l2: 21.8807
[90]	training's l2: 22.1846	valid_1's l2: 21.6389
[100]	training's l2: 21.944	valid_1's l2: 21.4523
[110]	training's l2: 21.7406	valid_1's l2: 21.3073
[120]	training's l2: 21.5648	valid_1's l2: 21.1889
[130]	training's l2: 21.4085	valid_1's l2: 21.0871
[140]	training's l2: 21.2738	valid_1's l2: 21.0062
[150]	training's l2: 21.1475	valid_1's l2: 20.9338
[160]	training's l2: 21.035	valid_1's l2: 20.8742
[170]	training's l2: 20.9239	valid_1's l2: 20.8196
[180]	training's l2: 20.821	valid_1's l2: 20.7748
[190]	training's l2: 20.7255	valid_1's l2: 20.737
[200]	training's l2: 20.6352	valid_1's l2: 20.703
[210]	training's l2: 20.5495	valid_1's l2: 20.6737
[220]	training's l2: 20.4686	valid_1's l2: 20.6492
[230]	training's l2: 20.3897	valid_1's l2: 20.6282
[240]	training's l2: 20.3118	valid_1's l2: 20.604
[250]	training's l2: 20.2374	valid_1's l2: 20.5839
[260]	training's l2: 20.1664	valid_1's l2: 20.5668
[270]	training's l2: 20.094	valid_1's l2: 20.5507
[280]	training's l2: 20.0256	valid_1's l2: 20.5363
[290]	training's l2: 19.9574	valid_1's l2: 20.5213
[300]	training's l2: 19.8899	valid_1's l2: 20.5084
[310]	training's l2: 19.8245	valid_1's l2: 20.4959
[320]	training's l2: 19.7598	valid_1's l2: 20.488
[330]	training's l2: 19.6952	valid_1's l2: 20.4752
[340]	training's l2: 19.631	valid_1's l2: 20.4642
[350]	training's l2: 19.5684	valid_1's l2: 20.4542
[360]	training's l2: 19.5064	valid_1's l2: 20.4445
[370]	training's l2: 19.4443	valid_1's l2: 20.4345
[380]	training's l2: 19.3851	valid_1's l2: 20.4247
[390]	training's l2: 19.3255	valid_1's l2: 20.4154
[400]	training's l2: 19.267	valid_1's l2: 20.4087
[410]	training's l2: 19.2089	valid_1's l2: 20.4006
[420]	training's l2: 19.1512	valid_1's l2: 20.393
[430]	training's l2: 19.0943	valid_1's l2: 20.3855
[440]	training's l2: 19.0383	valid_1's l2: 20.3795
[450]	training's l2: 18.9821	valid_1's l2: 20.3736
[460]	training's l2: 18.9269	valid_1's l2: 20.3687
[470]	training's l2: 18.8729	valid_1's l2: 20.3649
[480]	training's l2: 18.8189	valid_1's l2: 20.3613
[490]	training's l2: 18.7645	valid_1's l2: 20.3572
[500]	training's l2: 18.7115	valid_1's l2: 20.352
[510]	training's l2: 18.6588	valid_1's l2: 20.3481
[520]	training's l2: 18.6062	valid_1's l2: 20.3423
[530]	training's l2: 18.5541	valid_1's l2: 20.3382
[540]	training's l2: 18.5019	valid_1's l2: 20.3326
[550]	training's l2: 18.4513	valid_1's l2: 20.3283
[560]	training's l2: 18.4014	valid_1's l2: 20.3261
[570]	training's l2: 18.3509	valid_1's l2: 20.3224
[580]	training's l2: 18.3021	valid_1's l2: 20.3179
[590]	training's l2: 18.2522	valid_1's l2: 20.3143
[600]	training's l2: 18.2033	valid_1's l2: 20.312
[610]	training's l2: 18.1554	valid_1's l2: 20.3106
[620]	training's l2: 18.1064	valid_1's l2: 20.3057
[630]	training's l2: 18.0596	valid_1's l2: 20.3024
[640]	training's l2: 18.0128	valid_1's l2: 20.299
[650]	training's l2: 17.965	valid_1's l2: 20.2945
[660]	training's l2: 17.9193	valid_1's l2: 20.2912
[670]	training's l2: 17.874	valid_1's l2: 20.2916
[680]	training's l2: 17.8275	valid_1's l2: 20.2892
[690]	training's l2: 17.7811	valid_1's l2: 20.2874
[700]	training's l2: 17.737	valid_1's l2: 20.2857
[710]	training's l2: 17.6929	valid_1's l2: 20.2828
[720]	training's l2: 17.6502	valid_1's l2: 20.2825
[730]	training's l2: 17.6081	valid_1's l2: 20.2801
[740]	training's l2: 17.567	valid_1's l2: 20.2805
[750]	training's l2: 17.5266	valid_1's l2: 20.2789
[760]	training's l2: 17.4842	valid_1's l2: 20.2795
[770]	training's l2: 17.4431	valid_1's l2: 20.2789
[780]	training's l2: 17.4024	valid_1's l2: 20.2782
[790]	training's l2: 17.3606	valid_1's l2: 20.2785
[800]	training's l2: 17.3183	valid_1's l2: 20.2775
[810]	training's l2: 17.2784	valid_1's l2: 20.2765
[820]	training's l2: 17.2377	valid_1's l2: 20.2754
[830]	training's l2: 17.1981	valid_1's l2: 20.2748
[840]	training's l2: 17.1582	valid_1's l2: 20.2749
[850]	training's l2: 17.1174	valid_1's l2: 20.2732
[860]	training's l2: 17.0779	valid_1's l2: 20.2726
[870]	training's l2: 17.0369	valid_1's l2: 20.2717
[880]	training's l2: 16.9973	valid_1's l2: 20.272
[890]	training's l2: 16.957	valid_1's l2: 20.2708
[900]	training's l2: 16.919	valid_1's l2: 20.2687
[910]	training's l2: 16.8803	valid_1's l2: 20.2677
[920]	training's l2: 16.8413	valid_1's l2: 20.2682
[930]	training's l2: 16.8029	valid_1's l2: 20.2678
[940]	training's l2: 16.7636	valid_1's l2: 20.2667
[950]	training's l2: 16.7259	valid_1's l2: 20.2665
[960]	training's l2: 16.6869	valid_1's l2: 20.2659
[970]	training's l2: 16.6512	valid_1's l2: 20.2648
[980]	training's l2: 16.6157	valid_1's l2: 20.2646
[990]	training's l2: 16.5778	valid_1's l2: 20.2656
[1000]	training's l2: 16.5411	valid_1's l2: 20.2663
Early stopping, best iteration is:
[978]	training's l2: 16.6228	valid_1's l2: 20.2645
score1: 3.836775252213277
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254216 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.7279	valid_1's l2: 30.824
[20]	training's l2: 29.854	valid_1's l2: 28.9607
[30]	training's l2: 28.3574	valid_1's l2: 27.4719
[40]	training's l2: 27.1579	valid_1's l2: 26.2844
[50]	training's l2: 26.1908	valid_1's l2: 25.3305
[60]	training's l2: 25.4048	valid_1's l2: 24.5591
[70]	training's l2: 24.7651	valid_1's l2: 23.937
[80]	training's l2: 24.2414	valid_1's l2: 23.4337
[90]	training's l2: 23.8084	valid_1's l2: 23.0199
[100]	training's l2: 23.4498	valid_1's l2: 22.6816
[110]	training's l2: 23.1512	valid_1's l2: 22.4049
[120]	training's l2: 22.9003	valid_1's l2: 22.1766
[130]	training's l2: 22.6836	valid_1's l2: 21.9852
[140]	training's l2: 22.4945	valid_1's l2: 21.821
[150]	training's l2: 22.3296	valid_1's l2: 21.6804
[160]	training's l2: 22.1859	valid_1's l2: 21.5631
[170]	training's l2: 22.0563	valid_1's l2: 21.4603
[180]	training's l2: 21.941	valid_1's l2: 21.3724
[190]	training's l2: 21.8364	valid_1's l2: 21.2958
[200]	training's l2: 21.7388	valid_1's l2: 21.2267
[210]	training's l2: 21.6483	valid_1's l2: 21.162
[220]	training's l2: 21.5652	valid_1's l2: 21.1051
[230]	training's l2: 21.4882	valid_1's l2: 21.0554
[240]	training's l2: 21.4162	valid_1's l2: 21.0095
[250]	training's l2: 21.3494	valid_1's l2: 20.9692
[260]	training's l2: 21.2873	valid_1's l2: 20.9329
[270]	training's l2: 21.2252	valid_1's l2: 20.8977
[280]	training's l2: 21.1654	valid_1's l2: 20.8647
[290]	training's l2: 21.1095	valid_1's l2: 20.8345
[300]	training's l2: 21.0554	valid_1's l2: 20.81
[310]	training's l2: 21.0028	valid_1's l2: 20.7854
[320]	training's l2: 20.9525	valid_1's l2: 20.7625
[330]	training's l2: 20.9045	valid_1's l2: 20.7417
[340]	training's l2: 20.8582	valid_1's l2: 20.7222
[350]	training's l2: 20.8122	valid_1's l2: 20.7031
[360]	training's l2: 20.7683	valid_1's l2: 20.6857
[370]	training's l2: 20.7258	valid_1's l2: 20.669
[380]	training's l2: 20.6835	valid_1's l2: 20.6536
[390]	training's l2: 20.643	valid_1's l2: 20.6399
[400]	training's l2: 20.6027	valid_1's l2: 20.6256
[410]	training's l2: 20.5626	valid_1's l2: 20.6123
[420]	training's l2: 20.5233	valid_1's l2: 20.601
[430]	training's l2: 20.485	valid_1's l2: 20.5889
[440]	training's l2: 20.4473	valid_1's l2: 20.5769
[450]	training's l2: 20.4098	valid_1's l2: 20.5656
[460]	training's l2: 20.3726	valid_1's l2: 20.5551
[470]	training's l2: 20.3372	valid_1's l2: 20.5451
[480]	training's l2: 20.3019	valid_1's l2: 20.5357
[490]	training's l2: 20.2666	valid_1's l2: 20.5268
[500]	training's l2: 20.2319	valid_1's l2: 20.5182
[510]	training's l2: 20.198	valid_1's l2: 20.5112
[520]	training's l2: 20.164	valid_1's l2: 20.5034
[530]	training's l2: 20.1304	valid_1's l2: 20.4964
[540]	training's l2: 20.0974	valid_1's l2: 20.4908
[550]	training's l2: 20.0646	valid_1's l2: 20.4845
[560]	training's l2: 20.0319	valid_1's l2: 20.4794
[570]	training's l2: 19.9997	valid_1's l2: 20.4731
[580]	training's l2: 19.9675	valid_1's l2: 20.4667
[590]	training's l2: 19.9356	valid_1's l2: 20.4604
[600]	training's l2: 19.9034	valid_1's l2: 20.4542
[610]	training's l2: 19.8716	valid_1's l2: 20.4476
[620]	training's l2: 19.8394	valid_1's l2: 20.4408
[630]	training's l2: 19.8074	valid_1's l2: 20.4353
[640]	training's l2: 19.7765	valid_1's l2: 20.4292
[650]	training's l2: 19.7457	valid_1's l2: 20.424
[660]	training's l2: 19.7149	valid_1's l2: 20.418
[670]	training's l2: 19.6847	valid_1's l2: 20.414
[680]	training's l2: 19.6549	valid_1's l2: 20.409
[690]	training's l2: 19.6249	valid_1's l2: 20.4037
[700]	training's l2: 19.5953	valid_1's l2: 20.3995
[710]	training's l2: 19.5668	valid_1's l2: 20.3963
[720]	training's l2: 19.538	valid_1's l2: 20.3922
[730]	training's l2: 19.5084	valid_1's l2: 20.3877
[740]	training's l2: 19.4797	valid_1's l2: 20.385
[750]	training's l2: 19.4514	valid_1's l2: 20.3806
[760]	training's l2: 19.4226	valid_1's l2: 20.3766
[770]	training's l2: 19.3939	valid_1's l2: 20.3726
[780]	training's l2: 19.3655	valid_1's l2: 20.3693
[790]	training's l2: 19.3373	valid_1's l2: 20.3662
[800]	training's l2: 19.3093	valid_1's l2: 20.363
[810]	training's l2: 19.2815	valid_1's l2: 20.3592
[820]	training's l2: 19.2541	valid_1's l2: 20.3568
[830]	training's l2: 19.2271	valid_1's l2: 20.3539
[840]	training's l2: 19.1992	valid_1's l2: 20.3507
[850]	training's l2: 19.1721	valid_1's l2: 20.3474
[860]	training's l2: 19.1444	valid_1's l2: 20.345
[870]	training's l2: 19.1173	valid_1's l2: 20.3432
[880]	training's l2: 19.0901	valid_1's l2: 20.3405
[890]	training's l2: 19.0641	valid_1's l2: 20.3382
[900]	training's l2: 19.0376	valid_1's l2: 20.3364
[910]	training's l2: 19.0113	valid_1's l2: 20.3339
[920]	training's l2: 18.9848	valid_1's l2: 20.3309
[930]	training's l2: 18.959	valid_1's l2: 20.3289
[940]	training's l2: 18.9331	valid_1's l2: 20.3268
[950]	training's l2: 18.9074	valid_1's l2: 20.3246
[960]	training's l2: 18.8819	valid_1's l2: 20.3215
[970]	training's l2: 18.8565	valid_1's l2: 20.3196
[980]	training's l2: 18.8311	valid_1's l2: 20.3169
[990]	training's l2: 18.8051	valid_1's l2: 20.3145
[1000]	training's l2: 18.7802	valid_1's l2: 20.3124
[1010]	training's l2: 18.7549	valid_1's l2: 20.31
[1020]	training's l2: 18.7304	valid_1's l2: 20.3086
[1030]	training's l2: 18.7053	valid_1's l2: 20.3062
[1040]	training's l2: 18.6809	valid_1's l2: 20.3039
[1050]	training's l2: 18.6569	valid_1's l2: 20.3022
[1060]	training's l2: 18.6324	valid_1's l2: 20.3006
[1070]	training's l2: 18.6087	valid_1's l2: 20.2988
[1080]	training's l2: 18.584	valid_1's l2: 20.2972
[1090]	training's l2: 18.56	valid_1's l2: 20.2966
[1100]	training's l2: 18.5359	valid_1's l2: 20.295
[1110]	training's l2: 18.5127	valid_1's l2: 20.293
[1120]	training's l2: 18.4904	valid_1's l2: 20.2917
[1130]	training's l2: 18.4666	valid_1's l2: 20.2902
[1140]	training's l2: 18.4434	valid_1's l2: 20.2891
[1150]	training's l2: 18.4199	valid_1's l2: 20.2875
[1160]	training's l2: 18.3971	valid_1's l2: 20.286
[1170]	training's l2: 18.3744	valid_1's l2: 20.2852
[1180]	training's l2: 18.3518	valid_1's l2: 20.2836
[1190]	training's l2: 18.3285	valid_1's l2: 20.2812
[1200]	training's l2: 18.3066	valid_1's l2: 20.2806
[1210]	training's l2: 18.2853	valid_1's l2: 20.2796
[1220]	training's l2: 18.263	valid_1's l2: 20.2788
[1230]	training's l2: 18.2402	valid_1's l2: 20.2776
[1240]	training's l2: 18.2181	valid_1's l2: 20.2771
[1250]	training's l2: 18.1961	valid_1's l2: 20.2759
[1260]	training's l2: 18.1745	valid_1's l2: 20.2749
[1270]	training's l2: 18.1524	valid_1's l2: 20.2742
[1280]	training's l2: 18.1315	valid_1's l2: 20.2738
[1290]	training's l2: 18.1102	valid_1's l2: 20.2723
[1300]	training's l2: 18.089	valid_1's l2: 20.2712
[1310]	training's l2: 18.0669	valid_1's l2: 20.2709
[1320]	training's l2: 18.0457	valid_1's l2: 20.2695
[1330]	training's l2: 18.0241	valid_1's l2: 20.2686
[1340]	training's l2: 18.0032	valid_1's l2: 20.268
[1350]	training's l2: 17.9819	valid_1's l2: 20.2683
[1360]	training's l2: 17.9616	valid_1's l2: 20.2673
[1370]	training's l2: 17.9409	valid_1's l2: 20.2667
[1380]	training's l2: 17.9201	valid_1's l2: 20.2656
[1390]	training's l2: 17.8994	valid_1's l2: 20.2649
[1400]	training's l2: 17.8787	valid_1's l2: 20.2643
[1410]	training's l2: 17.8583	valid_1's l2: 20.2637
[1420]	training's l2: 17.8376	valid_1's l2: 20.2634
[1430]	training's l2: 17.8178	valid_1's l2: 20.2627
[1440]	training's l2: 17.7973	valid_1's l2: 20.2622
[1450]	training's l2: 17.7774	valid_1's l2: 20.2613
[1460]	training's l2: 17.7589	valid_1's l2: 20.2607
[1470]	training's l2: 17.738	valid_1's l2: 20.2595
[1480]	training's l2: 17.7172	valid_1's l2: 20.2588
[1490]	training's l2: 17.6976	valid_1's l2: 20.2585
[1500]	training's l2: 17.6775	valid_1's l2: 20.2584
[1510]	training's l2: 17.6568	valid_1's l2: 20.2577
[1520]	training's l2: 17.6367	valid_1's l2: 20.2566
[1530]	training's l2: 17.6171	valid_1's l2: 20.2559
[1540]	training's l2: 17.5979	valid_1's l2: 20.2553
[1550]	training's l2: 17.5785	valid_1's l2: 20.2548
[1560]	training's l2: 17.5579	valid_1's l2: 20.2538
[1570]	training's l2: 17.5382	valid_1's l2: 20.2527
[1580]	training's l2: 17.5189	valid_1's l2: 20.2521
[1590]	training's l2: 17.4991	valid_1's l2: 20.2516
[1600]	training's l2: 17.4789	valid_1's l2: 20.2507
[1610]	training's l2: 17.4597	valid_1's l2: 20.2504
[1620]	training's l2: 17.4399	valid_1's l2: 20.2496
[1630]	training's l2: 17.4209	valid_1's l2: 20.2493
[1640]	training's l2: 17.4011	valid_1's l2: 20.2491
[1650]	training's l2: 17.3823	valid_1's l2: 20.2485
[1660]	training's l2: 17.3628	valid_1's l2: 20.2481
[1670]	training's l2: 17.3435	valid_1's l2: 20.2477
[1680]	training's l2: 17.3255	valid_1's l2: 20.2472
[1690]	training's l2: 17.3067	valid_1's l2: 20.247
[1700]	training's l2: 17.2872	valid_1's l2: 20.2462
[1710]	training's l2: 17.2686	valid_1's l2: 20.2465
[1720]	training's l2: 17.2492	valid_1's l2: 20.2457
[1730]	training's l2: 17.2304	valid_1's l2: 20.2452
[1740]	training's l2: 17.2121	valid_1's l2: 20.2445
[1750]	training's l2: 17.1932	valid_1's l2: 20.2445
[1760]	training's l2: 17.1746	valid_1's l2: 20.2447
[1770]	training's l2: 17.1564	valid_1's l2: 20.2444
[1780]	training's l2: 17.1386	valid_1's l2: 20.2441
[1790]	training's l2: 17.12	valid_1's l2: 20.2433
[1800]	training's l2: 17.1014	valid_1's l2: 20.2434
[1810]	training's l2: 17.0835	valid_1's l2: 20.2427
[1820]	training's l2: 17.0656	valid_1's l2: 20.2421
[1830]	training's l2: 17.0473	valid_1's l2: 20.2415
[1840]	training's l2: 17.0297	valid_1's l2: 20.2417
[1850]	training's l2: 17.0119	valid_1's l2: 20.2412
[1860]	training's l2: 16.9932	valid_1's l2: 20.2414
[1870]	training's l2: 16.9754	valid_1's l2: 20.2421
[1880]	training's l2: 16.9571	valid_1's l2: 20.2412
[1890]	training's l2: 16.9399	valid_1's l2: 20.2407
[1900]	training's l2: 16.9219	valid_1's l2: 20.2401
[1910]	training's l2: 16.9035	valid_1's l2: 20.2408
[1920]	training's l2: 16.886	valid_1's l2: 20.2404
Early stopping, best iteration is:
[1899]	training's l2: 16.9238	valid_1's l2: 20.2401
score1: 3.8360673406031176
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257857 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.0807	valid_1's l2: 30.1842
[20]	training's l2: 28.8511	valid_1's l2: 27.9719
[30]	training's l2: 27.1889	valid_1's l2: 26.3282
[40]	training's l2: 25.9369	valid_1's l2: 25.0987
[50]	training's l2: 24.9836	valid_1's l2: 24.1733
[60]	training's l2: 24.2501	valid_1's l2: 23.4655
[70]	training's l2: 23.6803	valid_1's l2: 22.929
[80]	training's l2: 23.2329	valid_1's l2: 22.5155
[90]	training's l2: 22.8802	valid_1's l2: 22.1948
[100]	training's l2: 22.5917	valid_1's l2: 21.9417
[110]	training's l2: 22.3505	valid_1's l2: 21.7395
[120]	training's l2: 22.1478	valid_1's l2: 21.5723
[130]	training's l2: 21.9718	valid_1's l2: 21.4364
[140]	training's l2: 21.8198	valid_1's l2: 21.3251
[150]	training's l2: 21.6848	valid_1's l2: 21.2315
[160]	training's l2: 21.5603	valid_1's l2: 21.1483
[170]	training's l2: 21.4513	valid_1's l2: 21.0793
[180]	training's l2: 21.3491	valid_1's l2: 21.0161
[190]	training's l2: 21.2563	valid_1's l2: 20.9629
[200]	training's l2: 21.1707	valid_1's l2: 20.9158
[210]	training's l2: 21.0866	valid_1's l2: 20.8701
[220]	training's l2: 21.0074	valid_1's l2: 20.8314
[230]	training's l2: 20.9321	valid_1's l2: 20.7964
[240]	training's l2: 20.8616	valid_1's l2: 20.7663
[250]	training's l2: 20.7922	valid_1's l2: 20.737
[260]	training's l2: 20.7282	valid_1's l2: 20.7145
[270]	training's l2: 20.6656	valid_1's l2: 20.6924
[280]	training's l2: 20.6049	valid_1's l2: 20.6719
[290]	training's l2: 20.5467	valid_1's l2: 20.6517
[300]	training's l2: 20.4903	valid_1's l2: 20.6335
[310]	training's l2: 20.4343	valid_1's l2: 20.6161
[320]	training's l2: 20.3801	valid_1's l2: 20.6008
[330]	training's l2: 20.3268	valid_1's l2: 20.5833
[340]	training's l2: 20.2736	valid_1's l2: 20.5699
[350]	training's l2: 20.2211	valid_1's l2: 20.5559
[360]	training's l2: 20.1712	valid_1's l2: 20.5444
[370]	training's l2: 20.1207	valid_1's l2: 20.5335
[380]	training's l2: 20.0711	valid_1's l2: 20.5217
[390]	training's l2: 20.0225	valid_1's l2: 20.512
[400]	training's l2: 19.9746	valid_1's l2: 20.5009
[410]	training's l2: 19.9253	valid_1's l2: 20.4913
[420]	training's l2: 19.8787	valid_1's l2: 20.4819
[430]	training's l2: 19.8328	valid_1's l2: 20.4741
[440]	training's l2: 19.7863	valid_1's l2: 20.4666
[450]	training's l2: 19.7405	valid_1's l2: 20.458
[460]	training's l2: 19.6938	valid_1's l2: 20.448
[470]	training's l2: 19.6488	valid_1's l2: 20.44
[480]	training's l2: 19.6045	valid_1's l2: 20.4339
[490]	training's l2: 19.5603	valid_1's l2: 20.4275
[500]	training's l2: 19.517	valid_1's l2: 20.4202
[510]	training's l2: 19.4746	valid_1's l2: 20.4145
[520]	training's l2: 19.4324	valid_1's l2: 20.4083
[530]	training's l2: 19.3897	valid_1's l2: 20.402
[540]	training's l2: 19.3477	valid_1's l2: 20.3961
[550]	training's l2: 19.3065	valid_1's l2: 20.3909
[560]	training's l2: 19.2653	valid_1's l2: 20.3867
[570]	training's l2: 19.2241	valid_1's l2: 20.382
[580]	training's l2: 19.1837	valid_1's l2: 20.3786
[590]	training's l2: 19.1444	valid_1's l2: 20.375
[600]	training's l2: 19.1032	valid_1's l2: 20.3703
[610]	training's l2: 19.0636	valid_1's l2: 20.367
[620]	training's l2: 19.0245	valid_1's l2: 20.3627
[630]	training's l2: 18.9849	valid_1's l2: 20.3586
[640]	training's l2: 18.946	valid_1's l2: 20.3547
[650]	training's l2: 18.9067	valid_1's l2: 20.3519
[660]	training's l2: 18.8685	valid_1's l2: 20.3487
[670]	training's l2: 18.8307	valid_1's l2: 20.3465
[680]	training's l2: 18.7931	valid_1's l2: 20.3434
[690]	training's l2: 18.755	valid_1's l2: 20.3392
[700]	training's l2: 18.7183	valid_1's l2: 20.3372
[710]	training's l2: 18.681	valid_1's l2: 20.3337
[720]	training's l2: 18.6444	valid_1's l2: 20.3312
[730]	training's l2: 18.6079	valid_1's l2: 20.3288
[740]	training's l2: 18.572	valid_1's l2: 20.3253
[750]	training's l2: 18.5363	valid_1's l2: 20.3227
[760]	training's l2: 18.5003	valid_1's l2: 20.3205
[770]	training's l2: 18.465	valid_1's l2: 20.3175
[780]	training's l2: 18.4296	valid_1's l2: 20.315
[790]	training's l2: 18.3946	valid_1's l2: 20.3123
[800]	training's l2: 18.3598	valid_1's l2: 20.3105
[810]	training's l2: 18.3251	valid_1's l2: 20.3077
[820]	training's l2: 18.2915	valid_1's l2: 20.3054
[830]	training's l2: 18.2571	valid_1's l2: 20.3036
[840]	training's l2: 18.224	valid_1's l2: 20.3008
[850]	training's l2: 18.1916	valid_1's l2: 20.3002
[860]	training's l2: 18.1588	valid_1's l2: 20.2983
[870]	training's l2: 18.1259	valid_1's l2: 20.2965
[880]	training's l2: 18.0933	valid_1's l2: 20.2957
[890]	training's l2: 18.0614	valid_1's l2: 20.2954
[900]	training's l2: 18.0285	valid_1's l2: 20.2928
[910]	training's l2: 17.9969	valid_1's l2: 20.2923
[920]	training's l2: 17.9647	valid_1's l2: 20.2907
[930]	training's l2: 17.9341	valid_1's l2: 20.2903
[940]	training's l2: 17.9023	valid_1's l2: 20.2886
[950]	training's l2: 17.8714	valid_1's l2: 20.2877
[960]	training's l2: 17.8414	valid_1's l2: 20.2871
[970]	training's l2: 17.8082	valid_1's l2: 20.2858
[980]	training's l2: 17.7772	valid_1's l2: 20.2847
[990]	training's l2: 17.7475	valid_1's l2: 20.2841
[1000]	training's l2: 17.717	valid_1's l2: 20.2834
[1010]	training's l2: 17.6863	valid_1's l2: 20.2828
[1020]	training's l2: 17.6565	valid_1's l2: 20.2825
[1030]	training's l2: 17.6263	valid_1's l2: 20.2812
[1040]	training's l2: 17.5966	valid_1's l2: 20.2796
[1050]	training's l2: 17.5673	valid_1's l2: 20.2794
[1060]	training's l2: 17.5377	valid_1's l2: 20.2792
[1070]	training's l2: 17.5084	valid_1's l2: 20.2787
[1080]	training's l2: 17.4788	valid_1's l2: 20.2779
[1090]	training's l2: 17.4503	valid_1's l2: 20.2775
[1100]	training's l2: 17.4224	valid_1's l2: 20.2774
[1110]	training's l2: 17.3938	valid_1's l2: 20.2768
[1120]	training's l2: 17.3655	valid_1's l2: 20.2768
[1130]	training's l2: 17.3376	valid_1's l2: 20.2769
[1140]	training's l2: 17.3105	valid_1's l2: 20.2763
[1150]	training's l2: 17.281	valid_1's l2: 20.2759
[1160]	training's l2: 17.2526	valid_1's l2: 20.2749
[1170]	training's l2: 17.2236	valid_1's l2: 20.2744
[1180]	training's l2: 17.1948	valid_1's l2: 20.2736
[1190]	training's l2: 17.167	valid_1's l2: 20.2735
[1200]	training's l2: 17.1392	valid_1's l2: 20.273
[1210]	training's l2: 17.1124	valid_1's l2: 20.2719
[1220]	training's l2: 17.0852	valid_1's l2: 20.2713
[1230]	training's l2: 17.0603	valid_1's l2: 20.2708
[1240]	training's l2: 17.0336	valid_1's l2: 20.2709
[1250]	training's l2: 17.0061	valid_1's l2: 20.2692
[1260]	training's l2: 16.9779	valid_1's l2: 20.2673
[1270]	training's l2: 16.9507	valid_1's l2: 20.2673
[1280]	training's l2: 16.9236	valid_1's l2: 20.2683
[1290]	training's l2: 16.8982	valid_1's l2: 20.2681
Early stopping, best iteration is:
[1264]	training's l2: 16.9666	valid_1's l2: 20.2671
score1: 3.8402519248622733
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.242245 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.4866	valid_1's l2: 30.5898
[20]	training's l2: 29.4679	valid_1's l2: 28.589
[30]	training's l2: 27.8915	valid_1's l2: 27.0334
[40]	training's l2: 26.6547	valid_1's l2: 25.8171
[50]	training's l2: 25.6771	valid_1's l2: 24.8585
[60]	training's l2: 24.8984	valid_1's l2: 24.1045
[70]	training's l2: 24.2746	valid_1's l2: 23.5094
[80]	training's l2: 23.7695	valid_1's l2: 23.0324
[90]	training's l2: 23.3578	valid_1's l2: 22.6529
[100]	training's l2: 23.0216	valid_1's l2: 22.3482
[110]	training's l2: 22.7434	valid_1's l2: 22.1011
[120]	training's l2: 22.5055	valid_1's l2: 21.8973
[130]	training's l2: 22.2999	valid_1's l2: 21.7252
[140]	training's l2: 22.1236	valid_1's l2: 21.5841
[150]	training's l2: 21.9682	valid_1's l2: 21.4639
[160]	training's l2: 21.8277	valid_1's l2: 21.3622
[170]	training's l2: 21.7042	valid_1's l2: 21.2747
[180]	training's l2: 21.5892	valid_1's l2: 21.1969
[190]	training's l2: 21.483	valid_1's l2: 21.1266
[200]	training's l2: 21.3875	valid_1's l2: 21.0674
[210]	training's l2: 21.2993	valid_1's l2: 21.015
[220]	training's l2: 21.2152	valid_1's l2: 20.9666
[230]	training's l2: 21.137	valid_1's l2: 20.9239
[240]	training's l2: 21.063	valid_1's l2: 20.884
[250]	training's l2: 20.9899	valid_1's l2: 20.8453
[260]	training's l2: 20.92	valid_1's l2: 20.8112
[270]	training's l2: 20.8543	valid_1's l2: 20.7825
[280]	training's l2: 20.7893	valid_1's l2: 20.7543
[290]	training's l2: 20.7283	valid_1's l2: 20.7283
[300]	training's l2: 20.6715	valid_1's l2: 20.7078
[310]	training's l2: 20.6147	valid_1's l2: 20.6866
[320]	training's l2: 20.5597	valid_1's l2: 20.6681
[330]	training's l2: 20.5074	valid_1's l2: 20.6511
[340]	training's l2: 20.4564	valid_1's l2: 20.6349
[350]	training's l2: 20.4049	valid_1's l2: 20.6177
[360]	training's l2: 20.3552	valid_1's l2: 20.603
[370]	training's l2: 20.3059	valid_1's l2: 20.5891
[380]	training's l2: 20.2577	valid_1's l2: 20.5754
[390]	training's l2: 20.211	valid_1's l2: 20.5634
[400]	training's l2: 20.1641	valid_1's l2: 20.5513
[410]	training's l2: 20.1183	valid_1's l2: 20.5402
[420]	training's l2: 20.0734	valid_1's l2: 20.5302
[430]	training's l2: 20.0285	valid_1's l2: 20.5208
[440]	training's l2: 19.9855	valid_1's l2: 20.5126
[450]	training's l2: 19.941	valid_1's l2: 20.504
[460]	training's l2: 19.898	valid_1's l2: 20.4945
[470]	training's l2: 19.8556	valid_1's l2: 20.4872
[480]	training's l2: 19.8137	valid_1's l2: 20.4801
[490]	training's l2: 19.7716	valid_1's l2: 20.4718
[500]	training's l2: 19.7302	valid_1's l2: 20.4635
[510]	training's l2: 19.6894	valid_1's l2: 20.4559
[520]	training's l2: 19.6481	valid_1's l2: 20.4485
[530]	training's l2: 19.6069	valid_1's l2: 20.4409
[540]	training's l2: 19.5658	valid_1's l2: 20.4346
[550]	training's l2: 19.5261	valid_1's l2: 20.4289
[560]	training's l2: 19.4867	valid_1's l2: 20.422
[570]	training's l2: 19.448	valid_1's l2: 20.4175
[580]	training's l2: 19.4089	valid_1's l2: 20.4113
[590]	training's l2: 19.3699	valid_1's l2: 20.406
[600]	training's l2: 19.3309	valid_1's l2: 20.3997
[610]	training's l2: 19.2931	valid_1's l2: 20.3958
[620]	training's l2: 19.2554	valid_1's l2: 20.3902
[630]	training's l2: 19.2181	valid_1's l2: 20.3857
[640]	training's l2: 19.1804	valid_1's l2: 20.3817
[650]	training's l2: 19.1441	valid_1's l2: 20.378
[660]	training's l2: 19.107	valid_1's l2: 20.3735
[670]	training's l2: 19.0705	valid_1's l2: 20.3693
[680]	training's l2: 19.0346	valid_1's l2: 20.3663
[690]	training's l2: 18.9995	valid_1's l2: 20.3623
[700]	training's l2: 18.9635	valid_1's l2: 20.3576
[710]	training's l2: 18.9278	valid_1's l2: 20.3529
[720]	training's l2: 18.8929	valid_1's l2: 20.3489
[730]	training's l2: 18.8584	valid_1's l2: 20.3456
[740]	training's l2: 18.8232	valid_1's l2: 20.343
[750]	training's l2: 18.7891	valid_1's l2: 20.3412
[760]	training's l2: 18.7547	valid_1's l2: 20.3383
[770]	training's l2: 18.7207	valid_1's l2: 20.3359
[780]	training's l2: 18.6863	valid_1's l2: 20.3327
[790]	training's l2: 18.6527	valid_1's l2: 20.3298
[800]	training's l2: 18.6189	valid_1's l2: 20.3265
[810]	training's l2: 18.5851	valid_1's l2: 20.3234
[820]	training's l2: 18.5525	valid_1's l2: 20.3214
[830]	training's l2: 18.5186	valid_1's l2: 20.3188
[840]	training's l2: 18.4859	valid_1's l2: 20.3161
[850]	training's l2: 18.4526	valid_1's l2: 20.3132
[860]	training's l2: 18.4203	valid_1's l2: 20.3111
[870]	training's l2: 18.3875	valid_1's l2: 20.3088
[880]	training's l2: 18.3556	valid_1's l2: 20.3063
[890]	training's l2: 18.3235	valid_1's l2: 20.3043
[900]	training's l2: 18.2917	valid_1's l2: 20.3023
[910]	training's l2: 18.26	valid_1's l2: 20.3011
[920]	training's l2: 18.2284	valid_1's l2: 20.299
[930]	training's l2: 18.1967	valid_1's l2: 20.2964
[940]	training's l2: 18.1672	valid_1's l2: 20.295
[950]	training's l2: 18.1366	valid_1's l2: 20.2928
[960]	training's l2: 18.1054	valid_1's l2: 20.2909
[970]	training's l2: 18.0757	valid_1's l2: 20.2892
[980]	training's l2: 18.0463	valid_1's l2: 20.2872
[990]	training's l2: 18.0161	valid_1's l2: 20.2858
[1000]	training's l2: 17.9866	valid_1's l2: 20.2841
[1010]	training's l2: 17.9585	valid_1's l2: 20.2833
[1020]	training's l2: 17.9298	valid_1's l2: 20.282
[1030]	training's l2: 17.9009	valid_1's l2: 20.2806
[1040]	training's l2: 17.8735	valid_1's l2: 20.2798
[1050]	training's l2: 17.8459	valid_1's l2: 20.2792
[1060]	training's l2: 17.817	valid_1's l2: 20.2777
[1070]	training's l2: 17.789	valid_1's l2: 20.2762
[1080]	training's l2: 17.7612	valid_1's l2: 20.275
[1090]	training's l2: 17.7326	valid_1's l2: 20.2746
[1100]	training's l2: 17.7049	valid_1's l2: 20.2739
[1110]	training's l2: 17.6754	valid_1's l2: 20.2721
[1120]	training's l2: 17.648	valid_1's l2: 20.2711
[1130]	training's l2: 17.619	valid_1's l2: 20.27
[1140]	training's l2: 17.5908	valid_1's l2: 20.2693
[1150]	training's l2: 17.5638	valid_1's l2: 20.2688
[1160]	training's l2: 17.5363	valid_1's l2: 20.2683
[1170]	training's l2: 17.5099	valid_1's l2: 20.2679
[1180]	training's l2: 17.4837	valid_1's l2: 20.2663
[1190]	training's l2: 17.4583	valid_1's l2: 20.2654
[1200]	training's l2: 17.4317	valid_1's l2: 20.2648
[1210]	training's l2: 17.4043	valid_1's l2: 20.2639
[1220]	training's l2: 17.3781	valid_1's l2: 20.2626
[1230]	training's l2: 17.3533	valid_1's l2: 20.2616
[1240]	training's l2: 17.3279	valid_1's l2: 20.2616
[1250]	training's l2: 17.3008	valid_1's l2: 20.2609
[1260]	training's l2: 17.2738	valid_1's l2: 20.2609
[1270]	training's l2: 17.2479	valid_1's l2: 20.2606
[1280]	training's l2: 17.2232	valid_1's l2: 20.2595
[1290]	training's l2: 17.198	valid_1's l2: 20.2587
[1300]	training's l2: 17.1723	valid_1's l2: 20.2584
[1310]	training's l2: 17.1456	valid_1's l2: 20.2573
[1320]	training's l2: 17.1219	valid_1's l2: 20.2578
[1330]	training's l2: 17.0963	valid_1's l2: 20.2581
[1340]	training's l2: 17.0727	valid_1's l2: 20.2584
Early stopping, best iteration is:
[1310]	training's l2: 17.1456	valid_1's l2: 20.2573
score1: 3.838599972773629
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257977 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 32.121	valid_1's l2: 31.2104
[20]	training's l2: 30.4961	valid_1's l2: 29.5882
[30]	training's l2: 29.147	valid_1's l2: 28.2433
[40]	training's l2: 28.0268	valid_1's l2: 27.1274
[50]	training's l2: 27.0899	valid_1's l2: 26.1955
[60]	training's l2: 26.3067	valid_1's l2: 25.4207
[70]	training's l2: 25.6471	valid_1's l2: 24.7706
[80]	training's l2: 25.0897	valid_1's l2: 24.2244
[90]	training's l2: 24.6195	valid_1's l2: 23.7658
[100]	training's l2: 24.2183	valid_1's l2: 23.3761
[110]	training's l2: 23.8762	valid_1's l2: 23.0474
[120]	training's l2: 23.5826	valid_1's l2: 22.7682
[130]	training's l2: 23.3323	valid_1's l2: 22.5326
[140]	training's l2: 23.1164	valid_1's l2: 22.3301
[150]	training's l2: 22.9274	valid_1's l2: 22.1552
[160]	training's l2: 22.7563	valid_1's l2: 22.0002
[170]	training's l2: 22.6057	valid_1's l2: 21.8653
[180]	training's l2: 22.4726	valid_1's l2: 21.7493
[190]	training's l2: 22.3518	valid_1's l2: 21.6463
[200]	training's l2: 22.2444	valid_1's l2: 21.5553
[210]	training's l2: 22.1461	valid_1's l2: 21.4755
[220]	training's l2: 22.0573	valid_1's l2: 21.4043
[230]	training's l2: 21.9724	valid_1's l2: 21.3377
[240]	training's l2: 21.8943	valid_1's l2: 21.277
[250]	training's l2: 21.8219	valid_1's l2: 21.2231
[260]	training's l2: 21.7535	valid_1's l2: 21.1738
[270]	training's l2: 21.6902	valid_1's l2: 21.1285
[280]	training's l2: 21.6308	valid_1's l2: 21.086
[290]	training's l2: 21.5756	valid_1's l2: 21.0482
[300]	training's l2: 21.5227	valid_1's l2: 21.0132
[310]	training's l2: 21.4741	valid_1's l2: 20.9822
[320]	training's l2: 21.4265	valid_1's l2: 20.952
[330]	training's l2: 21.3802	valid_1's l2: 20.9235
[340]	training's l2: 21.3347	valid_1's l2: 20.8953
[350]	training's l2: 21.2926	valid_1's l2: 20.8714
[360]	training's l2: 21.2512	valid_1's l2: 20.8491
[370]	training's l2: 21.2112	valid_1's l2: 20.8271
[380]	training's l2: 21.1728	valid_1's l2: 20.8065
[390]	training's l2: 21.1359	valid_1's l2: 20.7874
[400]	training's l2: 21.0999	valid_1's l2: 20.7695
[410]	training's l2: 21.0648	valid_1's l2: 20.7531
[420]	training's l2: 21.0307	valid_1's l2: 20.7372
[430]	training's l2: 20.9977	valid_1's l2: 20.7226
[440]	training's l2: 20.9647	valid_1's l2: 20.7082
[450]	training's l2: 20.9328	valid_1's l2: 20.6955
[460]	training's l2: 20.9017	valid_1's l2: 20.6829
[470]	training's l2: 20.8711	valid_1's l2: 20.6716
[480]	training's l2: 20.8407	valid_1's l2: 20.6588
[490]	training's l2: 20.8113	valid_1's l2: 20.6473
[500]	training's l2: 20.7827	valid_1's l2: 20.6373
[510]	training's l2: 20.7546	valid_1's l2: 20.6273
[520]	training's l2: 20.7269	valid_1's l2: 20.6173
[530]	training's l2: 20.6995	valid_1's l2: 20.6067
[540]	training's l2: 20.6722	valid_1's l2: 20.5971
[550]	training's l2: 20.6457	valid_1's l2: 20.589
[560]	training's l2: 20.6187	valid_1's l2: 20.5801
[570]	training's l2: 20.5928	valid_1's l2: 20.5719
[580]	training's l2: 20.5673	valid_1's l2: 20.5649
[590]	training's l2: 20.5421	valid_1's l2: 20.5572
[600]	training's l2: 20.5165	valid_1's l2: 20.5502
[610]	training's l2: 20.4916	valid_1's l2: 20.5438
[620]	training's l2: 20.4669	valid_1's l2: 20.5366
[630]	training's l2: 20.4423	valid_1's l2: 20.5304
[640]	training's l2: 20.418	valid_1's l2: 20.5243
[650]	training's l2: 20.3932	valid_1's l2: 20.5176
[660]	training's l2: 20.369	valid_1's l2: 20.5117
[670]	training's l2: 20.3454	valid_1's l2: 20.5057
[680]	training's l2: 20.3221	valid_1's l2: 20.5002
[690]	training's l2: 20.2991	valid_1's l2: 20.495
[700]	training's l2: 20.2754	valid_1's l2: 20.4893
[710]	training's l2: 20.252	valid_1's l2: 20.4836
[720]	training's l2: 20.2285	valid_1's l2: 20.478
[730]	training's l2: 20.2058	valid_1's l2: 20.4737
[740]	training's l2: 20.1831	valid_1's l2: 20.4694
[750]	training's l2: 20.1603	valid_1's l2: 20.4639
[760]	training's l2: 20.1377	valid_1's l2: 20.4588
[770]	training's l2: 20.1148	valid_1's l2: 20.4533
[780]	training's l2: 20.0921	valid_1's l2: 20.4484
[790]	training's l2: 20.0695	valid_1's l2: 20.4429
[800]	training's l2: 20.0477	valid_1's l2: 20.4391
[810]	training's l2: 20.0261	valid_1's l2: 20.4344
[820]	training's l2: 20.0044	valid_1's l2: 20.4298
[830]	training's l2: 19.9825	valid_1's l2: 20.4251
[840]	training's l2: 19.9614	valid_1's l2: 20.4212
[850]	training's l2: 19.9403	valid_1's l2: 20.4177
[860]	training's l2: 19.919	valid_1's l2: 20.4135
[870]	training's l2: 19.8981	valid_1's l2: 20.4101
[880]	training's l2: 19.8768	valid_1's l2: 20.4066
[890]	training's l2: 19.8561	valid_1's l2: 20.4031
[900]	training's l2: 19.8353	valid_1's l2: 20.3996
[910]	training's l2: 19.8144	valid_1's l2: 20.3954
[920]	training's l2: 19.7937	valid_1's l2: 20.3925
[930]	training's l2: 19.7735	valid_1's l2: 20.3886
[940]	training's l2: 19.7531	valid_1's l2: 20.3842
[950]	training's l2: 19.7332	valid_1's l2: 20.3816
[960]	training's l2: 19.7132	valid_1's l2: 20.3789
[970]	training's l2: 19.6934	valid_1's l2: 20.3757
[980]	training's l2: 19.6739	valid_1's l2: 20.3734
[990]	training's l2: 19.6546	valid_1's l2: 20.3712
[1000]	training's l2: 19.6349	valid_1's l2: 20.3677
[1010]	training's l2: 19.6154	valid_1's l2: 20.3646
[1020]	training's l2: 19.5957	valid_1's l2: 20.3618
[1030]	training's l2: 19.576	valid_1's l2: 20.3586
[1040]	training's l2: 19.5568	valid_1's l2: 20.356
[1050]	training's l2: 19.5376	valid_1's l2: 20.3538
[1060]	training's l2: 19.5183	valid_1's l2: 20.3514
[1070]	training's l2: 19.4995	valid_1's l2: 20.3498
[1080]	training's l2: 19.4802	valid_1's l2: 20.3467
[1090]	training's l2: 19.4617	valid_1's l2: 20.3448
[1100]	training's l2: 19.4433	valid_1's l2: 20.3431
[1110]	training's l2: 19.4248	valid_1's l2: 20.3409
[1120]	training's l2: 19.406	valid_1's l2: 20.3387
[1130]	training's l2: 19.3875	valid_1's l2: 20.3365
[1140]	training's l2: 19.3692	valid_1's l2: 20.3354
[1150]	training's l2: 19.3513	valid_1's l2: 20.3336
[1160]	training's l2: 19.3327	valid_1's l2: 20.3314
[1170]	training's l2: 19.3145	valid_1's l2: 20.33
[1180]	training's l2: 19.2962	valid_1's l2: 20.3283
[1190]	training's l2: 19.2781	valid_1's l2: 20.3268
[1200]	training's l2: 19.26	valid_1's l2: 20.3264
[1210]	training's l2: 19.2419	valid_1's l2: 20.3242
[1220]	training's l2: 19.224	valid_1's l2: 20.3231
[1230]	training's l2: 19.2055	valid_1's l2: 20.3213
[1240]	training's l2: 19.1879	valid_1's l2: 20.3204
[1250]	training's l2: 19.1705	valid_1's l2: 20.3193
[1260]	training's l2: 19.1532	valid_1's l2: 20.3176
[1270]	training's l2: 19.1355	valid_1's l2: 20.3162
[1280]	training's l2: 19.1183	valid_1's l2: 20.3149
[1290]	training's l2: 19.1012	valid_1's l2: 20.3133
[1300]	training's l2: 19.0837	valid_1's l2: 20.3123
[1310]	training's l2: 19.0661	valid_1's l2: 20.3113
[1320]	training's l2: 19.0492	valid_1's l2: 20.3103
[1330]	training's l2: 19.0315	valid_1's l2: 20.3096
[1340]	training's l2: 19.0146	valid_1's l2: 20.3085
[1350]	training's l2: 18.9973	valid_1's l2: 20.3067
[1360]	training's l2: 18.9795	valid_1's l2: 20.305
[1370]	training's l2: 18.9629	valid_1's l2: 20.3041
[1380]	training's l2: 18.9469	valid_1's l2: 20.3036
[1390]	training's l2: 18.9302	valid_1's l2: 20.3024
[1400]	training's l2: 18.9132	valid_1's l2: 20.3015
[1410]	training's l2: 18.8968	valid_1's l2: 20.3007
[1420]	training's l2: 18.8802	valid_1's l2: 20.2994
[1430]	training's l2: 18.8635	valid_1's l2: 20.2985
[1440]	training's l2: 18.8472	valid_1's l2: 20.2978
[1450]	training's l2: 18.8313	valid_1's l2: 20.2959
[1460]	training's l2: 18.8149	valid_1's l2: 20.295
[1470]	training's l2: 18.7993	valid_1's l2: 20.294
[1480]	training's l2: 18.7836	valid_1's l2: 20.2932
[1490]	training's l2: 18.7682	valid_1's l2: 20.2924
[1500]	training's l2: 18.7519	valid_1's l2: 20.2917
[1510]	training's l2: 18.7362	valid_1's l2: 20.2904
[1520]	training's l2: 18.7208	valid_1's l2: 20.2897
[1530]	training's l2: 18.7046	valid_1's l2: 20.2882
[1540]	training's l2: 18.6886	valid_1's l2: 20.2866
[1550]	training's l2: 18.6735	valid_1's l2: 20.2856
[1560]	training's l2: 18.6573	valid_1's l2: 20.2846
[1570]	training's l2: 18.6421	valid_1's l2: 20.2843
[1580]	training's l2: 18.6268	valid_1's l2: 20.2833
[1590]	training's l2: 18.6112	valid_1's l2: 20.2822
[1600]	training's l2: 18.5955	valid_1's l2: 20.2808
[1610]	training's l2: 18.5804	valid_1's l2: 20.2802
[1620]	training's l2: 18.5659	valid_1's l2: 20.2801
[1630]	training's l2: 18.5509	valid_1's l2: 20.2796
[1640]	training's l2: 18.5357	valid_1's l2: 20.2791
[1650]	training's l2: 18.5201	valid_1's l2: 20.2786
[1660]	training's l2: 18.505	valid_1's l2: 20.2782
[1670]	training's l2: 18.4906	valid_1's l2: 20.2773
[1680]	training's l2: 18.4754	valid_1's l2: 20.2766
[1690]	training's l2: 18.4612	valid_1's l2: 20.276
[1700]	training's l2: 18.4463	valid_1's l2: 20.2753
[1710]	training's l2: 18.4315	valid_1's l2: 20.275
[1720]	training's l2: 18.4177	valid_1's l2: 20.2741
[1730]	training's l2: 18.4026	valid_1's l2: 20.2741
[1740]	training's l2: 18.388	valid_1's l2: 20.2731
[1750]	training's l2: 18.3743	valid_1's l2: 20.2731
[1760]	training's l2: 18.3595	valid_1's l2: 20.2724
[1770]	training's l2: 18.3447	valid_1's l2: 20.2711
[1780]	training's l2: 18.3299	valid_1's l2: 20.2706
[1790]	training's l2: 18.3151	valid_1's l2: 20.2707
[1800]	training's l2: 18.3007	valid_1's l2: 20.2705
[1810]	training's l2: 18.2861	valid_1's l2: 20.2702
[1820]	training's l2: 18.2726	valid_1's l2: 20.2699
[1830]	training's l2: 18.2585	valid_1's l2: 20.2691
[1840]	training's l2: 18.2435	valid_1's l2: 20.2686
[1850]	training's l2: 18.2301	valid_1's l2: 20.2683
[1860]	training's l2: 18.2165	valid_1's l2: 20.2678
[1870]	training's l2: 18.2019	valid_1's l2: 20.2679
[1880]	training's l2: 18.1878	valid_1's l2: 20.2667
[1890]	training's l2: 18.174	valid_1's l2: 20.2661
[1900]	training's l2: 18.1598	valid_1's l2: 20.2659
[1910]	training's l2: 18.1464	valid_1's l2: 20.2648
[1920]	training's l2: 18.1325	valid_1's l2: 20.2649
[1930]	training's l2: 18.1179	valid_1's l2: 20.264
[1940]	training's l2: 18.1032	valid_1's l2: 20.2635
[1950]	training's l2: 18.0893	valid_1's l2: 20.2629
[1960]	training's l2: 18.0755	valid_1's l2: 20.2621
[1970]	training's l2: 18.0624	valid_1's l2: 20.2624
[1980]	training's l2: 18.0486	valid_1's l2: 20.2625
[1990]	training's l2: 18.0348	valid_1's l2: 20.262
[2000]	training's l2: 18.0212	valid_1's l2: 20.2614
[2010]	training's l2: 18.008	valid_1's l2: 20.261
[2020]	training's l2: 17.9937	valid_1's l2: 20.2609
[2030]	training's l2: 17.9806	valid_1's l2: 20.2606
[2040]	training's l2: 17.9674	valid_1's l2: 20.2606
[2050]	training's l2: 17.9536	valid_1's l2: 20.2609
[2060]	training's l2: 17.9403	valid_1's l2: 20.2604
[2070]	training's l2: 17.9277	valid_1's l2: 20.26
[2080]	training's l2: 17.9143	valid_1's l2: 20.2595
[2090]	training's l2: 17.9002	valid_1's l2: 20.2588
[2100]	training's l2: 17.8868	valid_1's l2: 20.2587
[2110]	training's l2: 17.874	valid_1's l2: 20.258
[2120]	training's l2: 17.8611	valid_1's l2: 20.258
[2130]	training's l2: 17.8476	valid_1's l2: 20.2573
[2140]	training's l2: 17.8344	valid_1's l2: 20.2576
[2150]	training's l2: 17.821	valid_1's l2: 20.2576
[2160]	training's l2: 17.8073	valid_1's l2: 20.2578
Early stopping, best iteration is:
[2132]	training's l2: 17.8449	valid_1's l2: 20.2572
score1: 3.8368580148883638
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243229 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.9882	valid_1's l2: 30.0852
[20]	training's l2: 28.7193	valid_1's l2: 27.8294
[30]	training's l2: 27.0467	valid_1's l2: 26.1685
[40]	training's l2: 25.802	valid_1's l2: 24.9425
[50]	training's l2: 24.8645	valid_1's l2: 24.026
[60]	training's l2: 24.1513	valid_1's l2: 23.3383
[70]	training's l2: 23.6028	valid_1's l2: 22.8179
[80]	training's l2: 23.1778	valid_1's l2: 22.4207
[90]	training's l2: 22.8429	valid_1's l2: 22.1168
[100]	training's l2: 22.5661	valid_1's l2: 21.8727
[110]	training's l2: 22.3401	valid_1's l2: 21.6795
[120]	training's l2: 22.1485	valid_1's l2: 21.5212
[130]	training's l2: 21.9847	valid_1's l2: 21.3935
[140]	training's l2: 21.8425	valid_1's l2: 21.29
[150]	training's l2: 21.712	valid_1's l2: 21.1947
[160]	training's l2: 21.597	valid_1's l2: 21.1146
[170]	training's l2: 21.494	valid_1's l2: 21.0448
[180]	training's l2: 21.3994	valid_1's l2: 20.9859
[190]	training's l2: 21.3138	valid_1's l2: 20.9353
[200]	training's l2: 21.2316	valid_1's l2: 20.8883
[210]	training's l2: 21.1536	valid_1's l2: 20.8464
[220]	training's l2: 21.0798	valid_1's l2: 20.8103
[230]	training's l2: 21.0103	valid_1's l2: 20.7779
[240]	training's l2: 20.944	valid_1's l2: 20.7494
[250]	training's l2: 20.8819	valid_1's l2: 20.7238
[260]	training's l2: 20.8221	valid_1's l2: 20.6991
[270]	training's l2: 20.7631	valid_1's l2: 20.6749
[280]	training's l2: 20.7072	valid_1's l2: 20.655
[290]	training's l2: 20.6532	valid_1's l2: 20.6364
[300]	training's l2: 20.6001	valid_1's l2: 20.6201
[310]	training's l2: 20.5476	valid_1's l2: 20.6032
[320]	training's l2: 20.4972	valid_1's l2: 20.5876
[330]	training's l2: 20.4466	valid_1's l2: 20.5727
[340]	training's l2: 20.398	valid_1's l2: 20.5598
[350]	training's l2: 20.3502	valid_1's l2: 20.5485
[360]	training's l2: 20.3036	valid_1's l2: 20.5377
[370]	training's l2: 20.2575	valid_1's l2: 20.5269
[380]	training's l2: 20.212	valid_1's l2: 20.5172
[390]	training's l2: 20.1667	valid_1's l2: 20.5072
[400]	training's l2: 20.1223	valid_1's l2: 20.4975
[410]	training's l2: 20.0785	valid_1's l2: 20.489
[420]	training's l2: 20.0347	valid_1's l2: 20.4795
[430]	training's l2: 19.9909	valid_1's l2: 20.4704
[440]	training's l2: 19.9474	valid_1's l2: 20.4613
[450]	training's l2: 19.9052	valid_1's l2: 20.4531
[460]	training's l2: 19.8623	valid_1's l2: 20.444
[470]	training's l2: 19.8212	valid_1's l2: 20.4378
[480]	training's l2: 19.7797	valid_1's l2: 20.4312
[490]	training's l2: 19.7392	valid_1's l2: 20.4239
[500]	training's l2: 19.699	valid_1's l2: 20.4177
[510]	training's l2: 19.6585	valid_1's l2: 20.4114
[520]	training's l2: 19.6187	valid_1's l2: 20.4047
[530]	training's l2: 19.5794	valid_1's l2: 20.399
[540]	training's l2: 19.5404	valid_1's l2: 20.3938
[550]	training's l2: 19.5015	valid_1's l2: 20.3869
[560]	training's l2: 19.4638	valid_1's l2: 20.382
[570]	training's l2: 19.4267	valid_1's l2: 20.3763
[580]	training's l2: 19.3882	valid_1's l2: 20.3701
[590]	training's l2: 19.3514	valid_1's l2: 20.366
[600]	training's l2: 19.3143	valid_1's l2: 20.3616
[610]	training's l2: 19.2776	valid_1's l2: 20.3577
[620]	training's l2: 19.2416	valid_1's l2: 20.3526
[630]	training's l2: 19.205	valid_1's l2: 20.3479
[640]	training's l2: 19.1688	valid_1's l2: 20.345
[650]	training's l2: 19.1331	valid_1's l2: 20.3414
[660]	training's l2: 19.0973	valid_1's l2: 20.3378
[670]	training's l2: 19.0618	valid_1's l2: 20.3339
[680]	training's l2: 19.0263	valid_1's l2: 20.3311
[690]	training's l2: 18.9908	valid_1's l2: 20.3275
[700]	training's l2: 18.9559	valid_1's l2: 20.3241
[710]	training's l2: 18.922	valid_1's l2: 20.321
[720]	training's l2: 18.8879	valid_1's l2: 20.3171
[730]	training's l2: 18.8541	valid_1's l2: 20.314
[740]	training's l2: 18.8194	valid_1's l2: 20.3109
[750]	training's l2: 18.7857	valid_1's l2: 20.3092
[760]	training's l2: 18.7527	valid_1's l2: 20.3066
[770]	training's l2: 18.7198	valid_1's l2: 20.3048
[780]	training's l2: 18.687	valid_1's l2: 20.3014
[790]	training's l2: 18.6549	valid_1's l2: 20.2981
[800]	training's l2: 18.6234	valid_1's l2: 20.2956
[810]	training's l2: 18.592	valid_1's l2: 20.2938
[820]	training's l2: 18.5607	valid_1's l2: 20.2916
[830]	training's l2: 18.5288	valid_1's l2: 20.29
[840]	training's l2: 18.4985	valid_1's l2: 20.2882
[850]	training's l2: 18.4675	valid_1's l2: 20.2866
[860]	training's l2: 18.4374	valid_1's l2: 20.2856
[870]	training's l2: 18.4076	valid_1's l2: 20.2839
[880]	training's l2: 18.3773	valid_1's l2: 20.2824
[890]	training's l2: 18.3475	valid_1's l2: 20.2816
[900]	training's l2: 18.3175	valid_1's l2: 20.2802
[910]	training's l2: 18.2873	valid_1's l2: 20.2798
[920]	training's l2: 18.2576	valid_1's l2: 20.2793
[930]	training's l2: 18.2298	valid_1's l2: 20.2784
[940]	training's l2: 18.1987	valid_1's l2: 20.2783
[950]	training's l2: 18.1694	valid_1's l2: 20.2759
[960]	training's l2: 18.1391	valid_1's l2: 20.2744
[970]	training's l2: 18.1101	valid_1's l2: 20.2741
[980]	training's l2: 18.081	valid_1's l2: 20.2719
[990]	training's l2: 18.0537	valid_1's l2: 20.2707
[1000]	training's l2: 18.0253	valid_1's l2: 20.269
[1010]	training's l2: 17.9974	valid_1's l2: 20.2686
[1020]	training's l2: 17.9712	valid_1's l2: 20.2673
[1030]	training's l2: 17.9435	valid_1's l2: 20.2672
[1040]	training's l2: 17.9161	valid_1's l2: 20.2668
[1050]	training's l2: 17.8886	valid_1's l2: 20.2658
[1060]	training's l2: 17.8605	valid_1's l2: 20.2655
[1070]	training's l2: 17.8329	valid_1's l2: 20.2652
[1080]	training's l2: 17.8048	valid_1's l2: 20.2649
[1090]	training's l2: 17.7776	valid_1's l2: 20.2643
[1100]	training's l2: 17.7514	valid_1's l2: 20.2624
[1110]	training's l2: 17.7248	valid_1's l2: 20.2624
[1120]	training's l2: 17.6993	valid_1's l2: 20.2609
[1130]	training's l2: 17.6735	valid_1's l2: 20.261
[1140]	training's l2: 17.6451	valid_1's l2: 20.2614
[1150]	training's l2: 17.6184	valid_1's l2: 20.2602
[1160]	training's l2: 17.5922	valid_1's l2: 20.2601
[1170]	training's l2: 17.5675	valid_1's l2: 20.2603
[1180]	training's l2: 17.541	valid_1's l2: 20.2599
[1190]	training's l2: 17.5149	valid_1's l2: 20.2596
[1200]	training's l2: 17.4884	valid_1's l2: 20.2591
[1210]	training's l2: 17.4632	valid_1's l2: 20.2588
[1220]	training's l2: 17.4375	valid_1's l2: 20.2575
[1230]	training's l2: 17.4113	valid_1's l2: 20.2572
[1240]	training's l2: 17.3864	valid_1's l2: 20.2563
[1250]	training's l2: 17.3601	valid_1's l2: 20.2558
[1260]	training's l2: 17.3351	valid_1's l2: 20.2553
[1270]	training's l2: 17.3086	valid_1's l2: 20.2543
[1280]	training's l2: 17.2822	valid_1's l2: 20.2538
[1290]	training's l2: 17.2575	valid_1's l2: 20.2543
[1300]	training's l2: 17.2334	valid_1's l2: 20.2546
[1310]	training's l2: 17.21	valid_1's l2: 20.2544
Early stopping, best iteration is:
[1284]	training's l2: 17.2722	valid_1's l2: 20.2537
score1: 3.8390701658878488
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268926 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.703	valid_1's l2: 30.8014
[20]	training's l2: 29.8121	valid_1's l2: 28.9238
[30]	training's l2: 28.3039	valid_1's l2: 27.4288
[40]	training's l2: 27.0985	valid_1's l2: 26.2371
[50]	training's l2: 26.1261	valid_1's l2: 25.2795
[60]	training's l2: 25.3387	valid_1's l2: 24.5092
[70]	training's l2: 24.6973	valid_1's l2: 23.8888
[80]	training's l2: 24.1723	valid_1's l2: 23.3845
[90]	training's l2: 23.7391	valid_1's l2: 22.9757
[100]	training's l2: 23.3785	valid_1's l2: 22.6394
[110]	training's l2: 23.0794	valid_1's l2: 22.3633
[120]	training's l2: 22.8283	valid_1's l2: 22.1364
[130]	training's l2: 22.6112	valid_1's l2: 21.9466
[140]	training's l2: 22.4219	valid_1's l2: 21.7854
[150]	training's l2: 22.2566	valid_1's l2: 21.6484
[160]	training's l2: 22.1113	valid_1's l2: 21.531
[170]	training's l2: 21.9812	valid_1's l2: 21.431
[180]	training's l2: 21.8639	valid_1's l2: 21.3445
[190]	training's l2: 21.7582	valid_1's l2: 21.2693
[200]	training's l2: 21.6571	valid_1's l2: 21.1989
[210]	training's l2: 21.566	valid_1's l2: 21.138
[220]	training's l2: 21.4821	valid_1's l2: 21.0836
[230]	training's l2: 21.4038	valid_1's l2: 21.0352
[240]	training's l2: 21.3286	valid_1's l2: 20.989
[250]	training's l2: 21.2607	valid_1's l2: 20.9504
[260]	training's l2: 21.1959	valid_1's l2: 20.9126
[270]	training's l2: 21.1319	valid_1's l2: 20.877
[280]	training's l2: 21.0703	valid_1's l2: 20.8442
[290]	training's l2: 21.012	valid_1's l2: 20.8155
[300]	training's l2: 20.9543	valid_1's l2: 20.788
[310]	training's l2: 20.9001	valid_1's l2: 20.7642
[320]	training's l2: 20.8483	valid_1's l2: 20.7425
[330]	training's l2: 20.798	valid_1's l2: 20.7222
[340]	training's l2: 20.7501	valid_1's l2: 20.7043
[350]	training's l2: 20.703	valid_1's l2: 20.6867
[360]	training's l2: 20.6573	valid_1's l2: 20.6692
[370]	training's l2: 20.6121	valid_1's l2: 20.6539
[380]	training's l2: 20.5668	valid_1's l2: 20.6385
[390]	training's l2: 20.5241	valid_1's l2: 20.6257
[400]	training's l2: 20.4824	valid_1's l2: 20.6128
[410]	training's l2: 20.4407	valid_1's l2: 20.5995
[420]	training's l2: 20.3996	valid_1's l2: 20.5869
[430]	training's l2: 20.3587	valid_1's l2: 20.5749
[440]	training's l2: 20.3192	valid_1's l2: 20.5644
[450]	training's l2: 20.2805	valid_1's l2: 20.5547
[460]	training's l2: 20.2419	valid_1's l2: 20.5451
[470]	training's l2: 20.2042	valid_1's l2: 20.5363
[480]	training's l2: 20.1669	valid_1's l2: 20.5273
[490]	training's l2: 20.1298	valid_1's l2: 20.5186
[500]	training's l2: 20.0931	valid_1's l2: 20.5096
[510]	training's l2: 20.0572	valid_1's l2: 20.5023
[520]	training's l2: 20.0219	valid_1's l2: 20.4957
[530]	training's l2: 19.9866	valid_1's l2: 20.4898
[540]	training's l2: 19.9509	valid_1's l2: 20.4831
[550]	training's l2: 19.9164	valid_1's l2: 20.478
[560]	training's l2: 19.8814	valid_1's l2: 20.4715
[570]	training's l2: 19.8464	valid_1's l2: 20.4651
[580]	training's l2: 19.8114	valid_1's l2: 20.4591
[590]	training's l2: 19.777	valid_1's l2: 20.4527
[600]	training's l2: 19.7422	valid_1's l2: 20.4456
[610]	training's l2: 19.7084	valid_1's l2: 20.4392
[620]	training's l2: 19.6747	valid_1's l2: 20.4318
[630]	training's l2: 19.6406	valid_1's l2: 20.4251
[640]	training's l2: 19.607	valid_1's l2: 20.4189
[650]	training's l2: 19.5745	valid_1's l2: 20.4149
[660]	training's l2: 19.5424	valid_1's l2: 20.4097
[670]	training's l2: 19.5103	valid_1's l2: 20.4043
[680]	training's l2: 19.478	valid_1's l2: 20.3997
[690]	training's l2: 19.4457	valid_1's l2: 20.3946
[700]	training's l2: 19.4142	valid_1's l2: 20.39
[710]	training's l2: 19.3824	valid_1's l2: 20.387
[720]	training's l2: 19.3515	valid_1's l2: 20.3816
[730]	training's l2: 19.3211	valid_1's l2: 20.3778
[740]	training's l2: 19.2901	valid_1's l2: 20.3733
[750]	training's l2: 19.2597	valid_1's l2: 20.3704
[760]	training's l2: 19.2293	valid_1's l2: 20.3662
[770]	training's l2: 19.1993	valid_1's l2: 20.3633
[780]	training's l2: 19.1693	valid_1's l2: 20.36
[790]	training's l2: 19.1397	valid_1's l2: 20.3568
[800]	training's l2: 19.1097	valid_1's l2: 20.3532
[810]	training's l2: 19.0798	valid_1's l2: 20.3502
[820]	training's l2: 19.0504	valid_1's l2: 20.3467
[830]	training's l2: 19.021	valid_1's l2: 20.3447
[840]	training's l2: 18.992	valid_1's l2: 20.3419
[850]	training's l2: 18.9633	valid_1's l2: 20.3395
[860]	training's l2: 18.9348	valid_1's l2: 20.3375
[870]	training's l2: 18.9063	valid_1's l2: 20.3354
[880]	training's l2: 18.8777	valid_1's l2: 20.3319
[890]	training's l2: 18.849	valid_1's l2: 20.3296
[900]	training's l2: 18.8213	valid_1's l2: 20.327
[910]	training's l2: 18.7927	valid_1's l2: 20.3246
[920]	training's l2: 18.7645	valid_1's l2: 20.3231
[930]	training's l2: 18.7371	valid_1's l2: 20.3206
[940]	training's l2: 18.7088	valid_1's l2: 20.318
[950]	training's l2: 18.6811	valid_1's l2: 20.3157
[960]	training's l2: 18.6542	valid_1's l2: 20.3134
[970]	training's l2: 18.6267	valid_1's l2: 20.3112
[980]	training's l2: 18.5993	valid_1's l2: 20.3092
[990]	training's l2: 18.5719	valid_1's l2: 20.3072
[1000]	training's l2: 18.5449	valid_1's l2: 20.3047
[1010]	training's l2: 18.5179	valid_1's l2: 20.3025
[1020]	training's l2: 18.492	valid_1's l2: 20.3009
[1030]	training's l2: 18.4654	valid_1's l2: 20.2994
[1040]	training's l2: 18.4391	valid_1's l2: 20.2986
[1050]	training's l2: 18.4129	valid_1's l2: 20.2969
[1060]	training's l2: 18.3872	valid_1's l2: 20.295
[1070]	training's l2: 18.3619	valid_1's l2: 20.293
[1080]	training's l2: 18.3366	valid_1's l2: 20.2917
[1090]	training's l2: 18.3108	valid_1's l2: 20.2901
[1100]	training's l2: 18.2853	valid_1's l2: 20.2887
[1110]	training's l2: 18.2611	valid_1's l2: 20.2873
[1120]	training's l2: 18.237	valid_1's l2: 20.2872
[1130]	training's l2: 18.2132	valid_1's l2: 20.2876
[1140]	training's l2: 18.1888	valid_1's l2: 20.2853
[1150]	training's l2: 18.1636	valid_1's l2: 20.2851
[1160]	training's l2: 18.1393	valid_1's l2: 20.2846
[1170]	training's l2: 18.1158	valid_1's l2: 20.2841
[1180]	training's l2: 18.0924	valid_1's l2: 20.2834
[1190]	training's l2: 18.0686	valid_1's l2: 20.2819
[1200]	training's l2: 18.0447	valid_1's l2: 20.2812
[1210]	training's l2: 18.0206	valid_1's l2: 20.2798
[1220]	training's l2: 17.997	valid_1's l2: 20.2789
[1230]	training's l2: 17.9748	valid_1's l2: 20.2786
[1240]	training's l2: 17.9513	valid_1's l2: 20.2779
[1250]	training's l2: 17.9284	valid_1's l2: 20.2768
[1260]	training's l2: 17.906	valid_1's l2: 20.276
[1270]	training's l2: 17.8817	valid_1's l2: 20.2752
[1280]	training's l2: 17.8588	valid_1's l2: 20.2733
[1290]	training's l2: 17.8368	valid_1's l2: 20.2736
[1300]	training's l2: 17.8137	valid_1's l2: 20.2724
[1310]	training's l2: 17.7904	valid_1's l2: 20.2723
[1320]	training's l2: 17.7671	valid_1's l2: 20.2715
[1330]	training's l2: 17.7448	valid_1's l2: 20.2707
[1340]	training's l2: 17.7214	valid_1's l2: 20.2696
[1350]	training's l2: 17.699	valid_1's l2: 20.2694
[1360]	training's l2: 17.6768	valid_1's l2: 20.2681
[1370]	training's l2: 17.6545	valid_1's l2: 20.2673
[1380]	training's l2: 17.6325	valid_1's l2: 20.2675
[1390]	training's l2: 17.6106	valid_1's l2: 20.2661
[1400]	training's l2: 17.5891	valid_1's l2: 20.2655
[1410]	training's l2: 17.5672	valid_1's l2: 20.2657
[1420]	training's l2: 17.546	valid_1's l2: 20.2646
[1430]	training's l2: 17.5238	valid_1's l2: 20.2645
[1440]	training's l2: 17.5026	valid_1's l2: 20.2632
[1450]	training's l2: 17.4816	valid_1's l2: 20.2627
[1460]	training's l2: 17.4592	valid_1's l2: 20.2632
[1470]	training's l2: 17.4376	valid_1's l2: 20.2628
[1480]	training's l2: 17.4159	valid_1's l2: 20.2624
[1490]	training's l2: 17.3942	valid_1's l2: 20.2619
[1500]	training's l2: 17.3735	valid_1's l2: 20.2608
[1510]	training's l2: 17.3518	valid_1's l2: 20.2605
[1520]	training's l2: 17.3298	valid_1's l2: 20.2592
[1530]	training's l2: 17.3085	valid_1's l2: 20.2589
[1540]	training's l2: 17.2876	valid_1's l2: 20.2587
[1550]	training's l2: 17.2661	valid_1's l2: 20.2576
[1560]	training's l2: 17.2465	valid_1's l2: 20.2565
[1570]	training's l2: 17.2255	valid_1's l2: 20.2557
[1580]	training's l2: 17.2045	valid_1's l2: 20.2553
[1590]	training's l2: 17.1841	valid_1's l2: 20.2554
[1600]	training's l2: 17.1626	valid_1's l2: 20.2548
[1610]	training's l2: 17.1417	valid_1's l2: 20.2539
[1620]	training's l2: 17.1215	valid_1's l2: 20.2525
[1630]	training's l2: 17.1021	valid_1's l2: 20.2521
[1640]	training's l2: 17.0817	valid_1's l2: 20.2515
[1650]	training's l2: 17.0628	valid_1's l2: 20.2515
[1660]	training's l2: 17.0422	valid_1's l2: 20.2509
[1670]	training's l2: 17.0215	valid_1's l2: 20.2508
[1680]	training's l2: 17.0009	valid_1's l2: 20.2508
[1690]	training's l2: 16.9802	valid_1's l2: 20.2504
[1700]	training's l2: 16.9603	valid_1's l2: 20.2496
[1710]	training's l2: 16.94	valid_1's l2: 20.2492
[1720]	training's l2: 16.9191	valid_1's l2: 20.2495
[1730]	training's l2: 16.8993	valid_1's l2: 20.2495
[1740]	training's l2: 16.8795	valid_1's l2: 20.2498
[1750]	training's l2: 16.859	valid_1's l2: 20.2495
[1760]	training's l2: 16.8389	valid_1's l2: 20.2491
[1770]	training's l2: 16.8195	valid_1's l2: 20.2494
[1780]	training's l2: 16.7994	valid_1's l2: 20.2492
[1790]	training's l2: 16.7803	valid_1's l2: 20.2486
[1800]	training's l2: 16.7613	valid_1's l2: 20.2483
[1810]	training's l2: 16.7421	valid_1's l2: 20.2475
[1820]	training's l2: 16.7229	valid_1's l2: 20.2477
[1830]	training's l2: 16.7044	valid_1's l2: 20.2477
[1840]	training's l2: 16.6853	valid_1's l2: 20.2486
Early stopping, best iteration is:
[1814]	training's l2: 16.734	valid_1's l2: 20.247
score1: 3.8374525468949785
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261301 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.6399	valid_1's l2: 29.7285
[20]	training's l2: 28.2321	valid_1's l2: 27.3249
[30]	training's l2: 26.5304	valid_1's l2: 25.6332
[40]	training's l2: 25.3111	valid_1's l2: 24.428
[50]	training's l2: 24.4275	valid_1's l2: 23.5623
[60]	training's l2: 23.7777	valid_1's l2: 22.9352
[70]	training's l2: 23.2932	valid_1's l2: 22.4751
[80]	training's l2: 22.9215	valid_1's l2: 22.1254
[90]	training's l2: 22.628	valid_1's l2: 21.8583
[100]	training's l2: 22.3924	valid_1's l2: 21.6522
[110]	training's l2: 22.1996	valid_1's l2: 21.49
[120]	training's l2: 22.0357	valid_1's l2: 21.357
[130]	training's l2: 21.8932	valid_1's l2: 21.2435
[140]	training's l2: 21.7696	valid_1's l2: 21.1513
[150]	training's l2: 21.6597	valid_1's l2: 21.0711
[160]	training's l2: 21.5634	valid_1's l2: 21.0063
[170]	training's l2: 21.4747	valid_1's l2: 20.9482
[180]	training's l2: 21.3938	valid_1's l2: 20.8969
[190]	training's l2: 21.3177	valid_1's l2: 20.8523
[200]	training's l2: 21.2458	valid_1's l2: 20.8115
[210]	training's l2: 21.1799	valid_1's l2: 20.7779
[220]	training's l2: 21.1167	valid_1's l2: 20.7478
[230]	training's l2: 21.0566	valid_1's l2: 20.7175
[240]	training's l2: 20.9988	valid_1's l2: 20.6916
[250]	training's l2: 20.9436	valid_1's l2: 20.6676
[260]	training's l2: 20.891	valid_1's l2: 20.6468
[270]	training's l2: 20.8398	valid_1's l2: 20.626
[280]	training's l2: 20.7904	valid_1's l2: 20.6071
[290]	training's l2: 20.7414	valid_1's l2: 20.5896
[300]	training's l2: 20.695	valid_1's l2: 20.574
[310]	training's l2: 20.6502	valid_1's l2: 20.5615
[320]	training's l2: 20.6043	valid_1's l2: 20.5457
[330]	training's l2: 20.5598	valid_1's l2: 20.5328
[340]	training's l2: 20.5167	valid_1's l2: 20.5206
[350]	training's l2: 20.4741	valid_1's l2: 20.5104
[360]	training's l2: 20.4316	valid_1's l2: 20.4987
[370]	training's l2: 20.39	valid_1's l2: 20.4867
[380]	training's l2: 20.3482	valid_1's l2: 20.475
[390]	training's l2: 20.3068	valid_1's l2: 20.466
[400]	training's l2: 20.266	valid_1's l2: 20.4547
[410]	training's l2: 20.2262	valid_1's l2: 20.4453
[420]	training's l2: 20.1861	valid_1's l2: 20.4345
[430]	training's l2: 20.147	valid_1's l2: 20.4265
[440]	training's l2: 20.1095	valid_1's l2: 20.4179
[450]	training's l2: 20.0718	valid_1's l2: 20.411
[460]	training's l2: 20.0343	valid_1's l2: 20.4047
[470]	training's l2: 19.9968	valid_1's l2: 20.3968
[480]	training's l2: 19.9602	valid_1's l2: 20.3901
[490]	training's l2: 19.924	valid_1's l2: 20.3856
[500]	training's l2: 19.8885	valid_1's l2: 20.3789
[510]	training's l2: 19.8538	valid_1's l2: 20.3727
[520]	training's l2: 19.8187	valid_1's l2: 20.3692
[530]	training's l2: 19.7841	valid_1's l2: 20.3634
[540]	training's l2: 19.7498	valid_1's l2: 20.3576
[550]	training's l2: 19.7151	valid_1's l2: 20.3526
[560]	training's l2: 19.681	valid_1's l2: 20.3478
[570]	training's l2: 19.6474	valid_1's l2: 20.3423
[580]	training's l2: 19.614	valid_1's l2: 20.3396
[590]	training's l2: 19.5804	valid_1's l2: 20.3348
[600]	training's l2: 19.5477	valid_1's l2: 20.331
[610]	training's l2: 19.5151	valid_1's l2: 20.327
[620]	training's l2: 19.4828	valid_1's l2: 20.3233
[630]	training's l2: 19.4499	valid_1's l2: 20.3204
[640]	training's l2: 19.4185	valid_1's l2: 20.3182
[650]	training's l2: 19.3877	valid_1's l2: 20.315
[660]	training's l2: 19.3563	valid_1's l2: 20.3115
[670]	training's l2: 19.325	valid_1's l2: 20.3087
[680]	training's l2: 19.2932	valid_1's l2: 20.3061
[690]	training's l2: 19.2626	valid_1's l2: 20.3047
[700]	training's l2: 19.2317	valid_1's l2: 20.3022
[710]	training's l2: 19.2017	valid_1's l2: 20.2994
[720]	training's l2: 19.1722	valid_1's l2: 20.2972
[730]	training's l2: 19.1428	valid_1's l2: 20.2952
[740]	training's l2: 19.1127	valid_1's l2: 20.2929
[750]	training's l2: 19.0832	valid_1's l2: 20.291
[760]	training's l2: 19.0538	valid_1's l2: 20.2888
[770]	training's l2: 19.0252	valid_1's l2: 20.2865
[780]	training's l2: 18.9971	valid_1's l2: 20.285
[790]	training's l2: 18.9687	valid_1's l2: 20.2832
[800]	training's l2: 18.9405	valid_1's l2: 20.2819
[810]	training's l2: 18.9132	valid_1's l2: 20.2806
[820]	training's l2: 18.8868	valid_1's l2: 20.2796
[830]	training's l2: 18.8588	valid_1's l2: 20.2784
[840]	training's l2: 18.8311	valid_1's l2: 20.2774
[850]	training's l2: 18.8047	valid_1's l2: 20.2756
[860]	training's l2: 18.7773	valid_1's l2: 20.2744
[870]	training's l2: 18.751	valid_1's l2: 20.2732
[880]	training's l2: 18.725	valid_1's l2: 20.2719
[890]	training's l2: 18.6995	valid_1's l2: 20.2705
[900]	training's l2: 18.6738	valid_1's l2: 20.2696
[910]	training's l2: 18.6483	valid_1's l2: 20.2678
[920]	training's l2: 18.6218	valid_1's l2: 20.2662
[930]	training's l2: 18.5967	valid_1's l2: 20.2652
[940]	training's l2: 18.5707	valid_1's l2: 20.2642
[950]	training's l2: 18.5453	valid_1's l2: 20.262
[960]	training's l2: 18.518	valid_1's l2: 20.2607
[970]	training's l2: 18.4933	valid_1's l2: 20.2606
[980]	training's l2: 18.4679	valid_1's l2: 20.2593
[990]	training's l2: 18.4437	valid_1's l2: 20.2579
[1000]	training's l2: 18.4183	valid_1's l2: 20.258
[1010]	training's l2: 18.395	valid_1's l2: 20.2575
[1020]	training's l2: 18.3699	valid_1's l2: 20.2564
[1030]	training's l2: 18.3467	valid_1's l2: 20.2553
[1040]	training's l2: 18.3242	valid_1's l2: 20.2552
[1050]	training's l2: 18.2997	valid_1's l2: 20.2545
[1060]	training's l2: 18.2768	valid_1's l2: 20.2542
[1070]	training's l2: 18.2527	valid_1's l2: 20.2534
[1080]	training's l2: 18.2282	valid_1's l2: 20.252
[1090]	training's l2: 18.2045	valid_1's l2: 20.2516
[1100]	training's l2: 18.1818	valid_1's l2: 20.2527
[1110]	training's l2: 18.1564	valid_1's l2: 20.251
[1120]	training's l2: 18.1332	valid_1's l2: 20.2503
[1130]	training's l2: 18.1102	valid_1's l2: 20.2491
[1140]	training's l2: 18.0868	valid_1's l2: 20.2492
[1150]	training's l2: 18.0635	valid_1's l2: 20.2479
[1160]	training's l2: 18.0387	valid_1's l2: 20.2469
[1170]	training's l2: 18.016	valid_1's l2: 20.2462
[1180]	training's l2: 17.9941	valid_1's l2: 20.2462
[1190]	training's l2: 17.9725	valid_1's l2: 20.2455
[1200]	training's l2: 17.9502	valid_1's l2: 20.2454
[1210]	training's l2: 17.9275	valid_1's l2: 20.2459
[1220]	training's l2: 17.9036	valid_1's l2: 20.2448
[1230]	training's l2: 17.8807	valid_1's l2: 20.2445
[1240]	training's l2: 17.8575	valid_1's l2: 20.2445
[1250]	training's l2: 17.834	valid_1's l2: 20.2435
[1260]	training's l2: 17.8129	valid_1's l2: 20.2433
[1270]	training's l2: 17.7902	valid_1's l2: 20.2426
[1280]	training's l2: 17.7662	valid_1's l2: 20.2423
[1290]	training's l2: 17.7426	valid_1's l2: 20.243
[1300]	training's l2: 17.7222	valid_1's l2: 20.2426
[1310]	training's l2: 17.7013	valid_1's l2: 20.2422
[1320]	training's l2: 17.6805	valid_1's l2: 20.2418
[1330]	training's l2: 17.6587	valid_1's l2: 20.2417
[1340]	training's l2: 17.6354	valid_1's l2: 20.2418
[1350]	training's l2: 17.6129	valid_1's l2: 20.2417
[1360]	training's l2: 17.5909	valid_1's l2: 20.2412
[1370]	training's l2: 17.5679	valid_1's l2: 20.241
[1380]	training's l2: 17.5455	valid_1's l2: 20.2409
[1390]	training's l2: 17.5254	valid_1's l2: 20.2411
[1400]	training's l2: 17.5043	valid_1's l2: 20.2408
Early stopping, best iteration is:
[1374]	training's l2: 17.5586	valid_1's l2: 20.2404
score1: 3.839172468798234
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254357 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.1515	valid_1's l2: 30.2445
[20]	training's l2: 28.9661	valid_1's l2: 28.0644
[30]	training's l2: 27.3292	valid_1's l2: 26.4413
[40]	training's l2: 26.0903	valid_1's l2: 25.2138
[50]	training's l2: 25.1447	valid_1's l2: 24.2865
[60]	training's l2: 24.4178	valid_1's l2: 23.579
[70]	training's l2: 23.8524	valid_1's l2: 23.034
[80]	training's l2: 23.4073	valid_1's l2: 22.6124
[90]	training's l2: 23.057	valid_1's l2: 22.2859
[100]	training's l2: 22.7707	valid_1's l2: 22.0251
[110]	training's l2: 22.5318	valid_1's l2: 21.8119
[120]	training's l2: 22.3331	valid_1's l2: 21.643
[130]	training's l2: 22.1615	valid_1's l2: 21.5014
[140]	training's l2: 22.0172	valid_1's l2: 21.3876
[150]	training's l2: 21.8867	valid_1's l2: 21.2875
[160]	training's l2: 21.768	valid_1's l2: 21.1992
[170]	training's l2: 21.663	valid_1's l2: 21.1252
[180]	training's l2: 21.5692	valid_1's l2: 21.0603
[190]	training's l2: 21.4839	valid_1's l2: 21.0032
[200]	training's l2: 21.4048	valid_1's l2: 20.9538
[210]	training's l2: 21.3296	valid_1's l2: 20.909
[220]	training's l2: 21.258	valid_1's l2: 20.8687
[230]	training's l2: 21.1905	valid_1's l2: 20.8323
[240]	training's l2: 21.126	valid_1's l2: 20.7985
[250]	training's l2: 21.0652	valid_1's l2: 20.767
[260]	training's l2: 21.008	valid_1's l2: 20.74
[270]	training's l2: 20.9527	valid_1's l2: 20.716
[280]	training's l2: 20.9004	valid_1's l2: 20.6948
[290]	training's l2: 20.8509	valid_1's l2: 20.6752
[300]	training's l2: 20.8007	valid_1's l2: 20.6561
[310]	training's l2: 20.7513	valid_1's l2: 20.6376
[320]	training's l2: 20.7039	valid_1's l2: 20.6206
[330]	training's l2: 20.6588	valid_1's l2: 20.6044
[340]	training's l2: 20.6146	valid_1's l2: 20.5898
[350]	training's l2: 20.5701	valid_1's l2: 20.5756
[360]	training's l2: 20.5266	valid_1's l2: 20.5628
[370]	training's l2: 20.4849	valid_1's l2: 20.5524
[380]	training's l2: 20.443	valid_1's l2: 20.5406
[390]	training's l2: 20.4023	valid_1's l2: 20.53
[400]	training's l2: 20.3627	valid_1's l2: 20.52
[410]	training's l2: 20.3219	valid_1's l2: 20.5077
[420]	training's l2: 20.2824	valid_1's l2: 20.4979
[430]	training's l2: 20.2433	valid_1's l2: 20.4894
[440]	training's l2: 20.2049	valid_1's l2: 20.4814
[450]	training's l2: 20.1657	valid_1's l2: 20.4716
[460]	training's l2: 20.1275	valid_1's l2: 20.4639
[470]	training's l2: 20.0897	valid_1's l2: 20.4563
[480]	training's l2: 20.0526	valid_1's l2: 20.4484
[490]	training's l2: 20.0153	valid_1's l2: 20.4399
[500]	training's l2: 19.9794	valid_1's l2: 20.4335
[510]	training's l2: 19.9436	valid_1's l2: 20.4275
[520]	training's l2: 19.9074	valid_1's l2: 20.4195
[530]	training's l2: 19.8718	valid_1's l2: 20.4135
[540]	training's l2: 19.8366	valid_1's l2: 20.4072
[550]	training's l2: 19.8016	valid_1's l2: 20.4013
[560]	training's l2: 19.7669	valid_1's l2: 20.3954
[570]	training's l2: 19.7324	valid_1's l2: 20.3901
[580]	training's l2: 19.6982	valid_1's l2: 20.3853
[590]	training's l2: 19.6643	valid_1's l2: 20.3799
[600]	training's l2: 19.6311	valid_1's l2: 20.3742
[610]	training's l2: 19.5982	valid_1's l2: 20.3689
[620]	training's l2: 19.5649	valid_1's l2: 20.3645
[630]	training's l2: 19.5327	valid_1's l2: 20.3612
[640]	training's l2: 19.5002	valid_1's l2: 20.3565
[650]	training's l2: 19.4683	valid_1's l2: 20.3532
[660]	training's l2: 19.4362	valid_1's l2: 20.3489
[670]	training's l2: 19.4048	valid_1's l2: 20.3459
[680]	training's l2: 19.3735	valid_1's l2: 20.3431
[690]	training's l2: 19.3422	valid_1's l2: 20.3394
[700]	training's l2: 19.3112	valid_1's l2: 20.3365
[710]	training's l2: 19.2807	valid_1's l2: 20.3328
[720]	training's l2: 19.25	valid_1's l2: 20.3304
[730]	training's l2: 19.22	valid_1's l2: 20.3277
[740]	training's l2: 19.1892	valid_1's l2: 20.3253
[750]	training's l2: 19.1588	valid_1's l2: 20.3224
[760]	training's l2: 19.1289	valid_1's l2: 20.319
[770]	training's l2: 19.0988	valid_1's l2: 20.3155
[780]	training's l2: 19.0698	valid_1's l2: 20.3134
[790]	training's l2: 19.0406	valid_1's l2: 20.3108
[800]	training's l2: 19.0115	valid_1's l2: 20.3083
[810]	training's l2: 18.9827	valid_1's l2: 20.3056
[820]	training's l2: 18.9541	valid_1's l2: 20.3039
[830]	training's l2: 18.9251	valid_1's l2: 20.3006
[840]	training's l2: 18.8965	valid_1's l2: 20.2981
[850]	training's l2: 18.8683	valid_1's l2: 20.2962
[860]	training's l2: 18.8398	valid_1's l2: 20.2946
[870]	training's l2: 18.8127	valid_1's l2: 20.2935
[880]	training's l2: 18.7857	valid_1's l2: 20.2926
[890]	training's l2: 18.7582	valid_1's l2: 20.2904
[900]	training's l2: 18.731	valid_1's l2: 20.2889
[910]	training's l2: 18.7034	valid_1's l2: 20.2872
[920]	training's l2: 18.6769	valid_1's l2: 20.285
[930]	training's l2: 18.6497	valid_1's l2: 20.2833
[940]	training's l2: 18.6233	valid_1's l2: 20.2827
[950]	training's l2: 18.597	valid_1's l2: 20.2808
[960]	training's l2: 18.5705	valid_1's l2: 20.2801
[970]	training's l2: 18.5451	valid_1's l2: 20.2785
[980]	training's l2: 18.5203	valid_1's l2: 20.2777
[990]	training's l2: 18.4943	valid_1's l2: 20.2759
[1000]	training's l2: 18.4702	valid_1's l2: 20.2745
[1010]	training's l2: 18.4445	valid_1's l2: 20.2732
[1020]	training's l2: 18.4196	valid_1's l2: 20.271
[1030]	training's l2: 18.3941	valid_1's l2: 20.2704
[1040]	training's l2: 18.3703	valid_1's l2: 20.2689
[1050]	training's l2: 18.3463	valid_1's l2: 20.2678
[1060]	training's l2: 18.3224	valid_1's l2: 20.2669
[1070]	training's l2: 18.2987	valid_1's l2: 20.2664
[1080]	training's l2: 18.2736	valid_1's l2: 20.2653
[1090]	training's l2: 18.2486	valid_1's l2: 20.2641
[1100]	training's l2: 18.2246	valid_1's l2: 20.2635
[1110]	training's l2: 18.1995	valid_1's l2: 20.263
[1120]	training's l2: 18.1762	valid_1's l2: 20.263
[1130]	training's l2: 18.1524	valid_1's l2: 20.2629
[1140]	training's l2: 18.1274	valid_1's l2: 20.2619
[1150]	training's l2: 18.1043	valid_1's l2: 20.2609
[1160]	training's l2: 18.0809	valid_1's l2: 20.2607
[1170]	training's l2: 18.0559	valid_1's l2: 20.2589
[1180]	training's l2: 18.033	valid_1's l2: 20.2584
[1190]	training's l2: 18.0104	valid_1's l2: 20.2579
[1200]	training's l2: 17.9867	valid_1's l2: 20.257
[1210]	training's l2: 17.9634	valid_1's l2: 20.2568
[1220]	training's l2: 17.9399	valid_1's l2: 20.2567
[1230]	training's l2: 17.9174	valid_1's l2: 20.2551
[1240]	training's l2: 17.8958	valid_1's l2: 20.2543
[1250]	training's l2: 17.8736	valid_1's l2: 20.255
[1260]	training's l2: 17.8509	valid_1's l2: 20.2541
[1270]	training's l2: 17.8276	valid_1's l2: 20.2536
[1280]	training's l2: 17.8041	valid_1's l2: 20.2528
[1290]	training's l2: 17.7817	valid_1's l2: 20.2523
[1300]	training's l2: 17.7604	valid_1's l2: 20.2525
[1310]	training's l2: 17.7381	valid_1's l2: 20.252
[1320]	training's l2: 17.7149	valid_1's l2: 20.2511
[1330]	training's l2: 17.6932	valid_1's l2: 20.2506
[1340]	training's l2: 17.6719	valid_1's l2: 20.2503
[1350]	training's l2: 17.651	valid_1's l2: 20.2501
[1360]	training's l2: 17.6287	valid_1's l2: 20.2494
[1370]	training's l2: 17.6065	valid_1's l2: 20.248
[1380]	training's l2: 17.5843	valid_1's l2: 20.2478
[1390]	training's l2: 17.5637	valid_1's l2: 20.2472
[1400]	training's l2: 17.5413	valid_1's l2: 20.2476
[1410]	training's l2: 17.5199	valid_1's l2: 20.2467
[1420]	training's l2: 17.4992	valid_1's l2: 20.2464
[1430]	training's l2: 17.4762	valid_1's l2: 20.2458
[1440]	training's l2: 17.4556	valid_1's l2: 20.2456
[1450]	training's l2: 17.4331	valid_1's l2: 20.2456
[1460]	training's l2: 17.413	valid_1's l2: 20.2448
[1470]	training's l2: 17.3928	valid_1's l2: 20.245
[1480]	training's l2: 17.3726	valid_1's l2: 20.2445
[1490]	training's l2: 17.3519	valid_1's l2: 20.2439
[1500]	training's l2: 17.3299	valid_1's l2: 20.2444
[1510]	training's l2: 17.3094	valid_1's l2: 20.2447
[1520]	training's l2: 17.2877	valid_1's l2: 20.2444
Early stopping, best iteration is:
[1493]	training's l2: 17.3446	valid_1's l2: 20.2434
score1: 3.836797032037701
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.249236 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.6144	valid_1's l2: 30.7197
[20]	training's l2: 29.667	valid_1's l2: 28.7933
[30]	training's l2: 28.1243	valid_1's l2: 27.2719
[40]	training's l2: 26.8969	valid_1's l2: 26.0651
[50]	training's l2: 25.9152	valid_1's l2: 25.1055
[60]	training's l2: 25.124	valid_1's l2: 24.3371
[70]	training's l2: 24.4821	valid_1's l2: 23.7236
[80]	training's l2: 23.9582	valid_1's l2: 23.2277
[90]	training's l2: 23.5264	valid_1's l2: 22.8269
[100]	training's l2: 23.1697	valid_1's l2: 22.5022
[110]	training's l2: 22.8727	valid_1's l2: 22.2381
[120]	training's l2: 22.6207	valid_1's l2: 22.0175
[130]	training's l2: 22.4045	valid_1's l2: 21.836
[140]	training's l2: 22.2158	valid_1's l2: 21.6838
[150]	training's l2: 22.0508	valid_1's l2: 21.5528
[160]	training's l2: 21.9031	valid_1's l2: 21.4436
[170]	training's l2: 21.7696	valid_1's l2: 21.347
[180]	training's l2: 21.6497	valid_1's l2: 21.2644
[190]	training's l2: 21.5397	valid_1's l2: 21.1916
[200]	training's l2: 21.4362	valid_1's l2: 21.1237
[210]	training's l2: 21.3417	valid_1's l2: 21.0656
[220]	training's l2: 21.2547	valid_1's l2: 21.0143
[230]	training's l2: 21.172	valid_1's l2: 20.9678
[240]	training's l2: 21.0942	valid_1's l2: 20.9254
[250]	training's l2: 21.0201	valid_1's l2: 20.8866
[260]	training's l2: 20.9502	valid_1's l2: 20.8517
[270]	training's l2: 20.88	valid_1's l2: 20.8172
[280]	training's l2: 20.8124	valid_1's l2: 20.7857
[290]	training's l2: 20.7482	valid_1's l2: 20.7593
[300]	training's l2: 20.6881	valid_1's l2: 20.7361
[310]	training's l2: 20.6288	valid_1's l2: 20.7126
[320]	training's l2: 20.5718	valid_1's l2: 20.6915
[330]	training's l2: 20.5163	valid_1's l2: 20.6725
[340]	training's l2: 20.4623	valid_1's l2: 20.6543
[350]	training's l2: 20.4108	valid_1's l2: 20.6377
[360]	training's l2: 20.3598	valid_1's l2: 20.623
[370]	training's l2: 20.3102	valid_1's l2: 20.6083
[380]	training's l2: 20.2611	valid_1's l2: 20.5948
[390]	training's l2: 20.2127	valid_1's l2: 20.5823
[400]	training's l2: 20.1651	valid_1's l2: 20.5709
[410]	training's l2: 20.1185	valid_1's l2: 20.5587
[420]	training's l2: 20.0723	valid_1's l2: 20.5474
[430]	training's l2: 20.0276	valid_1's l2: 20.5381
[440]	training's l2: 19.9822	valid_1's l2: 20.5287
[450]	training's l2: 19.9376	valid_1's l2: 20.5196
[460]	training's l2: 19.8946	valid_1's l2: 20.5105
[470]	training's l2: 19.8511	valid_1's l2: 20.5015
[480]	training's l2: 19.8087	valid_1's l2: 20.4928
[490]	training's l2: 19.7666	valid_1's l2: 20.4843
[500]	training's l2: 19.7239	valid_1's l2: 20.4774
[510]	training's l2: 19.6825	valid_1's l2: 20.4711
[520]	training's l2: 19.6408	valid_1's l2: 20.4631
[530]	training's l2: 19.6002	valid_1's l2: 20.4559
[540]	training's l2: 19.5588	valid_1's l2: 20.4482
[550]	training's l2: 19.5183	valid_1's l2: 20.4418
[560]	training's l2: 19.4777	valid_1's l2: 20.4345
[570]	training's l2: 19.4372	valid_1's l2: 20.4267
[580]	training's l2: 19.3977	valid_1's l2: 20.4209
[590]	training's l2: 19.359	valid_1's l2: 20.4147
[600]	training's l2: 19.3196	valid_1's l2: 20.4077
[610]	training's l2: 19.2814	valid_1's l2: 20.4024
[620]	training's l2: 19.2431	valid_1's l2: 20.3968
[630]	training's l2: 19.2052	valid_1's l2: 20.3902
[640]	training's l2: 19.1671	valid_1's l2: 20.3845
[650]	training's l2: 19.1296	valid_1's l2: 20.38
[660]	training's l2: 19.0921	valid_1's l2: 20.3751
[670]	training's l2: 19.0558	valid_1's l2: 20.3706
[680]	training's l2: 19.0192	valid_1's l2: 20.3666
[690]	training's l2: 18.9833	valid_1's l2: 20.3626
[700]	training's l2: 18.9471	valid_1's l2: 20.3586
[710]	training's l2: 18.9112	valid_1's l2: 20.3546
[720]	training's l2: 18.8754	valid_1's l2: 20.3506
[730]	training's l2: 18.8399	valid_1's l2: 20.3467
[740]	training's l2: 18.8044	valid_1's l2: 20.3436
[750]	training's l2: 18.7697	valid_1's l2: 20.3404
[760]	training's l2: 18.7351	valid_1's l2: 20.3372
[770]	training's l2: 18.7001	valid_1's l2: 20.3338
[780]	training's l2: 18.6658	valid_1's l2: 20.3319
[790]	training's l2: 18.6318	valid_1's l2: 20.3293
[800]	training's l2: 18.5973	valid_1's l2: 20.326
[810]	training's l2: 18.5637	valid_1's l2: 20.3221
[820]	training's l2: 18.5297	valid_1's l2: 20.3197
[830]	training's l2: 18.4965	valid_1's l2: 20.3169
[840]	training's l2: 18.4631	valid_1's l2: 20.3151
[850]	training's l2: 18.4299	valid_1's l2: 20.3134
[860]	training's l2: 18.3972	valid_1's l2: 20.3106
[870]	training's l2: 18.365	valid_1's l2: 20.3089
[880]	training's l2: 18.332	valid_1's l2: 20.3068
[890]	training's l2: 18.3004	valid_1's l2: 20.3051
[900]	training's l2: 18.2689	valid_1's l2: 20.3042
[910]	training's l2: 18.2363	valid_1's l2: 20.3018
[920]	training's l2: 18.2045	valid_1's l2: 20.3007
[930]	training's l2: 18.173	valid_1's l2: 20.3001
[940]	training's l2: 18.142	valid_1's l2: 20.2992
[950]	training's l2: 18.1115	valid_1's l2: 20.298
[960]	training's l2: 18.08	valid_1's l2: 20.296
[970]	training's l2: 18.0492	valid_1's l2: 20.2949
[980]	training's l2: 18.0179	valid_1's l2: 20.2928
[990]	training's l2: 17.9874	valid_1's l2: 20.2915
[1000]	training's l2: 17.9566	valid_1's l2: 20.2892
[1010]	training's l2: 17.9265	valid_1's l2: 20.2875
[1020]	training's l2: 17.8965	valid_1's l2: 20.2865
[1030]	training's l2: 17.8662	valid_1's l2: 20.2849
[1040]	training's l2: 17.8361	valid_1's l2: 20.2836
[1050]	training's l2: 17.8065	valid_1's l2: 20.2826
[1060]	training's l2: 17.7767	valid_1's l2: 20.2821
[1070]	training's l2: 17.7488	valid_1's l2: 20.2806
[1080]	training's l2: 17.7195	valid_1's l2: 20.2788
[1090]	training's l2: 17.6908	valid_1's l2: 20.2777
[1100]	training's l2: 17.6625	valid_1's l2: 20.2769
[1110]	training's l2: 17.6347	valid_1's l2: 20.276
[1120]	training's l2: 17.6062	valid_1's l2: 20.2743
[1130]	training's l2: 17.5781	valid_1's l2: 20.2723
[1140]	training's l2: 17.5505	valid_1's l2: 20.2715
[1150]	training's l2: 17.5226	valid_1's l2: 20.2716
[1160]	training's l2: 17.4948	valid_1's l2: 20.2696
[1170]	training's l2: 17.468	valid_1's l2: 20.2687
[1180]	training's l2: 17.4412	valid_1's l2: 20.268
[1190]	training's l2: 17.4142	valid_1's l2: 20.2673
[1200]	training's l2: 17.3879	valid_1's l2: 20.2663
[1210]	training's l2: 17.3615	valid_1's l2: 20.266
[1220]	training's l2: 17.3351	valid_1's l2: 20.2653
[1230]	training's l2: 17.3078	valid_1's l2: 20.264
[1240]	training's l2: 17.2819	valid_1's l2: 20.2633
[1250]	training's l2: 17.2554	valid_1's l2: 20.2626
[1260]	training's l2: 17.2296	valid_1's l2: 20.2624
[1270]	training's l2: 17.2031	valid_1's l2: 20.2611
[1280]	training's l2: 17.1772	valid_1's l2: 20.261
[1290]	training's l2: 17.1501	valid_1's l2: 20.2605
[1300]	training's l2: 17.1238	valid_1's l2: 20.2603
[1310]	training's l2: 17.0979	valid_1's l2: 20.2595
[1320]	training's l2: 17.0727	valid_1's l2: 20.259
[1330]	training's l2: 17.0473	valid_1's l2: 20.2587
[1340]	training's l2: 17.0226	valid_1's l2: 20.2572
[1350]	training's l2: 16.9975	valid_1's l2: 20.2574
[1360]	training's l2: 16.9724	valid_1's l2: 20.2565
[1370]	training's l2: 16.9476	valid_1's l2: 20.255
[1380]	training's l2: 16.9232	valid_1's l2: 20.2548
[1390]	training's l2: 16.8987	valid_1's l2: 20.2549
[1400]	training's l2: 16.8756	valid_1's l2: 20.2541
[1410]	training's l2: 16.8499	valid_1's l2: 20.254
[1420]	training's l2: 16.824	valid_1's l2: 20.2536
[1430]	training's l2: 16.7987	valid_1's l2: 20.253
[1440]	training's l2: 16.774	valid_1's l2: 20.2521
[1450]	training's l2: 16.7494	valid_1's l2: 20.2515
[1460]	training's l2: 16.7244	valid_1's l2: 20.2509
[1470]	training's l2: 16.7015	valid_1's l2: 20.2514
[1480]	training's l2: 16.6766	valid_1's l2: 20.2512
[1490]	training's l2: 16.6526	valid_1's l2: 20.2504
[1500]	training's l2: 16.6285	valid_1's l2: 20.2492
[1510]	training's l2: 16.6051	valid_1's l2: 20.2492
[1520]	training's l2: 16.5809	valid_1's l2: 20.2488
[1530]	training's l2: 16.557	valid_1's l2: 20.2486
[1540]	training's l2: 16.5326	valid_1's l2: 20.2484
[1550]	training's l2: 16.5088	valid_1's l2: 20.248
[1560]	training's l2: 16.4851	valid_1's l2: 20.2475
[1570]	training's l2: 16.4604	valid_1's l2: 20.2466
[1580]	training's l2: 16.438	valid_1's l2: 20.246
[1590]	training's l2: 16.4155	valid_1's l2: 20.2461
[1600]	training's l2: 16.3925	valid_1's l2: 20.2455
[1610]	training's l2: 16.369	valid_1's l2: 20.2449
[1620]	training's l2: 16.3461	valid_1's l2: 20.2449
[1630]	training's l2: 16.3234	valid_1's l2: 20.2448
[1640]	training's l2: 16.3008	valid_1's l2: 20.2446
[1650]	training's l2: 16.2783	valid_1's l2: 20.2445
[1660]	training's l2: 16.2552	valid_1's l2: 20.2445
[1670]	training's l2: 16.2319	valid_1's l2: 20.2435
[1680]	training's l2: 16.21	valid_1's l2: 20.2436
[1690]	training's l2: 16.1877	valid_1's l2: 20.243
[1700]	training's l2: 16.1641	valid_1's l2: 20.243
[1710]	training's l2: 16.1414	valid_1's l2: 20.2425
[1720]	training's l2: 16.1181	valid_1's l2: 20.2423
[1730]	training's l2: 16.0953	valid_1's l2: 20.2418
[1740]	training's l2: 16.0729	valid_1's l2: 20.2417
[1750]	training's l2: 16.0504	valid_1's l2: 20.2413
[1760]	training's l2: 16.0277	valid_1's l2: 20.2415
[1770]	training's l2: 16.0041	valid_1's l2: 20.2416
Early stopping, best iteration is:
[1744]	training's l2: 16.0643	valid_1's l2: 20.2411
score1: 3.8345922866035
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261187 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 29.8218	valid_1's l2: 28.9462
[20]	training's l2: 27.0951	valid_1's l2: 26.2569
[30]	training's l2: 25.3188	valid_1's l2: 24.5257
[40]	training's l2: 24.1358	valid_1's l2: 23.3957
[50]	training's l2: 23.3239	valid_1's l2: 22.6408
[60]	training's l2: 22.755	valid_1's l2: 22.1344
[70]	training's l2: 22.3335	valid_1's l2: 21.7794
[80]	training's l2: 22.0119	valid_1's l2: 21.5264
[90]	training's l2: 21.7508	valid_1's l2: 21.337
[100]	training's l2: 21.5323	valid_1's l2: 21.1907
[110]	training's l2: 21.3453	valid_1's l2: 21.0731
[120]	training's l2: 21.1836	valid_1's l2: 20.9817
[130]	training's l2: 21.0341	valid_1's l2: 20.9042
[140]	training's l2: 20.9004	valid_1's l2: 20.8379
[150]	training's l2: 20.7723	valid_1's l2: 20.7813
[160]	training's l2: 20.6534	valid_1's l2: 20.7364
[170]	training's l2: 20.544	valid_1's l2: 20.6976
[180]	training's l2: 20.4408	valid_1's l2: 20.6648
[190]	training's l2: 20.3435	valid_1's l2: 20.6367
[200]	training's l2: 20.2473	valid_1's l2: 20.6081
[210]	training's l2: 20.1544	valid_1's l2: 20.5861
[220]	training's l2: 20.065	valid_1's l2: 20.5651
[230]	training's l2: 19.9798	valid_1's l2: 20.5485
[240]	training's l2: 19.8961	valid_1's l2: 20.533
[250]	training's l2: 19.8119	valid_1's l2: 20.5179
[260]	training's l2: 19.7302	valid_1's l2: 20.5037
[270]	training's l2: 19.6485	valid_1's l2: 20.4853
[280]	training's l2: 19.569	valid_1's l2: 20.4723
[290]	training's l2: 19.4899	valid_1's l2: 20.4587
[300]	training's l2: 19.413	valid_1's l2: 20.4463
[310]	training's l2: 19.337	valid_1's l2: 20.4353
[320]	training's l2: 19.2619	valid_1's l2: 20.4226
[330]	training's l2: 19.1877	valid_1's l2: 20.4109
[340]	training's l2: 19.1144	valid_1's l2: 20.4015
[350]	training's l2: 19.0435	valid_1's l2: 20.3943
[360]	training's l2: 18.9723	valid_1's l2: 20.3851
[370]	training's l2: 18.9014	valid_1's l2: 20.3779
[380]	training's l2: 18.8324	valid_1's l2: 20.3697
[390]	training's l2: 18.7649	valid_1's l2: 20.3654
[400]	training's l2: 18.6969	valid_1's l2: 20.3609
[410]	training's l2: 18.6312	valid_1's l2: 20.3547
[420]	training's l2: 18.5656	valid_1's l2: 20.3498
[430]	training's l2: 18.5004	valid_1's l2: 20.346
[440]	training's l2: 18.4342	valid_1's l2: 20.3409
[450]	training's l2: 18.3703	valid_1's l2: 20.338
[460]	training's l2: 18.306	valid_1's l2: 20.3348
[470]	training's l2: 18.2438	valid_1's l2: 20.3298
[480]	training's l2: 18.1813	valid_1's l2: 20.3241
[490]	training's l2: 18.1192	valid_1's l2: 20.3216
[500]	training's l2: 18.0583	valid_1's l2: 20.3182
[510]	training's l2: 17.9965	valid_1's l2: 20.315
[520]	training's l2: 17.9367	valid_1's l2: 20.3115
[530]	training's l2: 17.8773	valid_1's l2: 20.3075
[540]	training's l2: 17.8186	valid_1's l2: 20.304
[550]	training's l2: 17.7618	valid_1's l2: 20.3033
[560]	training's l2: 17.7055	valid_1's l2: 20.3017
[570]	training's l2: 17.6507	valid_1's l2: 20.2989
[580]	training's l2: 17.596	valid_1's l2: 20.2964
[590]	training's l2: 17.5411	valid_1's l2: 20.2953
[600]	training's l2: 17.4859	valid_1's l2: 20.2921
[610]	training's l2: 17.4334	valid_1's l2: 20.2908
[620]	training's l2: 17.3803	valid_1's l2: 20.2886
[630]	training's l2: 17.3278	valid_1's l2: 20.2885
[640]	training's l2: 17.2771	valid_1's l2: 20.2852
[650]	training's l2: 17.2264	valid_1's l2: 20.2841
[660]	training's l2: 17.177	valid_1's l2: 20.2843
[670]	training's l2: 17.1241	valid_1's l2: 20.283
[680]	training's l2: 17.0756	valid_1's l2: 20.2818
[690]	training's l2: 17.0232	valid_1's l2: 20.2813
[700]	training's l2: 16.974	valid_1's l2: 20.2796
[710]	training's l2: 16.9242	valid_1's l2: 20.2796
[720]	training's l2: 16.8767	valid_1's l2: 20.2789
[730]	training's l2: 16.8273	valid_1's l2: 20.2783
[740]	training's l2: 16.7795	valid_1's l2: 20.278
[750]	training's l2: 16.7325	valid_1's l2: 20.2773
[760]	training's l2: 16.6843	valid_1's l2: 20.2776
[770]	training's l2: 16.6382	valid_1's l2: 20.2764
[780]	training's l2: 16.59	valid_1's l2: 20.2769
[790]	training's l2: 16.5443	valid_1's l2: 20.2766
Early stopping, best iteration is:
[767]	training's l2: 16.6523	valid_1's l2: 20.2758
score1: 3.8401257138116534
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.245321 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.7532	valid_1's l2: 30.8537
[20]	training's l2: 29.89	valid_1's l2: 29.0065
[30]	training's l2: 28.3948	valid_1's l2: 27.5267
[40]	training's l2: 27.1895	valid_1's l2: 26.3372
[50]	training's l2: 26.2147	valid_1's l2: 25.3801
[60]	training's l2: 25.4208	valid_1's l2: 24.6051
[70]	training's l2: 24.7712	valid_1's l2: 23.9792
[80]	training's l2: 24.2361	valid_1's l2: 23.4664
[90]	training's l2: 23.7921	valid_1's l2: 23.0481
[100]	training's l2: 23.4215	valid_1's l2: 22.7042
[110]	training's l2: 23.1114	valid_1's l2: 22.4197
[120]	training's l2: 22.851	valid_1's l2: 22.1857
[130]	training's l2: 22.6269	valid_1's l2: 21.9896
[140]	training's l2: 22.4314	valid_1's l2: 21.8245
[150]	training's l2: 22.2598	valid_1's l2: 21.6832
[160]	training's l2: 22.1091	valid_1's l2: 21.5618
[170]	training's l2: 21.9728	valid_1's l2: 21.4577
[180]	training's l2: 21.8495	valid_1's l2: 21.3665
[190]	training's l2: 21.7388	valid_1's l2: 21.2879
[200]	training's l2: 21.6353	valid_1's l2: 21.2166
[210]	training's l2: 21.5398	valid_1's l2: 21.1526
[220]	training's l2: 21.4512	valid_1's l2: 21.0955
[230]	training's l2: 21.3701	valid_1's l2: 21.0449
[240]	training's l2: 21.2938	valid_1's l2: 20.9983
[250]	training's l2: 21.2207	valid_1's l2: 20.9558
[260]	training's l2: 21.1517	valid_1's l2: 20.9179
[270]	training's l2: 21.0868	valid_1's l2: 20.8839
[280]	training's l2: 21.0236	valid_1's l2: 20.8511
[290]	training's l2: 20.9614	valid_1's l2: 20.8193
[300]	training's l2: 20.9012	valid_1's l2: 20.7913
[310]	training's l2: 20.8449	valid_1's l2: 20.7674
[320]	training's l2: 20.7892	valid_1's l2: 20.7429
[330]	training's l2: 20.7367	valid_1's l2: 20.7209
[340]	training's l2: 20.6864	valid_1's l2: 20.7021
[350]	training's l2: 20.637	valid_1's l2: 20.6838
[360]	training's l2: 20.5893	valid_1's l2: 20.6672
[370]	training's l2: 20.5435	valid_1's l2: 20.6518
[380]	training's l2: 20.4976	valid_1's l2: 20.6365
[390]	training's l2: 20.4528	valid_1's l2: 20.623
[400]	training's l2: 20.4087	valid_1's l2: 20.6088
[410]	training's l2: 20.3651	valid_1's l2: 20.5964
[420]	training's l2: 20.3227	valid_1's l2: 20.5849
[430]	training's l2: 20.2812	valid_1's l2: 20.5734
[440]	training's l2: 20.2398	valid_1's l2: 20.5615
[450]	training's l2: 20.1993	valid_1's l2: 20.5509
[460]	training's l2: 20.1587	valid_1's l2: 20.5418
[470]	training's l2: 20.1192	valid_1's l2: 20.5322
[480]	training's l2: 20.0796	valid_1's l2: 20.5236
[490]	training's l2: 20.0412	valid_1's l2: 20.5147
[500]	training's l2: 20.0033	valid_1's l2: 20.5065
[510]	training's l2: 19.9652	valid_1's l2: 20.4984
[520]	training's l2: 19.9271	valid_1's l2: 20.49
[530]	training's l2: 19.8898	valid_1's l2: 20.484
[540]	training's l2: 19.8531	valid_1's l2: 20.4772
[550]	training's l2: 19.8165	valid_1's l2: 20.4709
[560]	training's l2: 19.78	valid_1's l2: 20.4645
[570]	training's l2: 19.744	valid_1's l2: 20.459
[580]	training's l2: 19.7076	valid_1's l2: 20.4527
[590]	training's l2: 19.6722	valid_1's l2: 20.4464
[600]	training's l2: 19.6361	valid_1's l2: 20.4408
[610]	training's l2: 19.6012	valid_1's l2: 20.4355
[620]	training's l2: 19.5663	valid_1's l2: 20.4288
[630]	training's l2: 19.5314	valid_1's l2: 20.4237
[640]	training's l2: 19.4969	valid_1's l2: 20.418
[650]	training's l2: 19.4629	valid_1's l2: 20.4127
[660]	training's l2: 19.4288	valid_1's l2: 20.4067
[670]	training's l2: 19.3954	valid_1's l2: 20.4012
[680]	training's l2: 19.3621	valid_1's l2: 20.3963
[690]	training's l2: 19.329	valid_1's l2: 20.3916
[700]	training's l2: 19.2962	valid_1's l2: 20.3866
[710]	training's l2: 19.2636	valid_1's l2: 20.3825
[720]	training's l2: 19.2313	valid_1's l2: 20.3782
[730]	training's l2: 19.1986	valid_1's l2: 20.3739
[740]	training's l2: 19.1668	valid_1's l2: 20.37
[750]	training's l2: 19.135	valid_1's l2: 20.3659
[760]	training's l2: 19.1034	valid_1's l2: 20.3628
[770]	training's l2: 19.0718	valid_1's l2: 20.3584
[780]	training's l2: 19.0403	valid_1's l2: 20.3554
[790]	training's l2: 19.0093	valid_1's l2: 20.3513
[800]	training's l2: 18.9787	valid_1's l2: 20.3474
[810]	training's l2: 18.948	valid_1's l2: 20.3442
[820]	training's l2: 18.9177	valid_1's l2: 20.3413
[830]	training's l2: 18.8874	valid_1's l2: 20.3387
[840]	training's l2: 18.8569	valid_1's l2: 20.3358
[850]	training's l2: 18.8264	valid_1's l2: 20.3327
[860]	training's l2: 18.7961	valid_1's l2: 20.3311
[870]	training's l2: 18.7663	valid_1's l2: 20.3289
[880]	training's l2: 18.7364	valid_1's l2: 20.3263
[890]	training's l2: 18.7067	valid_1's l2: 20.3237
[900]	training's l2: 18.6778	valid_1's l2: 20.322
[910]	training's l2: 18.6485	valid_1's l2: 20.3191
[920]	training's l2: 18.6195	valid_1's l2: 20.3165
[930]	training's l2: 18.5907	valid_1's l2: 20.3139
[940]	training's l2: 18.5612	valid_1's l2: 20.3119
[950]	training's l2: 18.5327	valid_1's l2: 20.3098
[960]	training's l2: 18.5036	valid_1's l2: 20.307
[970]	training's l2: 18.4751	valid_1's l2: 20.306
[980]	training's l2: 18.4473	valid_1's l2: 20.3042
[990]	training's l2: 18.4192	valid_1's l2: 20.3015
[1000]	training's l2: 18.3915	valid_1's l2: 20.2992
[1010]	training's l2: 18.3639	valid_1's l2: 20.2976
[1020]	training's l2: 18.3363	valid_1's l2: 20.2945
[1030]	training's l2: 18.3084	valid_1's l2: 20.2928
[1040]	training's l2: 18.2805	valid_1's l2: 20.2916
[1050]	training's l2: 18.2544	valid_1's l2: 20.2896
[1060]	training's l2: 18.2276	valid_1's l2: 20.2884
[1070]	training's l2: 18.1999	valid_1's l2: 20.286
[1080]	training's l2: 18.173	valid_1's l2: 20.2844
[1090]	training's l2: 18.1455	valid_1's l2: 20.2824
[1100]	training's l2: 18.1194	valid_1's l2: 20.2806
[1110]	training's l2: 18.0927	valid_1's l2: 20.2792
[1120]	training's l2: 18.067	valid_1's l2: 20.2775
[1130]	training's l2: 18.0415	valid_1's l2: 20.2767
[1140]	training's l2: 18.0161	valid_1's l2: 20.2752
[1150]	training's l2: 17.9908	valid_1's l2: 20.2749
[1160]	training's l2: 17.9644	valid_1's l2: 20.2738
[1170]	training's l2: 17.9399	valid_1's l2: 20.2725
[1180]	training's l2: 17.9136	valid_1's l2: 20.2716
[1190]	training's l2: 17.8891	valid_1's l2: 20.2709
[1200]	training's l2: 17.8635	valid_1's l2: 20.2693
[1210]	training's l2: 17.8391	valid_1's l2: 20.2686
[1220]	training's l2: 17.8142	valid_1's l2: 20.2669
[1230]	training's l2: 17.7905	valid_1's l2: 20.267
[1240]	training's l2: 17.7658	valid_1's l2: 20.2661
[1250]	training's l2: 17.7408	valid_1's l2: 20.2657
[1260]	training's l2: 17.717	valid_1's l2: 20.2648
[1270]	training's l2: 17.6931	valid_1's l2: 20.264
[1280]	training's l2: 17.6687	valid_1's l2: 20.263
[1290]	training's l2: 17.6451	valid_1's l2: 20.2623
[1300]	training's l2: 17.6206	valid_1's l2: 20.2612
[1310]	training's l2: 17.5961	valid_1's l2: 20.2604
[1320]	training's l2: 17.5728	valid_1's l2: 20.2593
[1330]	training's l2: 17.5498	valid_1's l2: 20.2587
[1340]	training's l2: 17.5268	valid_1's l2: 20.2582
[1350]	training's l2: 17.5038	valid_1's l2: 20.2578
[1360]	training's l2: 17.4812	valid_1's l2: 20.2563
[1370]	training's l2: 17.4583	valid_1's l2: 20.2561
[1380]	training's l2: 17.4359	valid_1's l2: 20.2547
[1390]	training's l2: 17.4128	valid_1's l2: 20.2547
[1400]	training's l2: 17.3901	valid_1's l2: 20.2548
[1410]	training's l2: 17.3679	valid_1's l2: 20.2544
[1420]	training's l2: 17.3457	valid_1's l2: 20.2537
[1430]	training's l2: 17.3232	valid_1's l2: 20.2529
[1440]	training's l2: 17.3006	valid_1's l2: 20.2522
[1450]	training's l2: 17.2781	valid_1's l2: 20.2516
[1460]	training's l2: 17.2567	valid_1's l2: 20.2502
[1470]	training's l2: 17.2343	valid_1's l2: 20.2506
[1480]	training's l2: 17.2118	valid_1's l2: 20.2506
[1490]	training's l2: 17.1896	valid_1's l2: 20.25
[1500]	training's l2: 17.1676	valid_1's l2: 20.249
[1510]	training's l2: 17.1459	valid_1's l2: 20.2488
[1520]	training's l2: 17.1242	valid_1's l2: 20.2486
[1530]	training's l2: 17.103	valid_1's l2: 20.2479
[1540]	training's l2: 17.0815	valid_1's l2: 20.247
[1550]	training's l2: 17.0604	valid_1's l2: 20.2463
[1560]	training's l2: 17.0378	valid_1's l2: 20.2469
[1570]	training's l2: 17.0156	valid_1's l2: 20.2461
[1580]	training's l2: 16.994	valid_1's l2: 20.2455
[1590]	training's l2: 16.9732	valid_1's l2: 20.2453
[1600]	training's l2: 16.9527	valid_1's l2: 20.2456
[1610]	training's l2: 16.9323	valid_1's l2: 20.2458
Early stopping, best iteration is:
[1587]	training's l2: 16.9793	valid_1's l2: 20.2446
score1: 3.836084767687951
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.260186 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.2123	valid_1's l2: 30.3213
[20]	training's l2: 29.0455	valid_1's l2: 28.1762
[30]	training's l2: 27.4002	valid_1's l2: 26.5541
[40]	training's l2: 26.143	valid_1's l2: 25.323
[50]	training's l2: 25.1721	valid_1's l2: 24.38
[60]	training's l2: 24.4154	valid_1's l2: 23.6574
[70]	training's l2: 23.8203	valid_1's l2: 23.0972
[80]	training's l2: 23.3471	valid_1's l2: 22.6597
[90]	training's l2: 22.969	valid_1's l2: 22.3181
[100]	training's l2: 22.6622	valid_1's l2: 22.0494
[110]	training's l2: 22.4044	valid_1's l2: 21.8295
[120]	training's l2: 22.1874	valid_1's l2: 21.6531
[130]	training's l2: 22.0003	valid_1's l2: 21.5053
[140]	training's l2: 21.8352	valid_1's l2: 21.3833
[150]	training's l2: 21.6898	valid_1's l2: 21.2818
[160]	training's l2: 21.5585	valid_1's l2: 21.1939
[170]	training's l2: 21.4374	valid_1's l2: 21.1136
[180]	training's l2: 21.3296	valid_1's l2: 21.0486
[190]	training's l2: 21.2307	valid_1's l2: 20.9918
[200]	training's l2: 21.1368	valid_1's l2: 20.9393
[210]	training's l2: 21.0503	valid_1's l2: 20.8937
[220]	training's l2: 20.9678	valid_1's l2: 20.8508
[230]	training's l2: 20.8866	valid_1's l2: 20.812
[240]	training's l2: 20.8095	valid_1's l2: 20.7782
[250]	training's l2: 20.7368	valid_1's l2: 20.7466
[260]	training's l2: 20.6668	valid_1's l2: 20.7185
[270]	training's l2: 20.6007	valid_1's l2: 20.6949
[280]	training's l2: 20.5365	valid_1's l2: 20.6718
[290]	training's l2: 20.475	valid_1's l2: 20.6508
[300]	training's l2: 20.4162	valid_1's l2: 20.6319
[310]	training's l2: 20.3572	valid_1's l2: 20.6137
[320]	training's l2: 20.2988	valid_1's l2: 20.5957
[330]	training's l2: 20.2418	valid_1's l2: 20.58
[340]	training's l2: 20.1868	valid_1's l2: 20.5664
[350]	training's l2: 20.1323	valid_1's l2: 20.5527
[360]	training's l2: 20.0784	valid_1's l2: 20.5386
[370]	training's l2: 20.0267	valid_1's l2: 20.5275
[380]	training's l2: 19.9751	valid_1's l2: 20.5182
[390]	training's l2: 19.9246	valid_1's l2: 20.5069
[400]	training's l2: 19.8739	valid_1's l2: 20.4975
[410]	training's l2: 19.8249	valid_1's l2: 20.4885
[420]	training's l2: 19.7765	valid_1's l2: 20.4803
[430]	training's l2: 19.7274	valid_1's l2: 20.4705
[440]	training's l2: 19.6795	valid_1's l2: 20.4625
[450]	training's l2: 19.6316	valid_1's l2: 20.4546
[460]	training's l2: 19.5847	valid_1's l2: 20.4475
[470]	training's l2: 19.5373	valid_1's l2: 20.4399
[480]	training's l2: 19.4904	valid_1's l2: 20.4291
[490]	training's l2: 19.4432	valid_1's l2: 20.4223
[500]	training's l2: 19.3979	valid_1's l2: 20.4152
[510]	training's l2: 19.3521	valid_1's l2: 20.4073
[520]	training's l2: 19.307	valid_1's l2: 20.4002
[530]	training's l2: 19.2632	valid_1's l2: 20.3946
[540]	training's l2: 19.2191	valid_1's l2: 20.3889
[550]	training's l2: 19.1752	valid_1's l2: 20.3823
[560]	training's l2: 19.1321	valid_1's l2: 20.3769
[570]	training's l2: 19.0888	valid_1's l2: 20.3708
[580]	training's l2: 19.0469	valid_1's l2: 20.365
[590]	training's l2: 19.005	valid_1's l2: 20.3603
[600]	training's l2: 18.9636	valid_1's l2: 20.3562
[610]	training's l2: 18.9232	valid_1's l2: 20.3517
[620]	training's l2: 18.8823	valid_1's l2: 20.348
[630]	training's l2: 18.8413	valid_1's l2: 20.3448
[640]	training's l2: 18.8005	valid_1's l2: 20.3408
[650]	training's l2: 18.7602	valid_1's l2: 20.3385
[660]	training's l2: 18.7193	valid_1's l2: 20.3361
[670]	training's l2: 18.6792	valid_1's l2: 20.3336
[680]	training's l2: 18.6398	valid_1's l2: 20.3291
[690]	training's l2: 18.6004	valid_1's l2: 20.3261
[700]	training's l2: 18.561	valid_1's l2: 20.3234
[710]	training's l2: 18.5223	valid_1's l2: 20.3205
[720]	training's l2: 18.4839	valid_1's l2: 20.317
[730]	training's l2: 18.4462	valid_1's l2: 20.3136
[740]	training's l2: 18.4086	valid_1's l2: 20.3109
[750]	training's l2: 18.3706	valid_1's l2: 20.3086
[760]	training's l2: 18.3323	valid_1's l2: 20.3063
[770]	training's l2: 18.2952	valid_1's l2: 20.3042
[780]	training's l2: 18.2584	valid_1's l2: 20.3021
[790]	training's l2: 18.2216	valid_1's l2: 20.2989
[800]	training's l2: 18.1854	valid_1's l2: 20.2956
[810]	training's l2: 18.1483	valid_1's l2: 20.2942
[820]	training's l2: 18.1126	valid_1's l2: 20.2918
[830]	training's l2: 18.0771	valid_1's l2: 20.2898
[840]	training's l2: 18.0405	valid_1's l2: 20.2882
[850]	training's l2: 18.0061	valid_1's l2: 20.2867
[860]	training's l2: 17.9703	valid_1's l2: 20.2848
[870]	training's l2: 17.9352	valid_1's l2: 20.2829
[880]	training's l2: 17.9007	valid_1's l2: 20.282
[890]	training's l2: 17.8667	valid_1's l2: 20.2815
[900]	training's l2: 17.8322	valid_1's l2: 20.2807
[910]	training's l2: 17.799	valid_1's l2: 20.2792
[920]	training's l2: 17.7653	valid_1's l2: 20.279
[930]	training's l2: 17.7327	valid_1's l2: 20.2779
[940]	training's l2: 17.6999	valid_1's l2: 20.2771
[950]	training's l2: 17.6669	valid_1's l2: 20.2769
[960]	training's l2: 17.6348	valid_1's l2: 20.2768
[970]	training's l2: 17.6016	valid_1's l2: 20.2759
[980]	training's l2: 17.5691	valid_1's l2: 20.2742
[990]	training's l2: 17.5381	valid_1's l2: 20.2746
[1000]	training's l2: 17.5068	valid_1's l2: 20.2738
[1010]	training's l2: 17.4751	valid_1's l2: 20.272
[1020]	training's l2: 17.445	valid_1's l2: 20.2714
[1030]	training's l2: 17.4137	valid_1's l2: 20.2707
[1040]	training's l2: 17.3827	valid_1's l2: 20.2697
[1050]	training's l2: 17.3527	valid_1's l2: 20.2696
[1060]	training's l2: 17.3205	valid_1's l2: 20.2682
[1070]	training's l2: 17.289	valid_1's l2: 20.2672
[1080]	training's l2: 17.2586	valid_1's l2: 20.267
[1090]	training's l2: 17.2282	valid_1's l2: 20.2669
[1100]	training's l2: 17.198	valid_1's l2: 20.2656
[1110]	training's l2: 17.1675	valid_1's l2: 20.2646
[1120]	training's l2: 17.1382	valid_1's l2: 20.2635
[1130]	training's l2: 17.1095	valid_1's l2: 20.2633
[1140]	training's l2: 17.0799	valid_1's l2: 20.2624
[1150]	training's l2: 17.051	valid_1's l2: 20.2617
[1160]	training's l2: 17.0222	valid_1's l2: 20.2605
[1170]	training's l2: 16.9917	valid_1's l2: 20.2602
[1180]	training's l2: 16.9626	valid_1's l2: 20.2596
[1190]	training's l2: 16.9327	valid_1's l2: 20.2591
[1200]	training's l2: 16.9048	valid_1's l2: 20.2581
[1210]	training's l2: 16.8759	valid_1's l2: 20.2575
[1220]	training's l2: 16.8461	valid_1's l2: 20.2572
[1230]	training's l2: 16.8168	valid_1's l2: 20.2571
[1240]	training's l2: 16.7887	valid_1's l2: 20.2557
[1250]	training's l2: 16.7601	valid_1's l2: 20.2565
[1260]	training's l2: 16.7318	valid_1's l2: 20.2547
[1270]	training's l2: 16.7023	valid_1's l2: 20.2544
[1280]	training's l2: 16.6739	valid_1's l2: 20.2551
[1290]	training's l2: 16.6478	valid_1's l2: 20.255
Early stopping, best iteration is:
[1267]	training's l2: 16.7114	valid_1's l2: 20.2542
score1: 3.834797128612595
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.253314 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.1557	valid_1's l2: 30.2649
[20]	training's l2: 28.9584	valid_1's l2: 28.091
[30]	training's l2: 27.3015	valid_1's l2: 26.4586
[40]	training's l2: 26.0396	valid_1's l2: 25.2231
[50]	training's l2: 25.0733	valid_1's l2: 24.2867
[60]	training's l2: 24.3219	valid_1's l2: 23.5704
[70]	training's l2: 23.734	valid_1's l2: 23.017
[80]	training's l2: 23.2677	valid_1's l2: 22.5885
[90]	training's l2: 22.8957	valid_1's l2: 22.2541
[100]	training's l2: 22.5945	valid_1's l2: 21.9931
[110]	training's l2: 22.3406	valid_1's l2: 21.7809
[120]	training's l2: 22.1272	valid_1's l2: 21.6097
[130]	training's l2: 21.9435	valid_1's l2: 21.4689
[140]	training's l2: 21.7801	valid_1's l2: 21.3498
[150]	training's l2: 21.637	valid_1's l2: 21.2525
[160]	training's l2: 21.5078	valid_1's l2: 21.1682
[170]	training's l2: 21.387	valid_1's l2: 21.0908
[180]	training's l2: 21.281	valid_1's l2: 21.0269
[190]	training's l2: 21.18	valid_1's l2: 20.9691
[200]	training's l2: 21.0869	valid_1's l2: 20.9178
[210]	training's l2: 21.0005	valid_1's l2: 20.8722
[220]	training's l2: 20.9144	valid_1's l2: 20.8283
[230]	training's l2: 20.8334	valid_1's l2: 20.7916
[240]	training's l2: 20.7562	valid_1's l2: 20.759
[250]	training's l2: 20.6814	valid_1's l2: 20.7289
[260]	training's l2: 20.611	valid_1's l2: 20.7022
[270]	training's l2: 20.5446	valid_1's l2: 20.6807
[280]	training's l2: 20.4796	valid_1's l2: 20.6592
[290]	training's l2: 20.4168	valid_1's l2: 20.638
[300]	training's l2: 20.3565	valid_1's l2: 20.6208
[310]	training's l2: 20.2966	valid_1's l2: 20.6038
[320]	training's l2: 20.2373	valid_1's l2: 20.5879
[330]	training's l2: 20.1796	valid_1's l2: 20.573
[340]	training's l2: 20.1223	valid_1's l2: 20.5583
[350]	training's l2: 20.068	valid_1's l2: 20.547
[360]	training's l2: 20.014	valid_1's l2: 20.535
[370]	training's l2: 19.9609	valid_1's l2: 20.5236
[380]	training's l2: 19.9074	valid_1's l2: 20.5124
[390]	training's l2: 19.8557	valid_1's l2: 20.502
[400]	training's l2: 19.8042	valid_1's l2: 20.4913
[410]	training's l2: 19.7531	valid_1's l2: 20.4823
[420]	training's l2: 19.7036	valid_1's l2: 20.4744
[430]	training's l2: 19.6546	valid_1's l2: 20.4657
[440]	training's l2: 19.605	valid_1's l2: 20.4576
[450]	training's l2: 19.5568	valid_1's l2: 20.4507
[460]	training's l2: 19.5073	valid_1's l2: 20.4425
[470]	training's l2: 19.4595	valid_1's l2: 20.4346
[480]	training's l2: 19.4112	valid_1's l2: 20.4266
[490]	training's l2: 19.3646	valid_1's l2: 20.4187
[500]	training's l2: 19.3179	valid_1's l2: 20.4109
[510]	training's l2: 19.2704	valid_1's l2: 20.4023
[520]	training's l2: 19.2245	valid_1's l2: 20.3966
[530]	training's l2: 19.1787	valid_1's l2: 20.3911
[540]	training's l2: 19.1336	valid_1's l2: 20.3844
[550]	training's l2: 19.0894	valid_1's l2: 20.3792
[560]	training's l2: 19.0446	valid_1's l2: 20.3739
[570]	training's l2: 19.001	valid_1's l2: 20.369
[580]	training's l2: 18.9578	valid_1's l2: 20.3638
[590]	training's l2: 18.9147	valid_1's l2: 20.3609
[600]	training's l2: 18.8716	valid_1's l2: 20.3562
[610]	training's l2: 18.8296	valid_1's l2: 20.3526
[620]	training's l2: 18.7867	valid_1's l2: 20.3491
[630]	training's l2: 18.7456	valid_1's l2: 20.3452
[640]	training's l2: 18.7039	valid_1's l2: 20.343
[650]	training's l2: 18.6633	valid_1's l2: 20.3397
[660]	training's l2: 18.622	valid_1's l2: 20.3363
[670]	training's l2: 18.5809	valid_1's l2: 20.3329
[680]	training's l2: 18.5404	valid_1's l2: 20.3291
[690]	training's l2: 18.5003	valid_1's l2: 20.3263
[700]	training's l2: 18.4596	valid_1's l2: 20.3229
[710]	training's l2: 18.4194	valid_1's l2: 20.3194
[720]	training's l2: 18.3796	valid_1's l2: 20.3165
[730]	training's l2: 18.3397	valid_1's l2: 20.3136
[740]	training's l2: 18.3005	valid_1's l2: 20.311
[750]	training's l2: 18.2619	valid_1's l2: 20.3085
[760]	training's l2: 18.2237	valid_1's l2: 20.3069
[770]	training's l2: 18.1856	valid_1's l2: 20.3067
[780]	training's l2: 18.148	valid_1's l2: 20.3045
[790]	training's l2: 18.1098	valid_1's l2: 20.3014
[800]	training's l2: 18.0718	valid_1's l2: 20.2994
[810]	training's l2: 18.0342	valid_1's l2: 20.2974
[820]	training's l2: 17.9981	valid_1's l2: 20.2964
[830]	training's l2: 17.9625	valid_1's l2: 20.2941
[840]	training's l2: 17.9265	valid_1's l2: 20.2922
[850]	training's l2: 17.8899	valid_1's l2: 20.2908
[860]	training's l2: 17.855	valid_1's l2: 20.2883
[870]	training's l2: 17.8192	valid_1's l2: 20.2868
[880]	training's l2: 17.784	valid_1's l2: 20.2857
[890]	training's l2: 17.7486	valid_1's l2: 20.2845
[900]	training's l2: 17.7137	valid_1's l2: 20.283
[910]	training's l2: 17.6811	valid_1's l2: 20.2828
[920]	training's l2: 17.6464	valid_1's l2: 20.2824
[930]	training's l2: 17.6134	valid_1's l2: 20.2826
[940]	training's l2: 17.581	valid_1's l2: 20.282
[950]	training's l2: 17.5482	valid_1's l2: 20.2813
[960]	training's l2: 17.5154	valid_1's l2: 20.2801
[970]	training's l2: 17.4835	valid_1's l2: 20.2797
[980]	training's l2: 17.45	valid_1's l2: 20.2795
[990]	training's l2: 17.4175	valid_1's l2: 20.2781
[1000]	training's l2: 17.3843	valid_1's l2: 20.2761
[1010]	training's l2: 17.3526	valid_1's l2: 20.2747
[1020]	training's l2: 17.3183	valid_1's l2: 20.273
[1030]	training's l2: 17.2881	valid_1's l2: 20.2726
[1040]	training's l2: 17.2556	valid_1's l2: 20.2705
[1050]	training's l2: 17.2233	valid_1's l2: 20.2698
[1060]	training's l2: 17.1907	valid_1's l2: 20.2687
[1070]	training's l2: 17.1609	valid_1's l2: 20.2687
[1080]	training's l2: 17.1297	valid_1's l2: 20.2675
[1090]	training's l2: 17.1002	valid_1's l2: 20.267
[1100]	training's l2: 17.0681	valid_1's l2: 20.2647
[1110]	training's l2: 17.0374	valid_1's l2: 20.2643
[1120]	training's l2: 17.0068	valid_1's l2: 20.264
[1130]	training's l2: 16.9781	valid_1's l2: 20.2642
[1140]	training's l2: 16.9476	valid_1's l2: 20.2638
[1150]	training's l2: 16.9165	valid_1's l2: 20.2629
[1160]	training's l2: 16.8866	valid_1's l2: 20.2623
[1170]	training's l2: 16.8572	valid_1's l2: 20.2619
[1180]	training's l2: 16.8266	valid_1's l2: 20.2614
[1190]	training's l2: 16.7966	valid_1's l2: 20.2603
[1200]	training's l2: 16.7681	valid_1's l2: 20.2605
[1210]	training's l2: 16.7409	valid_1's l2: 20.2597
[1220]	training's l2: 16.7101	valid_1's l2: 20.2592
[1230]	training's l2: 16.6803	valid_1's l2: 20.2587
[1240]	training's l2: 16.6508	valid_1's l2: 20.2581
[1250]	training's l2: 16.6209	valid_1's l2: 20.2578
[1260]	training's l2: 16.5919	valid_1's l2: 20.2576
[1270]	training's l2: 16.5634	valid_1's l2: 20.2571
[1280]	training's l2: 16.5346	valid_1's l2: 20.2572
[1290]	training's l2: 16.5055	valid_1's l2: 20.2572
[1300]	training's l2: 16.4754	valid_1's l2: 20.2572
[1310]	training's l2: 16.448	valid_1's l2: 20.258
Early stopping, best iteration is:
[1284]	training's l2: 16.5229	valid_1's l2: 20.2566
score1: 3.837184692228007
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262806 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.803	valid_1's l2: 29.9175
[20]	training's l2: 28.4361	valid_1's l2: 27.5789
[30]	training's l2: 26.717	valid_1's l2: 25.8892
[40]	training's l2: 25.4538	valid_1's l2: 24.6608
[50]	training's l2: 24.5121	valid_1's l2: 23.7576
[60]	training's l2: 23.8029	valid_1's l2: 23.0891
[70]	training's l2: 23.259	valid_1's l2: 22.5912
[80]	training's l2: 22.8402	valid_1's l2: 22.2173
[90]	training's l2: 22.5075	valid_1's l2: 21.9303
[100]	training's l2: 22.2332	valid_1's l2: 21.7062
[110]	training's l2: 22.0064	valid_1's l2: 21.5276
[120]	training's l2: 21.8089	valid_1's l2: 21.3838
[130]	training's l2: 21.6401	valid_1's l2: 21.267
[140]	training's l2: 21.4895	valid_1's l2: 21.1675
[150]	training's l2: 21.3549	valid_1's l2: 21.0837
[160]	training's l2: 21.2327	valid_1's l2: 21.0122
[170]	training's l2: 21.119	valid_1's l2: 20.9471
[180]	training's l2: 21.0171	valid_1's l2: 20.8943
[190]	training's l2: 20.9184	valid_1's l2: 20.8454
[200]	training's l2: 20.8237	valid_1's l2: 20.8007
[210]	training's l2: 20.7343	valid_1's l2: 20.7626
[220]	training's l2: 20.649	valid_1's l2: 20.7291
[230]	training's l2: 20.57	valid_1's l2: 20.6998
[240]	training's l2: 20.4944	valid_1's l2: 20.674
[250]	training's l2: 20.4206	valid_1's l2: 20.651
[260]	training's l2: 20.3506	valid_1's l2: 20.631
[270]	training's l2: 20.2816	valid_1's l2: 20.6122
[280]	training's l2: 20.2136	valid_1's l2: 20.5934
[290]	training's l2: 20.1473	valid_1's l2: 20.576
[300]	training's l2: 20.082	valid_1's l2: 20.5605
[310]	training's l2: 20.0199	valid_1's l2: 20.5466
[320]	training's l2: 19.9577	valid_1's l2: 20.5341
[330]	training's l2: 19.8963	valid_1's l2: 20.5207
[340]	training's l2: 19.8361	valid_1's l2: 20.5082
[350]	training's l2: 19.7758	valid_1's l2: 20.4953
[360]	training's l2: 19.7177	valid_1's l2: 20.4843
[370]	training's l2: 19.6591	valid_1's l2: 20.4732
[380]	training's l2: 19.6016	valid_1's l2: 20.4634
[390]	training's l2: 19.5443	valid_1's l2: 20.4543
[400]	training's l2: 19.4876	valid_1's l2: 20.4445
[410]	training's l2: 19.4312	valid_1's l2: 20.432
[420]	training's l2: 19.3759	valid_1's l2: 20.4227
[430]	training's l2: 19.3218	valid_1's l2: 20.4165
[440]	training's l2: 19.2692	valid_1's l2: 20.4097
[450]	training's l2: 19.2163	valid_1's l2: 20.4022
[460]	training's l2: 19.164	valid_1's l2: 20.3952
[470]	training's l2: 19.1123	valid_1's l2: 20.3897
[480]	training's l2: 19.0616	valid_1's l2: 20.3846
[490]	training's l2: 19.0102	valid_1's l2: 20.3793
[500]	training's l2: 18.9586	valid_1's l2: 20.3727
[510]	training's l2: 18.9085	valid_1's l2: 20.3674
[520]	training's l2: 18.8582	valid_1's l2: 20.3626
[530]	training's l2: 18.8089	valid_1's l2: 20.3584
[540]	training's l2: 18.7605	valid_1's l2: 20.3544
[550]	training's l2: 18.7127	valid_1's l2: 20.3503
[560]	training's l2: 18.6639	valid_1's l2: 20.3485
[570]	training's l2: 18.6164	valid_1's l2: 20.3451
[580]	training's l2: 18.5695	valid_1's l2: 20.3415
[590]	training's l2: 18.5234	valid_1's l2: 20.3382
[600]	training's l2: 18.4773	valid_1's l2: 20.3351
[610]	training's l2: 18.4313	valid_1's l2: 20.3311
[620]	training's l2: 18.3849	valid_1's l2: 20.3265
[630]	training's l2: 18.3392	valid_1's l2: 20.3227
[640]	training's l2: 18.2938	valid_1's l2: 20.3185
[650]	training's l2: 18.2484	valid_1's l2: 20.3156
[660]	training's l2: 18.2032	valid_1's l2: 20.3128
[670]	training's l2: 18.1602	valid_1's l2: 20.3106
[680]	training's l2: 18.117	valid_1's l2: 20.3084
[690]	training's l2: 18.0737	valid_1's l2: 20.3055
[700]	training's l2: 18.0302	valid_1's l2: 20.3022
[710]	training's l2: 17.9867	valid_1's l2: 20.2994
[720]	training's l2: 17.9441	valid_1's l2: 20.2971
[730]	training's l2: 17.9024	valid_1's l2: 20.2951
[740]	training's l2: 17.8615	valid_1's l2: 20.2932
[750]	training's l2: 17.8196	valid_1's l2: 20.2896
[760]	training's l2: 17.7802	valid_1's l2: 20.2885
[770]	training's l2: 17.7399	valid_1's l2: 20.2872
[780]	training's l2: 17.7008	valid_1's l2: 20.2869
[790]	training's l2: 17.6619	valid_1's l2: 20.2861
[800]	training's l2: 17.6231	valid_1's l2: 20.2845
[810]	training's l2: 17.5846	valid_1's l2: 20.2835
[820]	training's l2: 17.5447	valid_1's l2: 20.2829
[830]	training's l2: 17.5069	valid_1's l2: 20.2803
[840]	training's l2: 17.4692	valid_1's l2: 20.2795
[850]	training's l2: 17.4295	valid_1's l2: 20.2771
[860]	training's l2: 17.3916	valid_1's l2: 20.2766
[870]	training's l2: 17.3544	valid_1's l2: 20.2755
[880]	training's l2: 17.3178	valid_1's l2: 20.2745
[890]	training's l2: 17.2801	valid_1's l2: 20.273
[900]	training's l2: 17.2443	valid_1's l2: 20.2721
[910]	training's l2: 17.2067	valid_1's l2: 20.2717
[920]	training's l2: 17.1721	valid_1's l2: 20.2708
[930]	training's l2: 17.1369	valid_1's l2: 20.2701
[940]	training's l2: 17.1012	valid_1's l2: 20.27
[950]	training's l2: 17.065	valid_1's l2: 20.2692
[960]	training's l2: 17.0302	valid_1's l2: 20.2686
[970]	training's l2: 16.9948	valid_1's l2: 20.2689
[980]	training's l2: 16.9605	valid_1's l2: 20.2686
[990]	training's l2: 16.9274	valid_1's l2: 20.268
[1000]	training's l2: 16.8937	valid_1's l2: 20.2671
[1010]	training's l2: 16.8603	valid_1's l2: 20.268
[1020]	training's l2: 16.8248	valid_1's l2: 20.2679
[1030]	training's l2: 16.7924	valid_1's l2: 20.2676
Early stopping, best iteration is:
[1006]	training's l2: 16.875	valid_1's l2: 20.2671
score1: 3.838919971569667
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.240531 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.3097	valid_1's l2: 30.4119
[20]	training's l2: 29.1974	valid_1's l2: 28.316
[30]	training's l2: 27.5797	valid_1's l2: 26.7146
[40]	training's l2: 26.3319	valid_1's l2: 25.4868
[50]	training's l2: 25.364	valid_1's l2: 24.5408
[60]	training's l2: 24.6055	valid_1's l2: 23.8107
[70]	training's l2: 24.0051	valid_1's l2: 23.2377
[80]	training's l2: 23.5255	valid_1's l2: 22.7887
[90]	training's l2: 23.1407	valid_1's l2: 22.4357
[100]	training's l2: 22.8292	valid_1's l2: 22.1563
[110]	training's l2: 22.569	valid_1's l2: 21.9269
[120]	training's l2: 22.3486	valid_1's l2: 21.7403
[130]	training's l2: 22.1613	valid_1's l2: 21.5863
[140]	training's l2: 21.9947	valid_1's l2: 21.4571
[150]	training's l2: 21.8495	valid_1's l2: 21.349
[160]	training's l2: 21.7221	valid_1's l2: 21.2589
[170]	training's l2: 21.6013	valid_1's l2: 21.175
[180]	training's l2: 21.4926	valid_1's l2: 21.1026
[190]	training's l2: 21.395	valid_1's l2: 21.0421
[200]	training's l2: 21.3034	valid_1's l2: 20.9871
[210]	training's l2: 21.2188	valid_1's l2: 20.9371
[220]	training's l2: 21.1419	valid_1's l2: 20.8938
[230]	training's l2: 21.0641	valid_1's l2: 20.8512
[240]	training's l2: 20.9907	valid_1's l2: 20.8132
[250]	training's l2: 20.9215	valid_1's l2: 20.7809
[260]	training's l2: 20.8547	valid_1's l2: 20.7519
[270]	training's l2: 20.7912	valid_1's l2: 20.7243
[280]	training's l2: 20.7311	valid_1's l2: 20.7011
[290]	training's l2: 20.6734	valid_1's l2: 20.6805
[300]	training's l2: 20.6168	valid_1's l2: 20.6617
[310]	training's l2: 20.5611	valid_1's l2: 20.6418
[320]	training's l2: 20.5079	valid_1's l2: 20.6255
[330]	training's l2: 20.4551	valid_1's l2: 20.6079
[340]	training's l2: 20.4044	valid_1's l2: 20.5938
[350]	training's l2: 20.3542	valid_1's l2: 20.5797
[360]	training's l2: 20.3044	valid_1's l2: 20.5661
[370]	training's l2: 20.256	valid_1's l2: 20.5542
[380]	training's l2: 20.2082	valid_1's l2: 20.5434
[390]	training's l2: 20.1615	valid_1's l2: 20.5325
[400]	training's l2: 20.1163	valid_1's l2: 20.5228
[410]	training's l2: 20.0708	valid_1's l2: 20.5131
[420]	training's l2: 20.0266	valid_1's l2: 20.5041
[430]	training's l2: 19.9821	valid_1's l2: 20.494
[440]	training's l2: 19.9384	valid_1's l2: 20.4869
[450]	training's l2: 19.8954	valid_1's l2: 20.4784
[460]	training's l2: 19.8523	valid_1's l2: 20.4695
[470]	training's l2: 19.8091	valid_1's l2: 20.4616
[480]	training's l2: 19.7662	valid_1's l2: 20.4539
[490]	training's l2: 19.7238	valid_1's l2: 20.4455
[500]	training's l2: 19.6815	valid_1's l2: 20.4371
[510]	training's l2: 19.6403	valid_1's l2: 20.4299
[520]	training's l2: 19.5993	valid_1's l2: 20.4226
[530]	training's l2: 19.559	valid_1's l2: 20.4169
[540]	training's l2: 19.5187	valid_1's l2: 20.41
[550]	training's l2: 19.4791	valid_1's l2: 20.4031
[560]	training's l2: 19.4394	valid_1's l2: 20.3976
[570]	training's l2: 19.3999	valid_1's l2: 20.3922
[580]	training's l2: 19.3613	valid_1's l2: 20.3871
[590]	training's l2: 19.3228	valid_1's l2: 20.382
[600]	training's l2: 19.2849	valid_1's l2: 20.3763
[610]	training's l2: 19.2464	valid_1's l2: 20.3704
[620]	training's l2: 19.2084	valid_1's l2: 20.365
[630]	training's l2: 19.1702	valid_1's l2: 20.3598
[640]	training's l2: 19.1342	valid_1's l2: 20.3566
[650]	training's l2: 19.0974	valid_1's l2: 20.3532
[660]	training's l2: 19.0606	valid_1's l2: 20.3485
[670]	training's l2: 19.0236	valid_1's l2: 20.344
[680]	training's l2: 18.9872	valid_1's l2: 20.3413
[690]	training's l2: 18.9509	valid_1's l2: 20.3376
[700]	training's l2: 18.9158	valid_1's l2: 20.3348
[710]	training's l2: 18.8801	valid_1's l2: 20.3317
[720]	training's l2: 18.8448	valid_1's l2: 20.3294
[730]	training's l2: 18.8104	valid_1's l2: 20.326
[740]	training's l2: 18.776	valid_1's l2: 20.3231
[750]	training's l2: 18.7416	valid_1's l2: 20.3195
[760]	training's l2: 18.7063	valid_1's l2: 20.3153
[770]	training's l2: 18.6723	valid_1's l2: 20.3126
[780]	training's l2: 18.6386	valid_1's l2: 20.3092
[790]	training's l2: 18.605	valid_1's l2: 20.3058
[800]	training's l2: 18.5708	valid_1's l2: 20.3036
[810]	training's l2: 18.5373	valid_1's l2: 20.3015
[820]	training's l2: 18.5042	valid_1's l2: 20.2997
[830]	training's l2: 18.4712	valid_1's l2: 20.2973
[840]	training's l2: 18.4385	valid_1's l2: 20.296
[850]	training's l2: 18.406	valid_1's l2: 20.2952
[860]	training's l2: 18.3732	valid_1's l2: 20.2942
[870]	training's l2: 18.3412	valid_1's l2: 20.2923
[880]	training's l2: 18.3103	valid_1's l2: 20.2909
[890]	training's l2: 18.2786	valid_1's l2: 20.2896
[900]	training's l2: 18.2465	valid_1's l2: 20.2873
[910]	training's l2: 18.2158	valid_1's l2: 20.2864
[920]	training's l2: 18.1843	valid_1's l2: 20.2838
[930]	training's l2: 18.1546	valid_1's l2: 20.2818
[940]	training's l2: 18.1238	valid_1's l2: 20.2804
[950]	training's l2: 18.0945	valid_1's l2: 20.2797
[960]	training's l2: 18.0642	valid_1's l2: 20.2798
[970]	training's l2: 18.0349	valid_1's l2: 20.2781
[980]	training's l2: 18.0056	valid_1's l2: 20.2779
[990]	training's l2: 17.9762	valid_1's l2: 20.2758
[1000]	training's l2: 17.9456	valid_1's l2: 20.2743
[1010]	training's l2: 17.9169	valid_1's l2: 20.2734
[1020]	training's l2: 17.8891	valid_1's l2: 20.2728
[1030]	training's l2: 17.8611	valid_1's l2: 20.2713
[1040]	training's l2: 17.8325	valid_1's l2: 20.2701
[1050]	training's l2: 17.8044	valid_1's l2: 20.2698
[1060]	training's l2: 17.7757	valid_1's l2: 20.269
[1070]	training's l2: 17.7477	valid_1's l2: 20.2687
[1080]	training's l2: 17.7192	valid_1's l2: 20.2675
[1090]	training's l2: 17.6919	valid_1's l2: 20.2656
[1100]	training's l2: 17.6639	valid_1's l2: 20.2644
[1110]	training's l2: 17.6369	valid_1's l2: 20.2636
[1120]	training's l2: 17.6098	valid_1's l2: 20.2635
[1130]	training's l2: 17.5822	valid_1's l2: 20.2625
[1140]	training's l2: 17.5564	valid_1's l2: 20.2612
[1150]	training's l2: 17.5292	valid_1's l2: 20.2599
[1160]	training's l2: 17.5032	valid_1's l2: 20.2593
[1170]	training's l2: 17.4764	valid_1's l2: 20.2589
[1180]	training's l2: 17.4507	valid_1's l2: 20.2585
[1190]	training's l2: 17.4238	valid_1's l2: 20.2579
[1200]	training's l2: 17.3965	valid_1's l2: 20.2575
[1210]	training's l2: 17.3698	valid_1's l2: 20.2577
[1220]	training's l2: 17.3444	valid_1's l2: 20.2573
[1230]	training's l2: 17.3189	valid_1's l2: 20.2565
[1240]	training's l2: 17.2937	valid_1's l2: 20.2557
[1250]	training's l2: 17.2688	valid_1's l2: 20.2548
[1260]	training's l2: 17.2439	valid_1's l2: 20.2543
[1270]	training's l2: 17.2183	valid_1's l2: 20.2535
[1280]	training's l2: 17.1925	valid_1's l2: 20.2532
[1290]	training's l2: 17.1663	valid_1's l2: 20.2522
[1300]	training's l2: 17.1418	valid_1's l2: 20.2522
[1310]	training's l2: 17.1169	valid_1's l2: 20.2518
[1320]	training's l2: 17.0913	valid_1's l2: 20.2523
[1330]	training's l2: 17.0678	valid_1's l2: 20.253
[1340]	training's l2: 17.0432	valid_1's l2: 20.2528
Early stopping, best iteration is:
[1310]	training's l2: 17.1169	valid_1's l2: 20.2518
score1: 3.8399059225977368
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257509 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.6199	valid_1's l2: 30.7229
[20]	training's l2: 29.6769	valid_1's l2: 28.7998
[30]	training's l2: 28.1365	valid_1's l2: 27.2776
[40]	training's l2: 26.9131	valid_1's l2: 26.0734
[50]	training's l2: 25.9328	valid_1's l2: 25.1149
[60]	training's l2: 25.1417	valid_1's l2: 24.3464
[70]	training's l2: 24.5023	valid_1's l2: 23.735
[80]	training's l2: 23.9803	valid_1's l2: 23.2391
[90]	training's l2: 23.5499	valid_1's l2: 22.839
[100]	training's l2: 23.1936	valid_1's l2: 22.5127
[110]	training's l2: 22.8974	valid_1's l2: 22.2485
[120]	training's l2: 22.6487	valid_1's l2: 22.0306
[130]	training's l2: 22.4336	valid_1's l2: 21.8489
[140]	training's l2: 22.2462	valid_1's l2: 21.6957
[150]	training's l2: 22.082	valid_1's l2: 21.5636
[160]	training's l2: 21.9359	valid_1's l2: 21.4524
[170]	training's l2: 21.8046	valid_1's l2: 21.3571
[180]	training's l2: 21.6863	valid_1's l2: 21.2758
[190]	training's l2: 21.577	valid_1's l2: 21.203
[200]	training's l2: 21.4739	valid_1's l2: 21.1345
[210]	training's l2: 21.3805	valid_1's l2: 21.0758
[220]	training's l2: 21.2947	valid_1's l2: 21.0249
[230]	training's l2: 21.213	valid_1's l2: 20.9772
[240]	training's l2: 21.1364	valid_1's l2: 20.9349
[250]	training's l2: 21.065	valid_1's l2: 20.8969
[260]	training's l2: 20.9953	valid_1's l2: 20.8616
[270]	training's l2: 20.9261	valid_1's l2: 20.8261
[280]	training's l2: 20.8602	valid_1's l2: 20.7947
[290]	training's l2: 20.7979	valid_1's l2: 20.7678
[300]	training's l2: 20.7379	valid_1's l2: 20.743
[310]	training's l2: 20.6795	valid_1's l2: 20.7204
[320]	training's l2: 20.6239	valid_1's l2: 20.7005
[330]	training's l2: 20.5704	valid_1's l2: 20.6813
[340]	training's l2: 20.5182	valid_1's l2: 20.6623
[350]	training's l2: 20.4672	valid_1's l2: 20.6457
[360]	training's l2: 20.4172	valid_1's l2: 20.6301
[370]	training's l2: 20.3673	valid_1's l2: 20.6152
[380]	training's l2: 20.3201	valid_1's l2: 20.6022
[390]	training's l2: 20.2727	valid_1's l2: 20.5881
[400]	training's l2: 20.2257	valid_1's l2: 20.5759
[410]	training's l2: 20.1799	valid_1's l2: 20.5633
[420]	training's l2: 20.1355	valid_1's l2: 20.5526
[430]	training's l2: 20.0906	valid_1's l2: 20.5424
[440]	training's l2: 20.0468	valid_1's l2: 20.5327
[450]	training's l2: 20.0039	valid_1's l2: 20.5245
[460]	training's l2: 19.9608	valid_1's l2: 20.5143
[470]	training's l2: 19.9187	valid_1's l2: 20.5054
[480]	training's l2: 19.8773	valid_1's l2: 20.4969
[490]	training's l2: 19.8367	valid_1's l2: 20.4897
[500]	training's l2: 19.7959	valid_1's l2: 20.4825
[510]	training's l2: 19.7559	valid_1's l2: 20.4756
[520]	training's l2: 19.7156	valid_1's l2: 20.4684
[530]	training's l2: 19.676	valid_1's l2: 20.4617
[540]	training's l2: 19.6363	valid_1's l2: 20.4567
[550]	training's l2: 19.5962	valid_1's l2: 20.4483
[560]	training's l2: 19.5565	valid_1's l2: 20.441
[570]	training's l2: 19.5175	valid_1's l2: 20.4351
[580]	training's l2: 19.4789	valid_1's l2: 20.4289
[590]	training's l2: 19.4408	valid_1's l2: 20.4231
[600]	training's l2: 19.403	valid_1's l2: 20.418
[610]	training's l2: 19.3655	valid_1's l2: 20.4125
[620]	training's l2: 19.3287	valid_1's l2: 20.4069
[630]	training's l2: 19.2912	valid_1's l2: 20.4004
[640]	training's l2: 19.2545	valid_1's l2: 20.3951
[650]	training's l2: 19.2182	valid_1's l2: 20.3905
[660]	training's l2: 19.182	valid_1's l2: 20.3847
[670]	training's l2: 19.1462	valid_1's l2: 20.38
[680]	training's l2: 19.1108	valid_1's l2: 20.3748
[690]	training's l2: 19.0753	valid_1's l2: 20.3711
[700]	training's l2: 19.0401	valid_1's l2: 20.368
[710]	training's l2: 19.0049	valid_1's l2: 20.3646
[720]	training's l2: 18.9697	valid_1's l2: 20.3609
[730]	training's l2: 18.9355	valid_1's l2: 20.3574
[740]	training's l2: 18.9015	valid_1's l2: 20.3536
[750]	training's l2: 18.8674	valid_1's l2: 20.3496
[760]	training's l2: 18.8335	valid_1's l2: 20.3464
[770]	training's l2: 18.7996	valid_1's l2: 20.3437
[780]	training's l2: 18.7659	valid_1's l2: 20.341
[790]	training's l2: 18.7327	valid_1's l2: 20.3389
[800]	training's l2: 18.6992	valid_1's l2: 20.3361
[810]	training's l2: 18.6664	valid_1's l2: 20.3331
[820]	training's l2: 18.6334	valid_1's l2: 20.3307
[830]	training's l2: 18.6007	valid_1's l2: 20.3276
[840]	training's l2: 18.5685	valid_1's l2: 20.3252
[850]	training's l2: 18.5363	valid_1's l2: 20.3226
[860]	training's l2: 18.5044	valid_1's l2: 20.3195
[870]	training's l2: 18.4727	valid_1's l2: 20.317
[880]	training's l2: 18.4397	valid_1's l2: 20.3138
[890]	training's l2: 18.408	valid_1's l2: 20.3115
[900]	training's l2: 18.3762	valid_1's l2: 20.3088
[910]	training's l2: 18.3455	valid_1's l2: 20.3067
[920]	training's l2: 18.3146	valid_1's l2: 20.3053
[930]	training's l2: 18.2833	valid_1's l2: 20.3029
[940]	training's l2: 18.2522	valid_1's l2: 20.3011
[950]	training's l2: 18.2224	valid_1's l2: 20.2996
[960]	training's l2: 18.1918	valid_1's l2: 20.2984
[970]	training's l2: 18.162	valid_1's l2: 20.2966
[980]	training's l2: 18.1315	valid_1's l2: 20.2949
[990]	training's l2: 18.1016	valid_1's l2: 20.2935
[1000]	training's l2: 18.0716	valid_1's l2: 20.2914
[1010]	training's l2: 18.0424	valid_1's l2: 20.2909
[1020]	training's l2: 18.0126	valid_1's l2: 20.2893
[1030]	training's l2: 17.9828	valid_1's l2: 20.2876
[1040]	training's l2: 17.9533	valid_1's l2: 20.2859
[1050]	training's l2: 17.9239	valid_1's l2: 20.285
[1060]	training's l2: 17.8959	valid_1's l2: 20.2841
[1070]	training's l2: 17.8674	valid_1's l2: 20.2829
[1080]	training's l2: 17.8391	valid_1's l2: 20.2814
[1090]	training's l2: 17.8111	valid_1's l2: 20.2807
[1100]	training's l2: 17.7826	valid_1's l2: 20.2797
[1110]	training's l2: 17.7547	valid_1's l2: 20.2792
[1120]	training's l2: 17.7265	valid_1's l2: 20.2784
[1130]	training's l2: 17.6998	valid_1's l2: 20.277
[1140]	training's l2: 17.6724	valid_1's l2: 20.2759
[1150]	training's l2: 17.6459	valid_1's l2: 20.2747
[1160]	training's l2: 17.6196	valid_1's l2: 20.2737
[1170]	training's l2: 17.5936	valid_1's l2: 20.273
[1180]	training's l2: 17.567	valid_1's l2: 20.2728
[1190]	training's l2: 17.5405	valid_1's l2: 20.2715
[1200]	training's l2: 17.5155	valid_1's l2: 20.2713
[1210]	training's l2: 17.49	valid_1's l2: 20.2712
[1220]	training's l2: 17.4649	valid_1's l2: 20.27
[1230]	training's l2: 17.4389	valid_1's l2: 20.2698
[1240]	training's l2: 17.4126	valid_1's l2: 20.2688
[1250]	training's l2: 17.3865	valid_1's l2: 20.2679
[1260]	training's l2: 17.3607	valid_1's l2: 20.2674
[1270]	training's l2: 17.3351	valid_1's l2: 20.2675
[1280]	training's l2: 17.3095	valid_1's l2: 20.2655
[1290]	training's l2: 17.285	valid_1's l2: 20.2645
[1300]	training's l2: 17.2598	valid_1's l2: 20.2644
[1310]	training's l2: 17.2344	valid_1's l2: 20.2635
[1320]	training's l2: 17.2094	valid_1's l2: 20.2629
[1330]	training's l2: 17.1845	valid_1's l2: 20.2629
[1340]	training's l2: 17.1591	valid_1's l2: 20.2619
[1350]	training's l2: 17.1351	valid_1's l2: 20.2608
[1360]	training's l2: 17.1104	valid_1's l2: 20.2598
[1370]	training's l2: 17.0869	valid_1's l2: 20.2588
[1380]	training's l2: 17.0616	valid_1's l2: 20.2583
[1390]	training's l2: 17.0377	valid_1's l2: 20.258
[1400]	training's l2: 17.0128	valid_1's l2: 20.2579
[1410]	training's l2: 16.9876	valid_1's l2: 20.2572
[1420]	training's l2: 16.9647	valid_1's l2: 20.2562
[1430]	training's l2: 16.9409	valid_1's l2: 20.2559
[1440]	training's l2: 16.9174	valid_1's l2: 20.2557
[1450]	training's l2: 16.8927	valid_1's l2: 20.2556
[1460]	training's l2: 16.8692	valid_1's l2: 20.2558
[1470]	training's l2: 16.8446	valid_1's l2: 20.2558
[1480]	training's l2: 16.8202	valid_1's l2: 20.2553
[1490]	training's l2: 16.7964	valid_1's l2: 20.2547
[1500]	training's l2: 16.7726	valid_1's l2: 20.2541
[1510]	training's l2: 16.749	valid_1's l2: 20.2527
[1520]	training's l2: 16.7254	valid_1's l2: 20.253
[1530]	training's l2: 16.7021	valid_1's l2: 20.2529
[1540]	training's l2: 16.6793	valid_1's l2: 20.2519
[1550]	training's l2: 16.6557	valid_1's l2: 20.2522
[1560]	training's l2: 16.6322	valid_1's l2: 20.2514
[1570]	training's l2: 16.6083	valid_1's l2: 20.2514
[1580]	training's l2: 16.5846	valid_1's l2: 20.2514
[1590]	training's l2: 16.5625	valid_1's l2: 20.2505
[1600]	training's l2: 16.5393	valid_1's l2: 20.2502
[1610]	training's l2: 16.5158	valid_1's l2: 20.2504
[1620]	training's l2: 16.4926	valid_1's l2: 20.2499
[1630]	training's l2: 16.4701	valid_1's l2: 20.2499
[1640]	training's l2: 16.4475	valid_1's l2: 20.2496
[1650]	training's l2: 16.4242	valid_1's l2: 20.2503
Early stopping, best iteration is:
[1628]	training's l2: 16.4746	valid_1's l2: 20.2493
score1: 3.836045556635742
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.274252 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.786	valid_1's l2: 30.8868
[20]	training's l2: 29.9429	valid_1's l2: 29.0593
[30]	training's l2: 28.4575	valid_1's l2: 27.5886
[40]	training's l2: 27.2583	valid_1's l2: 26.4063
[50]	training's l2: 26.2834	valid_1's l2: 25.4507
[60]	training's l2: 25.4874	valid_1's l2: 24.6728
[70]	training's l2: 24.8333	valid_1's l2: 24.0434
[80]	training's l2: 24.2952	valid_1's l2: 23.5297
[90]	training's l2: 23.8462	valid_1's l2: 23.1055
[100]	training's l2: 23.4712	valid_1's l2: 22.757
[110]	training's l2: 23.1564	valid_1's l2: 22.4692
[120]	training's l2: 22.8908	valid_1's l2: 22.2308
[130]	training's l2: 22.6639	valid_1's l2: 22.0297
[140]	training's l2: 22.4652	valid_1's l2: 21.86
[150]	training's l2: 22.291	valid_1's l2: 21.7159
[160]	training's l2: 22.1369	valid_1's l2: 21.5925
[170]	training's l2: 21.9996	valid_1's l2: 21.4848
[180]	training's l2: 21.8742	valid_1's l2: 21.3911
[190]	training's l2: 21.7616	valid_1's l2: 21.31
[200]	training's l2: 21.6588	valid_1's l2: 21.239
[210]	training's l2: 21.5608	valid_1's l2: 21.1732
[220]	training's l2: 21.4709	valid_1's l2: 21.1145
[230]	training's l2: 21.3889	valid_1's l2: 21.0627
[240]	training's l2: 21.3111	valid_1's l2: 21.0165
[250]	training's l2: 21.2372	valid_1's l2: 20.9733
[260]	training's l2: 21.1671	valid_1's l2: 20.9338
[270]	training's l2: 21.1012	valid_1's l2: 20.8983
[280]	training's l2: 21.0393	valid_1's l2: 20.8666
[290]	training's l2: 20.976	valid_1's l2: 20.833
[300]	training's l2: 20.9161	valid_1's l2: 20.8037
[310]	training's l2: 20.8584	valid_1's l2: 20.7778
[320]	training's l2: 20.802	valid_1's l2: 20.7538
[330]	training's l2: 20.7483	valid_1's l2: 20.7301
[340]	training's l2: 20.6971	valid_1's l2: 20.7114
[350]	training's l2: 20.6479	valid_1's l2: 20.6935
[360]	training's l2: 20.5988	valid_1's l2: 20.6757
[370]	training's l2: 20.5514	valid_1's l2: 20.659
[380]	training's l2: 20.5054	valid_1's l2: 20.6435
[390]	training's l2: 20.4604	valid_1's l2: 20.6285
[400]	training's l2: 20.4163	valid_1's l2: 20.6148
[410]	training's l2: 20.3725	valid_1's l2: 20.6013
[420]	training's l2: 20.3293	valid_1's l2: 20.5884
[430]	training's l2: 20.2869	valid_1's l2: 20.5775
[440]	training's l2: 20.2453	valid_1's l2: 20.5659
[450]	training's l2: 20.2046	valid_1's l2: 20.5557
[460]	training's l2: 20.1642	valid_1's l2: 20.5463
[470]	training's l2: 20.1243	valid_1's l2: 20.5367
[480]	training's l2: 20.0848	valid_1's l2: 20.5273
[490]	training's l2: 20.0459	valid_1's l2: 20.519
[500]	training's l2: 20.0073	valid_1's l2: 20.5108
[510]	training's l2: 19.9689	valid_1's l2: 20.5028
[520]	training's l2: 19.9312	valid_1's l2: 20.4947
[530]	training's l2: 19.8947	valid_1's l2: 20.4891
[540]	training's l2: 19.8577	valid_1's l2: 20.481
[550]	training's l2: 19.8212	valid_1's l2: 20.4737
[560]	training's l2: 19.7842	valid_1's l2: 20.4672
[570]	training's l2: 19.7481	valid_1's l2: 20.4605
[580]	training's l2: 19.7112	valid_1's l2: 20.4533
[590]	training's l2: 19.6751	valid_1's l2: 20.4463
[600]	training's l2: 19.6394	valid_1's l2: 20.4409
[610]	training's l2: 19.6038	valid_1's l2: 20.4347
[620]	training's l2: 19.5686	valid_1's l2: 20.4288
[630]	training's l2: 19.5341	valid_1's l2: 20.4233
[640]	training's l2: 19.499	valid_1's l2: 20.4174
[650]	training's l2: 19.4653	valid_1's l2: 20.412
[660]	training's l2: 19.4311	valid_1's l2: 20.406
[670]	training's l2: 19.3972	valid_1's l2: 20.4009
[680]	training's l2: 19.3632	valid_1's l2: 20.3954
[690]	training's l2: 19.3296	valid_1's l2: 20.3898
[700]	training's l2: 19.2967	valid_1's l2: 20.3849
[710]	training's l2: 19.2638	valid_1's l2: 20.3794
[720]	training's l2: 19.2312	valid_1's l2: 20.3744
[730]	training's l2: 19.1989	valid_1's l2: 20.3702
[740]	training's l2: 19.1667	valid_1's l2: 20.3674
[750]	training's l2: 19.1347	valid_1's l2: 20.3638
[760]	training's l2: 19.1031	valid_1's l2: 20.3609
[770]	training's l2: 19.0719	valid_1's l2: 20.3578
[780]	training's l2: 19.0403	valid_1's l2: 20.3544
[790]	training's l2: 19.0093	valid_1's l2: 20.3512
[800]	training's l2: 18.9782	valid_1's l2: 20.3477
[810]	training's l2: 18.9472	valid_1's l2: 20.3452
[820]	training's l2: 18.9163	valid_1's l2: 20.3417
[830]	training's l2: 18.886	valid_1's l2: 20.3398
[840]	training's l2: 18.8555	valid_1's l2: 20.3377
[850]	training's l2: 18.8251	valid_1's l2: 20.3356
[860]	training's l2: 18.7955	valid_1's l2: 20.3329
[870]	training's l2: 18.7657	valid_1's l2: 20.3296
[880]	training's l2: 18.7353	valid_1's l2: 20.3265
[890]	training's l2: 18.7061	valid_1's l2: 20.3246
[900]	training's l2: 18.6766	valid_1's l2: 20.3222
[910]	training's l2: 18.647	valid_1's l2: 20.3183
[920]	training's l2: 18.6171	valid_1's l2: 20.3155
[930]	training's l2: 18.5883	valid_1's l2: 20.3137
[940]	training's l2: 18.5596	valid_1's l2: 20.3115
[950]	training's l2: 18.5313	valid_1's l2: 20.3094
[960]	training's l2: 18.5029	valid_1's l2: 20.3077
[970]	training's l2: 18.474	valid_1's l2: 20.3055
[980]	training's l2: 18.4454	valid_1's l2: 20.3039
[990]	training's l2: 18.4169	valid_1's l2: 20.3018
[1000]	training's l2: 18.3891	valid_1's l2: 20.2992
[1010]	training's l2: 18.3608	valid_1's l2: 20.2964
[1020]	training's l2: 18.3327	valid_1's l2: 20.2941
[1030]	training's l2: 18.3051	valid_1's l2: 20.2919
[1040]	training's l2: 18.2773	valid_1's l2: 20.2901
[1050]	training's l2: 18.25	valid_1's l2: 20.2885
[1060]	training's l2: 18.2225	valid_1's l2: 20.2863
[1070]	training's l2: 18.1958	valid_1's l2: 20.2847
[1080]	training's l2: 18.1688	valid_1's l2: 20.2829
[1090]	training's l2: 18.1425	valid_1's l2: 20.2814
[1100]	training's l2: 18.1164	valid_1's l2: 20.2804
[1110]	training's l2: 18.0895	valid_1's l2: 20.2785
[1120]	training's l2: 18.0636	valid_1's l2: 20.2778
[1130]	training's l2: 18.0377	valid_1's l2: 20.2764
[1140]	training's l2: 18.0122	valid_1's l2: 20.2759
[1150]	training's l2: 17.9866	valid_1's l2: 20.274
[1160]	training's l2: 17.9606	valid_1's l2: 20.2739
[1170]	training's l2: 17.9345	valid_1's l2: 20.2727
[1180]	training's l2: 17.9084	valid_1's l2: 20.2719
[1190]	training's l2: 17.8825	valid_1's l2: 20.2697
[1200]	training's l2: 17.857	valid_1's l2: 20.268
[1210]	training's l2: 17.8322	valid_1's l2: 20.2675
[1220]	training's l2: 17.807	valid_1's l2: 20.2667
[1230]	training's l2: 17.7826	valid_1's l2: 20.266
[1240]	training's l2: 17.7578	valid_1's l2: 20.2645
[1250]	training's l2: 17.7331	valid_1's l2: 20.2636
[1260]	training's l2: 17.7084	valid_1's l2: 20.2613
[1270]	training's l2: 17.6842	valid_1's l2: 20.2599
[1280]	training's l2: 17.6614	valid_1's l2: 20.259
[1290]	training's l2: 17.6368	valid_1's l2: 20.2592
[1300]	training's l2: 17.6124	valid_1's l2: 20.2587
[1310]	training's l2: 17.5887	valid_1's l2: 20.2585
[1320]	training's l2: 17.5647	valid_1's l2: 20.2581
[1330]	training's l2: 17.5413	valid_1's l2: 20.2573
[1340]	training's l2: 17.5174	valid_1's l2: 20.2566
[1350]	training's l2: 17.4934	valid_1's l2: 20.2554
[1360]	training's l2: 17.4697	valid_1's l2: 20.2555
[1370]	training's l2: 17.4469	valid_1's l2: 20.2547
[1380]	training's l2: 17.4227	valid_1's l2: 20.254
[1390]	training's l2: 17.3997	valid_1's l2: 20.2542
[1400]	training's l2: 17.376	valid_1's l2: 20.254
[1410]	training's l2: 17.3532	valid_1's l2: 20.253
[1420]	training's l2: 17.3313	valid_1's l2: 20.2523
[1430]	training's l2: 17.309	valid_1's l2: 20.2519
[1440]	training's l2: 17.2867	valid_1's l2: 20.2524
[1450]	training's l2: 17.2645	valid_1's l2: 20.2525
Early stopping, best iteration is:
[1426]	training's l2: 17.3174	valid_1's l2: 20.2518
score1: 3.837816189569743
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263137 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.2535	valid_1's l2: 30.3554
[20]	training's l2: 29.1116	valid_1's l2: 28.2285
[30]	training's l2: 27.4832	valid_1's l2: 26.6175
[40]	training's l2: 26.2352	valid_1's l2: 25.3891
[50]	training's l2: 25.2708	valid_1's l2: 24.4478
[60]	training's l2: 24.5185	valid_1's l2: 23.722
[70]	training's l2: 23.9262	valid_1's l2: 23.1569
[80]	training's l2: 23.4554	valid_1's l2: 22.7169
[90]	training's l2: 23.0797	valid_1's l2: 22.3715
[100]	training's l2: 22.7753	valid_1's l2: 22.0997
[110]	training's l2: 22.52	valid_1's l2: 21.8775
[120]	training's l2: 22.3043	valid_1's l2: 21.6961
[130]	training's l2: 22.1199	valid_1's l2: 21.5463
[140]	training's l2: 21.9591	valid_1's l2: 21.423
[150]	training's l2: 21.8181	valid_1's l2: 21.32
[160]	training's l2: 21.6904	valid_1's l2: 21.2304
[170]	training's l2: 21.5742	valid_1's l2: 21.1526
[180]	training's l2: 21.4705	valid_1's l2: 21.0847
[190]	training's l2: 21.3739	valid_1's l2: 21.0244
[200]	training's l2: 21.2855	valid_1's l2: 20.9722
[210]	training's l2: 21.2028	valid_1's l2: 20.9252
[220]	training's l2: 21.1248	valid_1's l2: 20.882
[230]	training's l2: 21.0478	valid_1's l2: 20.8405
[240]	training's l2: 20.9761	valid_1's l2: 20.8064
[250]	training's l2: 20.9077	valid_1's l2: 20.7739
[260]	training's l2: 20.8417	valid_1's l2: 20.7449
[270]	training's l2: 20.7804	valid_1's l2: 20.719
[280]	training's l2: 20.7212	valid_1's l2: 20.6966
[290]	training's l2: 20.6637	valid_1's l2: 20.675
[300]	training's l2: 20.6086	valid_1's l2: 20.6558
[310]	training's l2: 20.5541	valid_1's l2: 20.6369
[320]	training's l2: 20.5017	valid_1's l2: 20.6203
[330]	training's l2: 20.4493	valid_1's l2: 20.603
[340]	training's l2: 20.3983	valid_1's l2: 20.5873
[350]	training's l2: 20.3491	valid_1's l2: 20.5724
[360]	training's l2: 20.2997	valid_1's l2: 20.5592
[370]	training's l2: 20.2525	valid_1's l2: 20.5481
[380]	training's l2: 20.2045	valid_1's l2: 20.5368
[390]	training's l2: 20.1581	valid_1's l2: 20.5273
[400]	training's l2: 20.1128	valid_1's l2: 20.5171
[410]	training's l2: 20.0674	valid_1's l2: 20.5061
[420]	training's l2: 20.0236	valid_1's l2: 20.4971
[430]	training's l2: 19.9797	valid_1's l2: 20.489
[440]	training's l2: 19.9359	valid_1's l2: 20.4808
[450]	training's l2: 19.8929	valid_1's l2: 20.4723
[460]	training's l2: 19.8499	valid_1's l2: 20.4632
[470]	training's l2: 19.8069	valid_1's l2: 20.4566
[480]	training's l2: 19.7644	valid_1's l2: 20.4481
[490]	training's l2: 19.7224	valid_1's l2: 20.4403
[500]	training's l2: 19.6807	valid_1's l2: 20.4336
[510]	training's l2: 19.6401	valid_1's l2: 20.4269
[520]	training's l2: 19.5994	valid_1's l2: 20.4198
[530]	training's l2: 19.5591	valid_1's l2: 20.4132
[540]	training's l2: 19.5189	valid_1's l2: 20.4071
[550]	training's l2: 19.4789	valid_1's l2: 20.4007
[560]	training's l2: 19.4403	valid_1's l2: 20.396
[570]	training's l2: 19.4012	valid_1's l2: 20.3901
[580]	training's l2: 19.3627	valid_1's l2: 20.385
[590]	training's l2: 19.3244	valid_1's l2: 20.3788
[600]	training's l2: 19.2857	valid_1's l2: 20.3742
[610]	training's l2: 19.2482	valid_1's l2: 20.37
[620]	training's l2: 19.2107	valid_1's l2: 20.3657
[630]	training's l2: 19.1732	valid_1's l2: 20.3621
[640]	training's l2: 19.1366	valid_1's l2: 20.3578
[650]	training's l2: 19.0997	valid_1's l2: 20.3527
[660]	training's l2: 19.0629	valid_1's l2: 20.3483
[670]	training's l2: 19.027	valid_1's l2: 20.346
[680]	training's l2: 18.9908	valid_1's l2: 20.3433
[690]	training's l2: 18.9557	valid_1's l2: 20.3399
[700]	training's l2: 18.9204	valid_1's l2: 20.3369
[710]	training's l2: 18.8856	valid_1's l2: 20.3331
[720]	training's l2: 18.8503	valid_1's l2: 20.3299
[730]	training's l2: 18.8156	valid_1's l2: 20.3263
[740]	training's l2: 18.7808	valid_1's l2: 20.3228
[750]	training's l2: 18.7466	valid_1's l2: 20.3201
[760]	training's l2: 18.7132	valid_1's l2: 20.3174
[770]	training's l2: 18.6794	valid_1's l2: 20.3157
[780]	training's l2: 18.6459	valid_1's l2: 20.3134
[790]	training's l2: 18.6132	valid_1's l2: 20.3104
[800]	training's l2: 18.58	valid_1's l2: 20.3088
[810]	training's l2: 18.5463	valid_1's l2: 20.3074
[820]	training's l2: 18.513	valid_1's l2: 20.3055
[830]	training's l2: 18.4799	valid_1's l2: 20.3037
[840]	training's l2: 18.4472	valid_1's l2: 20.3028
[850]	training's l2: 18.4147	valid_1's l2: 20.3009
[860]	training's l2: 18.3831	valid_1's l2: 20.2991
[870]	training's l2: 18.3512	valid_1's l2: 20.2974
[880]	training's l2: 18.3202	valid_1's l2: 20.2957
[890]	training's l2: 18.2882	valid_1's l2: 20.2937
[900]	training's l2: 18.2578	valid_1's l2: 20.2928
[910]	training's l2: 18.227	valid_1's l2: 20.2913
[920]	training's l2: 18.1969	valid_1's l2: 20.2898
[930]	training's l2: 18.1666	valid_1's l2: 20.2883
[940]	training's l2: 18.1369	valid_1's l2: 20.2869
[950]	training's l2: 18.1073	valid_1's l2: 20.2858
[960]	training's l2: 18.0782	valid_1's l2: 20.2851
[970]	training's l2: 18.0489	valid_1's l2: 20.2834
[980]	training's l2: 18.0209	valid_1's l2: 20.2839
[990]	training's l2: 17.9924	valid_1's l2: 20.2826
[1000]	training's l2: 17.9633	valid_1's l2: 20.2806
[1010]	training's l2: 17.9348	valid_1's l2: 20.2796
[1020]	training's l2: 17.9072	valid_1's l2: 20.279
[1030]	training's l2: 17.8783	valid_1's l2: 20.278
[1040]	training's l2: 17.8505	valid_1's l2: 20.2773
[1050]	training's l2: 17.8236	valid_1's l2: 20.2764
[1060]	training's l2: 17.7961	valid_1's l2: 20.2758
[1070]	training's l2: 17.7684	valid_1's l2: 20.2755
[1080]	training's l2: 17.7405	valid_1's l2: 20.2747
[1090]	training's l2: 17.7126	valid_1's l2: 20.2731
[1100]	training's l2: 17.6857	valid_1's l2: 20.2726
[1110]	training's l2: 17.6572	valid_1's l2: 20.2711
[1120]	training's l2: 17.6314	valid_1's l2: 20.2707
[1130]	training's l2: 17.6043	valid_1's l2: 20.2696
[1140]	training's l2: 17.5759	valid_1's l2: 20.2697
[1150]	training's l2: 17.5478	valid_1's l2: 20.2687
[1160]	training's l2: 17.5205	valid_1's l2: 20.2685
[1170]	training's l2: 17.4955	valid_1's l2: 20.2687
[1180]	training's l2: 17.4691	valid_1's l2: 20.2683
[1190]	training's l2: 17.4413	valid_1's l2: 20.269
[1200]	training's l2: 17.4147	valid_1's l2: 20.2687
[1210]	training's l2: 17.3886	valid_1's l2: 20.2675
[1220]	training's l2: 17.3629	valid_1's l2: 20.2664
[1230]	training's l2: 17.3364	valid_1's l2: 20.2658
[1240]	training's l2: 17.3113	valid_1's l2: 20.2654
[1250]	training's l2: 17.2849	valid_1's l2: 20.2653
[1260]	training's l2: 17.2597	valid_1's l2: 20.2654
Early stopping, best iteration is:
[1236]	training's l2: 17.3214	valid_1's l2: 20.265
score1: 3.840491279255303
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243404 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.3824	valid_1's l2: 29.4596
[20]	training's l2: 27.8981	valid_1's l2: 26.9662
[30]	training's l2: 26.2027	valid_1's l2: 25.2691
[40]	training's l2: 25.0327	valid_1's l2: 24.1041
[50]	training's l2: 24.2117	valid_1's l2: 23.2899
[60]	training's l2: 23.6264	valid_1's l2: 22.7148
[70]	training's l2: 23.1941	valid_1's l2: 22.2974
[80]	training's l2: 22.863	valid_1's l2: 21.9831
[90]	training's l2: 22.6113	valid_1's l2: 21.752
[100]	training's l2: 22.4104	valid_1's l2: 21.5693
[110]	training's l2: 22.2421	valid_1's l2: 21.4218
[120]	training's l2: 22.1031	valid_1's l2: 21.3048
[130]	training's l2: 21.9834	valid_1's l2: 21.2054
[140]	training's l2: 21.8802	valid_1's l2: 21.1235
[150]	training's l2: 21.7897	valid_1's l2: 21.0551
[160]	training's l2: 21.7083	valid_1's l2: 20.9954
[170]	training's l2: 21.634	valid_1's l2: 20.9422
[180]	training's l2: 21.565	valid_1's l2: 20.8944
[190]	training's l2: 21.5026	valid_1's l2: 20.8542
[200]	training's l2: 21.4462	valid_1's l2: 20.8204
[210]	training's l2: 21.3912	valid_1's l2: 20.7878
[220]	training's l2: 21.3387	valid_1's l2: 20.7584
[230]	training's l2: 21.2893	valid_1's l2: 20.7308
[240]	training's l2: 21.242	valid_1's l2: 20.7064
[250]	training's l2: 21.1967	valid_1's l2: 20.6834
[260]	training's l2: 21.1542	valid_1's l2: 20.6642
[270]	training's l2: 21.1128	valid_1's l2: 20.6473
[280]	training's l2: 21.0724	valid_1's l2: 20.6291
[290]	training's l2: 21.0352	valid_1's l2: 20.6147
[300]	training's l2: 20.997	valid_1's l2: 20.6005
[310]	training's l2: 20.9582	valid_1's l2: 20.585
[320]	training's l2: 20.9232	valid_1's l2: 20.573
[330]	training's l2: 20.888	valid_1's l2: 20.5592
[340]	training's l2: 20.8546	valid_1's l2: 20.5482
[350]	training's l2: 20.8196	valid_1's l2: 20.5347
[360]	training's l2: 20.7848	valid_1's l2: 20.5216
[370]	training's l2: 20.7524	valid_1's l2: 20.511
[380]	training's l2: 20.7189	valid_1's l2: 20.4999
[390]	training's l2: 20.6871	valid_1's l2: 20.4894
[400]	training's l2: 20.6561	valid_1's l2: 20.4796
[410]	training's l2: 20.6235	valid_1's l2: 20.4697
[420]	training's l2: 20.5934	valid_1's l2: 20.4612
[430]	training's l2: 20.5629	valid_1's l2: 20.453
[440]	training's l2: 20.5332	valid_1's l2: 20.4453
[450]	training's l2: 20.5037	valid_1's l2: 20.4386
[460]	training's l2: 20.4747	valid_1's l2: 20.4321
[470]	training's l2: 20.4457	valid_1's l2: 20.425
[480]	training's l2: 20.4171	valid_1's l2: 20.419
[490]	training's l2: 20.3895	valid_1's l2: 20.4131
[500]	training's l2: 20.3618	valid_1's l2: 20.4078
[510]	training's l2: 20.3344	valid_1's l2: 20.4025
[520]	training's l2: 20.306	valid_1's l2: 20.3961
[530]	training's l2: 20.2792	valid_1's l2: 20.3916
[540]	training's l2: 20.2516	valid_1's l2: 20.3858
[550]	training's l2: 20.2245	valid_1's l2: 20.3812
[560]	training's l2: 20.1982	valid_1's l2: 20.3762
[570]	training's l2: 20.1717	valid_1's l2: 20.3706
[580]	training's l2: 20.1458	valid_1's l2: 20.3661
[590]	training's l2: 20.12	valid_1's l2: 20.3623
[600]	training's l2: 20.094	valid_1's l2: 20.3593
[610]	training's l2: 20.0689	valid_1's l2: 20.3564
[620]	training's l2: 20.0436	valid_1's l2: 20.3537
[630]	training's l2: 20.0186	valid_1's l2: 20.3503
[640]	training's l2: 19.9944	valid_1's l2: 20.3472
[650]	training's l2: 19.9701	valid_1's l2: 20.3434
[660]	training's l2: 19.9453	valid_1's l2: 20.3389
[670]	training's l2: 19.922	valid_1's l2: 20.3379
[680]	training's l2: 19.8989	valid_1's l2: 20.336
[690]	training's l2: 19.8754	valid_1's l2: 20.3334
[700]	training's l2: 19.8535	valid_1's l2: 20.3317
[710]	training's l2: 19.83	valid_1's l2: 20.3286
[720]	training's l2: 19.8066	valid_1's l2: 20.3271
[730]	training's l2: 19.7833	valid_1's l2: 20.3242
[740]	training's l2: 19.7613	valid_1's l2: 20.3227
[750]	training's l2: 19.739	valid_1's l2: 20.3195
[760]	training's l2: 19.718	valid_1's l2: 20.3194
[770]	training's l2: 19.6957	valid_1's l2: 20.3171
[780]	training's l2: 19.6742	valid_1's l2: 20.314
[790]	training's l2: 19.6529	valid_1's l2: 20.3128
[800]	training's l2: 19.6304	valid_1's l2: 20.3114
[810]	training's l2: 19.6101	valid_1's l2: 20.3095
[820]	training's l2: 19.589	valid_1's l2: 20.3078
[830]	training's l2: 19.5675	valid_1's l2: 20.3055
[840]	training's l2: 19.5483	valid_1's l2: 20.305
[850]	training's l2: 19.5281	valid_1's l2: 20.3033
[860]	training's l2: 19.5067	valid_1's l2: 20.3011
[870]	training's l2: 19.4853	valid_1's l2: 20.2986
[880]	training's l2: 19.4655	valid_1's l2: 20.2991
[890]	training's l2: 19.4447	valid_1's l2: 20.2977
[900]	training's l2: 19.4257	valid_1's l2: 20.2976
[910]	training's l2: 19.4059	valid_1's l2: 20.297
[920]	training's l2: 19.386	valid_1's l2: 20.2958
[930]	training's l2: 19.3676	valid_1's l2: 20.294
[940]	training's l2: 19.3475	valid_1's l2: 20.2939
[950]	training's l2: 19.3283	valid_1's l2: 20.2925
[960]	training's l2: 19.3095	valid_1's l2: 20.2909
[970]	training's l2: 19.2893	valid_1's l2: 20.2894
[980]	training's l2: 19.2698	valid_1's l2: 20.2875
[990]	training's l2: 19.2512	valid_1's l2: 20.2867
[1000]	training's l2: 19.2314	valid_1's l2: 20.2856
[1010]	training's l2: 19.2119	valid_1's l2: 20.2851
[1020]	training's l2: 19.1927	valid_1's l2: 20.2842
[1030]	training's l2: 19.1746	valid_1's l2: 20.2841
[1040]	training's l2: 19.1555	valid_1's l2: 20.2832
[1050]	training's l2: 19.1362	valid_1's l2: 20.2817
[1060]	training's l2: 19.1171	valid_1's l2: 20.2804
[1070]	training's l2: 19.0993	valid_1's l2: 20.2792
[1080]	training's l2: 19.0796	valid_1's l2: 20.278
[1090]	training's l2: 19.0621	valid_1's l2: 20.2773
[1100]	training's l2: 19.0457	valid_1's l2: 20.2765
[1110]	training's l2: 19.0271	valid_1's l2: 20.2758
[1120]	training's l2: 19.008	valid_1's l2: 20.2748
[1130]	training's l2: 18.9898	valid_1's l2: 20.2741
[1140]	training's l2: 18.9719	valid_1's l2: 20.2739
[1150]	training's l2: 18.954	valid_1's l2: 20.2727
[1160]	training's l2: 18.9369	valid_1's l2: 20.2728
[1170]	training's l2: 18.9193	valid_1's l2: 20.2721
[1180]	training's l2: 18.9004	valid_1's l2: 20.2706
[1190]	training's l2: 18.8825	valid_1's l2: 20.2707
[1200]	training's l2: 18.8652	valid_1's l2: 20.2711
[1210]	training's l2: 18.8482	valid_1's l2: 20.2702
[1220]	training's l2: 18.8305	valid_1's l2: 20.2705
[1230]	training's l2: 18.8128	valid_1's l2: 20.2695
[1240]	training's l2: 18.7958	valid_1's l2: 20.2683
[1250]	training's l2: 18.7783	valid_1's l2: 20.2668
[1260]	training's l2: 18.7595	valid_1's l2: 20.2659
[1270]	training's l2: 18.7413	valid_1's l2: 20.2654
[1280]	training's l2: 18.7249	valid_1's l2: 20.2649
[1290]	training's l2: 18.7078	valid_1's l2: 20.265
[1300]	training's l2: 18.6912	valid_1's l2: 20.2641
[1310]	training's l2: 18.6738	valid_1's l2: 20.2633
[1320]	training's l2: 18.6569	valid_1's l2: 20.2637
[1330]	training's l2: 18.6391	valid_1's l2: 20.2639
[1340]	training's l2: 18.621	valid_1's l2: 20.2635
Early stopping, best iteration is:
[1317]	training's l2: 18.6619	valid_1's l2: 20.2629
score1: 3.840526093197041
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263323 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.899	valid_1's l2: 29.9957
[20]	training's l2: 28.5902	valid_1's l2: 27.6977
[30]	training's l2: 26.9074	valid_1's l2: 26.0288
[40]	training's l2: 25.6653	valid_1's l2: 24.8019
[50]	training's l2: 24.7384	valid_1's l2: 23.897
[60]	training's l2: 24.0403	valid_1's l2: 23.2231
[70]	training's l2: 23.5064	valid_1's l2: 22.7171
[80]	training's l2: 23.0964	valid_1's l2: 22.335
[90]	training's l2: 22.7717	valid_1's l2: 22.0405
[100]	training's l2: 22.5054	valid_1's l2: 21.8064
[110]	training's l2: 22.2859	valid_1's l2: 21.6199
[120]	training's l2: 22.1018	valid_1's l2: 21.4705
[130]	training's l2: 21.9439	valid_1's l2: 21.3482
[140]	training's l2: 21.8049	valid_1's l2: 21.2454
[150]	training's l2: 21.6801	valid_1's l2: 21.1556
[160]	training's l2: 21.5691	valid_1's l2: 21.0808
[170]	training's l2: 21.4702	valid_1's l2: 21.0161
[180]	training's l2: 21.379	valid_1's l2: 20.9591
[190]	training's l2: 21.295	valid_1's l2: 20.9102
[200]	training's l2: 21.2137	valid_1's l2: 20.8637
[210]	training's l2: 21.1389	valid_1's l2: 20.8241
[220]	training's l2: 21.0675	valid_1's l2: 20.7873
[230]	training's l2: 21.0006	valid_1's l2: 20.7552
[240]	training's l2: 20.9375	valid_1's l2: 20.7265
[250]	training's l2: 20.8765	valid_1's l2: 20.7012
[260]	training's l2: 20.8181	valid_1's l2: 20.6784
[270]	training's l2: 20.7605	valid_1's l2: 20.6575
[280]	training's l2: 20.706	valid_1's l2: 20.6382
[290]	training's l2: 20.6514	valid_1's l2: 20.6182
[300]	training's l2: 20.5989	valid_1's l2: 20.6011
[310]	training's l2: 20.5481	valid_1's l2: 20.5845
[320]	training's l2: 20.4987	valid_1's l2: 20.571
[330]	training's l2: 20.451	valid_1's l2: 20.5573
[340]	training's l2: 20.4026	valid_1's l2: 20.5434
[350]	training's l2: 20.3569	valid_1's l2: 20.5318
[360]	training's l2: 20.3112	valid_1's l2: 20.5203
[370]	training's l2: 20.2657	valid_1's l2: 20.5085
[380]	training's l2: 20.221	valid_1's l2: 20.4985
[390]	training's l2: 20.1773	valid_1's l2: 20.4892
[400]	training's l2: 20.1333	valid_1's l2: 20.4807
[410]	training's l2: 20.0891	valid_1's l2: 20.4707
[420]	training's l2: 20.0445	valid_1's l2: 20.4588
[430]	training's l2: 20.002	valid_1's l2: 20.4512
[440]	training's l2: 19.9595	valid_1's l2: 20.4424
[450]	training's l2: 19.9174	valid_1's l2: 20.435
[460]	training's l2: 19.876	valid_1's l2: 20.4283
[470]	training's l2: 19.8355	valid_1's l2: 20.4197
[480]	training's l2: 19.7945	valid_1's l2: 20.4135
[490]	training's l2: 19.7543	valid_1's l2: 20.4072
[500]	training's l2: 19.7147	valid_1's l2: 20.4007
[510]	training's l2: 19.6765	valid_1's l2: 20.3944
[520]	training's l2: 19.6378	valid_1's l2: 20.3882
[530]	training's l2: 19.5995	valid_1's l2: 20.3825
[540]	training's l2: 19.5618	valid_1's l2: 20.377
[550]	training's l2: 19.5241	valid_1's l2: 20.3718
[560]	training's l2: 19.4857	valid_1's l2: 20.367
[570]	training's l2: 19.4485	valid_1's l2: 20.3625
[580]	training's l2: 19.4121	valid_1's l2: 20.3581
[590]	training's l2: 19.3764	valid_1's l2: 20.3543
[600]	training's l2: 19.3402	valid_1's l2: 20.3496
[610]	training's l2: 19.3042	valid_1's l2: 20.3455
[620]	training's l2: 19.2686	valid_1's l2: 20.3414
[630]	training's l2: 19.2328	valid_1's l2: 20.3372
[640]	training's l2: 19.198	valid_1's l2: 20.3338
[650]	training's l2: 19.1637	valid_1's l2: 20.3303
[660]	training's l2: 19.1296	valid_1's l2: 20.3276
[670]	training's l2: 19.0945	valid_1's l2: 20.3252
[680]	training's l2: 19.0598	valid_1's l2: 20.3212
[690]	training's l2: 19.0257	valid_1's l2: 20.3178
[700]	training's l2: 18.9923	valid_1's l2: 20.3156
[710]	training's l2: 18.9579	valid_1's l2: 20.3114
[720]	training's l2: 18.9245	valid_1's l2: 20.3077
[730]	training's l2: 18.8918	valid_1's l2: 20.3047
[740]	training's l2: 18.8597	valid_1's l2: 20.3025
[750]	training's l2: 18.8281	valid_1's l2: 20.3006
[760]	training's l2: 18.7949	valid_1's l2: 20.2973
[770]	training's l2: 18.7629	valid_1's l2: 20.2962
[780]	training's l2: 18.7322	valid_1's l2: 20.2922
[790]	training's l2: 18.6997	valid_1's l2: 20.2893
[800]	training's l2: 18.6676	valid_1's l2: 20.2874
[810]	training's l2: 18.6359	valid_1's l2: 20.2852
[820]	training's l2: 18.6058	valid_1's l2: 20.2848
[830]	training's l2: 18.5756	valid_1's l2: 20.2841
[840]	training's l2: 18.5454	valid_1's l2: 20.2822
[850]	training's l2: 18.5152	valid_1's l2: 20.2797
[860]	training's l2: 18.4848	valid_1's l2: 20.2776
[870]	training's l2: 18.4543	valid_1's l2: 20.2751
[880]	training's l2: 18.4254	valid_1's l2: 20.2741
[890]	training's l2: 18.3968	valid_1's l2: 20.2732
[900]	training's l2: 18.3675	valid_1's l2: 20.2714
[910]	training's l2: 18.3386	valid_1's l2: 20.2713
[920]	training's l2: 18.3105	valid_1's l2: 20.2699
[930]	training's l2: 18.2822	valid_1's l2: 20.269
[940]	training's l2: 18.2535	valid_1's l2: 20.2686
[950]	training's l2: 18.2266	valid_1's l2: 20.2679
[960]	training's l2: 18.1989	valid_1's l2: 20.2676
[970]	training's l2: 18.1703	valid_1's l2: 20.2672
[980]	training's l2: 18.1421	valid_1's l2: 20.2667
[990]	training's l2: 18.1138	valid_1's l2: 20.2662
[1000]	training's l2: 18.0871	valid_1's l2: 20.2656
[1010]	training's l2: 18.0581	valid_1's l2: 20.2641
[1020]	training's l2: 18.0315	valid_1's l2: 20.2633
[1030]	training's l2: 18.0053	valid_1's l2: 20.262
[1040]	training's l2: 17.9796	valid_1's l2: 20.262
[1050]	training's l2: 17.9523	valid_1's l2: 20.2634
[1060]	training's l2: 17.9249	valid_1's l2: 20.2625
[1070]	training's l2: 17.8993	valid_1's l2: 20.2622
Early stopping, best iteration is:
[1041]	training's l2: 17.9766	valid_1's l2: 20.2618
score1: 3.83956424196879
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.249586 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.8734	valid_1's l2: 30.9705
[20]	training's l2: 30.0868	valid_1's l2: 29.1935
[30]	training's l2: 28.6376	valid_1's l2: 27.755
[40]	training's l2: 27.4581	valid_1's l2: 26.5866
[50]	training's l2: 26.4933	valid_1's l2: 25.6337
[60]	training's l2: 25.7017	valid_1's l2: 24.8555
[70]	training's l2: 25.0471	valid_1's l2: 24.2195
[80]	training's l2: 24.5046	valid_1's l2: 23.6951
[90]	training's l2: 24.0537	valid_1's l2: 23.2624
[100]	training's l2: 23.6734	valid_1's l2: 22.9032
[110]	training's l2: 23.353	valid_1's l2: 22.6055
[120]	training's l2: 23.0839	valid_1's l2: 22.3569
[130]	training's l2: 22.8534	valid_1's l2: 22.1481
[140]	training's l2: 22.6526	valid_1's l2: 21.9711
[150]	training's l2: 22.4757	valid_1's l2: 21.8183
[160]	training's l2: 22.3204	valid_1's l2: 21.6882
[170]	training's l2: 22.1827	valid_1's l2: 21.5748
[180]	training's l2: 22.0583	valid_1's l2: 21.4766
[190]	training's l2: 21.9445	valid_1's l2: 21.3901
[200]	training's l2: 21.8445	valid_1's l2: 21.3168
[210]	training's l2: 21.7492	valid_1's l2: 21.2487
[220]	training's l2: 21.6603	valid_1's l2: 21.1863
[230]	training's l2: 21.5783	valid_1's l2: 21.1296
[240]	training's l2: 21.504	valid_1's l2: 21.081
[250]	training's l2: 21.4316	valid_1's l2: 21.0346
[260]	training's l2: 21.3653	valid_1's l2: 20.9942
[270]	training's l2: 21.3022	valid_1's l2: 20.9569
[280]	training's l2: 21.2426	valid_1's l2: 20.9229
[290]	training's l2: 21.1851	valid_1's l2: 20.8908
[300]	training's l2: 21.128	valid_1's l2: 20.8589
[310]	training's l2: 21.074	valid_1's l2: 20.8301
[320]	training's l2: 21.0227	valid_1's l2: 20.8054
[330]	training's l2: 20.972	valid_1's l2: 20.7821
[340]	training's l2: 20.924	valid_1's l2: 20.76
[350]	training's l2: 20.8774	valid_1's l2: 20.7392
[360]	training's l2: 20.8332	valid_1's l2: 20.7211
[370]	training's l2: 20.7901	valid_1's l2: 20.703
[380]	training's l2: 20.7477	valid_1's l2: 20.6867
[390]	training's l2: 20.7062	valid_1's l2: 20.6717
[400]	training's l2: 20.6668	valid_1's l2: 20.6571
[410]	training's l2: 20.6269	valid_1's l2: 20.6424
[420]	training's l2: 20.5874	valid_1's l2: 20.629
[430]	training's l2: 20.5491	valid_1's l2: 20.6161
[440]	training's l2: 20.512	valid_1's l2: 20.6041
[450]	training's l2: 20.4755	valid_1's l2: 20.5931
[460]	training's l2: 20.4388	valid_1's l2: 20.581
[470]	training's l2: 20.4028	valid_1's l2: 20.5693
[480]	training's l2: 20.3673	valid_1's l2: 20.5593
[490]	training's l2: 20.333	valid_1's l2: 20.5511
[500]	training's l2: 20.2986	valid_1's l2: 20.5423
[510]	training's l2: 20.2652	valid_1's l2: 20.5347
[520]	training's l2: 20.2314	valid_1's l2: 20.5265
[530]	training's l2: 20.1978	valid_1's l2: 20.5187
[540]	training's l2: 20.1645	valid_1's l2: 20.5096
[550]	training's l2: 20.1324	valid_1's l2: 20.503
[560]	training's l2: 20.0997	valid_1's l2: 20.495
[570]	training's l2: 20.0672	valid_1's l2: 20.4874
[580]	training's l2: 20.0357	valid_1's l2: 20.4807
[590]	training's l2: 20.0043	valid_1's l2: 20.4744
[600]	training's l2: 19.9729	valid_1's l2: 20.4684
[610]	training's l2: 19.942	valid_1's l2: 20.462
[620]	training's l2: 19.9107	valid_1's l2: 20.4561
[630]	training's l2: 19.88	valid_1's l2: 20.4498
[640]	training's l2: 19.8492	valid_1's l2: 20.4443
[650]	training's l2: 19.8186	valid_1's l2: 20.4382
[660]	training's l2: 19.7883	valid_1's l2: 20.4322
[670]	training's l2: 19.7587	valid_1's l2: 20.427
[680]	training's l2: 19.729	valid_1's l2: 20.4214
[690]	training's l2: 19.6994	valid_1's l2: 20.416
[700]	training's l2: 19.6701	valid_1's l2: 20.4111
[710]	training's l2: 19.6417	valid_1's l2: 20.4067
[720]	training's l2: 19.6132	valid_1's l2: 20.402
[730]	training's l2: 19.5848	valid_1's l2: 20.3967
[740]	training's l2: 19.5561	valid_1's l2: 20.3928
[750]	training's l2: 19.5281	valid_1's l2: 20.3892
[760]	training's l2: 19.4991	valid_1's l2: 20.3845
[770]	training's l2: 19.4716	valid_1's l2: 20.3819
[780]	training's l2: 19.4439	valid_1's l2: 20.3771
[790]	training's l2: 19.416	valid_1's l2: 20.3729
[800]	training's l2: 19.3884	valid_1's l2: 20.3684
[810]	training's l2: 19.3612	valid_1's l2: 20.3652
[820]	training's l2: 19.3344	valid_1's l2: 20.3622
[830]	training's l2: 19.3076	valid_1's l2: 20.3595
[840]	training's l2: 19.2812	valid_1's l2: 20.3565
[850]	training's l2: 19.2544	valid_1's l2: 20.3537
[860]	training's l2: 19.2276	valid_1's l2: 20.3504
[870]	training's l2: 19.2013	valid_1's l2: 20.3475
[880]	training's l2: 19.1746	valid_1's l2: 20.3455
[890]	training's l2: 19.1486	valid_1's l2: 20.3435
[900]	training's l2: 19.1225	valid_1's l2: 20.3398
[910]	training's l2: 19.0969	valid_1's l2: 20.3367
[920]	training's l2: 19.0712	valid_1's l2: 20.334
[930]	training's l2: 19.0456	valid_1's l2: 20.3314
[940]	training's l2: 19.0202	valid_1's l2: 20.3287
[950]	training's l2: 18.995	valid_1's l2: 20.3266
[960]	training's l2: 18.9701	valid_1's l2: 20.3239
[970]	training's l2: 18.9448	valid_1's l2: 20.3211
[980]	training's l2: 18.9196	valid_1's l2: 20.3186
[990]	training's l2: 18.894	valid_1's l2: 20.3164
[1000]	training's l2: 18.8699	valid_1's l2: 20.3151
[1010]	training's l2: 18.8457	valid_1's l2: 20.3122
[1020]	training's l2: 18.8208	valid_1's l2: 20.3097
[1030]	training's l2: 18.7959	valid_1's l2: 20.3076
[1040]	training's l2: 18.771	valid_1's l2: 20.3051
[1050]	training's l2: 18.7465	valid_1's l2: 20.303
[1060]	training's l2: 18.7228	valid_1's l2: 20.3006
[1070]	training's l2: 18.699	valid_1's l2: 20.2988
[1080]	training's l2: 18.6748	valid_1's l2: 20.297
[1090]	training's l2: 18.651	valid_1's l2: 20.2956
[1100]	training's l2: 18.6279	valid_1's l2: 20.2942
[1110]	training's l2: 18.6042	valid_1's l2: 20.2918
[1120]	training's l2: 18.5812	valid_1's l2: 20.2908
[1130]	training's l2: 18.5577	valid_1's l2: 20.2891
[1140]	training's l2: 18.5346	valid_1's l2: 20.2869
[1150]	training's l2: 18.5115	valid_1's l2: 20.2855
[1160]	training's l2: 18.4884	valid_1's l2: 20.2828
[1170]	training's l2: 18.4645	valid_1's l2: 20.2811
[1180]	training's l2: 18.4424	valid_1's l2: 20.2799
[1190]	training's l2: 18.4191	valid_1's l2: 20.2776
[1200]	training's l2: 18.3969	valid_1's l2: 20.2765
[1210]	training's l2: 18.3746	valid_1's l2: 20.2758
[1220]	training's l2: 18.3521	valid_1's l2: 20.2735
[1230]	training's l2: 18.3297	valid_1's l2: 20.2726
[1240]	training's l2: 18.3076	valid_1's l2: 20.2714
[1250]	training's l2: 18.2862	valid_1's l2: 20.2699
[1260]	training's l2: 18.2645	valid_1's l2: 20.2691
[1270]	training's l2: 18.2429	valid_1's l2: 20.2682
[1280]	training's l2: 18.2215	valid_1's l2: 20.2674
[1290]	training's l2: 18.2006	valid_1's l2: 20.2666
[1300]	training's l2: 18.1789	valid_1's l2: 20.2651
[1310]	training's l2: 18.1581	valid_1's l2: 20.265
[1320]	training's l2: 18.1367	valid_1's l2: 20.2641
[1330]	training's l2: 18.1162	valid_1's l2: 20.2634
[1340]	training's l2: 18.0954	valid_1's l2: 20.2626
[1350]	training's l2: 18.0746	valid_1's l2: 20.2622
[1360]	training's l2: 18.0536	valid_1's l2: 20.2608
[1370]	training's l2: 18.0331	valid_1's l2: 20.26
[1380]	training's l2: 18.013	valid_1's l2: 20.259
[1390]	training's l2: 17.992	valid_1's l2: 20.2586
[1400]	training's l2: 17.9717	valid_1's l2: 20.2575
[1410]	training's l2: 17.9516	valid_1's l2: 20.2571
[1420]	training's l2: 17.9319	valid_1's l2: 20.2565
[1430]	training's l2: 17.9122	valid_1's l2: 20.2558
[1440]	training's l2: 17.8923	valid_1's l2: 20.2555
[1450]	training's l2: 17.8727	valid_1's l2: 20.2551
[1460]	training's l2: 17.8519	valid_1's l2: 20.2539
[1470]	training's l2: 17.8331	valid_1's l2: 20.2523
[1480]	training's l2: 17.8139	valid_1's l2: 20.2511
[1490]	training's l2: 17.7942	valid_1's l2: 20.2504
[1500]	training's l2: 17.7743	valid_1's l2: 20.25
[1510]	training's l2: 17.7544	valid_1's l2: 20.25
[1520]	training's l2: 17.7346	valid_1's l2: 20.2491
[1530]	training's l2: 17.7158	valid_1's l2: 20.2486
[1540]	training's l2: 17.6964	valid_1's l2: 20.2487
[1550]	training's l2: 17.677	valid_1's l2: 20.248
[1560]	training's l2: 17.657	valid_1's l2: 20.2475
[1570]	training's l2: 17.6384	valid_1's l2: 20.2474
[1580]	training's l2: 17.6196	valid_1's l2: 20.2475
[1590]	training's l2: 17.6004	valid_1's l2: 20.2467
[1600]	training's l2: 17.5811	valid_1's l2: 20.2457
[1610]	training's l2: 17.5625	valid_1's l2: 20.2452
[1620]	training's l2: 17.5433	valid_1's l2: 20.2453
[1630]	training's l2: 17.5255	valid_1's l2: 20.2456
[1640]	training's l2: 17.5063	valid_1's l2: 20.2442
[1650]	training's l2: 17.4878	valid_1's l2: 20.2444
[1660]	training's l2: 17.469	valid_1's l2: 20.2445
[1670]	training's l2: 17.4502	valid_1's l2: 20.2441
[1680]	training's l2: 17.4316	valid_1's l2: 20.244
[1690]	training's l2: 17.4124	valid_1's l2: 20.2439
[1700]	training's l2: 17.3941	valid_1's l2: 20.2437
[1710]	training's l2: 17.3756	valid_1's l2: 20.2438
[1720]	training's l2: 17.3576	valid_1's l2: 20.2438
[1730]	training's l2: 17.3396	valid_1's l2: 20.244
Early stopping, best iteration is:
[1705]	training's l2: 17.385	valid_1's l2: 20.2436
score1: 3.83457049425643
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.248403 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.9161	valid_1's l2: 31.0123
[20]	training's l2: 30.1565	valid_1's l2: 29.2621
[30]	training's l2: 28.7232	valid_1's l2: 27.8396
[40]	training's l2: 27.5519	valid_1's l2: 26.6773
[50]	training's l2: 26.5906	valid_1's l2: 25.7289
[60]	training's l2: 25.7993	valid_1's l2: 24.9506
[70]	training's l2: 25.1423	valid_1's l2: 24.3091
[80]	training's l2: 24.5962	valid_1's l2: 23.7819
[90]	training's l2: 24.1403	valid_1's l2: 23.3438
[100]	training's l2: 23.7562	valid_1's l2: 22.9791
[110]	training's l2: 23.4315	valid_1's l2: 22.6748
[120]	training's l2: 23.1573	valid_1's l2: 22.4204
[130]	training's l2: 22.9238	valid_1's l2: 22.2082
[140]	training's l2: 22.7205	valid_1's l2: 22.0265
[150]	training's l2: 22.5414	valid_1's l2: 21.8698
[160]	training's l2: 22.3829	valid_1's l2: 21.734
[170]	training's l2: 22.244	valid_1's l2: 21.6173
[180]	training's l2: 22.118	valid_1's l2: 21.5167
[190]	training's l2: 22.003	valid_1's l2: 21.4266
[200]	training's l2: 21.9006	valid_1's l2: 21.3494
[210]	training's l2: 21.8062	valid_1's l2: 21.2805
[220]	training's l2: 21.7163	valid_1's l2: 21.2155
[230]	training's l2: 21.6329	valid_1's l2: 21.157
[240]	training's l2: 21.5565	valid_1's l2: 21.1058
[250]	training's l2: 21.4854	valid_1's l2: 21.0598
[260]	training's l2: 21.4173	valid_1's l2: 21.0159
[270]	training's l2: 21.3543	valid_1's l2: 20.9771
[280]	training's l2: 21.296	valid_1's l2: 20.944
[290]	training's l2: 21.2388	valid_1's l2: 20.911
[300]	training's l2: 21.1834	valid_1's l2: 20.8801
[310]	training's l2: 21.1293	valid_1's l2: 20.8506
[320]	training's l2: 21.0775	valid_1's l2: 20.8231
[330]	training's l2: 21.027	valid_1's l2: 20.7984
[340]	training's l2: 20.9789	valid_1's l2: 20.7758
[350]	training's l2: 20.9324	valid_1's l2: 20.7541
[360]	training's l2: 20.888	valid_1's l2: 20.7358
[370]	training's l2: 20.8458	valid_1's l2: 20.7187
[380]	training's l2: 20.8035	valid_1's l2: 20.7018
[390]	training's l2: 20.7625	valid_1's l2: 20.6851
[400]	training's l2: 20.7228	valid_1's l2: 20.6697
[410]	training's l2: 20.6849	valid_1's l2: 20.6564
[420]	training's l2: 20.6467	valid_1's l2: 20.6426
[430]	training's l2: 20.6093	valid_1's l2: 20.6308
[440]	training's l2: 20.5726	valid_1's l2: 20.6175
[450]	training's l2: 20.5367	valid_1's l2: 20.6058
[460]	training's l2: 20.5011	valid_1's l2: 20.5959
[470]	training's l2: 20.466	valid_1's l2: 20.5843
[480]	training's l2: 20.4315	valid_1's l2: 20.5741
[490]	training's l2: 20.3975	valid_1's l2: 20.5639
[500]	training's l2: 20.3643	valid_1's l2: 20.5557
[510]	training's l2: 20.3307	valid_1's l2: 20.5468
[520]	training's l2: 20.298	valid_1's l2: 20.5385
[530]	training's l2: 20.2659	valid_1's l2: 20.5294
[540]	training's l2: 20.2343	valid_1's l2: 20.5221
[550]	training's l2: 20.2029	valid_1's l2: 20.5153
[560]	training's l2: 20.1718	valid_1's l2: 20.5081
[570]	training's l2: 20.1395	valid_1's l2: 20.4999
[580]	training's l2: 20.1084	valid_1's l2: 20.4939
[590]	training's l2: 20.0778	valid_1's l2: 20.4869
[600]	training's l2: 20.0476	valid_1's l2: 20.481
[610]	training's l2: 20.0173	valid_1's l2: 20.4755
[620]	training's l2: 19.9873	valid_1's l2: 20.4685
[630]	training's l2: 19.9567	valid_1's l2: 20.4623
[640]	training's l2: 19.927	valid_1's l2: 20.4567
[650]	training's l2: 19.8972	valid_1's l2: 20.4495
[660]	training's l2: 19.8682	valid_1's l2: 20.444
[670]	training's l2: 19.839	valid_1's l2: 20.4383
[680]	training's l2: 19.8101	valid_1's l2: 20.4336
[690]	training's l2: 19.781	valid_1's l2: 20.4275
[700]	training's l2: 19.7524	valid_1's l2: 20.4225
[710]	training's l2: 19.7243	valid_1's l2: 20.4171
[720]	training's l2: 19.696	valid_1's l2: 20.4121
[730]	training's l2: 19.6686	valid_1's l2: 20.4075
[740]	training's l2: 19.641	valid_1's l2: 20.4033
[750]	training's l2: 19.6136	valid_1's l2: 20.399
[760]	training's l2: 19.5865	valid_1's l2: 20.395
[770]	training's l2: 19.5597	valid_1's l2: 20.3904
[780]	training's l2: 19.533	valid_1's l2: 20.3866
[790]	training's l2: 19.5058	valid_1's l2: 20.3821
[800]	training's l2: 19.4791	valid_1's l2: 20.3782
[810]	training's l2: 19.4525	valid_1's l2: 20.3743
[820]	training's l2: 19.4263	valid_1's l2: 20.3715
[830]	training's l2: 19.3997	valid_1's l2: 20.3681
[840]	training's l2: 19.3737	valid_1's l2: 20.3649
[850]	training's l2: 19.3481	valid_1's l2: 20.3617
[860]	training's l2: 19.3225	valid_1's l2: 20.3586
[870]	training's l2: 19.2969	valid_1's l2: 20.3555
[880]	training's l2: 19.2711	valid_1's l2: 20.353
[890]	training's l2: 19.2457	valid_1's l2: 20.3499
[900]	training's l2: 19.2205	valid_1's l2: 20.347
[910]	training's l2: 19.1952	valid_1's l2: 20.3448
[920]	training's l2: 19.1701	valid_1's l2: 20.3418
[930]	training's l2: 19.1456	valid_1's l2: 20.339
[940]	training's l2: 19.1205	valid_1's l2: 20.3362
[950]	training's l2: 19.0957	valid_1's l2: 20.3335
[960]	training's l2: 19.0711	valid_1's l2: 20.3313
[970]	training's l2: 19.0473	valid_1's l2: 20.3297
[980]	training's l2: 19.0228	valid_1's l2: 20.3268
[990]	training's l2: 18.9981	valid_1's l2: 20.3241
[1000]	training's l2: 18.9741	valid_1's l2: 20.322
[1010]	training's l2: 18.9498	valid_1's l2: 20.3197
[1020]	training's l2: 18.9249	valid_1's l2: 20.317
[1030]	training's l2: 18.9012	valid_1's l2: 20.3142
[1040]	training's l2: 18.877	valid_1's l2: 20.312
[1050]	training's l2: 18.854	valid_1's l2: 20.31
[1060]	training's l2: 18.8299	valid_1's l2: 20.3081
[1070]	training's l2: 18.8064	valid_1's l2: 20.3057
[1080]	training's l2: 18.7835	valid_1's l2: 20.3041
[1090]	training's l2: 18.7602	valid_1's l2: 20.3019
[1100]	training's l2: 18.737	valid_1's l2: 20.2999
[1110]	training's l2: 18.714	valid_1's l2: 20.2973
[1120]	training's l2: 18.6911	valid_1's l2: 20.2959
[1130]	training's l2: 18.6685	valid_1's l2: 20.2938
[1140]	training's l2: 18.6462	valid_1's l2: 20.2922
[1150]	training's l2: 18.6236	valid_1's l2: 20.2908
[1160]	training's l2: 18.6017	valid_1's l2: 20.2889
[1170]	training's l2: 18.5793	valid_1's l2: 20.2876
[1180]	training's l2: 18.557	valid_1's l2: 20.2861
[1190]	training's l2: 18.535	valid_1's l2: 20.2847
[1200]	training's l2: 18.5128	valid_1's l2: 20.2839
[1210]	training's l2: 18.4909	valid_1's l2: 20.2833
[1220]	training's l2: 18.4696	valid_1's l2: 20.2825
[1230]	training's l2: 18.4476	valid_1's l2: 20.2808
[1240]	training's l2: 18.4258	valid_1's l2: 20.28
[1250]	training's l2: 18.4046	valid_1's l2: 20.2788
[1260]	training's l2: 18.3838	valid_1's l2: 20.2783
[1270]	training's l2: 18.3629	valid_1's l2: 20.2772
[1280]	training's l2: 18.3425	valid_1's l2: 20.2757
[1290]	training's l2: 18.321	valid_1's l2: 20.2743
[1300]	training's l2: 18.3011	valid_1's l2: 20.2733
[1310]	training's l2: 18.2807	valid_1's l2: 20.2722
[1320]	training's l2: 18.2614	valid_1's l2: 20.2709
[1330]	training's l2: 18.2413	valid_1's l2: 20.2692
[1340]	training's l2: 18.2208	valid_1's l2: 20.2683
[1350]	training's l2: 18.2003	valid_1's l2: 20.2671
[1360]	training's l2: 18.1807	valid_1's l2: 20.2657
[1370]	training's l2: 18.1609	valid_1's l2: 20.2651
[1380]	training's l2: 18.1407	valid_1's l2: 20.2632
[1390]	training's l2: 18.1214	valid_1's l2: 20.2635
[1400]	training's l2: 18.1011	valid_1's l2: 20.2629
[1410]	training's l2: 18.0809	valid_1's l2: 20.2618
[1420]	training's l2: 18.061	valid_1's l2: 20.2611
[1430]	training's l2: 18.0406	valid_1's l2: 20.2602
[1440]	training's l2: 18.0219	valid_1's l2: 20.2605
[1450]	training's l2: 18.0029	valid_1's l2: 20.2599
[1460]	training's l2: 17.9849	valid_1's l2: 20.2592
[1470]	training's l2: 17.9658	valid_1's l2: 20.258
[1480]	training's l2: 17.947	valid_1's l2: 20.2572
[1490]	training's l2: 17.9279	valid_1's l2: 20.257
[1500]	training's l2: 17.9084	valid_1's l2: 20.2568
[1510]	training's l2: 17.8891	valid_1's l2: 20.2568
[1520]	training's l2: 17.8691	valid_1's l2: 20.2564
[1530]	training's l2: 17.8507	valid_1's l2: 20.2559
[1540]	training's l2: 17.8312	valid_1's l2: 20.2553
[1550]	training's l2: 17.8129	valid_1's l2: 20.2541
[1560]	training's l2: 17.7957	valid_1's l2: 20.2534
[1570]	training's l2: 17.7772	valid_1's l2: 20.253
[1580]	training's l2: 17.7584	valid_1's l2: 20.2518
[1590]	training's l2: 17.7409	valid_1's l2: 20.2516
[1600]	training's l2: 17.7224	valid_1's l2: 20.2512
[1610]	training's l2: 17.7035	valid_1's l2: 20.2509
[1620]	training's l2: 17.6845	valid_1's l2: 20.2511
[1630]	training's l2: 17.6658	valid_1's l2: 20.2507
[1640]	training's l2: 17.648	valid_1's l2: 20.2499
[1650]	training's l2: 17.63	valid_1's l2: 20.2494
[1660]	training's l2: 17.6125	valid_1's l2: 20.2487
[1670]	training's l2: 17.5946	valid_1's l2: 20.248
[1680]	training's l2: 17.5762	valid_1's l2: 20.2476
[1690]	training's l2: 17.5581	valid_1's l2: 20.2467
[1700]	training's l2: 17.5398	valid_1's l2: 20.2473
[1710]	training's l2: 17.5218	valid_1's l2: 20.2481
[1720]	training's l2: 17.5044	valid_1's l2: 20.2479
Early stopping, best iteration is:
[1690]	training's l2: 17.5581	valid_1's l2: 20.2467
score1: 3.838099240856778
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254008 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.4443	valid_1's l2: 30.5418
[20]	training's l2: 29.4065	valid_1's l2: 28.5156
[30]	training's l2: 27.8264	valid_1's l2: 26.9489
[40]	training's l2: 26.5923	valid_1's l2: 25.7296
[50]	training's l2: 25.6241	valid_1's l2: 24.7799
[60]	training's l2: 24.8565	valid_1's l2: 24.0343
[70]	training's l2: 24.2457	valid_1's l2: 23.4451
[80]	training's l2: 23.7532	valid_1's l2: 22.9779
[90]	training's l2: 23.3543	valid_1's l2: 22.6058
[100]	training's l2: 23.0308	valid_1's l2: 22.3076
[110]	training's l2: 22.7623	valid_1's l2: 22.0665
[120]	training's l2: 22.533	valid_1's l2: 21.8665
[130]	training's l2: 22.3367	valid_1's l2: 21.6994
[140]	training's l2: 22.1671	valid_1's l2: 21.5602
[150]	training's l2: 22.018	valid_1's l2: 21.4428
[160]	training's l2: 21.888	valid_1's l2: 21.345
[170]	training's l2: 21.7706	valid_1's l2: 21.2606
[180]	training's l2: 21.66	valid_1's l2: 21.1834
[190]	training's l2: 21.5612	valid_1's l2: 21.1164
[200]	training's l2: 21.4718	valid_1's l2: 21.0572
[210]	training's l2: 21.3874	valid_1's l2: 21.0029
[220]	training's l2: 21.3096	valid_1's l2: 20.9566
[230]	training's l2: 21.2379	valid_1's l2: 20.915
[240]	training's l2: 21.1664	valid_1's l2: 20.8746
[250]	training's l2: 21.0986	valid_1's l2: 20.8376
[260]	training's l2: 21.0352	valid_1's l2: 20.8072
[270]	training's l2: 20.9728	valid_1's l2: 20.776
[280]	training's l2: 20.9147	valid_1's l2: 20.7484
[290]	training's l2: 20.8592	valid_1's l2: 20.7252
[300]	training's l2: 20.8055	valid_1's l2: 20.7024
[310]	training's l2: 20.7531	valid_1's l2: 20.6815
[320]	training's l2: 20.7033	valid_1's l2: 20.6628
[330]	training's l2: 20.6547	valid_1's l2: 20.6447
[340]	training's l2: 20.6066	valid_1's l2: 20.6283
[350]	training's l2: 20.5593	valid_1's l2: 20.6126
[360]	training's l2: 20.5127	valid_1's l2: 20.5972
[370]	training's l2: 20.468	valid_1's l2: 20.5838
[380]	training's l2: 20.4243	valid_1's l2: 20.5705
[390]	training's l2: 20.3809	valid_1's l2: 20.5581
[400]	training's l2: 20.3384	valid_1's l2: 20.5473
[410]	training's l2: 20.2971	valid_1's l2: 20.5365
[420]	training's l2: 20.2559	valid_1's l2: 20.5261
[430]	training's l2: 20.2156	valid_1's l2: 20.5171
[440]	training's l2: 20.1757	valid_1's l2: 20.5086
[450]	training's l2: 20.1364	valid_1's l2: 20.4982
[460]	training's l2: 20.0964	valid_1's l2: 20.4893
[470]	training's l2: 20.0577	valid_1's l2: 20.4818
[480]	training's l2: 20.0196	valid_1's l2: 20.4744
[490]	training's l2: 19.9813	valid_1's l2: 20.4671
[500]	training's l2: 19.9436	valid_1's l2: 20.4595
[510]	training's l2: 19.9059	valid_1's l2: 20.4522
[520]	training's l2: 19.8692	valid_1's l2: 20.4457
[530]	training's l2: 19.833	valid_1's l2: 20.4383
[540]	training's l2: 19.7966	valid_1's l2: 20.4322
[550]	training's l2: 19.7599	valid_1's l2: 20.4253
[560]	training's l2: 19.7237	valid_1's l2: 20.4189
[570]	training's l2: 19.6874	valid_1's l2: 20.4126
[580]	training's l2: 19.6517	valid_1's l2: 20.4081
[590]	training's l2: 19.6169	valid_1's l2: 20.4034
[600]	training's l2: 19.5822	valid_1's l2: 20.3981
[610]	training's l2: 19.5475	valid_1's l2: 20.3917
[620]	training's l2: 19.5128	valid_1's l2: 20.386
[630]	training's l2: 19.4788	valid_1's l2: 20.3816
[640]	training's l2: 19.4448	valid_1's l2: 20.3757
[650]	training's l2: 19.4112	valid_1's l2: 20.3712
[660]	training's l2: 19.3777	valid_1's l2: 20.3661
[670]	training's l2: 19.3447	valid_1's l2: 20.3612
[680]	training's l2: 19.312	valid_1's l2: 20.357
[690]	training's l2: 19.2798	valid_1's l2: 20.3539
[700]	training's l2: 19.2479	valid_1's l2: 20.3506
[710]	training's l2: 19.2153	valid_1's l2: 20.3473
[720]	training's l2: 19.1836	valid_1's l2: 20.3445
[730]	training's l2: 19.1516	valid_1's l2: 20.3405
[740]	training's l2: 19.1196	valid_1's l2: 20.3367
[750]	training's l2: 19.0891	valid_1's l2: 20.3348
[760]	training's l2: 19.0574	valid_1's l2: 20.3314
[770]	training's l2: 19.0265	valid_1's l2: 20.3288
[780]	training's l2: 18.9956	valid_1's l2: 20.3249
[790]	training's l2: 18.9661	valid_1's l2: 20.3216
[800]	training's l2: 18.9359	valid_1's l2: 20.3189
[810]	training's l2: 18.905	valid_1's l2: 20.3157
[820]	training's l2: 18.8746	valid_1's l2: 20.3126
[830]	training's l2: 18.8448	valid_1's l2: 20.3088
[840]	training's l2: 18.8148	valid_1's l2: 20.3049
[850]	training's l2: 18.7848	valid_1's l2: 20.3024
[860]	training's l2: 18.7558	valid_1's l2: 20.3002
[870]	training's l2: 18.7268	valid_1's l2: 20.2982
[880]	training's l2: 18.6984	valid_1's l2: 20.2957
[890]	training's l2: 18.6698	valid_1's l2: 20.2941
[900]	training's l2: 18.6403	valid_1's l2: 20.2915
[910]	training's l2: 18.6118	valid_1's l2: 20.2901
[920]	training's l2: 18.5837	valid_1's l2: 20.2877
[930]	training's l2: 18.5551	valid_1's l2: 20.286
[940]	training's l2: 18.5273	valid_1's l2: 20.2836
[950]	training's l2: 18.4994	valid_1's l2: 20.283
[960]	training's l2: 18.4721	valid_1's l2: 20.282
[970]	training's l2: 18.444	valid_1's l2: 20.2801
[980]	training's l2: 18.4159	valid_1's l2: 20.279
[990]	training's l2: 18.3898	valid_1's l2: 20.2782
[1000]	training's l2: 18.362	valid_1's l2: 20.277
[1010]	training's l2: 18.3353	valid_1's l2: 20.2755
[1020]	training's l2: 18.3087	valid_1's l2: 20.2747
[1030]	training's l2: 18.2833	valid_1's l2: 20.2737
[1040]	training's l2: 18.2571	valid_1's l2: 20.2738
[1050]	training's l2: 18.2307	valid_1's l2: 20.272
[1060]	training's l2: 18.2051	valid_1's l2: 20.2712
[1070]	training's l2: 18.1778	valid_1's l2: 20.2687
[1080]	training's l2: 18.1531	valid_1's l2: 20.2673
[1090]	training's l2: 18.1282	valid_1's l2: 20.2664
[1100]	training's l2: 18.1031	valid_1's l2: 20.2663
[1110]	training's l2: 18.078	valid_1's l2: 20.2652
[1120]	training's l2: 18.0538	valid_1's l2: 20.2644
[1130]	training's l2: 18.029	valid_1's l2: 20.2631
[1140]	training's l2: 18.0038	valid_1's l2: 20.2618
[1150]	training's l2: 17.9792	valid_1's l2: 20.2606
[1160]	training's l2: 17.9548	valid_1's l2: 20.259
[1170]	training's l2: 17.9301	valid_1's l2: 20.2575
[1180]	training's l2: 17.9046	valid_1's l2: 20.2572
[1190]	training's l2: 17.8803	valid_1's l2: 20.2563
[1200]	training's l2: 17.8558	valid_1's l2: 20.2568
[1210]	training's l2: 17.8324	valid_1's l2: 20.2565
[1220]	training's l2: 17.8085	valid_1's l2: 20.2566
[1230]	training's l2: 17.7842	valid_1's l2: 20.2549
[1240]	training's l2: 17.7603	valid_1's l2: 20.2548
[1250]	training's l2: 17.7379	valid_1's l2: 20.254
[1260]	training's l2: 17.7142	valid_1's l2: 20.2527
[1270]	training's l2: 17.6911	valid_1's l2: 20.253
[1280]	training's l2: 17.6669	valid_1's l2: 20.2525
[1290]	training's l2: 17.6433	valid_1's l2: 20.2516
[1300]	training's l2: 17.621	valid_1's l2: 20.2509
[1310]	training's l2: 17.5997	valid_1's l2: 20.251
[1320]	training's l2: 17.5756	valid_1's l2: 20.2505
[1330]	training's l2: 17.5531	valid_1's l2: 20.2496
[1340]	training's l2: 17.5306	valid_1's l2: 20.2487
[1350]	training's l2: 17.5072	valid_1's l2: 20.2483
[1360]	training's l2: 17.4844	valid_1's l2: 20.2479
[1370]	training's l2: 17.4612	valid_1's l2: 20.2484
[1380]	training's l2: 17.4381	valid_1's l2: 20.2474
[1390]	training's l2: 17.4158	valid_1's l2: 20.2472
[1400]	training's l2: 17.3932	valid_1's l2: 20.2472
[1410]	training's l2: 17.3716	valid_1's l2: 20.2468
[1420]	training's l2: 17.3509	valid_1's l2: 20.2471
[1430]	training's l2: 17.3283	valid_1's l2: 20.2471
[1440]	training's l2: 17.307	valid_1's l2: 20.2464
[1450]	training's l2: 17.2849	valid_1's l2: 20.2453
[1460]	training's l2: 17.2641	valid_1's l2: 20.2445
[1470]	training's l2: 17.2431	valid_1's l2: 20.2452
[1480]	training's l2: 17.2206	valid_1's l2: 20.2451
Early stopping, best iteration is:
[1459]	training's l2: 17.266	valid_1's l2: 20.2444
score1: 3.836030307784973
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261323 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.8073	valid_1's l2: 30.9125
[20]	training's l2: 29.9726	valid_1's l2: 29.0971
[30]	training's l2: 28.491	valid_1's l2: 27.6348
[40]	training's l2: 27.2894	valid_1's l2: 26.4528
[50]	training's l2: 26.3104	valid_1's l2: 25.4968
[60]	training's l2: 25.5078	valid_1's l2: 24.7155
[70]	training's l2: 24.848	valid_1's l2: 24.0802
[80]	training's l2: 24.3015	valid_1's l2: 23.5619
[90]	training's l2: 23.8452	valid_1's l2: 23.1341
[100]	training's l2: 23.4633	valid_1's l2: 22.782
[110]	training's l2: 23.1405	valid_1's l2: 22.4876
[120]	training's l2: 22.868	valid_1's l2: 22.2449
[130]	training's l2: 22.6345	valid_1's l2: 22.0433
[140]	training's l2: 22.431	valid_1's l2: 21.8708
[150]	training's l2: 22.2512	valid_1's l2: 21.7242
[160]	training's l2: 22.093	valid_1's l2: 21.5996
[170]	training's l2: 21.9508	valid_1's l2: 21.4905
[180]	training's l2: 21.8203	valid_1's l2: 21.3958
[190]	training's l2: 21.7027	valid_1's l2: 21.3128
[200]	training's l2: 21.5961	valid_1's l2: 21.2401
[210]	training's l2: 21.4954	valid_1's l2: 21.1747
[220]	training's l2: 21.401	valid_1's l2: 21.1154
[230]	training's l2: 21.3145	valid_1's l2: 21.0621
[240]	training's l2: 21.2336	valid_1's l2: 21.0156
[250]	training's l2: 21.1566	valid_1's l2: 20.9726
[260]	training's l2: 21.0829	valid_1's l2: 20.9319
[270]	training's l2: 21.0137	valid_1's l2: 20.8951
[280]	training's l2: 20.9476	valid_1's l2: 20.8624
[290]	training's l2: 20.8821	valid_1's l2: 20.8296
[300]	training's l2: 20.8186	valid_1's l2: 20.7989
[310]	training's l2: 20.7574	valid_1's l2: 20.7727
[320]	training's l2: 20.6985	valid_1's l2: 20.7489
[330]	training's l2: 20.6431	valid_1's l2: 20.7277
[340]	training's l2: 20.5886	valid_1's l2: 20.7069
[350]	training's l2: 20.5354	valid_1's l2: 20.6882
[360]	training's l2: 20.4837	valid_1's l2: 20.6702
[370]	training's l2: 20.4338	valid_1's l2: 20.6546
[380]	training's l2: 20.3847	valid_1's l2: 20.6395
[390]	training's l2: 20.3369	valid_1's l2: 20.6259
[400]	training's l2: 20.2906	valid_1's l2: 20.6136
[410]	training's l2: 20.2444	valid_1's l2: 20.5998
[420]	training's l2: 20.1986	valid_1's l2: 20.588
[430]	training's l2: 20.1538	valid_1's l2: 20.5757
[440]	training's l2: 20.1097	valid_1's l2: 20.5652
[450]	training's l2: 20.067	valid_1's l2: 20.5549
[460]	training's l2: 20.024	valid_1's l2: 20.5456
[470]	training's l2: 19.9814	valid_1's l2: 20.5361
[480]	training's l2: 19.9394	valid_1's l2: 20.5262
[490]	training's l2: 19.8977	valid_1's l2: 20.5174
[500]	training's l2: 19.8577	valid_1's l2: 20.5092
[510]	training's l2: 19.8172	valid_1's l2: 20.5015
[520]	training's l2: 19.7772	valid_1's l2: 20.4939
[530]	training's l2: 19.738	valid_1's l2: 20.4856
[540]	training's l2: 19.6987	valid_1's l2: 20.4793
[550]	training's l2: 19.6594	valid_1's l2: 20.4722
[560]	training's l2: 19.6203	valid_1's l2: 20.4654
[570]	training's l2: 19.5819	valid_1's l2: 20.4584
[580]	training's l2: 19.5438	valid_1's l2: 20.4526
[590]	training's l2: 19.5052	valid_1's l2: 20.4447
[600]	training's l2: 19.4668	valid_1's l2: 20.4384
[610]	training's l2: 19.4296	valid_1's l2: 20.4322
[620]	training's l2: 19.392	valid_1's l2: 20.427
[630]	training's l2: 19.3551	valid_1's l2: 20.4219
[640]	training's l2: 19.3183	valid_1's l2: 20.415
[650]	training's l2: 19.2815	valid_1's l2: 20.4085
[660]	training's l2: 19.2451	valid_1's l2: 20.4022
[670]	training's l2: 19.2093	valid_1's l2: 20.3973
[680]	training's l2: 19.1733	valid_1's l2: 20.3921
[690]	training's l2: 19.1379	valid_1's l2: 20.3867
[700]	training's l2: 19.1027	valid_1's l2: 20.3811
[710]	training's l2: 19.0676	valid_1's l2: 20.3773
[720]	training's l2: 19.0332	valid_1's l2: 20.3727
[730]	training's l2: 18.9988	valid_1's l2: 20.3695
[740]	training's l2: 18.9647	valid_1's l2: 20.3667
[750]	training's l2: 18.931	valid_1's l2: 20.3638
[760]	training's l2: 18.8976	valid_1's l2: 20.3616
[770]	training's l2: 18.8636	valid_1's l2: 20.3588
[780]	training's l2: 18.8303	valid_1's l2: 20.3559
[790]	training's l2: 18.7973	valid_1's l2: 20.3529
[800]	training's l2: 18.7646	valid_1's l2: 20.3506
[810]	training's l2: 18.7321	valid_1's l2: 20.3475
[820]	training's l2: 18.6995	valid_1's l2: 20.344
[830]	training's l2: 18.6677	valid_1's l2: 20.3423
[840]	training's l2: 18.6353	valid_1's l2: 20.3392
[850]	training's l2: 18.6028	valid_1's l2: 20.3365
[860]	training's l2: 18.5713	valid_1's l2: 20.3341
[870]	training's l2: 18.5392	valid_1's l2: 20.3317
[880]	training's l2: 18.5073	valid_1's l2: 20.3282
[890]	training's l2: 18.476	valid_1's l2: 20.3254
[900]	training's l2: 18.4449	valid_1's l2: 20.3234
[910]	training's l2: 18.4142	valid_1's l2: 20.3221
[920]	training's l2: 18.3831	valid_1's l2: 20.3194
[930]	training's l2: 18.3522	valid_1's l2: 20.3166
[940]	training's l2: 18.3216	valid_1's l2: 20.314
[950]	training's l2: 18.2904	valid_1's l2: 20.3121
[960]	training's l2: 18.26	valid_1's l2: 20.3105
[970]	training's l2: 18.2298	valid_1's l2: 20.3077
[980]	training's l2: 18.199	valid_1's l2: 20.3062
[990]	training's l2: 18.1691	valid_1's l2: 20.3044
[1000]	training's l2: 18.1392	valid_1's l2: 20.3021
[1010]	training's l2: 18.1102	valid_1's l2: 20.3004
[1020]	training's l2: 18.0813	valid_1's l2: 20.2985
[1030]	training's l2: 18.0516	valid_1's l2: 20.2973
[1040]	training's l2: 18.0226	valid_1's l2: 20.295
[1050]	training's l2: 17.993	valid_1's l2: 20.293
[1060]	training's l2: 17.9645	valid_1's l2: 20.2914
[1070]	training's l2: 17.9355	valid_1's l2: 20.2893
[1080]	training's l2: 17.9068	valid_1's l2: 20.2866
[1090]	training's l2: 17.8783	valid_1's l2: 20.2852
[1100]	training's l2: 17.8493	valid_1's l2: 20.2823
[1110]	training's l2: 17.8217	valid_1's l2: 20.2805
[1120]	training's l2: 17.7934	valid_1's l2: 20.2789
[1130]	training's l2: 17.7657	valid_1's l2: 20.2783
[1140]	training's l2: 17.7379	valid_1's l2: 20.2772
[1150]	training's l2: 17.7098	valid_1's l2: 20.2754
[1160]	training's l2: 17.6824	valid_1's l2: 20.2737
[1170]	training's l2: 17.6552	valid_1's l2: 20.2733
[1180]	training's l2: 17.6297	valid_1's l2: 20.2716
[1190]	training's l2: 17.6023	valid_1's l2: 20.2704
[1200]	training's l2: 17.5761	valid_1's l2: 20.2696
[1210]	training's l2: 17.5496	valid_1's l2: 20.2695
[1220]	training's l2: 17.5232	valid_1's l2: 20.2679
[1230]	training's l2: 17.4969	valid_1's l2: 20.2666
[1240]	training's l2: 17.4699	valid_1's l2: 20.2656
[1250]	training's l2: 17.4427	valid_1's l2: 20.2649
[1260]	training's l2: 17.4167	valid_1's l2: 20.2641
[1270]	training's l2: 17.3902	valid_1's l2: 20.2639
[1280]	training's l2: 17.365	valid_1's l2: 20.2631
[1290]	training's l2: 17.3387	valid_1's l2: 20.2617
[1300]	training's l2: 17.3126	valid_1's l2: 20.2609
[1310]	training's l2: 17.2874	valid_1's l2: 20.2607
[1320]	training's l2: 17.2618	valid_1's l2: 20.2603
[1330]	training's l2: 17.2371	valid_1's l2: 20.2598
[1340]	training's l2: 17.2133	valid_1's l2: 20.26
[1350]	training's l2: 17.1885	valid_1's l2: 20.2595
[1360]	training's l2: 17.1633	valid_1's l2: 20.2595
[1370]	training's l2: 17.139	valid_1's l2: 20.2595
Early stopping, best iteration is:
[1348]	training's l2: 17.193	valid_1's l2: 20.2592
score1: 3.8383496149805767
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243184 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.2012	valid_1's l2: 30.3026
[20]	training's l2: 29.0318	valid_1's l2: 28.1501
[30]	training's l2: 27.3926	valid_1's l2: 26.5275
[40]	training's l2: 26.143	valid_1's l2: 25.298
[50]	training's l2: 25.1815	valid_1's l2: 24.3572
[60]	training's l2: 24.4373	valid_1's l2: 23.6402
[70]	training's l2: 23.8529	valid_1's l2: 23.0836
[80]	training's l2: 23.39	valid_1's l2: 22.6516
[90]	training's l2: 23.0223	valid_1's l2: 22.3145
[100]	training's l2: 22.7246	valid_1's l2: 22.0507
[110]	training's l2: 22.4741	valid_1's l2: 21.8342
[120]	training's l2: 22.2615	valid_1's l2: 21.6575
[130]	training's l2: 22.0819	valid_1's l2: 21.5117
[140]	training's l2: 21.9235	valid_1's l2: 21.3902
[150]	training's l2: 21.7853	valid_1's l2: 21.2892
[160]	training's l2: 21.6592	valid_1's l2: 21.2014
[170]	training's l2: 21.5447	valid_1's l2: 21.123
[180]	training's l2: 21.4427	valid_1's l2: 21.0577
[190]	training's l2: 21.3489	valid_1's l2: 20.9983
[200]	training's l2: 21.261	valid_1's l2: 20.9472
[210]	training's l2: 21.1787	valid_1's l2: 20.9011
[220]	training's l2: 21.0998	valid_1's l2: 20.8583
[230]	training's l2: 21.024	valid_1's l2: 20.8192
[240]	training's l2: 20.953	valid_1's l2: 20.7865
[250]	training's l2: 20.8844	valid_1's l2: 20.7561
[260]	training's l2: 20.82	valid_1's l2: 20.7283
[270]	training's l2: 20.7592	valid_1's l2: 20.7041
[280]	training's l2: 20.7008	valid_1's l2: 20.6833
[290]	training's l2: 20.644	valid_1's l2: 20.6624
[300]	training's l2: 20.5896	valid_1's l2: 20.6447
[310]	training's l2: 20.5354	valid_1's l2: 20.6281
[320]	training's l2: 20.4816	valid_1's l2: 20.6097
[330]	training's l2: 20.4292	valid_1's l2: 20.5933
[340]	training's l2: 20.3786	valid_1's l2: 20.5782
[350]	training's l2: 20.3276	valid_1's l2: 20.5645
[360]	training's l2: 20.2789	valid_1's l2: 20.5528
[370]	training's l2: 20.2307	valid_1's l2: 20.5405
[380]	training's l2: 20.1831	valid_1's l2: 20.5291
[390]	training's l2: 20.1375	valid_1's l2: 20.5184
[400]	training's l2: 20.0917	valid_1's l2: 20.5087
[410]	training's l2: 20.0464	valid_1's l2: 20.4995
[420]	training's l2: 20.0018	valid_1's l2: 20.4899
[430]	training's l2: 19.9564	valid_1's l2: 20.4801
[440]	training's l2: 19.9129	valid_1's l2: 20.4726
[450]	training's l2: 19.87	valid_1's l2: 20.4647
[460]	training's l2: 19.8261	valid_1's l2: 20.4556
[470]	training's l2: 19.7829	valid_1's l2: 20.4476
[480]	training's l2: 19.7405	valid_1's l2: 20.4403
[490]	training's l2: 19.6978	valid_1's l2: 20.4326
[500]	training's l2: 19.6567	valid_1's l2: 20.4231
[510]	training's l2: 19.6139	valid_1's l2: 20.4135
[520]	training's l2: 19.573	valid_1's l2: 20.4061
[530]	training's l2: 19.5328	valid_1's l2: 20.4
[540]	training's l2: 19.4931	valid_1's l2: 20.3944
[550]	training's l2: 19.4532	valid_1's l2: 20.3892
[560]	training's l2: 19.4142	valid_1's l2: 20.3841
[570]	training's l2: 19.3747	valid_1's l2: 20.3779
[580]	training's l2: 19.336	valid_1's l2: 20.3738
[590]	training's l2: 19.2982	valid_1's l2: 20.3693
[600]	training's l2: 19.2602	valid_1's l2: 20.3642
[610]	training's l2: 19.222	valid_1's l2: 20.3593
[620]	training's l2: 19.1849	valid_1's l2: 20.3549
[630]	training's l2: 19.147	valid_1's l2: 20.3501
[640]	training's l2: 19.1101	valid_1's l2: 20.3459
[650]	training's l2: 19.0731	valid_1's l2: 20.3434
[660]	training's l2: 19.0361	valid_1's l2: 20.3402
[670]	training's l2: 18.9999	valid_1's l2: 20.3363
[680]	training's l2: 18.9634	valid_1's l2: 20.3335
[690]	training's l2: 18.9281	valid_1's l2: 20.3297
[700]	training's l2: 18.8916	valid_1's l2: 20.3258
[710]	training's l2: 18.8564	valid_1's l2: 20.3221
[720]	training's l2: 18.8206	valid_1's l2: 20.3193
[730]	training's l2: 18.7862	valid_1's l2: 20.3159
[740]	training's l2: 18.7518	valid_1's l2: 20.3121
[750]	training's l2: 18.7177	valid_1's l2: 20.3096
[760]	training's l2: 18.6831	valid_1's l2: 20.307
[770]	training's l2: 18.6496	valid_1's l2: 20.3057
[780]	training's l2: 18.6158	valid_1's l2: 20.3025
[790]	training's l2: 18.5821	valid_1's l2: 20.3001
[800]	training's l2: 18.5485	valid_1's l2: 20.298
[810]	training's l2: 18.5152	valid_1's l2: 20.2955
[820]	training's l2: 18.4825	valid_1's l2: 20.2924
[830]	training's l2: 18.4503	valid_1's l2: 20.2902
[840]	training's l2: 18.4178	valid_1's l2: 20.2879
[850]	training's l2: 18.3852	valid_1's l2: 20.2854
[860]	training's l2: 18.3527	valid_1's l2: 20.2834
[870]	training's l2: 18.3215	valid_1's l2: 20.2815
[880]	training's l2: 18.2895	valid_1's l2: 20.2796
[890]	training's l2: 18.2585	valid_1's l2: 20.2772
[900]	training's l2: 18.2276	valid_1's l2: 20.2768
[910]	training's l2: 18.1976	valid_1's l2: 20.276
[920]	training's l2: 18.1675	valid_1's l2: 20.2753
[930]	training's l2: 18.1373	valid_1's l2: 20.2745
[940]	training's l2: 18.1085	valid_1's l2: 20.2731
[950]	training's l2: 18.0785	valid_1's l2: 20.2723
[960]	training's l2: 18.0487	valid_1's l2: 20.2717
[970]	training's l2: 18.0192	valid_1's l2: 20.2701
[980]	training's l2: 17.9899	valid_1's l2: 20.2683
[990]	training's l2: 17.9611	valid_1's l2: 20.2674
[1000]	training's l2: 17.9322	valid_1's l2: 20.2668
[1010]	training's l2: 17.9048	valid_1's l2: 20.2657
[1020]	training's l2: 17.8769	valid_1's l2: 20.2646
[1030]	training's l2: 17.8482	valid_1's l2: 20.2646
[1040]	training's l2: 17.8203	valid_1's l2: 20.2643
[1050]	training's l2: 17.7928	valid_1's l2: 20.2628
[1060]	training's l2: 17.7646	valid_1's l2: 20.2625
[1070]	training's l2: 17.7376	valid_1's l2: 20.2621
[1080]	training's l2: 17.7089	valid_1's l2: 20.262
[1090]	training's l2: 17.6809	valid_1's l2: 20.2613
[1100]	training's l2: 17.6535	valid_1's l2: 20.2601
[1110]	training's l2: 17.6263	valid_1's l2: 20.2591
[1120]	training's l2: 17.5987	valid_1's l2: 20.2583
[1130]	training's l2: 17.5721	valid_1's l2: 20.2573
[1140]	training's l2: 17.5445	valid_1's l2: 20.2571
[1150]	training's l2: 17.5188	valid_1's l2: 20.2561
[1160]	training's l2: 17.4916	valid_1's l2: 20.2547
[1170]	training's l2: 17.463	valid_1's l2: 20.2539
[1180]	training's l2: 17.4366	valid_1's l2: 20.2539
[1190]	training's l2: 17.4097	valid_1's l2: 20.2538
[1200]	training's l2: 17.3843	valid_1's l2: 20.2532
[1210]	training's l2: 17.3576	valid_1's l2: 20.2522
[1220]	training's l2: 17.3308	valid_1's l2: 20.2527
[1230]	training's l2: 17.3056	valid_1's l2: 20.2523
Early stopping, best iteration is:
[1206]	training's l2: 17.3682	valid_1's l2: 20.2518
score1: 3.8347884121542584
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.253591 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.2595	valid_1's l2: 30.368
[20]	training's l2: 29.1171	valid_1's l2: 28.2476
[30]	training's l2: 27.4826	valid_1's l2: 26.6346
[40]	training's l2: 26.2265	valid_1's l2: 25.4026
[50]	training's l2: 25.2543	valid_1's l2: 24.4598
[60]	training's l2: 24.4936	valid_1's l2: 23.7313
[70]	training's l2: 23.8926	valid_1's l2: 23.1613
[80]	training's l2: 23.4134	valid_1's l2: 22.7162
[90]	training's l2: 23.0279	valid_1's l2: 22.3652
[100]	training's l2: 22.7156	valid_1's l2: 22.0886
[110]	training's l2: 22.4558	valid_1's l2: 21.8668
[120]	training's l2: 22.2332	valid_1's l2: 21.6824
[130]	training's l2: 22.0449	valid_1's l2: 21.5341
[140]	training's l2: 21.8775	valid_1's l2: 21.408
[150]	training's l2: 21.7296	valid_1's l2: 21.3021
[160]	training's l2: 21.5979	valid_1's l2: 21.2129
[170]	training's l2: 21.4763	valid_1's l2: 21.1337
[180]	training's l2: 21.3656	valid_1's l2: 21.0633
[190]	training's l2: 21.2643	valid_1's l2: 21.0036
[200]	training's l2: 21.1698	valid_1's l2: 20.9499
[210]	training's l2: 21.0831	valid_1's l2: 20.9032
[220]	training's l2: 21.0017	valid_1's l2: 20.8613
[230]	training's l2: 20.9209	valid_1's l2: 20.82
[240]	training's l2: 20.8448	valid_1's l2: 20.785
[250]	training's l2: 20.7711	valid_1's l2: 20.754
[260]	training's l2: 20.7011	valid_1's l2: 20.7266
[270]	training's l2: 20.6341	valid_1's l2: 20.7
[280]	training's l2: 20.5703	valid_1's l2: 20.6767
[290]	training's l2: 20.5088	valid_1's l2: 20.6552
[300]	training's l2: 20.4484	valid_1's l2: 20.6359
[310]	training's l2: 20.391	valid_1's l2: 20.6176
[320]	training's l2: 20.3344	valid_1's l2: 20.6008
[330]	training's l2: 20.278	valid_1's l2: 20.5844
[340]	training's l2: 20.2239	valid_1's l2: 20.5696
[350]	training's l2: 20.1701	valid_1's l2: 20.5558
[360]	training's l2: 20.1171	valid_1's l2: 20.5424
[370]	training's l2: 20.0651	valid_1's l2: 20.5306
[380]	training's l2: 20.0137	valid_1's l2: 20.5196
[390]	training's l2: 19.9629	valid_1's l2: 20.5085
[400]	training's l2: 19.913	valid_1's l2: 20.4986
[410]	training's l2: 19.8642	valid_1's l2: 20.4902
[420]	training's l2: 19.8162	valid_1's l2: 20.4807
[430]	training's l2: 19.7687	valid_1's l2: 20.4735
[440]	training's l2: 19.7213	valid_1's l2: 20.467
[450]	training's l2: 19.6745	valid_1's l2: 20.4597
[460]	training's l2: 19.6272	valid_1's l2: 20.4506
[470]	training's l2: 19.5806	valid_1's l2: 20.4429
[480]	training's l2: 19.5344	valid_1's l2: 20.4339
[490]	training's l2: 19.4886	valid_1's l2: 20.4254
[500]	training's l2: 19.4439	valid_1's l2: 20.4196
[510]	training's l2: 19.3998	valid_1's l2: 20.4124
[520]	training's l2: 19.3552	valid_1's l2: 20.4048
[530]	training's l2: 19.3114	valid_1's l2: 20.3986
[540]	training's l2: 19.267	valid_1's l2: 20.3909
[550]	training's l2: 19.2243	valid_1's l2: 20.3844
[560]	training's l2: 19.1817	valid_1's l2: 20.3789
[570]	training's l2: 19.139	valid_1's l2: 20.3725
[580]	training's l2: 19.0967	valid_1's l2: 20.3672
[590]	training's l2: 19.0541	valid_1's l2: 20.3605
[600]	training's l2: 19.0136	valid_1's l2: 20.3555
[610]	training's l2: 18.9723	valid_1's l2: 20.3507
[620]	training's l2: 18.9317	valid_1's l2: 20.3471
[630]	training's l2: 18.8911	valid_1's l2: 20.344
[640]	training's l2: 18.8507	valid_1's l2: 20.3395
[650]	training's l2: 18.8112	valid_1's l2: 20.3362
[660]	training's l2: 18.7716	valid_1's l2: 20.3328
[670]	training's l2: 18.7321	valid_1's l2: 20.3279
[680]	training's l2: 18.6928	valid_1's l2: 20.3246
[690]	training's l2: 18.6539	valid_1's l2: 20.3209
[700]	training's l2: 18.6143	valid_1's l2: 20.3175
[710]	training's l2: 18.5759	valid_1's l2: 20.3156
[720]	training's l2: 18.5365	valid_1's l2: 20.312
[730]	training's l2: 18.499	valid_1's l2: 20.3085
[740]	training's l2: 18.4603	valid_1's l2: 20.3055
[750]	training's l2: 18.4228	valid_1's l2: 20.3018
[760]	training's l2: 18.3862	valid_1's l2: 20.2986
[770]	training's l2: 18.3493	valid_1's l2: 20.2953
[780]	training's l2: 18.3124	valid_1's l2: 20.2929
[790]	training's l2: 18.2755	valid_1's l2: 20.2905
[800]	training's l2: 18.2392	valid_1's l2: 20.2875
[810]	training's l2: 18.2035	valid_1's l2: 20.2847
[820]	training's l2: 18.1683	valid_1's l2: 20.2828
[830]	training's l2: 18.1321	valid_1's l2: 20.2815
[840]	training's l2: 18.0966	valid_1's l2: 20.2786
[850]	training's l2: 18.0615	valid_1's l2: 20.2772
[860]	training's l2: 18.0265	valid_1's l2: 20.2753
[870]	training's l2: 17.9924	valid_1's l2: 20.2734
[880]	training's l2: 17.9579	valid_1's l2: 20.2719
[890]	training's l2: 17.9238	valid_1's l2: 20.2704
[900]	training's l2: 17.8904	valid_1's l2: 20.2692
[910]	training's l2: 17.8568	valid_1's l2: 20.2676
[920]	training's l2: 17.8234	valid_1's l2: 20.2666
[930]	training's l2: 17.7909	valid_1's l2: 20.2654
[940]	training's l2: 17.7584	valid_1's l2: 20.2648
[950]	training's l2: 17.7262	valid_1's l2: 20.2629
[960]	training's l2: 17.6935	valid_1's l2: 20.2608
[970]	training's l2: 17.6611	valid_1's l2: 20.2605
[980]	training's l2: 17.6297	valid_1's l2: 20.26
[990]	training's l2: 17.598	valid_1's l2: 20.2586
[1000]	training's l2: 17.5666	valid_1's l2: 20.2562
[1010]	training's l2: 17.5357	valid_1's l2: 20.2561
[1020]	training's l2: 17.5049	valid_1's l2: 20.2548
[1030]	training's l2: 17.4742	valid_1's l2: 20.2527
[1040]	training's l2: 17.4429	valid_1's l2: 20.253
[1050]	training's l2: 17.4126	valid_1's l2: 20.2526
[1060]	training's l2: 17.3827	valid_1's l2: 20.2514
[1070]	training's l2: 17.353	valid_1's l2: 20.2522
[1080]	training's l2: 17.3228	valid_1's l2: 20.2518
[1090]	training's l2: 17.2922	valid_1's l2: 20.2505
[1100]	training's l2: 17.2625	valid_1's l2: 20.251
[1110]	training's l2: 17.2328	valid_1's l2: 20.2502
[1120]	training's l2: 17.2026	valid_1's l2: 20.2504
[1130]	training's l2: 17.1733	valid_1's l2: 20.2504
[1140]	training's l2: 17.1439	valid_1's l2: 20.2508
[1150]	training's l2: 17.113	valid_1's l2: 20.2509
Early stopping, best iteration is:
[1129]	training's l2: 17.1763	valid_1's l2: 20.25
score1: 3.8367164460697643
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251390 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 32.1015	valid_1's l2: 31.1956
[20]	training's l2: 30.4596	valid_1's l2: 29.5624
[30]	training's l2: 29.0975	valid_1's l2: 28.2094
[40]	training's l2: 27.9645	valid_1's l2: 27.0851
[50]	training's l2: 27.0195	valid_1's l2: 26.1503
[60]	training's l2: 26.2264	valid_1's l2: 25.3681
[70]	training's l2: 25.5594	valid_1's l2: 24.7141
[80]	training's l2: 24.9972	valid_1's l2: 24.1676
[90]	training's l2: 24.5203	valid_1's l2: 23.7075
[100]	training's l2: 24.1147	valid_1's l2: 23.3183
[110]	training's l2: 23.768	valid_1's l2: 22.9892
[120]	training's l2: 23.4703	valid_1's l2: 22.7098
[130]	training's l2: 23.2145	valid_1's l2: 22.4733
[140]	training's l2: 22.994	valid_1's l2: 22.2716
[150]	training's l2: 22.8008	valid_1's l2: 22.0968
[160]	training's l2: 22.6291	valid_1's l2: 21.9445
[170]	training's l2: 22.4751	valid_1's l2: 21.812
[180]	training's l2: 22.3377	valid_1's l2: 21.696
[190]	training's l2: 22.2146	valid_1's l2: 21.5946
[200]	training's l2: 22.102	valid_1's l2: 21.5039
[210]	training's l2: 21.9988	valid_1's l2: 21.4232
[220]	training's l2: 21.9052	valid_1's l2: 21.353
[230]	training's l2: 21.8187	valid_1's l2: 21.2908
[240]	training's l2: 21.7368	valid_1's l2: 21.2323
[250]	training's l2: 21.6596	valid_1's l2: 21.1777
[260]	training's l2: 21.5881	valid_1's l2: 21.1288
[270]	training's l2: 21.5219	valid_1's l2: 21.0852
[280]	training's l2: 21.4583	valid_1's l2: 21.0439
[290]	training's l2: 21.3995	valid_1's l2: 21.0072
[300]	training's l2: 21.3426	valid_1's l2: 20.973
[310]	training's l2: 21.2895	valid_1's l2: 20.9416
[320]	training's l2: 21.2383	valid_1's l2: 20.912
[330]	training's l2: 21.1883	valid_1's l2: 20.884
[340]	training's l2: 21.1385	valid_1's l2: 20.856
[350]	training's l2: 21.0915	valid_1's l2: 20.831
[360]	training's l2: 21.0446	valid_1's l2: 20.8064
[370]	training's l2: 21.0005	valid_1's l2: 20.7855
[380]	training's l2: 20.9577	valid_1's l2: 20.7652
[390]	training's l2: 20.9161	valid_1's l2: 20.7469
[400]	training's l2: 20.8765	valid_1's l2: 20.7304
[410]	training's l2: 20.8379	valid_1's l2: 20.7143
[420]	training's l2: 20.8	valid_1's l2: 20.6992
[430]	training's l2: 20.7632	valid_1's l2: 20.6842
[440]	training's l2: 20.7275	valid_1's l2: 20.6706
[450]	training's l2: 20.6927	valid_1's l2: 20.6579
[460]	training's l2: 20.658	valid_1's l2: 20.6451
[470]	training's l2: 20.6234	valid_1's l2: 20.6331
[480]	training's l2: 20.5903	valid_1's l2: 20.6224
[490]	training's l2: 20.5566	valid_1's l2: 20.6104
[500]	training's l2: 20.5241	valid_1's l2: 20.6
[510]	training's l2: 20.4917	valid_1's l2: 20.59
[520]	training's l2: 20.4601	valid_1's l2: 20.5808
[530]	training's l2: 20.4288	valid_1's l2: 20.5708
[540]	training's l2: 20.398	valid_1's l2: 20.563
[550]	training's l2: 20.3675	valid_1's l2: 20.5549
[560]	training's l2: 20.3377	valid_1's l2: 20.5478
[570]	training's l2: 20.3075	valid_1's l2: 20.5401
[580]	training's l2: 20.2775	valid_1's l2: 20.5325
[590]	training's l2: 20.2481	valid_1's l2: 20.5248
[600]	training's l2: 20.2183	valid_1's l2: 20.5171
[610]	training's l2: 20.1898	valid_1's l2: 20.5111
[620]	training's l2: 20.1614	valid_1's l2: 20.504
[630]	training's l2: 20.1324	valid_1's l2: 20.4969
[640]	training's l2: 20.1041	valid_1's l2: 20.4904
[650]	training's l2: 20.0754	valid_1's l2: 20.4838
[660]	training's l2: 20.0482	valid_1's l2: 20.4789
[670]	training's l2: 20.0212	valid_1's l2: 20.4738
[680]	training's l2: 19.9939	valid_1's l2: 20.4672
[690]	training's l2: 19.9666	valid_1's l2: 20.4623
[700]	training's l2: 19.94	valid_1's l2: 20.4579
[710]	training's l2: 19.9124	valid_1's l2: 20.4525
[720]	training's l2: 19.8855	valid_1's l2: 20.447
[730]	training's l2: 19.8585	valid_1's l2: 20.4413
[740]	training's l2: 19.832	valid_1's l2: 20.4367
[750]	training's l2: 19.8055	valid_1's l2: 20.4311
[760]	training's l2: 19.7793	valid_1's l2: 20.4263
[770]	training's l2: 19.7535	valid_1's l2: 20.422
[780]	training's l2: 19.7277	valid_1's l2: 20.4174
[790]	training's l2: 19.7017	valid_1's l2: 20.4127
[800]	training's l2: 19.6765	valid_1's l2: 20.4078
[810]	training's l2: 19.6514	valid_1's l2: 20.4034
[820]	training's l2: 19.6265	valid_1's l2: 20.399
[830]	training's l2: 19.6016	valid_1's l2: 20.3946
[840]	training's l2: 19.5768	valid_1's l2: 20.3906
[850]	training's l2: 19.5518	valid_1's l2: 20.3868
[860]	training's l2: 19.5274	valid_1's l2: 20.3832
[870]	training's l2: 19.5032	valid_1's l2: 20.3794
[880]	training's l2: 19.4793	valid_1's l2: 20.3761
[890]	training's l2: 19.4555	valid_1's l2: 20.373
[900]	training's l2: 19.4313	valid_1's l2: 20.3696
[910]	training's l2: 19.4072	valid_1's l2: 20.3659
[920]	training's l2: 19.3836	valid_1's l2: 20.3626
[930]	training's l2: 19.36	valid_1's l2: 20.3597
[940]	training's l2: 19.3365	valid_1's l2: 20.3579
[950]	training's l2: 19.313	valid_1's l2: 20.3547
[960]	training's l2: 19.2892	valid_1's l2: 20.351
[970]	training's l2: 19.2661	valid_1's l2: 20.3489
[980]	training's l2: 19.2427	valid_1's l2: 20.3468
[990]	training's l2: 19.2198	valid_1's l2: 20.3442
[1000]	training's l2: 19.1969	valid_1's l2: 20.3419
[1010]	training's l2: 19.174	valid_1's l2: 20.3398
[1020]	training's l2: 19.1508	valid_1's l2: 20.3371
[1030]	training's l2: 19.1281	valid_1's l2: 20.3354
[1040]	training's l2: 19.1059	valid_1's l2: 20.3336
[1050]	training's l2: 19.083	valid_1's l2: 20.3318
[1060]	training's l2: 19.0607	valid_1's l2: 20.3295
[1070]	training's l2: 19.0387	valid_1's l2: 20.3274
[1080]	training's l2: 19.0167	valid_1's l2: 20.3255
[1090]	training's l2: 18.9949	valid_1's l2: 20.3234
[1100]	training's l2: 18.9729	valid_1's l2: 20.3216
[1110]	training's l2: 18.9506	valid_1's l2: 20.3193
[1120]	training's l2: 18.929	valid_1's l2: 20.3169
[1130]	training's l2: 18.9077	valid_1's l2: 20.3153
[1140]	training's l2: 18.8858	valid_1's l2: 20.3132
[1150]	training's l2: 18.8647	valid_1's l2: 20.3117
[1160]	training's l2: 18.8429	valid_1's l2: 20.31
[1170]	training's l2: 18.8217	valid_1's l2: 20.308
[1180]	training's l2: 18.8004	valid_1's l2: 20.306
[1190]	training's l2: 18.7793	valid_1's l2: 20.304
[1200]	training's l2: 18.7583	valid_1's l2: 20.3023
[1210]	training's l2: 18.7369	valid_1's l2: 20.3005
[1220]	training's l2: 18.7158	valid_1's l2: 20.2988
[1230]	training's l2: 18.695	valid_1's l2: 20.2976
[1240]	training's l2: 18.6743	valid_1's l2: 20.2965
[1250]	training's l2: 18.6535	valid_1's l2: 20.2953
[1260]	training's l2: 18.6329	valid_1's l2: 20.2926
[1270]	training's l2: 18.6121	valid_1's l2: 20.2914
[1280]	training's l2: 18.592	valid_1's l2: 20.2901
[1290]	training's l2: 18.572	valid_1's l2: 20.2886
[1300]	training's l2: 18.5512	valid_1's l2: 20.2873
[1310]	training's l2: 18.5314	valid_1's l2: 20.2856
[1320]	training's l2: 18.5112	valid_1's l2: 20.2842
[1330]	training's l2: 18.4918	valid_1's l2: 20.2843
[1340]	training's l2: 18.4724	valid_1's l2: 20.2833
[1350]	training's l2: 18.4522	valid_1's l2: 20.2824
[1360]	training's l2: 18.4334	valid_1's l2: 20.2818
[1370]	training's l2: 18.4137	valid_1's l2: 20.2812
[1380]	training's l2: 18.3955	valid_1's l2: 20.2804
[1390]	training's l2: 18.3763	valid_1's l2: 20.2791
[1400]	training's l2: 18.3564	valid_1's l2: 20.2778
[1410]	training's l2: 18.3369	valid_1's l2: 20.277
[1420]	training's l2: 18.3178	valid_1's l2: 20.2747
[1430]	training's l2: 18.2984	valid_1's l2: 20.2732
[1440]	training's l2: 18.2797	valid_1's l2: 20.2727
[1450]	training's l2: 18.2612	valid_1's l2: 20.272
[1460]	training's l2: 18.2425	valid_1's l2: 20.2716
[1470]	training's l2: 18.2237	valid_1's l2: 20.271
[1480]	training's l2: 18.2053	valid_1's l2: 20.2698
[1490]	training's l2: 18.1866	valid_1's l2: 20.2685
[1500]	training's l2: 18.1686	valid_1's l2: 20.2689
[1510]	training's l2: 18.1503	valid_1's l2: 20.2686
[1520]	training's l2: 18.1322	valid_1's l2: 20.2675
[1530]	training's l2: 18.1146	valid_1's l2: 20.267
[1540]	training's l2: 18.0971	valid_1's l2: 20.2673
[1550]	training's l2: 18.0795	valid_1's l2: 20.2668
[1560]	training's l2: 18.062	valid_1's l2: 20.2659
[1570]	training's l2: 18.0443	valid_1's l2: 20.2661
[1580]	training's l2: 18.0268	valid_1's l2: 20.2654
[1590]	training's l2: 18.0095	valid_1's l2: 20.2645
[1600]	training's l2: 17.9916	valid_1's l2: 20.2636
[1610]	training's l2: 17.9735	valid_1's l2: 20.263
[1620]	training's l2: 17.9553	valid_1's l2: 20.2619
[1630]	training's l2: 17.9372	valid_1's l2: 20.2613
[1640]	training's l2: 17.9202	valid_1's l2: 20.2604
[1650]	training's l2: 17.9027	valid_1's l2: 20.2604
[1660]	training's l2: 17.8859	valid_1's l2: 20.26
[1670]	training's l2: 17.8691	valid_1's l2: 20.2595
[1680]	training's l2: 17.8521	valid_1's l2: 20.2589
[1690]	training's l2: 17.8342	valid_1's l2: 20.2587
[1700]	training's l2: 17.8171	valid_1's l2: 20.2588
[1710]	training's l2: 17.7996	valid_1's l2: 20.2583
[1720]	training's l2: 17.7819	valid_1's l2: 20.2583
[1730]	training's l2: 17.7653	valid_1's l2: 20.2586
Early stopping, best iteration is:
[1708]	training's l2: 17.8034	valid_1's l2: 20.258
score1: 3.840978645724827
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.240309 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.696	valid_1's l2: 29.791
[20]	training's l2: 28.3054	valid_1's l2: 27.4074
[30]	training's l2: 26.5994	valid_1's l2: 25.714
[40]	training's l2: 25.3698	valid_1's l2: 24.5034
[50]	training's l2: 24.4722	valid_1's l2: 23.6269
[60]	training's l2: 23.8043	valid_1's l2: 22.9837
[70]	training's l2: 23.3055	valid_1's l2: 22.5121
[80]	training's l2: 22.9254	valid_1's l2: 22.1574
[90]	training's l2: 22.6208	valid_1's l2: 21.8835
[100]	training's l2: 22.3743	valid_1's l2: 21.6686
[110]	training's l2: 22.1721	valid_1's l2: 21.5006
[120]	training's l2: 22.0035	valid_1's l2: 21.3662
[130]	training's l2: 21.8522	valid_1's l2: 21.25
[140]	training's l2: 21.7236	valid_1's l2: 21.1568
[150]	training's l2: 21.6096	valid_1's l2: 21.0777
[160]	training's l2: 21.5069	valid_1's l2: 21.0091
[170]	training's l2: 21.4139	valid_1's l2: 20.9503
[180]	training's l2: 21.327	valid_1's l2: 20.8972
[190]	training's l2: 21.2462	valid_1's l2: 20.8526
[200]	training's l2: 21.1694	valid_1's l2: 20.8111
[210]	training's l2: 21.0982	valid_1's l2: 20.7776
[220]	training's l2: 21.0309	valid_1's l2: 20.745
[230]	training's l2: 20.9679	valid_1's l2: 20.7164
[240]	training's l2: 20.9076	valid_1's l2: 20.6901
[250]	training's l2: 20.8484	valid_1's l2: 20.6658
[260]	training's l2: 20.7915	valid_1's l2: 20.6427
[270]	training's l2: 20.7373	valid_1's l2: 20.6236
[280]	training's l2: 20.6832	valid_1's l2: 20.6055
[290]	training's l2: 20.6325	valid_1's l2: 20.5901
[300]	training's l2: 20.5824	valid_1's l2: 20.5737
[310]	training's l2: 20.5327	valid_1's l2: 20.5598
[320]	training's l2: 20.4836	valid_1's l2: 20.5451
[330]	training's l2: 20.4357	valid_1's l2: 20.531
[340]	training's l2: 20.389	valid_1's l2: 20.5185
[350]	training's l2: 20.3435	valid_1's l2: 20.5083
[360]	training's l2: 20.2985	valid_1's l2: 20.4969
[370]	training's l2: 20.2531	valid_1's l2: 20.4859
[380]	training's l2: 20.2089	valid_1's l2: 20.4748
[390]	training's l2: 20.1639	valid_1's l2: 20.4628
[400]	training's l2: 20.1204	valid_1's l2: 20.4527
[410]	training's l2: 20.077	valid_1's l2: 20.4436
[420]	training's l2: 20.0347	valid_1's l2: 20.4347
[430]	training's l2: 19.9926	valid_1's l2: 20.4257
[440]	training's l2: 19.9516	valid_1's l2: 20.4182
[450]	training's l2: 19.9106	valid_1's l2: 20.411
[460]	training's l2: 19.8702	valid_1's l2: 20.4032
[470]	training's l2: 19.8298	valid_1's l2: 20.395
[480]	training's l2: 19.7898	valid_1's l2: 20.3873
[490]	training's l2: 19.7519	valid_1's l2: 20.3831
[500]	training's l2: 19.7126	valid_1's l2: 20.3771
[510]	training's l2: 19.6737	valid_1's l2: 20.3716
[520]	training's l2: 19.6357	valid_1's l2: 20.366
[530]	training's l2: 19.5986	valid_1's l2: 20.3611
[540]	training's l2: 19.5617	valid_1's l2: 20.3569
[550]	training's l2: 19.5246	valid_1's l2: 20.3526
[560]	training's l2: 19.4879	valid_1's l2: 20.3479
[570]	training's l2: 19.4515	valid_1's l2: 20.3452
[580]	training's l2: 19.4148	valid_1's l2: 20.3395
[590]	training's l2: 19.3782	valid_1's l2: 20.3361
[600]	training's l2: 19.3429	valid_1's l2: 20.3338
[610]	training's l2: 19.308	valid_1's l2: 20.3311
[620]	training's l2: 19.2721	valid_1's l2: 20.3273
[630]	training's l2: 19.2377	valid_1's l2: 20.3243
[640]	training's l2: 19.2034	valid_1's l2: 20.3204
[650]	training's l2: 19.1681	valid_1's l2: 20.3174
[660]	training's l2: 19.1345	valid_1's l2: 20.3141
[670]	training's l2: 19.1009	valid_1's l2: 20.3122
[680]	training's l2: 19.0668	valid_1's l2: 20.3104
[690]	training's l2: 19.0333	valid_1's l2: 20.3062
[700]	training's l2: 18.9991	valid_1's l2: 20.3028
[710]	training's l2: 18.9653	valid_1's l2: 20.2994
[720]	training's l2: 18.933	valid_1's l2: 20.2974
[730]	training's l2: 18.9008	valid_1's l2: 20.2951
[740]	training's l2: 18.869	valid_1's l2: 20.2931
[750]	training's l2: 18.8373	valid_1's l2: 20.2916
[760]	training's l2: 18.806	valid_1's l2: 20.2895
[770]	training's l2: 18.7745	valid_1's l2: 20.2878
[780]	training's l2: 18.7444	valid_1's l2: 20.2865
[790]	training's l2: 18.7138	valid_1's l2: 20.2842
[800]	training's l2: 18.683	valid_1's l2: 20.2826
[810]	training's l2: 18.6533	valid_1's l2: 20.2817
[820]	training's l2: 18.6223	valid_1's l2: 20.2801
[830]	training's l2: 18.5927	valid_1's l2: 20.2786
[840]	training's l2: 18.5641	valid_1's l2: 20.2766
[850]	training's l2: 18.5349	valid_1's l2: 20.2767
[860]	training's l2: 18.5055	valid_1's l2: 20.2754
[870]	training's l2: 18.4764	valid_1's l2: 20.2755
[880]	training's l2: 18.4467	valid_1's l2: 20.2747
[890]	training's l2: 18.4189	valid_1's l2: 20.2745
[900]	training's l2: 18.3905	valid_1's l2: 20.2729
[910]	training's l2: 18.3626	valid_1's l2: 20.2729
[920]	training's l2: 18.3336	valid_1's l2: 20.2722
[930]	training's l2: 18.3052	valid_1's l2: 20.2716
[940]	training's l2: 18.2778	valid_1's l2: 20.2702
[950]	training's l2: 18.2505	valid_1's l2: 20.27
[960]	training's l2: 18.222	valid_1's l2: 20.2688
[970]	training's l2: 18.196	valid_1's l2: 20.2688
[980]	training's l2: 18.169	valid_1's l2: 20.2684
[990]	training's l2: 18.142	valid_1's l2: 20.2679
[1000]	training's l2: 18.1157	valid_1's l2: 20.2673
[1010]	training's l2: 18.0902	valid_1's l2: 20.2674
[1020]	training's l2: 18.0653	valid_1's l2: 20.2675
[1030]	training's l2: 18.0386	valid_1's l2: 20.2671
[1040]	training's l2: 18.0121	valid_1's l2: 20.2659
[1050]	training's l2: 17.9857	valid_1's l2: 20.2653
[1060]	training's l2: 17.9596	valid_1's l2: 20.2641
[1070]	training's l2: 17.9327	valid_1's l2: 20.264
[1080]	training's l2: 17.9067	valid_1's l2: 20.2631
[1090]	training's l2: 17.8805	valid_1's l2: 20.2627
[1100]	training's l2: 17.8557	valid_1's l2: 20.2634
[1110]	training's l2: 17.8302	valid_1's l2: 20.2628
Early stopping, best iteration is:
[1086]	training's l2: 17.8906	valid_1's l2: 20.2626
score1: 3.8417509079116106
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238435 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.9669	valid_1's l2: 30.0738
[20]	training's l2: 28.6809	valid_1's l2: 27.8067
[30]	training's l2: 26.9949	valid_1's l2: 26.143
[40]	training's l2: 25.7379	valid_1's l2: 24.9118
[50]	training's l2: 24.7896	valid_1's l2: 23.9923
[60]	training's l2: 24.0669	valid_1's l2: 23.3008
[70]	training's l2: 23.5093	valid_1's l2: 22.7789
[80]	training's l2: 23.075	valid_1's l2: 22.3807
[90]	training's l2: 22.7313	valid_1's l2: 22.0732
[100]	training's l2: 22.4517	valid_1's l2: 21.8332
[110]	training's l2: 22.2181	valid_1's l2: 21.6402
[120]	training's l2: 22.0203	valid_1's l2: 21.4829
[130]	training's l2: 21.8477	valid_1's l2: 21.3527
[140]	training's l2: 21.6985	valid_1's l2: 21.2474
[150]	training's l2: 21.5629	valid_1's l2: 21.1546
[160]	training's l2: 21.4415	valid_1's l2: 21.0764
[170]	training's l2: 21.3329	valid_1's l2: 21.0095
[180]	training's l2: 21.2333	valid_1's l2: 20.9507
[190]	training's l2: 21.1405	valid_1's l2: 20.8999
[200]	training's l2: 21.0526	valid_1's l2: 20.8538
[210]	training's l2: 20.967	valid_1's l2: 20.8102
[220]	training's l2: 20.8872	valid_1's l2: 20.7736
[230]	training's l2: 20.8118	valid_1's l2: 20.7408
[240]	training's l2: 20.7407	valid_1's l2: 20.713
[250]	training's l2: 20.6741	valid_1's l2: 20.6891
[260]	training's l2: 20.6065	valid_1's l2: 20.6649
[270]	training's l2: 20.5432	valid_1's l2: 20.6449
[280]	training's l2: 20.4811	valid_1's l2: 20.6251
[290]	training's l2: 20.4206	valid_1's l2: 20.6061
[300]	training's l2: 20.3618	valid_1's l2: 20.5875
[310]	training's l2: 20.3048	valid_1's l2: 20.5716
[320]	training's l2: 20.2484	valid_1's l2: 20.5573
[330]	training's l2: 20.1919	valid_1's l2: 20.5419
[340]	training's l2: 20.1379	valid_1's l2: 20.5296
[350]	training's l2: 20.0852	valid_1's l2: 20.5186
[360]	training's l2: 20.0321	valid_1's l2: 20.5072
[370]	training's l2: 19.9809	valid_1's l2: 20.4959
[380]	training's l2: 19.929	valid_1's l2: 20.4842
[390]	training's l2: 19.878	valid_1's l2: 20.4745
[400]	training's l2: 19.8277	valid_1's l2: 20.4641
[410]	training's l2: 19.7776	valid_1's l2: 20.4553
[420]	training's l2: 19.7276	valid_1's l2: 20.4463
[430]	training's l2: 19.6794	valid_1's l2: 20.4368
[440]	training's l2: 19.6312	valid_1's l2: 20.4269
[450]	training's l2: 19.5836	valid_1's l2: 20.4205
[460]	training's l2: 19.5367	valid_1's l2: 20.4134
[470]	training's l2: 19.4901	valid_1's l2: 20.4047
[480]	training's l2: 19.4446	valid_1's l2: 20.3977
[490]	training's l2: 19.3991	valid_1's l2: 20.3911
[500]	training's l2: 19.3537	valid_1's l2: 20.3846
[510]	training's l2: 19.3102	valid_1's l2: 20.3783
[520]	training's l2: 19.2663	valid_1's l2: 20.3721
[530]	training's l2: 19.2227	valid_1's l2: 20.3656
[540]	training's l2: 19.1794	valid_1's l2: 20.3605
[550]	training's l2: 19.1368	valid_1's l2: 20.3555
[560]	training's l2: 19.0937	valid_1's l2: 20.3515
[570]	training's l2: 19.0518	valid_1's l2: 20.3464
[580]	training's l2: 19.0096	valid_1's l2: 20.341
[590]	training's l2: 18.9676	valid_1's l2: 20.3373
[600]	training's l2: 18.926	valid_1's l2: 20.3339
[610]	training's l2: 18.8851	valid_1's l2: 20.3298
[620]	training's l2: 18.843	valid_1's l2: 20.3257
[630]	training's l2: 18.8019	valid_1's l2: 20.3219
[640]	training's l2: 18.7619	valid_1's l2: 20.3194
[650]	training's l2: 18.7216	valid_1's l2: 20.3151
[660]	training's l2: 18.6818	valid_1's l2: 20.3112
[670]	training's l2: 18.6423	valid_1's l2: 20.3077
[680]	training's l2: 18.6035	valid_1's l2: 20.3054
[690]	training's l2: 18.5642	valid_1's l2: 20.3024
[700]	training's l2: 18.5254	valid_1's l2: 20.299
[710]	training's l2: 18.4872	valid_1's l2: 20.2972
[720]	training's l2: 18.4493	valid_1's l2: 20.2955
[730]	training's l2: 18.4121	valid_1's l2: 20.2924
[740]	training's l2: 18.3736	valid_1's l2: 20.2887
[750]	training's l2: 18.3357	valid_1's l2: 20.2861
[760]	training's l2: 18.2984	valid_1's l2: 20.2846
[770]	training's l2: 18.2614	valid_1's l2: 20.2826
[780]	training's l2: 18.225	valid_1's l2: 20.2812
[790]	training's l2: 18.1894	valid_1's l2: 20.2784
[800]	training's l2: 18.1545	valid_1's l2: 20.2758
[810]	training's l2: 18.1192	valid_1's l2: 20.2747
[820]	training's l2: 18.0849	valid_1's l2: 20.2737
[830]	training's l2: 18.05	valid_1's l2: 20.2718
[840]	training's l2: 18.0158	valid_1's l2: 20.2715
[850]	training's l2: 17.9807	valid_1's l2: 20.2694
[860]	training's l2: 17.9476	valid_1's l2: 20.2684
[870]	training's l2: 17.9153	valid_1's l2: 20.266
[880]	training's l2: 17.8819	valid_1's l2: 20.2647
[890]	training's l2: 17.8488	valid_1's l2: 20.2637
[900]	training's l2: 17.8157	valid_1's l2: 20.2634
[910]	training's l2: 17.7833	valid_1's l2: 20.2627
[920]	training's l2: 17.7518	valid_1's l2: 20.2602
[930]	training's l2: 17.7188	valid_1's l2: 20.2586
[940]	training's l2: 17.6865	valid_1's l2: 20.2582
[950]	training's l2: 17.6548	valid_1's l2: 20.2575
[960]	training's l2: 17.6232	valid_1's l2: 20.2572
[970]	training's l2: 17.5917	valid_1's l2: 20.2568
[980]	training's l2: 17.56	valid_1's l2: 20.2547
[990]	training's l2: 17.5281	valid_1's l2: 20.2535
[1000]	training's l2: 17.4978	valid_1's l2: 20.2529
[1010]	training's l2: 17.4665	valid_1's l2: 20.2521
[1020]	training's l2: 17.4355	valid_1's l2: 20.2505
[1030]	training's l2: 17.4046	valid_1's l2: 20.2504
[1040]	training's l2: 17.3743	valid_1's l2: 20.2494
[1050]	training's l2: 17.3434	valid_1's l2: 20.2493
[1060]	training's l2: 17.3145	valid_1's l2: 20.2487
[1070]	training's l2: 17.2856	valid_1's l2: 20.2487
[1080]	training's l2: 17.2562	valid_1's l2: 20.2478
[1090]	training's l2: 17.2254	valid_1's l2: 20.2482
[1100]	training's l2: 17.1958	valid_1's l2: 20.2485
Early stopping, best iteration is:
[1076]	training's l2: 17.2676	valid_1's l2: 20.2474
score1: 3.836925530485307
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259149 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.527	valid_1's l2: 30.6228
[20]	training's l2: 29.537	valid_1's l2: 28.6432
[30]	training's l2: 27.9808	valid_1's l2: 27.0969
[40]	training's l2: 26.7582	valid_1's l2: 25.8861
[50]	training's l2: 25.7918	valid_1's l2: 24.935
[60]	training's l2: 25.0208	valid_1's l2: 24.1817
[70]	training's l2: 24.4036	valid_1's l2: 23.5832
[80]	training's l2: 23.9041	valid_1's l2: 23.1054
[90]	training's l2: 23.4978	valid_1's l2: 22.7232
[100]	training's l2: 23.1661	valid_1's l2: 22.4146
[110]	training's l2: 22.8924	valid_1's l2: 22.1649
[120]	training's l2: 22.6593	valid_1's l2: 21.9599
[130]	training's l2: 22.4589	valid_1's l2: 21.7853
[140]	training's l2: 22.2871	valid_1's l2: 21.6394
[150]	training's l2: 22.1353	valid_1's l2: 21.5163
[160]	training's l2: 22.0028	valid_1's l2: 21.4113
[170]	training's l2: 21.8843	valid_1's l2: 21.3239
[180]	training's l2: 21.7756	valid_1's l2: 21.2452
[190]	training's l2: 21.6749	valid_1's l2: 21.1732
[200]	training's l2: 21.5842	valid_1's l2: 21.1109
[210]	training's l2: 21.4999	valid_1's l2: 21.0557
[220]	training's l2: 21.4225	valid_1's l2: 21.0062
[230]	training's l2: 21.3503	valid_1's l2: 20.9622
[240]	training's l2: 21.2822	valid_1's l2: 20.9229
[250]	training's l2: 21.2163	valid_1's l2: 20.8857
[260]	training's l2: 21.1532	valid_1's l2: 20.8498
[270]	training's l2: 21.0924	valid_1's l2: 20.8181
[280]	training's l2: 21.035	valid_1's l2: 20.7906
[290]	training's l2: 20.9799	valid_1's l2: 20.7645
[300]	training's l2: 20.9285	valid_1's l2: 20.7425
[310]	training's l2: 20.8784	valid_1's l2: 20.7224
[320]	training's l2: 20.8309	valid_1's l2: 20.7042
[330]	training's l2: 20.7829	valid_1's l2: 20.6841
[340]	training's l2: 20.7379	valid_1's l2: 20.6677
[350]	training's l2: 20.6929	valid_1's l2: 20.6524
[360]	training's l2: 20.6489	valid_1's l2: 20.637
[370]	training's l2: 20.6059	valid_1's l2: 20.6225
[380]	training's l2: 20.5634	valid_1's l2: 20.6085
[390]	training's l2: 20.5224	valid_1's l2: 20.5949
[400]	training's l2: 20.4822	valid_1's l2: 20.5828
[410]	training's l2: 20.4425	valid_1's l2: 20.5717
[420]	training's l2: 20.403	valid_1's l2: 20.5604
[430]	training's l2: 20.3643	valid_1's l2: 20.5493
[440]	training's l2: 20.3264	valid_1's l2: 20.5394
[450]	training's l2: 20.289	valid_1's l2: 20.53
[460]	training's l2: 20.2525	valid_1's l2: 20.5213
[470]	training's l2: 20.2153	valid_1's l2: 20.5133
[480]	training's l2: 20.1791	valid_1's l2: 20.5041
[490]	training's l2: 20.143	valid_1's l2: 20.4964
[500]	training's l2: 20.1077	valid_1's l2: 20.4889
[510]	training's l2: 20.0724	valid_1's l2: 20.4822
[520]	training's l2: 20.0361	valid_1's l2: 20.4736
[530]	training's l2: 20.0006	valid_1's l2: 20.4667
[540]	training's l2: 19.9656	valid_1's l2: 20.4594
[550]	training's l2: 19.9313	valid_1's l2: 20.4528
[560]	training's l2: 19.8973	valid_1's l2: 20.4469
[570]	training's l2: 19.8636	valid_1's l2: 20.4409
[580]	training's l2: 19.8301	valid_1's l2: 20.4339
[590]	training's l2: 19.7973	valid_1's l2: 20.4284
[600]	training's l2: 19.7638	valid_1's l2: 20.4221
[610]	training's l2: 19.7318	valid_1's l2: 20.417
[620]	training's l2: 19.6993	valid_1's l2: 20.4108
[630]	training's l2: 19.6678	valid_1's l2: 20.4057
[640]	training's l2: 19.6361	valid_1's l2: 20.4007
[650]	training's l2: 19.6039	valid_1's l2: 20.3953
[660]	training's l2: 19.5723	valid_1's l2: 20.3904
[670]	training's l2: 19.5415	valid_1's l2: 20.3857
[680]	training's l2: 19.5102	valid_1's l2: 20.3817
[690]	training's l2: 19.4797	valid_1's l2: 20.3786
[700]	training's l2: 19.4493	valid_1's l2: 20.3744
[710]	training's l2: 19.4187	valid_1's l2: 20.3696
[720]	training's l2: 19.3882	valid_1's l2: 20.367
[730]	training's l2: 19.3588	valid_1's l2: 20.3637
[740]	training's l2: 19.3284	valid_1's l2: 20.3601
[750]	training's l2: 19.2984	valid_1's l2: 20.3557
[760]	training's l2: 19.2693	valid_1's l2: 20.3535
[770]	training's l2: 19.2403	valid_1's l2: 20.3496
[780]	training's l2: 19.2113	valid_1's l2: 20.3471
[790]	training's l2: 19.1824	valid_1's l2: 20.3447
[800]	training's l2: 19.1536	valid_1's l2: 20.3419
[810]	training's l2: 19.1253	valid_1's l2: 20.3403
[820]	training's l2: 19.0969	valid_1's l2: 20.3372
[830]	training's l2: 19.0687	valid_1's l2: 20.3344
[840]	training's l2: 19.0405	valid_1's l2: 20.3318
[850]	training's l2: 19.0122	valid_1's l2: 20.3293
[860]	training's l2: 18.985	valid_1's l2: 20.3257
[870]	training's l2: 18.9569	valid_1's l2: 20.3225
[880]	training's l2: 18.9288	valid_1's l2: 20.3195
[890]	training's l2: 18.9021	valid_1's l2: 20.318
[900]	training's l2: 18.8758	valid_1's l2: 20.3149
[910]	training's l2: 18.8482	valid_1's l2: 20.3127
[920]	training's l2: 18.8217	valid_1's l2: 20.3109
[930]	training's l2: 18.7947	valid_1's l2: 20.3096
[940]	training's l2: 18.768	valid_1's l2: 20.3065
[950]	training's l2: 18.7417	valid_1's l2: 20.3036
[960]	training's l2: 18.715	valid_1's l2: 20.3009
[970]	training's l2: 18.6883	valid_1's l2: 20.2993
[980]	training's l2: 18.6623	valid_1's l2: 20.2976
[990]	training's l2: 18.6366	valid_1's l2: 20.2965
[1000]	training's l2: 18.6109	valid_1's l2: 20.2946
[1010]	training's l2: 18.5848	valid_1's l2: 20.2933
[1020]	training's l2: 18.5599	valid_1's l2: 20.2922
[1030]	training's l2: 18.5353	valid_1's l2: 20.2915
[1040]	training's l2: 18.5108	valid_1's l2: 20.2902
[1050]	training's l2: 18.4861	valid_1's l2: 20.2895
[1060]	training's l2: 18.4617	valid_1's l2: 20.2891
[1070]	training's l2: 18.4365	valid_1's l2: 20.2887
[1080]	training's l2: 18.4121	valid_1's l2: 20.2867
[1090]	training's l2: 18.3873	valid_1's l2: 20.2852
[1100]	training's l2: 18.3635	valid_1's l2: 20.284
[1110]	training's l2: 18.3395	valid_1's l2: 20.2836
[1120]	training's l2: 18.3154	valid_1's l2: 20.2824
[1130]	training's l2: 18.2923	valid_1's l2: 20.2813
[1140]	training's l2: 18.2679	valid_1's l2: 20.2803
[1150]	training's l2: 18.2442	valid_1's l2: 20.2794
[1160]	training's l2: 18.2213	valid_1's l2: 20.2787
[1170]	training's l2: 18.1979	valid_1's l2: 20.2776
[1180]	training's l2: 18.1749	valid_1's l2: 20.277
[1190]	training's l2: 18.1512	valid_1's l2: 20.2762
[1200]	training's l2: 18.1286	valid_1's l2: 20.2754
[1210]	training's l2: 18.1058	valid_1's l2: 20.275
[1220]	training's l2: 18.0838	valid_1's l2: 20.275
[1230]	training's l2: 18.0606	valid_1's l2: 20.2743
[1240]	training's l2: 18.0395	valid_1's l2: 20.2739
[1250]	training's l2: 18.0177	valid_1's l2: 20.2734
[1260]	training's l2: 17.9957	valid_1's l2: 20.2735
[1270]	training's l2: 17.9732	valid_1's l2: 20.273
[1280]	training's l2: 17.9516	valid_1's l2: 20.272
[1290]	training's l2: 17.9291	valid_1's l2: 20.271
[1300]	training's l2: 17.9066	valid_1's l2: 20.2696
[1310]	training's l2: 17.8836	valid_1's l2: 20.2686
[1320]	training's l2: 17.8629	valid_1's l2: 20.2677
[1330]	training's l2: 17.8412	valid_1's l2: 20.268
[1340]	training's l2: 17.8191	valid_1's l2: 20.2676
[1350]	training's l2: 17.7978	valid_1's l2: 20.2669
[1360]	training's l2: 17.7767	valid_1's l2: 20.2663
[1370]	training's l2: 17.7543	valid_1's l2: 20.2654
[1380]	training's l2: 17.7336	valid_1's l2: 20.2649
[1390]	training's l2: 17.7122	valid_1's l2: 20.2642
[1400]	training's l2: 17.6919	valid_1's l2: 20.2631
[1410]	training's l2: 17.6716	valid_1's l2: 20.2628
[1420]	training's l2: 17.65	valid_1's l2: 20.2617
[1430]	training's l2: 17.6286	valid_1's l2: 20.2613
[1440]	training's l2: 17.6083	valid_1's l2: 20.2612
[1450]	training's l2: 17.5866	valid_1's l2: 20.2608
[1460]	training's l2: 17.5658	valid_1's l2: 20.2604
[1470]	training's l2: 17.5451	valid_1's l2: 20.2597
[1480]	training's l2: 17.5241	valid_1's l2: 20.2592
[1490]	training's l2: 17.5032	valid_1's l2: 20.2586
[1500]	training's l2: 17.4812	valid_1's l2: 20.2589
[1510]	training's l2: 17.4599	valid_1's l2: 20.2588
[1520]	training's l2: 17.4394	valid_1's l2: 20.2575
[1530]	training's l2: 17.4196	valid_1's l2: 20.2569
[1540]	training's l2: 17.3991	valid_1's l2: 20.2566
[1550]	training's l2: 17.3773	valid_1's l2: 20.2563
[1560]	training's l2: 17.3574	valid_1's l2: 20.2551
[1570]	training's l2: 17.3377	valid_1's l2: 20.2548
[1580]	training's l2: 17.317	valid_1's l2: 20.2537
[1590]	training's l2: 17.2973	valid_1's l2: 20.2547
[1600]	training's l2: 17.2773	valid_1's l2: 20.2544
[1610]	training's l2: 17.2584	valid_1's l2: 20.2545
Early stopping, best iteration is:
[1581]	training's l2: 17.3153	valid_1's l2: 20.2534
score1: 3.8385607874143406
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.238730 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.9269	valid_1's l2: 31.024
[20]	training's l2: 30.1728	valid_1's l2: 29.281
[30]	training's l2: 28.7411	valid_1's l2: 27.8612
[40]	training's l2: 27.5698	valid_1's l2: 26.7019
[50]	training's l2: 26.6059	valid_1's l2: 25.7524
[60]	training's l2: 25.8121	valid_1's l2: 24.9744
[70]	training's l2: 25.1529	valid_1's l2: 24.3313
[80]	training's l2: 24.6031	valid_1's l2: 23.8011
[90]	training's l2: 24.143	valid_1's l2: 23.3603
[100]	training's l2: 23.755	valid_1's l2: 22.9945
[110]	training's l2: 23.4264	valid_1's l2: 22.6891
[120]	training's l2: 23.1479	valid_1's l2: 22.4326
[130]	training's l2: 22.9107	valid_1's l2: 22.2166
[140]	training's l2: 22.7055	valid_1's l2: 22.0343
[150]	training's l2: 22.5237	valid_1's l2: 21.8769
[160]	training's l2: 22.362	valid_1's l2: 21.7412
[170]	training's l2: 22.2191	valid_1's l2: 21.6226
[180]	training's l2: 22.0911	valid_1's l2: 21.5211
[190]	training's l2: 21.9745	valid_1's l2: 21.4315
[200]	training's l2: 21.8687	valid_1's l2: 21.3515
[210]	training's l2: 21.7728	valid_1's l2: 21.2829
[220]	training's l2: 21.6817	valid_1's l2: 21.2201
[230]	training's l2: 21.5961	valid_1's l2: 21.1621
[240]	training's l2: 21.5187	valid_1's l2: 21.1107
[250]	training's l2: 21.4458	valid_1's l2: 21.0654
[260]	training's l2: 21.3767	valid_1's l2: 21.022
[270]	training's l2: 21.311	valid_1's l2: 20.9828
[280]	training's l2: 21.2508	valid_1's l2: 20.948
[290]	training's l2: 21.1926	valid_1's l2: 20.914
[300]	training's l2: 21.1357	valid_1's l2: 20.883
[310]	training's l2: 21.0793	valid_1's l2: 20.8528
[320]	training's l2: 21.0261	valid_1's l2: 20.8261
[330]	training's l2: 20.9749	valid_1's l2: 20.8017
[340]	training's l2: 20.9244	valid_1's l2: 20.7782
[350]	training's l2: 20.8772	valid_1's l2: 20.7574
[360]	training's l2: 20.8302	valid_1's l2: 20.7368
[370]	training's l2: 20.7856	valid_1's l2: 20.7195
[380]	training's l2: 20.7428	valid_1's l2: 20.7032
[390]	training's l2: 20.7006	valid_1's l2: 20.6878
[400]	training's l2: 20.659	valid_1's l2: 20.6713
[410]	training's l2: 20.6191	valid_1's l2: 20.6576
[420]	training's l2: 20.5795	valid_1's l2: 20.6437
[430]	training's l2: 20.5411	valid_1's l2: 20.6319
[440]	training's l2: 20.5028	valid_1's l2: 20.6194
[450]	training's l2: 20.4647	valid_1's l2: 20.6072
[460]	training's l2: 20.4271	valid_1's l2: 20.5961
[470]	training's l2: 20.3913	valid_1's l2: 20.5869
[480]	training's l2: 20.3552	valid_1's l2: 20.5766
[490]	training's l2: 20.3198	valid_1's l2: 20.5663
[500]	training's l2: 20.2842	valid_1's l2: 20.5568
[510]	training's l2: 20.25	valid_1's l2: 20.549
[520]	training's l2: 20.2161	valid_1's l2: 20.5405
[530]	training's l2: 20.182	valid_1's l2: 20.5316
[540]	training's l2: 20.1484	valid_1's l2: 20.5247
[550]	training's l2: 20.1152	valid_1's l2: 20.5177
[560]	training's l2: 20.0825	valid_1's l2: 20.5105
[570]	training's l2: 20.0496	valid_1's l2: 20.5028
[580]	training's l2: 20.0161	valid_1's l2: 20.495
[590]	training's l2: 19.9835	valid_1's l2: 20.4876
[600]	training's l2: 19.9515	valid_1's l2: 20.4819
[610]	training's l2: 19.9202	valid_1's l2: 20.4763
[620]	training's l2: 19.8891	valid_1's l2: 20.471
[630]	training's l2: 19.8572	valid_1's l2: 20.4644
[640]	training's l2: 19.8251	valid_1's l2: 20.4572
[650]	training's l2: 19.7938	valid_1's l2: 20.4512
[660]	training's l2: 19.7629	valid_1's l2: 20.4456
[670]	training's l2: 19.7324	valid_1's l2: 20.4404
[680]	training's l2: 19.7025	valid_1's l2: 20.4348
[690]	training's l2: 19.6729	valid_1's l2: 20.4287
[700]	training's l2: 19.6425	valid_1's l2: 20.4235
[710]	training's l2: 19.6128	valid_1's l2: 20.4181
[720]	training's l2: 19.583	valid_1's l2: 20.4133
[730]	training's l2: 19.5535	valid_1's l2: 20.4083
[740]	training's l2: 19.5244	valid_1's l2: 20.4045
[750]	training's l2: 19.4959	valid_1's l2: 20.3997
[760]	training's l2: 19.4671	valid_1's l2: 20.3966
[770]	training's l2: 19.439	valid_1's l2: 20.3922
[780]	training's l2: 19.4107	valid_1's l2: 20.3878
[790]	training's l2: 19.3822	valid_1's l2: 20.3827
[800]	training's l2: 19.3542	valid_1's l2: 20.3793
[810]	training's l2: 19.326	valid_1's l2: 20.3759
[820]	training's l2: 19.2983	valid_1's l2: 20.3722
[830]	training's l2: 19.271	valid_1's l2: 20.3683
[840]	training's l2: 19.2437	valid_1's l2: 20.3649
[850]	training's l2: 19.2165	valid_1's l2: 20.362
[860]	training's l2: 19.1886	valid_1's l2: 20.359
[870]	training's l2: 19.1621	valid_1's l2: 20.3569
[880]	training's l2: 19.1352	valid_1's l2: 20.3541
[890]	training's l2: 19.1089	valid_1's l2: 20.3518
[900]	training's l2: 19.0826	valid_1's l2: 20.349
[910]	training's l2: 19.0558	valid_1's l2: 20.3468
[920]	training's l2: 19.0296	valid_1's l2: 20.3443
[930]	training's l2: 19.0034	valid_1's l2: 20.3418
[940]	training's l2: 18.9777	valid_1's l2: 20.3383
[950]	training's l2: 18.9526	valid_1's l2: 20.3363
[960]	training's l2: 18.9268	valid_1's l2: 20.3342
[970]	training's l2: 18.9012	valid_1's l2: 20.3324
[980]	training's l2: 18.8754	valid_1's l2: 20.3308
[990]	training's l2: 18.8505	valid_1's l2: 20.3289
[1000]	training's l2: 18.8248	valid_1's l2: 20.3279
[1010]	training's l2: 18.7995	valid_1's l2: 20.3255
[1020]	training's l2: 18.7748	valid_1's l2: 20.3243
[1030]	training's l2: 18.7498	valid_1's l2: 20.3223
[1040]	training's l2: 18.7248	valid_1's l2: 20.3204
[1050]	training's l2: 18.7	valid_1's l2: 20.3178
[1060]	training's l2: 18.6754	valid_1's l2: 20.3161
[1070]	training's l2: 18.651	valid_1's l2: 20.3142
[1080]	training's l2: 18.6266	valid_1's l2: 20.3118
[1090]	training's l2: 18.6025	valid_1's l2: 20.3096
[1100]	training's l2: 18.5786	valid_1's l2: 20.3076
[1110]	training's l2: 18.5553	valid_1's l2: 20.306
[1120]	training's l2: 18.531	valid_1's l2: 20.3042
[1130]	training's l2: 18.5072	valid_1's l2: 20.3032
[1140]	training's l2: 18.4836	valid_1's l2: 20.3002
[1150]	training's l2: 18.4598	valid_1's l2: 20.2992
[1160]	training's l2: 18.4374	valid_1's l2: 20.2981
[1170]	training's l2: 18.4138	valid_1's l2: 20.2968
[1180]	training's l2: 18.3908	valid_1's l2: 20.2951
[1190]	training's l2: 18.3684	valid_1's l2: 20.2937
[1200]	training's l2: 18.3461	valid_1's l2: 20.2924
[1210]	training's l2: 18.3227	valid_1's l2: 20.2903
[1220]	training's l2: 18.3005	valid_1's l2: 20.2895
[1230]	training's l2: 18.2785	valid_1's l2: 20.2885
[1240]	training's l2: 18.2563	valid_1's l2: 20.2869
[1250]	training's l2: 18.2336	valid_1's l2: 20.2857
[1260]	training's l2: 18.2114	valid_1's l2: 20.285
[1270]	training's l2: 18.1893	valid_1's l2: 20.2844
[1280]	training's l2: 18.168	valid_1's l2: 20.2829
[1290]	training's l2: 18.1464	valid_1's l2: 20.2821
[1300]	training's l2: 18.1243	valid_1's l2: 20.2806
[1310]	training's l2: 18.1028	valid_1's l2: 20.2798
[1320]	training's l2: 18.0817	valid_1's l2: 20.2789
[1330]	training's l2: 18.061	valid_1's l2: 20.2785
[1340]	training's l2: 18.0387	valid_1's l2: 20.277
[1350]	training's l2: 18.0168	valid_1's l2: 20.2761
[1360]	training's l2: 17.9959	valid_1's l2: 20.2754
[1370]	training's l2: 17.9744	valid_1's l2: 20.2741
[1380]	training's l2: 17.9534	valid_1's l2: 20.2741
[1390]	training's l2: 17.9318	valid_1's l2: 20.2736
[1400]	training's l2: 17.9109	valid_1's l2: 20.273
[1410]	training's l2: 17.8895	valid_1's l2: 20.2719
[1420]	training's l2: 17.8684	valid_1's l2: 20.2711
[1430]	training's l2: 17.8477	valid_1's l2: 20.2698
[1440]	training's l2: 17.8281	valid_1's l2: 20.2694
[1450]	training's l2: 17.8079	valid_1's l2: 20.2692
[1460]	training's l2: 17.7875	valid_1's l2: 20.2679
[1470]	training's l2: 17.7673	valid_1's l2: 20.2669
[1480]	training's l2: 17.7469	valid_1's l2: 20.2662
[1490]	training's l2: 17.7265	valid_1's l2: 20.2656
[1500]	training's l2: 17.7067	valid_1's l2: 20.2657
[1510]	training's l2: 17.6859	valid_1's l2: 20.2649
[1520]	training's l2: 17.6659	valid_1's l2: 20.2638
[1530]	training's l2: 17.6459	valid_1's l2: 20.2632
[1540]	training's l2: 17.6267	valid_1's l2: 20.262
[1550]	training's l2: 17.606	valid_1's l2: 20.2608
[1560]	training's l2: 17.5864	valid_1's l2: 20.26
[1570]	training's l2: 17.5672	valid_1's l2: 20.2593
[1580]	training's l2: 17.5478	valid_1's l2: 20.2581
[1590]	training's l2: 17.528	valid_1's l2: 20.2573
[1600]	training's l2: 17.509	valid_1's l2: 20.2565
[1610]	training's l2: 17.4898	valid_1's l2: 20.2563
[1620]	training's l2: 17.47	valid_1's l2: 20.255
[1630]	training's l2: 17.4519	valid_1's l2: 20.2546
[1640]	training's l2: 17.432	valid_1's l2: 20.254
[1650]	training's l2: 17.413	valid_1's l2: 20.2538
[1660]	training's l2: 17.3938	valid_1's l2: 20.253
[1670]	training's l2: 17.3744	valid_1's l2: 20.253
[1680]	training's l2: 17.3568	valid_1's l2: 20.2525
[1690]	training's l2: 17.3384	valid_1's l2: 20.2521
[1700]	training's l2: 17.3199	valid_1's l2: 20.2523
[1710]	training's l2: 17.3006	valid_1's l2: 20.2525
[1720]	training's l2: 17.2817	valid_1's l2: 20.2522
Early stopping, best iteration is:
[1693]	training's l2: 17.3327	valid_1's l2: 20.2518
score1: 3.8346990673139714
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.275631 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.825	valid_1's l2: 30.9228
[20]	training's l2: 30.008	valid_1's l2: 29.1167
[30]	training's l2: 28.5418	valid_1's l2: 27.6613
[40]	training's l2: 27.3541	valid_1's l2: 26.4854
[50]	training's l2: 26.3853	valid_1's l2: 25.5295
[60]	training's l2: 25.5943	valid_1's l2: 24.7538
[70]	training's l2: 24.9439	valid_1's l2: 24.1219
[80]	training's l2: 24.406	valid_1's l2: 23.6045
[90]	training's l2: 23.9589	valid_1's l2: 23.1763
[100]	training's l2: 23.5846	valid_1's l2: 22.8253
[110]	training's l2: 23.2707	valid_1's l2: 22.5339
[120]	training's l2: 23.0066	valid_1's l2: 22.2909
[130]	training's l2: 22.7804	valid_1's l2: 22.0879
[140]	training's l2: 22.5824	valid_1's l2: 21.9156
[150]	training's l2: 22.4093	valid_1's l2: 21.7677
[160]	training's l2: 22.2566	valid_1's l2: 21.6398
[170]	training's l2: 22.1204	valid_1's l2: 21.531
[180]	training's l2: 21.9974	valid_1's l2: 21.4355
[190]	training's l2: 21.8886	valid_1's l2: 21.3535
[200]	training's l2: 21.7866	valid_1's l2: 21.28
[210]	training's l2: 21.6925	valid_1's l2: 21.2129
[220]	training's l2: 21.605	valid_1's l2: 21.1539
[230]	training's l2: 21.5247	valid_1's l2: 21.1003
[240]	training's l2: 21.4497	valid_1's l2: 21.0524
[250]	training's l2: 21.3788	valid_1's l2: 21.0082
[260]	training's l2: 21.3125	valid_1's l2: 20.9687
[270]	training's l2: 21.2501	valid_1's l2: 20.9317
[280]	training's l2: 21.1903	valid_1's l2: 20.8975
[290]	training's l2: 21.1312	valid_1's l2: 20.865
[300]	training's l2: 21.0742	valid_1's l2: 20.8345
[310]	training's l2: 21.0199	valid_1's l2: 20.8082
[320]	training's l2: 20.9676	valid_1's l2: 20.7834
[330]	training's l2: 20.9175	valid_1's l2: 20.76
[340]	training's l2: 20.8685	valid_1's l2: 20.7381
[350]	training's l2: 20.8231	valid_1's l2: 20.7198
[360]	training's l2: 20.7779	valid_1's l2: 20.7006
[370]	training's l2: 20.7334	valid_1's l2: 20.6836
[380]	training's l2: 20.6899	valid_1's l2: 20.6665
[390]	training's l2: 20.6486	valid_1's l2: 20.6514
[400]	training's l2: 20.6076	valid_1's l2: 20.6364
[410]	training's l2: 20.5677	valid_1's l2: 20.6237
[420]	training's l2: 20.5282	valid_1's l2: 20.6111
[430]	training's l2: 20.4896	valid_1's l2: 20.5995
[440]	training's l2: 20.4517	valid_1's l2: 20.5882
[450]	training's l2: 20.4138	valid_1's l2: 20.5765
[460]	training's l2: 20.3772	valid_1's l2: 20.5672
[470]	training's l2: 20.3402	valid_1's l2: 20.5567
[480]	training's l2: 20.3042	valid_1's l2: 20.5476
[490]	training's l2: 20.2685	valid_1's l2: 20.5387
[500]	training's l2: 20.2333	valid_1's l2: 20.5302
[510]	training's l2: 20.199	valid_1's l2: 20.5224
[520]	training's l2: 20.1638	valid_1's l2: 20.5139
[530]	training's l2: 20.1305	valid_1's l2: 20.5065
[540]	training's l2: 20.0962	valid_1's l2: 20.499
[550]	training's l2: 20.0633	valid_1's l2: 20.4928
[560]	training's l2: 20.0304	valid_1's l2: 20.4869
[570]	training's l2: 19.9983	valid_1's l2: 20.4804
[580]	training's l2: 19.9649	valid_1's l2: 20.473
[590]	training's l2: 19.9325	valid_1's l2: 20.4653
[600]	training's l2: 19.9002	valid_1's l2: 20.4579
[610]	training's l2: 19.8677	valid_1's l2: 20.4521
[620]	training's l2: 19.8357	valid_1's l2: 20.4458
[630]	training's l2: 19.8042	valid_1's l2: 20.4391
[640]	training's l2: 19.7726	valid_1's l2: 20.4331
[650]	training's l2: 19.7414	valid_1's l2: 20.4262
[660]	training's l2: 19.7106	valid_1's l2: 20.4208
[670]	training's l2: 19.6799	valid_1's l2: 20.4148
[680]	training's l2: 19.6497	valid_1's l2: 20.4094
[690]	training's l2: 19.6196	valid_1's l2: 20.4046
[700]	training's l2: 19.5898	valid_1's l2: 20.3994
[710]	training's l2: 19.56	valid_1's l2: 20.3946
[720]	training's l2: 19.5311	valid_1's l2: 20.3906
[730]	training's l2: 19.5018	valid_1's l2: 20.3858
[740]	training's l2: 19.4725	valid_1's l2: 20.3809
[750]	training's l2: 19.4433	valid_1's l2: 20.377
[760]	training's l2: 19.4142	valid_1's l2: 20.372
[770]	training's l2: 19.3859	valid_1's l2: 20.3693
[780]	training's l2: 19.3574	valid_1's l2: 20.366
[790]	training's l2: 19.3291	valid_1's l2: 20.3629
[800]	training's l2: 19.3008	valid_1's l2: 20.359
[810]	training's l2: 19.2722	valid_1's l2: 20.3552
[820]	training's l2: 19.2447	valid_1's l2: 20.3525
[830]	training's l2: 19.2169	valid_1's l2: 20.3488
[840]	training's l2: 19.1895	valid_1's l2: 20.346
[850]	training's l2: 19.1621	valid_1's l2: 20.3435
[860]	training's l2: 19.1348	valid_1's l2: 20.3398
[870]	training's l2: 19.1078	valid_1's l2: 20.3369
[880]	training's l2: 19.0805	valid_1's l2: 20.3349
[890]	training's l2: 19.0544	valid_1's l2: 20.3325
[900]	training's l2: 19.0276	valid_1's l2: 20.3297
[910]	training's l2: 19.0011	valid_1's l2: 20.3265
[920]	training's l2: 18.975	valid_1's l2: 20.3244
[930]	training's l2: 18.9483	valid_1's l2: 20.3209
[940]	training's l2: 18.9226	valid_1's l2: 20.3182
[950]	training's l2: 18.8963	valid_1's l2: 20.3155
[960]	training's l2: 18.8698	valid_1's l2: 20.313
[970]	training's l2: 18.8439	valid_1's l2: 20.3113
[980]	training's l2: 18.8183	valid_1's l2: 20.3088
[990]	training's l2: 18.7929	valid_1's l2: 20.3064
[1000]	training's l2: 18.7679	valid_1's l2: 20.3045
[1010]	training's l2: 18.7424	valid_1's l2: 20.3018
[1020]	training's l2: 18.7166	valid_1's l2: 20.299
[1030]	training's l2: 18.6918	valid_1's l2: 20.297
[1040]	training's l2: 18.6669	valid_1's l2: 20.2945
[1050]	training's l2: 18.6423	valid_1's l2: 20.2921
[1060]	training's l2: 18.6178	valid_1's l2: 20.2904
[1070]	training's l2: 18.593	valid_1's l2: 20.2883
[1080]	training's l2: 18.5686	valid_1's l2: 20.2867
[1090]	training's l2: 18.544	valid_1's l2: 20.2853
[1100]	training's l2: 18.5202	valid_1's l2: 20.2834
[1110]	training's l2: 18.4964	valid_1's l2: 20.2826
[1120]	training's l2: 18.4728	valid_1's l2: 20.2812
[1130]	training's l2: 18.4487	valid_1's l2: 20.2794
[1140]	training's l2: 18.4256	valid_1's l2: 20.2787
[1150]	training's l2: 18.4022	valid_1's l2: 20.2774
[1160]	training's l2: 18.3791	valid_1's l2: 20.2762
[1170]	training's l2: 18.3556	valid_1's l2: 20.2758
[1180]	training's l2: 18.3321	valid_1's l2: 20.2742
[1190]	training's l2: 18.309	valid_1's l2: 20.2733
[1200]	training's l2: 18.2858	valid_1's l2: 20.272
[1210]	training's l2: 18.2632	valid_1's l2: 20.2706
[1220]	training's l2: 18.2408	valid_1's l2: 20.2695
[1230]	training's l2: 18.2183	valid_1's l2: 20.2684
[1240]	training's l2: 18.1959	valid_1's l2: 20.2678
[1250]	training's l2: 18.1735	valid_1's l2: 20.2666
[1260]	training's l2: 18.1519	valid_1's l2: 20.2655
[1270]	training's l2: 18.1297	valid_1's l2: 20.2641
[1280]	training's l2: 18.1074	valid_1's l2: 20.2633
[1290]	training's l2: 18.0861	valid_1's l2: 20.2627
[1300]	training's l2: 18.0636	valid_1's l2: 20.2622
[1310]	training's l2: 18.0421	valid_1's l2: 20.2607
[1320]	training's l2: 18.021	valid_1's l2: 20.2594
[1330]	training's l2: 17.9995	valid_1's l2: 20.2582
[1340]	training's l2: 17.9781	valid_1's l2: 20.2576
[1350]	training's l2: 17.9558	valid_1's l2: 20.256
[1360]	training's l2: 17.9352	valid_1's l2: 20.255
[1370]	training's l2: 17.9143	valid_1's l2: 20.2537
[1380]	training's l2: 17.8923	valid_1's l2: 20.2526
[1390]	training's l2: 17.872	valid_1's l2: 20.2514
[1400]	training's l2: 17.8509	valid_1's l2: 20.2509
[1410]	training's l2: 17.8302	valid_1's l2: 20.2504
[1420]	training's l2: 17.8088	valid_1's l2: 20.2499
[1430]	training's l2: 17.7881	valid_1's l2: 20.2487
[1440]	training's l2: 17.7673	valid_1's l2: 20.2485
[1450]	training's l2: 17.7468	valid_1's l2: 20.2489
[1460]	training's l2: 17.726	valid_1's l2: 20.2486
[1470]	training's l2: 17.7053	valid_1's l2: 20.2476
[1480]	training's l2: 17.685	valid_1's l2: 20.2474
[1490]	training's l2: 17.6652	valid_1's l2: 20.2477
[1500]	training's l2: 17.6458	valid_1's l2: 20.2469
[1510]	training's l2: 17.6253	valid_1's l2: 20.246
[1520]	training's l2: 17.6053	valid_1's l2: 20.2455
[1530]	training's l2: 17.587	valid_1's l2: 20.2456
[1540]	training's l2: 17.5667	valid_1's l2: 20.2449
[1550]	training's l2: 17.5462	valid_1's l2: 20.2445
[1560]	training's l2: 17.5261	valid_1's l2: 20.2444
[1570]	training's l2: 17.5069	valid_1's l2: 20.2438
[1580]	training's l2: 17.4864	valid_1's l2: 20.2432
[1590]	training's l2: 17.4661	valid_1's l2: 20.2432
[1600]	training's l2: 17.4463	valid_1's l2: 20.242
[1610]	training's l2: 17.4263	valid_1's l2: 20.2419
[1620]	training's l2: 17.4072	valid_1's l2: 20.2419
[1630]	training's l2: 17.388	valid_1's l2: 20.2418
[1640]	training's l2: 17.3681	valid_1's l2: 20.2414
[1650]	training's l2: 17.3487	valid_1's l2: 20.2404
[1660]	training's l2: 17.3296	valid_1's l2: 20.2403
[1670]	training's l2: 17.3107	valid_1's l2: 20.2398
[1680]	training's l2: 17.2914	valid_1's l2: 20.2394
[1690]	training's l2: 17.2726	valid_1's l2: 20.2391
[1700]	training's l2: 17.2544	valid_1's l2: 20.2387
[1710]	training's l2: 17.2345	valid_1's l2: 20.2376
[1720]	training's l2: 17.2155	valid_1's l2: 20.2375
[1730]	training's l2: 17.197	valid_1's l2: 20.2372
[1740]	training's l2: 17.1781	valid_1's l2: 20.237
[1750]	training's l2: 17.1595	valid_1's l2: 20.2363
[1760]	training's l2: 17.1406	valid_1's l2: 20.2357
[1770]	training's l2: 17.1215	valid_1's l2: 20.2359
[1780]	training's l2: 17.1021	valid_1's l2: 20.2361
Early stopping, best iteration is:
[1758]	training's l2: 17.1445	valid_1's l2: 20.2355
score1: 3.8348319942478177
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.262676 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.1998	valid_1's l2: 30.3008
[20]	training's l2: 29.0311	valid_1's l2: 28.1474
[30]	training's l2: 27.3924	valid_1's l2: 26.5234
[40]	training's l2: 26.1444	valid_1's l2: 25.2954
[50]	training's l2: 25.1828	valid_1's l2: 24.3542
[60]	training's l2: 24.4397	valid_1's l2: 23.6379
[70]	training's l2: 23.8573	valid_1's l2: 23.083
[80]	training's l2: 23.3946	valid_1's l2: 22.6494
[90]	training's l2: 23.0284	valid_1's l2: 22.3112
[100]	training's l2: 22.7308	valid_1's l2: 22.0448
[110]	training's l2: 22.4826	valid_1's l2: 21.8303
[120]	training's l2: 22.2726	valid_1's l2: 21.6532
[130]	training's l2: 22.0935	valid_1's l2: 21.5098
[140]	training's l2: 21.9358	valid_1's l2: 21.3904
[150]	training's l2: 21.7982	valid_1's l2: 21.2902
[160]	training's l2: 21.6736	valid_1's l2: 21.2026
[170]	training's l2: 21.5587	valid_1's l2: 21.1244
[180]	training's l2: 21.4578	valid_1's l2: 21.0596
[190]	training's l2: 21.3616	valid_1's l2: 20.9997
[200]	training's l2: 21.2747	valid_1's l2: 20.9479
[210]	training's l2: 21.1942	valid_1's l2: 20.903
[220]	training's l2: 21.1153	valid_1's l2: 20.8589
[230]	training's l2: 21.0425	valid_1's l2: 20.822
[240]	training's l2: 20.9717	valid_1's l2: 20.7879
[250]	training's l2: 20.9043	valid_1's l2: 20.7586
[260]	training's l2: 20.8403	valid_1's l2: 20.7296
[270]	training's l2: 20.7809	valid_1's l2: 20.7069
[280]	training's l2: 20.7215	valid_1's l2: 20.6828
[290]	training's l2: 20.6648	valid_1's l2: 20.6628
[300]	training's l2: 20.6097	valid_1's l2: 20.6419
[310]	training's l2: 20.556	valid_1's l2: 20.6244
[320]	training's l2: 20.5028	valid_1's l2: 20.6074
[330]	training's l2: 20.4518	valid_1's l2: 20.5911
[340]	training's l2: 20.4017	valid_1's l2: 20.5753
[350]	training's l2: 20.3524	valid_1's l2: 20.562
[360]	training's l2: 20.304	valid_1's l2: 20.5495
[370]	training's l2: 20.2568	valid_1's l2: 20.5373
[380]	training's l2: 20.2103	valid_1's l2: 20.5267
[390]	training's l2: 20.1635	valid_1's l2: 20.5161
[400]	training's l2: 20.1176	valid_1's l2: 20.5051
[410]	training's l2: 20.0739	valid_1's l2: 20.4962
[420]	training's l2: 20.0291	valid_1's l2: 20.4879
[430]	training's l2: 19.9848	valid_1's l2: 20.4787
[440]	training's l2: 19.9421	valid_1's l2: 20.471
[450]	training's l2: 19.8993	valid_1's l2: 20.4629
[460]	training's l2: 19.8571	valid_1's l2: 20.4562
[470]	training's l2: 19.8142	valid_1's l2: 20.4482
[480]	training's l2: 19.772	valid_1's l2: 20.4409
[490]	training's l2: 19.7298	valid_1's l2: 20.4329
[500]	training's l2: 19.6886	valid_1's l2: 20.426
[510]	training's l2: 19.6477	valid_1's l2: 20.4199
[520]	training's l2: 19.6076	valid_1's l2: 20.4139
[530]	training's l2: 19.5683	valid_1's l2: 20.4068
[540]	training's l2: 19.5287	valid_1's l2: 20.4002
[550]	training's l2: 19.4897	valid_1's l2: 20.3939
[560]	training's l2: 19.4502	valid_1's l2: 20.3882
[570]	training's l2: 19.4114	valid_1's l2: 20.3834
[580]	training's l2: 19.3733	valid_1's l2: 20.3766
[590]	training's l2: 19.3353	valid_1's l2: 20.3722
[600]	training's l2: 19.2973	valid_1's l2: 20.3677
[610]	training's l2: 19.2593	valid_1's l2: 20.3637
[620]	training's l2: 19.2229	valid_1's l2: 20.36
[630]	training's l2: 19.1863	valid_1's l2: 20.3553
[640]	training's l2: 19.1505	valid_1's l2: 20.3518
[650]	training's l2: 19.1142	valid_1's l2: 20.3473
[660]	training's l2: 19.0775	valid_1's l2: 20.3432
[670]	training's l2: 19.0417	valid_1's l2: 20.3389
[680]	training's l2: 19.0062	valid_1's l2: 20.3353
[690]	training's l2: 18.971	valid_1's l2: 20.3327
[700]	training's l2: 18.9365	valid_1's l2: 20.331
[710]	training's l2: 18.9021	valid_1's l2: 20.3285
[720]	training's l2: 18.8675	valid_1's l2: 20.3267
[730]	training's l2: 18.8331	valid_1's l2: 20.3225
[740]	training's l2: 18.7989	valid_1's l2: 20.3207
[750]	training's l2: 18.7651	valid_1's l2: 20.3176
[760]	training's l2: 18.7318	valid_1's l2: 20.3161
[770]	training's l2: 18.6983	valid_1's l2: 20.3138
[780]	training's l2: 18.6651	valid_1's l2: 20.311
[790]	training's l2: 18.6322	valid_1's l2: 20.3075
[800]	training's l2: 18.5987	valid_1's l2: 20.3041
[810]	training's l2: 18.566	valid_1's l2: 20.3021
[820]	training's l2: 18.5348	valid_1's l2: 20.301
[830]	training's l2: 18.5039	valid_1's l2: 20.2991
[840]	training's l2: 18.4718	valid_1's l2: 20.2966
[850]	training's l2: 18.4393	valid_1's l2: 20.2942
[860]	training's l2: 18.4078	valid_1's l2: 20.2917
[870]	training's l2: 18.3765	valid_1's l2: 20.2895
[880]	training's l2: 18.3461	valid_1's l2: 20.287
[890]	training's l2: 18.3151	valid_1's l2: 20.2855
[900]	training's l2: 18.2844	valid_1's l2: 20.2831
[910]	training's l2: 18.2534	valid_1's l2: 20.2819
[920]	training's l2: 18.2223	valid_1's l2: 20.2806
[930]	training's l2: 18.1926	valid_1's l2: 20.2796
[940]	training's l2: 18.163	valid_1's l2: 20.2786
[950]	training's l2: 18.1336	valid_1's l2: 20.2764
[960]	training's l2: 18.104	valid_1's l2: 20.2755
[970]	training's l2: 18.0751	valid_1's l2: 20.274
[980]	training's l2: 18.0461	valid_1's l2: 20.2735
[990]	training's l2: 18.0176	valid_1's l2: 20.2728
[1000]	training's l2: 17.9895	valid_1's l2: 20.2732
[1010]	training's l2: 17.9604	valid_1's l2: 20.2719
[1020]	training's l2: 17.9313	valid_1's l2: 20.2703
[1030]	training's l2: 17.9025	valid_1's l2: 20.269
[1040]	training's l2: 17.875	valid_1's l2: 20.2678
[1050]	training's l2: 17.8471	valid_1's l2: 20.2671
[1060]	training's l2: 17.8193	valid_1's l2: 20.2672
[1070]	training's l2: 17.7916	valid_1's l2: 20.2665
[1080]	training's l2: 17.7639	valid_1's l2: 20.2646
[1090]	training's l2: 17.7357	valid_1's l2: 20.2637
[1100]	training's l2: 17.7091	valid_1's l2: 20.2627
[1110]	training's l2: 17.6826	valid_1's l2: 20.2618
[1120]	training's l2: 17.6556	valid_1's l2: 20.2623
[1130]	training's l2: 17.6277	valid_1's l2: 20.2616
[1140]	training's l2: 17.6023	valid_1's l2: 20.2618
[1150]	training's l2: 17.5765	valid_1's l2: 20.2605
[1160]	training's l2: 17.5493	valid_1's l2: 20.2589
[1170]	training's l2: 17.5231	valid_1's l2: 20.2591
[1180]	training's l2: 17.4975	valid_1's l2: 20.2594
[1190]	training's l2: 17.4718	valid_1's l2: 20.2591
[1200]	training's l2: 17.4456	valid_1's l2: 20.2593
[1210]	training's l2: 17.419	valid_1's l2: 20.2595
[1220]	training's l2: 17.393	valid_1's l2: 20.2595
Early stopping, best iteration is:
[1198]	training's l2: 17.4509	valid_1's l2: 20.2586
score1: 3.837726915218133
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.253101 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.5335	valid_1's l2: 30.6296
[20]	training's l2: 29.5461	valid_1's l2: 28.6537
[30]	training's l2: 27.9914	valid_1's l2: 27.1123
[40]	training's l2: 26.7678	valid_1's l2: 25.9015
[50]	training's l2: 25.8002	valid_1's l2: 24.9493
[60]	training's l2: 25.0274	valid_1's l2: 24.1941
[70]	training's l2: 24.4075	valid_1's l2: 23.5943
[80]	training's l2: 23.9069	valid_1's l2: 23.1159
[90]	training's l2: 23.498	valid_1's l2: 22.7316
[100]	training's l2: 23.1646	valid_1's l2: 22.4216
[110]	training's l2: 22.8888	valid_1's l2: 22.1697
[120]	training's l2: 22.6545	valid_1's l2: 21.9613
[130]	training's l2: 22.4534	valid_1's l2: 21.7897
[140]	training's l2: 22.2801	valid_1's l2: 21.6438
[150]	training's l2: 22.1282	valid_1's l2: 21.5201
[160]	training's l2: 21.9928	valid_1's l2: 21.4153
[170]	training's l2: 21.8745	valid_1's l2: 21.3269
[180]	training's l2: 21.7626	valid_1's l2: 21.2466
[190]	training's l2: 21.6613	valid_1's l2: 21.1747
[200]	training's l2: 21.5705	valid_1's l2: 21.1128
[210]	training's l2: 21.4852	valid_1's l2: 21.0567
[220]	training's l2: 21.4064	valid_1's l2: 21.0052
[230]	training's l2: 21.3343	valid_1's l2: 20.9622
[240]	training's l2: 21.2665	valid_1's l2: 20.9236
[250]	training's l2: 21.1993	valid_1's l2: 20.8857
[260]	training's l2: 21.1353	valid_1's l2: 20.8496
[270]	training's l2: 21.0745	valid_1's l2: 20.8188
[280]	training's l2: 21.0158	valid_1's l2: 20.7895
[290]	training's l2: 20.9602	valid_1's l2: 20.7638
[300]	training's l2: 20.9074	valid_1's l2: 20.7408
[310]	training's l2: 20.856	valid_1's l2: 20.7193
[320]	training's l2: 20.8074	valid_1's l2: 20.6996
[330]	training's l2: 20.759	valid_1's l2: 20.6802
[340]	training's l2: 20.7129	valid_1's l2: 20.664
[350]	training's l2: 20.6678	valid_1's l2: 20.6481
[360]	training's l2: 20.6226	valid_1's l2: 20.6314
[370]	training's l2: 20.5794	valid_1's l2: 20.6179
[380]	training's l2: 20.5361	valid_1's l2: 20.6037
[390]	training's l2: 20.4945	valid_1's l2: 20.5904
[400]	training's l2: 20.4533	valid_1's l2: 20.5766
[410]	training's l2: 20.4125	valid_1's l2: 20.5657
[420]	training's l2: 20.3728	valid_1's l2: 20.5539
[430]	training's l2: 20.3334	valid_1's l2: 20.543
[440]	training's l2: 20.2941	valid_1's l2: 20.5327
[450]	training's l2: 20.2563	valid_1's l2: 20.5231
[460]	training's l2: 20.2193	valid_1's l2: 20.5145
[470]	training's l2: 20.1813	valid_1's l2: 20.5058
[480]	training's l2: 20.1438	valid_1's l2: 20.4976
[490]	training's l2: 20.1076	valid_1's l2: 20.4913
[500]	training's l2: 20.0711	valid_1's l2: 20.483
[510]	training's l2: 20.0359	valid_1's l2: 20.4756
[520]	training's l2: 20.0005	valid_1's l2: 20.468
[530]	training's l2: 19.9649	valid_1's l2: 20.4603
[540]	training's l2: 19.9293	valid_1's l2: 20.4526
[550]	training's l2: 19.8943	valid_1's l2: 20.4461
[560]	training's l2: 19.8591	valid_1's l2: 20.4389
[570]	training's l2: 19.8248	valid_1's l2: 20.4338
[580]	training's l2: 19.7901	valid_1's l2: 20.4271
[590]	training's l2: 19.7566	valid_1's l2: 20.4198
[600]	training's l2: 19.7236	valid_1's l2: 20.4144
[610]	training's l2: 19.6896	valid_1's l2: 20.4082
[620]	training's l2: 19.6566	valid_1's l2: 20.4028
[630]	training's l2: 19.6236	valid_1's l2: 20.3979
[640]	training's l2: 19.5909	valid_1's l2: 20.3923
[650]	training's l2: 19.5586	valid_1's l2: 20.388
[660]	training's l2: 19.5264	valid_1's l2: 20.3829
[670]	training's l2: 19.4952	valid_1's l2: 20.3786
[680]	training's l2: 19.4627	valid_1's l2: 20.373
[690]	training's l2: 19.4315	valid_1's l2: 20.3704
[700]	training's l2: 19.4003	valid_1's l2: 20.3656
[710]	training's l2: 19.3697	valid_1's l2: 20.3614
[720]	training's l2: 19.3392	valid_1's l2: 20.3588
[730]	training's l2: 19.3079	valid_1's l2: 20.3543
[740]	training's l2: 19.2773	valid_1's l2: 20.3502
[750]	training's l2: 19.2471	valid_1's l2: 20.3464
[760]	training's l2: 19.2173	valid_1's l2: 20.3434
[770]	training's l2: 19.1872	valid_1's l2: 20.3403
[780]	training's l2: 19.1574	valid_1's l2: 20.3358
[790]	training's l2: 19.1279	valid_1's l2: 20.3323
[800]	training's l2: 19.0985	valid_1's l2: 20.3292
[810]	training's l2: 19.0696	valid_1's l2: 20.3262
[820]	training's l2: 19.0403	valid_1's l2: 20.3237
[830]	training's l2: 19.0113	valid_1's l2: 20.3216
[840]	training's l2: 18.9828	valid_1's l2: 20.3189
[850]	training's l2: 18.9539	valid_1's l2: 20.3169
[860]	training's l2: 18.9251	valid_1's l2: 20.3142
[870]	training's l2: 18.897	valid_1's l2: 20.3112
[880]	training's l2: 18.8687	valid_1's l2: 20.3081
[890]	training's l2: 18.8408	valid_1's l2: 20.3047
[900]	training's l2: 18.8136	valid_1's l2: 20.3025
[910]	training's l2: 18.7858	valid_1's l2: 20.3004
[920]	training's l2: 18.7585	valid_1's l2: 20.2983
[930]	training's l2: 18.7315	valid_1's l2: 20.2958
[940]	training's l2: 18.7048	valid_1's l2: 20.2939
[950]	training's l2: 18.6776	valid_1's l2: 20.292
[960]	training's l2: 18.6517	valid_1's l2: 20.2915
[970]	training's l2: 18.6257	valid_1's l2: 20.2901
[980]	training's l2: 18.6001	valid_1's l2: 20.289
[990]	training's l2: 18.5738	valid_1's l2: 20.2878
[1000]	training's l2: 18.548	valid_1's l2: 20.286
[1010]	training's l2: 18.5226	valid_1's l2: 20.2839
[1020]	training's l2: 18.4967	valid_1's l2: 20.281
[1030]	training's l2: 18.4702	valid_1's l2: 20.2796
[1040]	training's l2: 18.4452	valid_1's l2: 20.2787
[1050]	training's l2: 18.42	valid_1's l2: 20.2774
[1060]	training's l2: 18.3947	valid_1's l2: 20.2752
[1070]	training's l2: 18.3694	valid_1's l2: 20.274
[1080]	training's l2: 18.3448	valid_1's l2: 20.2737
[1090]	training's l2: 18.3207	valid_1's l2: 20.2731
[1100]	training's l2: 18.2959	valid_1's l2: 20.2715
[1110]	training's l2: 18.2713	valid_1's l2: 20.2706
[1120]	training's l2: 18.2472	valid_1's l2: 20.2691
[1130]	training's l2: 18.2229	valid_1's l2: 20.2674
[1140]	training's l2: 18.1985	valid_1's l2: 20.2668
[1150]	training's l2: 18.1746	valid_1's l2: 20.2655
[1160]	training's l2: 18.1504	valid_1's l2: 20.2637
[1170]	training's l2: 18.1263	valid_1's l2: 20.2629
[1180]	training's l2: 18.1021	valid_1's l2: 20.2619
[1190]	training's l2: 18.0782	valid_1's l2: 20.2603
[1200]	training's l2: 18.0557	valid_1's l2: 20.2596
[1210]	training's l2: 18.0319	valid_1's l2: 20.259
[1220]	training's l2: 18.0079	valid_1's l2: 20.2591
[1230]	training's l2: 17.9857	valid_1's l2: 20.2578
[1240]	training's l2: 17.963	valid_1's l2: 20.2578
[1250]	training's l2: 17.9403	valid_1's l2: 20.2571
[1260]	training's l2: 17.9171	valid_1's l2: 20.2572
[1270]	training's l2: 17.895	valid_1's l2: 20.2561
[1280]	training's l2: 17.8729	valid_1's l2: 20.2553
[1290]	training's l2: 17.8509	valid_1's l2: 20.2551
[1300]	training's l2: 17.8297	valid_1's l2: 20.2539
[1310]	training's l2: 17.8072	valid_1's l2: 20.2533
[1320]	training's l2: 17.7851	valid_1's l2: 20.2532
[1330]	training's l2: 17.7626	valid_1's l2: 20.2532
[1340]	training's l2: 17.7411	valid_1's l2: 20.2523
[1350]	training's l2: 17.7183	valid_1's l2: 20.2513
[1360]	training's l2: 17.6957	valid_1's l2: 20.2502
[1370]	training's l2: 17.6742	valid_1's l2: 20.2501
[1380]	training's l2: 17.6518	valid_1's l2: 20.2497
[1390]	training's l2: 17.6315	valid_1's l2: 20.2489
[1400]	training's l2: 17.6094	valid_1's l2: 20.2483
[1410]	training's l2: 17.5873	valid_1's l2: 20.2471
[1420]	training's l2: 17.5653	valid_1's l2: 20.2466
[1430]	training's l2: 17.5442	valid_1's l2: 20.246
[1440]	training's l2: 17.5219	valid_1's l2: 20.2454
[1450]	training's l2: 17.5004	valid_1's l2: 20.2442
[1460]	training's l2: 17.479	valid_1's l2: 20.2447
[1470]	training's l2: 17.4588	valid_1's l2: 20.2444
[1480]	training's l2: 17.437	valid_1's l2: 20.2437
[1490]	training's l2: 17.4169	valid_1's l2: 20.2433
[1500]	training's l2: 17.3969	valid_1's l2: 20.2429
[1510]	training's l2: 17.3758	valid_1's l2: 20.2423
[1520]	training's l2: 17.3561	valid_1's l2: 20.2424
[1530]	training's l2: 17.3355	valid_1's l2: 20.2418
[1540]	training's l2: 17.3149	valid_1's l2: 20.2422
[1550]	training's l2: 17.2945	valid_1's l2: 20.242
[1560]	training's l2: 17.2734	valid_1's l2: 20.2425
[1570]	training's l2: 17.2521	valid_1's l2: 20.2419
Early stopping, best iteration is:
[1543]	training's l2: 17.3082	valid_1's l2: 20.2415
score1: 3.836918996769783
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.246084 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.9009	valid_1's l2: 30.9984
[20]	training's l2: 30.1312	valid_1's l2: 29.2385
[30]	training's l2: 28.6915	valid_1's l2: 27.8102
[40]	training's l2: 27.5159	valid_1's l2: 26.6455
[50]	training's l2: 26.5523	valid_1's l2: 25.6944
[60]	training's l2: 25.7598	valid_1's l2: 24.9157
[70]	training's l2: 25.1025	valid_1's l2: 24.2755
[80]	training's l2: 24.5571	valid_1's l2: 23.7489
[90]	training's l2: 24.1014	valid_1's l2: 23.3131
[100]	training's l2: 23.7175	valid_1's l2: 22.9499
[110]	training's l2: 23.3937	valid_1's l2: 22.6485
[120]	training's l2: 23.1207	valid_1's l2: 22.3964
[130]	training's l2: 22.8875	valid_1's l2: 22.1855
[140]	training's l2: 22.6841	valid_1's l2: 22.0048
[150]	training's l2: 22.5058	valid_1's l2: 21.8514
[160]	training's l2: 22.3465	valid_1's l2: 21.7168
[170]	training's l2: 22.2064	valid_1's l2: 21.6008
[180]	training's l2: 22.0806	valid_1's l2: 21.5006
[190]	training's l2: 21.9672	valid_1's l2: 21.4138
[200]	training's l2: 21.8639	valid_1's l2: 21.3369
[210]	training's l2: 21.7683	valid_1's l2: 21.2687
[220]	training's l2: 21.6793	valid_1's l2: 21.2062
[230]	training's l2: 21.5946	valid_1's l2: 21.1475
[240]	training's l2: 21.5192	valid_1's l2: 21.0971
[250]	training's l2: 21.4472	valid_1's l2: 21.0511
[260]	training's l2: 21.3787	valid_1's l2: 21.0079
[270]	training's l2: 21.3149	valid_1's l2: 20.9694
[280]	training's l2: 21.2549	valid_1's l2: 20.9353
[290]	training's l2: 21.1986	valid_1's l2: 20.904
[300]	training's l2: 21.1413	valid_1's l2: 20.8714
[310]	training's l2: 21.0864	valid_1's l2: 20.8426
[320]	training's l2: 21.0337	valid_1's l2: 20.8153
[330]	training's l2: 20.983	valid_1's l2: 20.7915
[340]	training's l2: 20.9335	valid_1's l2: 20.768
[350]	training's l2: 20.8871	valid_1's l2: 20.7483
[360]	training's l2: 20.8412	valid_1's l2: 20.7285
[370]	training's l2: 20.7978	valid_1's l2: 20.7109
[380]	training's l2: 20.7557	valid_1's l2: 20.6939
[390]	training's l2: 20.7137	valid_1's l2: 20.6776
[400]	training's l2: 20.6741	valid_1's l2: 20.664
[410]	training's l2: 20.6349	valid_1's l2: 20.6509
[420]	training's l2: 20.5959	valid_1's l2: 20.6372
[430]	training's l2: 20.5572	valid_1's l2: 20.6233
[440]	training's l2: 20.5191	valid_1's l2: 20.6103
[450]	training's l2: 20.4815	valid_1's l2: 20.5983
[460]	training's l2: 20.4447	valid_1's l2: 20.587
[470]	training's l2: 20.4082	valid_1's l2: 20.5757
[480]	training's l2: 20.3732	valid_1's l2: 20.5667
[490]	training's l2: 20.3393	valid_1's l2: 20.5575
[500]	training's l2: 20.3043	valid_1's l2: 20.5477
[510]	training's l2: 20.27	valid_1's l2: 20.5393
[520]	training's l2: 20.2363	valid_1's l2: 20.5313
[530]	training's l2: 20.2029	valid_1's l2: 20.5233
[540]	training's l2: 20.1702	valid_1's l2: 20.5158
[550]	training's l2: 20.1376	valid_1's l2: 20.5086
[560]	training's l2: 20.1054	valid_1's l2: 20.5024
[570]	training's l2: 20.0731	valid_1's l2: 20.4961
[580]	training's l2: 20.0412	valid_1's l2: 20.4902
[590]	training's l2: 20.0094	valid_1's l2: 20.4844
[600]	training's l2: 19.9787	valid_1's l2: 20.4783
[610]	training's l2: 19.9475	valid_1's l2: 20.4724
[620]	training's l2: 19.9167	valid_1's l2: 20.4668
[630]	training's l2: 19.8861	valid_1's l2: 20.4617
[640]	training's l2: 19.8552	valid_1's l2: 20.456
[650]	training's l2: 19.8245	valid_1's l2: 20.45
[660]	training's l2: 19.7942	valid_1's l2: 20.4444
[670]	training's l2: 19.7643	valid_1's l2: 20.4388
[680]	training's l2: 19.7345	valid_1's l2: 20.4324
[690]	training's l2: 19.7046	valid_1's l2: 20.4268
[700]	training's l2: 19.6752	valid_1's l2: 20.4205
[710]	training's l2: 19.6457	valid_1's l2: 20.4149
[720]	training's l2: 19.6168	valid_1's l2: 20.4093
[730]	training's l2: 19.5879	valid_1's l2: 20.4042
[740]	training's l2: 19.5591	valid_1's l2: 20.3994
[750]	training's l2: 19.531	valid_1's l2: 20.3945
[760]	training's l2: 19.5029	valid_1's l2: 20.3905
[770]	training's l2: 19.4747	valid_1's l2: 20.3861
[780]	training's l2: 19.447	valid_1's l2: 20.3837
[790]	training's l2: 19.4184	valid_1's l2: 20.3803
[800]	training's l2: 19.3908	valid_1's l2: 20.3761
[810]	training's l2: 19.3631	valid_1's l2: 20.3726
[820]	training's l2: 19.336	valid_1's l2: 20.3692
[830]	training's l2: 19.3091	valid_1's l2: 20.3665
[840]	training's l2: 19.2824	valid_1's l2: 20.3631
[850]	training's l2: 19.2556	valid_1's l2: 20.36
[860]	training's l2: 19.2288	valid_1's l2: 20.3565
[870]	training's l2: 19.2026	valid_1's l2: 20.3554
[880]	training's l2: 19.1756	valid_1's l2: 20.3526
[890]	training's l2: 19.1492	valid_1's l2: 20.3502
[900]	training's l2: 19.123	valid_1's l2: 20.3471
[910]	training's l2: 19.0969	valid_1's l2: 20.3443
[920]	training's l2: 19.0707	valid_1's l2: 20.3407
[930]	training's l2: 19.0449	valid_1's l2: 20.3384
[940]	training's l2: 19.0191	valid_1's l2: 20.3354
[950]	training's l2: 18.9937	valid_1's l2: 20.3328
[960]	training's l2: 18.9689	valid_1's l2: 20.3308
[970]	training's l2: 18.9438	valid_1's l2: 20.3286
[980]	training's l2: 18.9189	valid_1's l2: 20.3259
[990]	training's l2: 18.8944	valid_1's l2: 20.3242
[1000]	training's l2: 18.8693	valid_1's l2: 20.3222
[1010]	training's l2: 18.8445	valid_1's l2: 20.3194
[1020]	training's l2: 18.8199	valid_1's l2: 20.3171
[1030]	training's l2: 18.7945	valid_1's l2: 20.3146
[1040]	training's l2: 18.7704	valid_1's l2: 20.3122
[1050]	training's l2: 18.746	valid_1's l2: 20.3099
[1060]	training's l2: 18.7217	valid_1's l2: 20.3079
[1070]	training's l2: 18.697	valid_1's l2: 20.3054
[1080]	training's l2: 18.673	valid_1's l2: 20.3034
[1090]	training's l2: 18.6493	valid_1's l2: 20.3014
[1100]	training's l2: 18.6255	valid_1's l2: 20.2996
[1110]	training's l2: 18.6015	valid_1's l2: 20.2979
[1120]	training's l2: 18.5776	valid_1's l2: 20.296
[1130]	training's l2: 18.5542	valid_1's l2: 20.2943
[1140]	training's l2: 18.5311	valid_1's l2: 20.2933
[1150]	training's l2: 18.508	valid_1's l2: 20.2915
[1160]	training's l2: 18.4856	valid_1's l2: 20.2895
[1170]	training's l2: 18.463	valid_1's l2: 20.2879
[1180]	training's l2: 18.4402	valid_1's l2: 20.2868
[1190]	training's l2: 18.4179	valid_1's l2: 20.2861
[1200]	training's l2: 18.3955	valid_1's l2: 20.2843
[1210]	training's l2: 18.3728	valid_1's l2: 20.2823
[1220]	training's l2: 18.3508	valid_1's l2: 20.2806
[1230]	training's l2: 18.3288	valid_1's l2: 20.2789
[1240]	training's l2: 18.3067	valid_1's l2: 20.278
[1250]	training's l2: 18.2847	valid_1's l2: 20.2766
[1260]	training's l2: 18.2636	valid_1's l2: 20.2761
[1270]	training's l2: 18.2418	valid_1's l2: 20.2754
[1280]	training's l2: 18.2204	valid_1's l2: 20.2747
[1290]	training's l2: 18.1995	valid_1's l2: 20.2744
[1300]	training's l2: 18.1773	valid_1's l2: 20.2733
[1310]	training's l2: 18.1555	valid_1's l2: 20.2727
[1320]	training's l2: 18.1345	valid_1's l2: 20.2719
[1330]	training's l2: 18.1143	valid_1's l2: 20.2719
[1340]	training's l2: 18.0937	valid_1's l2: 20.2708
[1350]	training's l2: 18.0723	valid_1's l2: 20.2691
[1360]	training's l2: 18.0519	valid_1's l2: 20.2691
[1370]	training's l2: 18.0312	valid_1's l2: 20.2686
[1380]	training's l2: 18.0102	valid_1's l2: 20.2682
[1390]	training's l2: 17.989	valid_1's l2: 20.2669
[1400]	training's l2: 17.969	valid_1's l2: 20.2659
[1410]	training's l2: 17.949	valid_1's l2: 20.2646
[1420]	training's l2: 17.9285	valid_1's l2: 20.2638
[1430]	training's l2: 17.9075	valid_1's l2: 20.263
[1440]	training's l2: 17.888	valid_1's l2: 20.2627
[1450]	training's l2: 17.8678	valid_1's l2: 20.2622
[1460]	training's l2: 17.8484	valid_1's l2: 20.2614
[1470]	training's l2: 17.8282	valid_1's l2: 20.2611
[1480]	training's l2: 17.8095	valid_1's l2: 20.2603
[1490]	training's l2: 17.7893	valid_1's l2: 20.259
[1500]	training's l2: 17.7696	valid_1's l2: 20.2581
[1510]	training's l2: 17.7507	valid_1's l2: 20.2572
[1520]	training's l2: 17.7302	valid_1's l2: 20.2569
[1530]	training's l2: 17.7103	valid_1's l2: 20.2568
[1540]	training's l2: 17.6906	valid_1's l2: 20.2567
[1550]	training's l2: 17.6718	valid_1's l2: 20.2563
[1560]	training's l2: 17.6524	valid_1's l2: 20.256
[1570]	training's l2: 17.634	valid_1's l2: 20.2556
[1580]	training's l2: 17.6148	valid_1's l2: 20.2552
[1590]	training's l2: 17.595	valid_1's l2: 20.2543
[1600]	training's l2: 17.5762	valid_1's l2: 20.2532
[1610]	training's l2: 17.5576	valid_1's l2: 20.2525
[1620]	training's l2: 17.5376	valid_1's l2: 20.2519
[1630]	training's l2: 17.519	valid_1's l2: 20.2516
[1640]	training's l2: 17.4998	valid_1's l2: 20.251
[1650]	training's l2: 17.4815	valid_1's l2: 20.2507
[1660]	training's l2: 17.4627	valid_1's l2: 20.2506
[1670]	training's l2: 17.4436	valid_1's l2: 20.2507
[1680]	training's l2: 17.4253	valid_1's l2: 20.2498
[1690]	training's l2: 17.4063	valid_1's l2: 20.2493
[1700]	training's l2: 17.3875	valid_1's l2: 20.2489
[1710]	training's l2: 17.3691	valid_1's l2: 20.2489
[1720]	training's l2: 17.3502	valid_1's l2: 20.2487
Early stopping, best iteration is:
[1693]	training's l2: 17.4005	valid_1's l2: 20.2487
score1: 3.8402062282367826
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254925 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.3944	valid_1's l2: 30.4877
[20]	training's l2: 29.3342	valid_1's l2: 28.4375
[30]	training's l2: 27.7468	valid_1's l2: 26.8603
[40]	training's l2: 26.5177	valid_1's l2: 25.6412
[50]	training's l2: 25.5571	valid_1's l2: 24.696
[60]	training's l2: 24.8	valid_1's l2: 23.9572
[70]	training's l2: 24.2007	valid_1's l2: 23.3765
[80]	training's l2: 23.7214	valid_1's l2: 22.9196
[90]	training's l2: 23.3365	valid_1's l2: 22.558
[100]	training's l2: 23.0246	valid_1's l2: 22.2679
[110]	training's l2: 22.7665	valid_1's l2: 22.0336
[120]	training's l2: 22.5453	valid_1's l2: 21.8385
[130]	training's l2: 22.3579	valid_1's l2: 21.6773
[140]	training's l2: 22.1946	valid_1's l2: 21.5422
[150]	training's l2: 22.0534	valid_1's l2: 21.4294
[160]	training's l2: 21.9283	valid_1's l2: 21.333
[170]	training's l2: 21.8135	valid_1's l2: 21.2474
[180]	training's l2: 21.7097	valid_1's l2: 21.1712
[190]	training's l2: 21.6155	valid_1's l2: 21.1063
[200]	training's l2: 21.5311	valid_1's l2: 21.0486
[210]	training's l2: 21.4521	valid_1's l2: 20.9977
[220]	training's l2: 21.3793	valid_1's l2: 20.953
[230]	training's l2: 21.3104	valid_1's l2: 20.9122
[240]	training's l2: 21.2421	valid_1's l2: 20.8729
[250]	training's l2: 21.179	valid_1's l2: 20.8391
[260]	training's l2: 21.1179	valid_1's l2: 20.8081
[270]	training's l2: 21.0612	valid_1's l2: 20.7789
[280]	training's l2: 21.0064	valid_1's l2: 20.7527
[290]	training's l2: 20.9537	valid_1's l2: 20.7293
[300]	training's l2: 20.9039	valid_1's l2: 20.7078
[310]	training's l2: 20.8549	valid_1's l2: 20.6864
[320]	training's l2: 20.8068	valid_1's l2: 20.6664
[330]	training's l2: 20.7607	valid_1's l2: 20.6509
[340]	training's l2: 20.7162	valid_1's l2: 20.6345
[350]	training's l2: 20.6726	valid_1's l2: 20.6201
[360]	training's l2: 20.6291	valid_1's l2: 20.6059
[370]	training's l2: 20.5877	valid_1's l2: 20.5923
[380]	training's l2: 20.5463	valid_1's l2: 20.5796
[390]	training's l2: 20.5059	valid_1's l2: 20.5673
[400]	training's l2: 20.4663	valid_1's l2: 20.558
[410]	training's l2: 20.4268	valid_1's l2: 20.5456
[420]	training's l2: 20.3875	valid_1's l2: 20.5352
[430]	training's l2: 20.3498	valid_1's l2: 20.5256
[440]	training's l2: 20.3121	valid_1's l2: 20.5157
[450]	training's l2: 20.2745	valid_1's l2: 20.5068
[460]	training's l2: 20.2373	valid_1's l2: 20.4986
[470]	training's l2: 20.2011	valid_1's l2: 20.49
[480]	training's l2: 20.1648	valid_1's l2: 20.4818
[490]	training's l2: 20.1291	valid_1's l2: 20.4747
[500]	training's l2: 20.0933	valid_1's l2: 20.4668
[510]	training's l2: 20.0579	valid_1's l2: 20.4583
[520]	training's l2: 20.0221	valid_1's l2: 20.4499
[530]	training's l2: 19.9867	valid_1's l2: 20.4427
[540]	training's l2: 19.9524	valid_1's l2: 20.4371
[550]	training's l2: 19.9179	valid_1's l2: 20.43
[560]	training's l2: 19.884	valid_1's l2: 20.4229
[570]	training's l2: 19.8508	valid_1's l2: 20.4173
[580]	training's l2: 19.8186	valid_1's l2: 20.4118
[590]	training's l2: 19.7853	valid_1's l2: 20.4059
[600]	training's l2: 19.7527	valid_1's l2: 20.4008
[610]	training's l2: 19.7205	valid_1's l2: 20.3952
[620]	training's l2: 19.6878	valid_1's l2: 20.3902
[630]	training's l2: 19.6562	valid_1's l2: 20.3869
[640]	training's l2: 19.6247	valid_1's l2: 20.3825
[650]	training's l2: 19.5934	valid_1's l2: 20.3768
[660]	training's l2: 19.562	valid_1's l2: 20.3722
[670]	training's l2: 19.5315	valid_1's l2: 20.3686
[680]	training's l2: 19.5006	valid_1's l2: 20.3649
[690]	training's l2: 19.4701	valid_1's l2: 20.3599
[700]	training's l2: 19.4403	valid_1's l2: 20.3568
[710]	training's l2: 19.41	valid_1's l2: 20.3522
[720]	training's l2: 19.3804	valid_1's l2: 20.3487
[730]	training's l2: 19.3506	valid_1's l2: 20.3457
[740]	training's l2: 19.3209	valid_1's l2: 20.3439
[750]	training's l2: 19.2918	valid_1's l2: 20.3415
[760]	training's l2: 19.2624	valid_1's l2: 20.3384
[770]	training's l2: 19.2332	valid_1's l2: 20.3358
[780]	training's l2: 19.2043	valid_1's l2: 20.3329
[790]	training's l2: 19.1758	valid_1's l2: 20.3299
[800]	training's l2: 19.1476	valid_1's l2: 20.3266
[810]	training's l2: 19.1194	valid_1's l2: 20.324
[820]	training's l2: 19.0912	valid_1's l2: 20.3211
[830]	training's l2: 19.0628	valid_1's l2: 20.3182
[840]	training's l2: 19.0347	valid_1's l2: 20.3154
[850]	training's l2: 19.0072	valid_1's l2: 20.3126
[860]	training's l2: 18.9794	valid_1's l2: 20.3104
[870]	training's l2: 18.9519	valid_1's l2: 20.3077
[880]	training's l2: 18.9246	valid_1's l2: 20.3058
[890]	training's l2: 18.8973	valid_1's l2: 20.3033
[900]	training's l2: 18.8701	valid_1's l2: 20.3013
[910]	training's l2: 18.8428	valid_1's l2: 20.2988
[920]	training's l2: 18.8162	valid_1's l2: 20.297
[930]	training's l2: 18.7901	valid_1's l2: 20.2967
[940]	training's l2: 18.7633	valid_1's l2: 20.2944
[950]	training's l2: 18.737	valid_1's l2: 20.2928
[960]	training's l2: 18.7117	valid_1's l2: 20.2914
[970]	training's l2: 18.6864	valid_1's l2: 20.2901
[980]	training's l2: 18.6614	valid_1's l2: 20.2897
[990]	training's l2: 18.6371	valid_1's l2: 20.2889
[1000]	training's l2: 18.6118	valid_1's l2: 20.2875
[1010]	training's l2: 18.5867	valid_1's l2: 20.2857
[1020]	training's l2: 18.5613	valid_1's l2: 20.2847
[1030]	training's l2: 18.5362	valid_1's l2: 20.2837
[1040]	training's l2: 18.5109	valid_1's l2: 20.2819
[1050]	training's l2: 18.4865	valid_1's l2: 20.2805
[1060]	training's l2: 18.4622	valid_1's l2: 20.2791
[1070]	training's l2: 18.4384	valid_1's l2: 20.2782
[1080]	training's l2: 18.4138	valid_1's l2: 20.2769
[1090]	training's l2: 18.3906	valid_1's l2: 20.2749
[1100]	training's l2: 18.3668	valid_1's l2: 20.274
[1110]	training's l2: 18.3431	valid_1's l2: 20.2726
[1120]	training's l2: 18.3198	valid_1's l2: 20.2723
[1130]	training's l2: 18.2965	valid_1's l2: 20.2714
[1140]	training's l2: 18.2727	valid_1's l2: 20.2707
[1150]	training's l2: 18.2502	valid_1's l2: 20.2698
[1160]	training's l2: 18.2267	valid_1's l2: 20.2695
[1170]	training's l2: 18.2035	valid_1's l2: 20.2696
[1180]	training's l2: 18.1815	valid_1's l2: 20.2686
[1190]	training's l2: 18.1589	valid_1's l2: 20.2679
[1200]	training's l2: 18.1365	valid_1's l2: 20.2682
[1210]	training's l2: 18.1144	valid_1's l2: 20.2669
[1220]	training's l2: 18.0913	valid_1's l2: 20.2657
[1230]	training's l2: 18.0689	valid_1's l2: 20.2651
[1240]	training's l2: 18.0472	valid_1's l2: 20.2644
[1250]	training's l2: 18.0248	valid_1's l2: 20.2642
[1260]	training's l2: 18.0035	valid_1's l2: 20.2628
[1270]	training's l2: 17.9815	valid_1's l2: 20.2618
[1280]	training's l2: 17.9604	valid_1's l2: 20.2609
[1290]	training's l2: 17.9392	valid_1's l2: 20.2596
[1300]	training's l2: 17.9182	valid_1's l2: 20.2583
[1310]	training's l2: 17.898	valid_1's l2: 20.2585
[1320]	training's l2: 17.8762	valid_1's l2: 20.2573
[1330]	training's l2: 17.8542	valid_1's l2: 20.2572
[1340]	training's l2: 17.832	valid_1's l2: 20.2567
[1350]	training's l2: 17.8109	valid_1's l2: 20.2556
[1360]	training's l2: 17.7901	valid_1's l2: 20.2556
[1370]	training's l2: 17.7682	valid_1's l2: 20.2543
[1380]	training's l2: 17.7467	valid_1's l2: 20.2546
[1390]	training's l2: 17.7263	valid_1's l2: 20.2532
[1400]	training's l2: 17.7057	valid_1's l2: 20.2531
[1410]	training's l2: 17.6838	valid_1's l2: 20.253
[1420]	training's l2: 17.6618	valid_1's l2: 20.2524
[1430]	training's l2: 17.6408	valid_1's l2: 20.2524
[1440]	training's l2: 17.6217	valid_1's l2: 20.252
[1450]	training's l2: 17.6005	valid_1's l2: 20.2522
[1460]	training's l2: 17.5802	valid_1's l2: 20.2516
[1470]	training's l2: 17.5597	valid_1's l2: 20.2511
[1480]	training's l2: 17.5389	valid_1's l2: 20.2512
[1490]	training's l2: 17.5189	valid_1's l2: 20.2503
[1500]	training's l2: 17.4986	valid_1's l2: 20.2501
[1510]	training's l2: 17.4786	valid_1's l2: 20.2499
[1520]	training's l2: 17.4589	valid_1's l2: 20.25
[1530]	training's l2: 17.4386	valid_1's l2: 20.2499
[1540]	training's l2: 17.4199	valid_1's l2: 20.2493
[1550]	training's l2: 17.3995	valid_1's l2: 20.249
[1560]	training's l2: 17.3803	valid_1's l2: 20.2486
[1570]	training's l2: 17.3594	valid_1's l2: 20.2484
[1580]	training's l2: 17.3399	valid_1's l2: 20.2472
[1590]	training's l2: 17.3197	valid_1's l2: 20.2471
[1600]	training's l2: 17.2993	valid_1's l2: 20.2461
[1610]	training's l2: 17.2794	valid_1's l2: 20.246
[1620]	training's l2: 17.2592	valid_1's l2: 20.2455
[1630]	training's l2: 17.2403	valid_1's l2: 20.246
[1640]	training's l2: 17.2209	valid_1's l2: 20.245
[1650]	training's l2: 17.2009	valid_1's l2: 20.2448
[1660]	training's l2: 17.1819	valid_1's l2: 20.2443
[1670]	training's l2: 17.1623	valid_1's l2: 20.2436
[1680]	training's l2: 17.1432	valid_1's l2: 20.2431
[1690]	training's l2: 17.1229	valid_1's l2: 20.2432
[1700]	training's l2: 17.1035	valid_1's l2: 20.2429
[1710]	training's l2: 17.0843	valid_1's l2: 20.2425
[1720]	training's l2: 17.0659	valid_1's l2: 20.2415
[1730]	training's l2: 17.0454	valid_1's l2: 20.2419
[1740]	training's l2: 17.0257	valid_1's l2: 20.2414
[1750]	training's l2: 17.0065	valid_1's l2: 20.2416
[1760]	training's l2: 16.987	valid_1's l2: 20.2421
[1770]	training's l2: 16.9682	valid_1's l2: 20.2424
Early stopping, best iteration is:
[1745]	training's l2: 17.0158	valid_1's l2: 20.2411
score1: 3.837208647471715
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.274154 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.8591	valid_1's l2: 30.9602
[20]	training's l2: 30.0584	valid_1's l2: 29.1738
[30]	training's l2: 28.5981	valid_1's l2: 27.7298
[40]	training's l2: 27.4097	valid_1's l2: 26.5566
[50]	training's l2: 26.4371	valid_1's l2: 25.6024
[60]	training's l2: 25.6385	valid_1's l2: 24.8229
[70]	training's l2: 24.9796	valid_1's l2: 24.185
[80]	training's l2: 24.4323	valid_1's l2: 23.66
[90]	training's l2: 23.9754	valid_1's l2: 23.2271
[100]	training's l2: 23.5911	valid_1's l2: 22.8683
[110]	training's l2: 23.2664	valid_1's l2: 22.5686
[120]	training's l2: 22.9919	valid_1's l2: 22.3207
[130]	training's l2: 22.7577	valid_1's l2: 22.1127
[140]	training's l2: 22.5536	valid_1's l2: 21.9356
[150]	training's l2: 22.374	valid_1's l2: 21.7854
[160]	training's l2: 22.2151	valid_1's l2: 21.6559
[170]	training's l2: 22.0737	valid_1's l2: 21.5427
[180]	training's l2: 21.946	valid_1's l2: 21.4457
[190]	training's l2: 21.83	valid_1's l2: 21.3606
[200]	training's l2: 21.7248	valid_1's l2: 21.2863
[210]	training's l2: 21.6269	valid_1's l2: 21.2207
[220]	training's l2: 21.5342	valid_1's l2: 21.1585
[230]	training's l2: 21.4481	valid_1's l2: 21.1032
[240]	training's l2: 21.3706	valid_1's l2: 21.0549
[250]	training's l2: 21.2948	valid_1's l2: 21.0092
[260]	training's l2: 21.2239	valid_1's l2: 20.9674
[270]	training's l2: 21.1572	valid_1's l2: 20.93
[280]	training's l2: 21.0939	valid_1's l2: 20.8952
[290]	training's l2: 21.033	valid_1's l2: 20.8626
[300]	training's l2: 20.9728	valid_1's l2: 20.8324
[310]	training's l2: 20.9148	valid_1's l2: 20.8051
[320]	training's l2: 20.859	valid_1's l2: 20.7792
[330]	training's l2: 20.8046	valid_1's l2: 20.7552
[340]	training's l2: 20.7518	valid_1's l2: 20.7332
[350]	training's l2: 20.7022	valid_1's l2: 20.7131
[360]	training's l2: 20.6542	valid_1's l2: 20.6959
[370]	training's l2: 20.6076	valid_1's l2: 20.6792
[380]	training's l2: 20.5614	valid_1's l2: 20.6623
[390]	training's l2: 20.5167	valid_1's l2: 20.6474
[400]	training's l2: 20.4737	valid_1's l2: 20.6334
[410]	training's l2: 20.4306	valid_1's l2: 20.6208
[420]	training's l2: 20.3892	valid_1's l2: 20.6083
[430]	training's l2: 20.3474	valid_1's l2: 20.5956
[440]	training's l2: 20.306	valid_1's l2: 20.5824
[450]	training's l2: 20.2657	valid_1's l2: 20.5714
[460]	training's l2: 20.2262	valid_1's l2: 20.5601
[470]	training's l2: 20.187	valid_1's l2: 20.55
[480]	training's l2: 20.1477	valid_1's l2: 20.5404
[490]	training's l2: 20.11	valid_1's l2: 20.5316
[500]	training's l2: 20.0724	valid_1's l2: 20.5242
[510]	training's l2: 20.0357	valid_1's l2: 20.5173
[520]	training's l2: 19.999	valid_1's l2: 20.5103
[530]	training's l2: 19.9629	valid_1's l2: 20.5031
[540]	training's l2: 19.9267	valid_1's l2: 20.4961
[550]	training's l2: 19.8911	valid_1's l2: 20.488
[560]	training's l2: 19.8557	valid_1's l2: 20.4808
[570]	training's l2: 19.8189	valid_1's l2: 20.4716
[580]	training's l2: 19.7832	valid_1's l2: 20.4642
[590]	training's l2: 19.7488	valid_1's l2: 20.4571
[600]	training's l2: 19.7137	valid_1's l2: 20.4491
[610]	training's l2: 19.679	valid_1's l2: 20.4431
[620]	training's l2: 19.6445	valid_1's l2: 20.4382
[630]	training's l2: 19.6102	valid_1's l2: 20.4314
[640]	training's l2: 19.5766	valid_1's l2: 20.4248
[650]	training's l2: 19.5423	valid_1's l2: 20.4191
[660]	training's l2: 19.5089	valid_1's l2: 20.4139
[670]	training's l2: 19.4755	valid_1's l2: 20.4087
[680]	training's l2: 19.4423	valid_1's l2: 20.4033
[690]	training's l2: 19.4099	valid_1's l2: 20.3978
[700]	training's l2: 19.3773	valid_1's l2: 20.3931
[710]	training's l2: 19.3454	valid_1's l2: 20.3886
[720]	training's l2: 19.3138	valid_1's l2: 20.3837
[730]	training's l2: 19.282	valid_1's l2: 20.3792
[740]	training's l2: 19.2507	valid_1's l2: 20.3741
[750]	training's l2: 19.2199	valid_1's l2: 20.3698
[760]	training's l2: 19.189	valid_1's l2: 20.3668
[770]	training's l2: 19.1581	valid_1's l2: 20.3634
[780]	training's l2: 19.1272	valid_1's l2: 20.3612
[790]	training's l2: 19.0968	valid_1's l2: 20.3584
[800]	training's l2: 19.0663	valid_1's l2: 20.3555
[810]	training's l2: 19.036	valid_1's l2: 20.3518
[820]	training's l2: 19.0061	valid_1's l2: 20.349
[830]	training's l2: 18.9761	valid_1's l2: 20.3458
[840]	training's l2: 18.9461	valid_1's l2: 20.3428
[850]	training's l2: 18.9165	valid_1's l2: 20.3397
[860]	training's l2: 18.8875	valid_1's l2: 20.3373
[870]	training's l2: 18.8576	valid_1's l2: 20.3342
[880]	training's l2: 18.8286	valid_1's l2: 20.3314
[890]	training's l2: 18.7995	valid_1's l2: 20.3285
[900]	training's l2: 18.7708	valid_1's l2: 20.3255
[910]	training's l2: 18.7422	valid_1's l2: 20.3231
[920]	training's l2: 18.7136	valid_1's l2: 20.3205
[930]	training's l2: 18.685	valid_1's l2: 20.3177
[940]	training's l2: 18.6565	valid_1's l2: 20.3158
[950]	training's l2: 18.628	valid_1's l2: 20.3137
[960]	training's l2: 18.6003	valid_1's l2: 20.3111
[970]	training's l2: 18.5718	valid_1's l2: 20.3091
[980]	training's l2: 18.5439	valid_1's l2: 20.3075
[990]	training's l2: 18.5164	valid_1's l2: 20.3055
[1000]	training's l2: 18.4884	valid_1's l2: 20.3033
[1010]	training's l2: 18.4609	valid_1's l2: 20.3019
[1020]	training's l2: 18.4339	valid_1's l2: 20.3
[1030]	training's l2: 18.4075	valid_1's l2: 20.2981
[1040]	training's l2: 18.3801	valid_1's l2: 20.2962
[1050]	training's l2: 18.3532	valid_1's l2: 20.2948
[1060]	training's l2: 18.3269	valid_1's l2: 20.2927
[1070]	training's l2: 18.3003	valid_1's l2: 20.2913
[1080]	training's l2: 18.2741	valid_1's l2: 20.2895
[1090]	training's l2: 18.2477	valid_1's l2: 20.2881
[1100]	training's l2: 18.2218	valid_1's l2: 20.2863
[1110]	training's l2: 18.1959	valid_1's l2: 20.2844
[1120]	training's l2: 18.1698	valid_1's l2: 20.2833
[1130]	training's l2: 18.144	valid_1's l2: 20.2823
[1140]	training's l2: 18.1186	valid_1's l2: 20.2804
[1150]	training's l2: 18.0932	valid_1's l2: 20.2791
[1160]	training's l2: 18.0677	valid_1's l2: 20.2783
[1170]	training's l2: 18.0422	valid_1's l2: 20.277
[1180]	training's l2: 18.0178	valid_1's l2: 20.2771
[1190]	training's l2: 17.9934	valid_1's l2: 20.2759
[1200]	training's l2: 17.9691	valid_1's l2: 20.2752
[1210]	training's l2: 17.9441	valid_1's l2: 20.2749
[1220]	training's l2: 17.9197	valid_1's l2: 20.2754
[1230]	training's l2: 17.8958	valid_1's l2: 20.2749
[1240]	training's l2: 17.8725	valid_1's l2: 20.2741
[1250]	training's l2: 17.8492	valid_1's l2: 20.274
[1260]	training's l2: 17.8258	valid_1's l2: 20.2733
[1270]	training's l2: 17.8023	valid_1's l2: 20.273
[1280]	training's l2: 17.7784	valid_1's l2: 20.2729
[1290]	training's l2: 17.7547	valid_1's l2: 20.2726
[1300]	training's l2: 17.7313	valid_1's l2: 20.2718
[1310]	training's l2: 17.708	valid_1's l2: 20.2712
[1320]	training's l2: 17.6851	valid_1's l2: 20.2701
[1330]	training's l2: 17.6616	valid_1's l2: 20.2685
[1340]	training's l2: 17.6377	valid_1's l2: 20.2673
[1350]	training's l2: 17.6156	valid_1's l2: 20.2667
[1360]	training's l2: 17.5923	valid_1's l2: 20.2663
[1370]	training's l2: 17.5705	valid_1's l2: 20.2663
[1380]	training's l2: 17.5475	valid_1's l2: 20.265
[1390]	training's l2: 17.5251	valid_1's l2: 20.2641
[1400]	training's l2: 17.5023	valid_1's l2: 20.2638
[1410]	training's l2: 17.4798	valid_1's l2: 20.2628
[1420]	training's l2: 17.4575	valid_1's l2: 20.2623
[1430]	training's l2: 17.4355	valid_1's l2: 20.2619
[1440]	training's l2: 17.4136	valid_1's l2: 20.2611
[1450]	training's l2: 17.3919	valid_1's l2: 20.2606
[1460]	training's l2: 17.3702	valid_1's l2: 20.2601
[1470]	training's l2: 17.3476	valid_1's l2: 20.2592
[1480]	training's l2: 17.3265	valid_1's l2: 20.2589
[1490]	training's l2: 17.3035	valid_1's l2: 20.2584
[1500]	training's l2: 17.2829	valid_1's l2: 20.2578
[1510]	training's l2: 17.2617	valid_1's l2: 20.2572
[1520]	training's l2: 17.2395	valid_1's l2: 20.2578
[1530]	training's l2: 17.219	valid_1's l2: 20.2583
[1540]	training's l2: 17.1978	valid_1's l2: 20.2582
Early stopping, best iteration is:
[1514]	training's l2: 17.2532	valid_1's l2: 20.2572
score1: 3.839226884129274
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.310133 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.0706	valid_1's l2: 30.1675
[20]	training's l2: 28.8418	valid_1's l2: 27.9495
[30]	training's l2: 27.1857	valid_1's l2: 26.3068
[40]	training's l2: 25.9422	valid_1's l2: 25.078
[50]	training's l2: 24.9981	valid_1's l2: 24.1532
[60]	training's l2: 24.2746	valid_1's l2: 23.4533
[70]	training's l2: 23.7165	valid_1's l2: 22.9225
[80]	training's l2: 23.28	valid_1's l2: 22.5136
[90]	training's l2: 22.9371	valid_1's l2: 22.1962
[100]	training's l2: 22.6563	valid_1's l2: 21.9456
[110]	training's l2: 22.4207	valid_1's l2: 21.7419
[120]	training's l2: 22.2262	valid_1's l2: 21.5781
[130]	training's l2: 22.0575	valid_1's l2: 21.4431
[140]	training's l2: 21.9121	valid_1's l2: 21.3329
[150]	training's l2: 21.7796	valid_1's l2: 21.2352
[160]	training's l2: 21.6638	valid_1's l2: 21.1537
[170]	training's l2: 21.5595	valid_1's l2: 21.0825
[180]	training's l2: 21.4636	valid_1's l2: 21.0194
[190]	training's l2: 21.3772	valid_1's l2: 20.9652
[200]	training's l2: 21.2963	valid_1's l2: 20.9159
[210]	training's l2: 21.2179	valid_1's l2: 20.8698
[220]	training's l2: 21.1457	valid_1's l2: 20.8315
[230]	training's l2: 21.0761	valid_1's l2: 20.7969
[240]	training's l2: 21.0117	valid_1's l2: 20.7655
[250]	training's l2: 20.9494	valid_1's l2: 20.7366
[260]	training's l2: 20.8905	valid_1's l2: 20.7102
[270]	training's l2: 20.8329	valid_1's l2: 20.6856
[280]	training's l2: 20.7786	valid_1's l2: 20.6649
[290]	training's l2: 20.7249	valid_1's l2: 20.646
[300]	training's l2: 20.6725	valid_1's l2: 20.6266
[310]	training's l2: 20.6218	valid_1's l2: 20.6091
[320]	training's l2: 20.5728	valid_1's l2: 20.5928
[330]	training's l2: 20.5244	valid_1's l2: 20.5767
[340]	training's l2: 20.4766	valid_1's l2: 20.5623
[350]	training's l2: 20.4313	valid_1's l2: 20.5503
[360]	training's l2: 20.3853	valid_1's l2: 20.5371
[370]	training's l2: 20.3408	valid_1's l2: 20.525
[380]	training's l2: 20.2976	valid_1's l2: 20.5157
[390]	training's l2: 20.2537	valid_1's l2: 20.5073
[400]	training's l2: 20.2108	valid_1's l2: 20.496
[410]	training's l2: 20.1685	valid_1's l2: 20.4865
[420]	training's l2: 20.1261	valid_1's l2: 20.4769
[430]	training's l2: 20.0838	valid_1's l2: 20.4678
[440]	training's l2: 20.0416	valid_1's l2: 20.4579
[450]	training's l2: 20.0011	valid_1's l2: 20.4496
[460]	training's l2: 19.9609	valid_1's l2: 20.4405
[470]	training's l2: 19.9208	valid_1's l2: 20.4316
[480]	training's l2: 19.8808	valid_1's l2: 20.424
[490]	training's l2: 19.8415	valid_1's l2: 20.4169
[500]	training's l2: 19.8021	valid_1's l2: 20.4105
[510]	training's l2: 19.763	valid_1's l2: 20.4056
[520]	training's l2: 19.7251	valid_1's l2: 20.3993
[530]	training's l2: 19.6867	valid_1's l2: 20.3929
[540]	training's l2: 19.6498	valid_1's l2: 20.3869
[550]	training's l2: 19.6121	valid_1's l2: 20.3805
[560]	training's l2: 19.5753	valid_1's l2: 20.3762
[570]	training's l2: 19.5392	valid_1's l2: 20.3713
[580]	training's l2: 19.5038	valid_1's l2: 20.3672
[590]	training's l2: 19.4674	valid_1's l2: 20.3636
[600]	training's l2: 19.4326	valid_1's l2: 20.3592
[610]	training's l2: 19.3975	valid_1's l2: 20.355
[620]	training's l2: 19.3625	valid_1's l2: 20.3514
[630]	training's l2: 19.3281	valid_1's l2: 20.3486
[640]	training's l2: 19.2937	valid_1's l2: 20.3452
[650]	training's l2: 19.26	valid_1's l2: 20.3413
[660]	training's l2: 19.2261	valid_1's l2: 20.3383
[670]	training's l2: 19.1922	valid_1's l2: 20.3351
[680]	training's l2: 19.1581	valid_1's l2: 20.3307
[690]	training's l2: 19.1249	valid_1's l2: 20.3286
[700]	training's l2: 19.0917	valid_1's l2: 20.3241
[710]	training's l2: 19.0585	valid_1's l2: 20.3191
[720]	training's l2: 19.0251	valid_1's l2: 20.3151
[730]	training's l2: 18.993	valid_1's l2: 20.3122
[740]	training's l2: 18.9613	valid_1's l2: 20.3101
[750]	training's l2: 18.9291	valid_1's l2: 20.3082
[760]	training's l2: 18.8966	valid_1's l2: 20.3043
[770]	training's l2: 18.8648	valid_1's l2: 20.3011
[780]	training's l2: 18.8335	valid_1's l2: 20.2987
[790]	training's l2: 18.8024	valid_1's l2: 20.2966
[800]	training's l2: 18.7716	valid_1's l2: 20.2946
[810]	training's l2: 18.7411	valid_1's l2: 20.2928
[820]	training's l2: 18.7104	valid_1's l2: 20.2906
[830]	training's l2: 18.6797	valid_1's l2: 20.2879
[840]	training's l2: 18.6498	valid_1's l2: 20.2851
[850]	training's l2: 18.6199	valid_1's l2: 20.284
[860]	training's l2: 18.5898	valid_1's l2: 20.2824
[870]	training's l2: 18.5607	valid_1's l2: 20.2811
[880]	training's l2: 18.5305	valid_1's l2: 20.2793
[890]	training's l2: 18.5022	valid_1's l2: 20.2769
[900]	training's l2: 18.4728	valid_1's l2: 20.2755
[910]	training's l2: 18.4447	valid_1's l2: 20.274
[920]	training's l2: 18.4169	valid_1's l2: 20.2726
[930]	training's l2: 18.3878	valid_1's l2: 20.2709
[940]	training's l2: 18.3603	valid_1's l2: 20.268
[950]	training's l2: 18.3316	valid_1's l2: 20.2675
[960]	training's l2: 18.3052	valid_1's l2: 20.2673
[970]	training's l2: 18.2775	valid_1's l2: 20.2655
[980]	training's l2: 18.2507	valid_1's l2: 20.2647
[990]	training's l2: 18.2232	valid_1's l2: 20.2642
[1000]	training's l2: 18.1965	valid_1's l2: 20.2637
[1010]	training's l2: 18.1696	valid_1's l2: 20.2629
[1020]	training's l2: 18.1425	valid_1's l2: 20.2614
[1030]	training's l2: 18.1155	valid_1's l2: 20.26
[1040]	training's l2: 18.0901	valid_1's l2: 20.2589
[1050]	training's l2: 18.0636	valid_1's l2: 20.2575
[1060]	training's l2: 18.0377	valid_1's l2: 20.2556
[1070]	training's l2: 18.0126	valid_1's l2: 20.2553
[1080]	training's l2: 17.9866	valid_1's l2: 20.2542
[1090]	training's l2: 17.9607	valid_1's l2: 20.254
[1100]	training's l2: 17.9362	valid_1's l2: 20.254
[1110]	training's l2: 17.9102	valid_1's l2: 20.2536
[1120]	training's l2: 17.8841	valid_1's l2: 20.2523
[1130]	training's l2: 17.8594	valid_1's l2: 20.2519
[1140]	training's l2: 17.8347	valid_1's l2: 20.252
[1150]	training's l2: 17.8085	valid_1's l2: 20.2519
[1160]	training's l2: 17.7824	valid_1's l2: 20.2516
[1170]	training's l2: 17.7577	valid_1's l2: 20.2515
[1180]	training's l2: 17.7333	valid_1's l2: 20.2506
[1190]	training's l2: 17.708	valid_1's l2: 20.2506
[1200]	training's l2: 17.6827	valid_1's l2: 20.2505
[1210]	training's l2: 17.6564	valid_1's l2: 20.2511
Early stopping, best iteration is:
[1187]	training's l2: 17.7154	valid_1's l2: 20.2502
score1: 3.8375875559815587
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256970 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.7977	valid_1's l2: 30.8712
[20]	training's l2: 29.9966	valid_1's l2: 29.0571
[30]	training's l2: 28.5647	valid_1's l2: 27.6117
[40]	training's l2: 27.4213	valid_1's l2: 26.4585
[50]	training's l2: 26.5014	valid_1's l2: 25.5313
[60]	training's l2: 25.7605	valid_1's l2: 24.7825
[70]	training's l2: 25.1623	valid_1's l2: 24.1797
[80]	training's l2: 24.6733	valid_1's l2: 23.6893
[90]	training's l2: 24.2725	valid_1's l2: 23.2863
[100]	training's l2: 23.9397	valid_1's l2: 22.951
[110]	training's l2: 23.6618	valid_1's l2: 22.6742
[120]	training's l2: 23.4283	valid_1's l2: 22.4433
[130]	training's l2: 23.2298	valid_1's l2: 22.2474
[140]	training's l2: 23.0607	valid_1's l2: 22.0821
[150]	training's l2: 22.9169	valid_1's l2: 21.9433
[160]	training's l2: 22.7847	valid_1's l2: 21.8171
[170]	training's l2: 22.6707	valid_1's l2: 21.7078
[180]	training's l2: 22.5719	valid_1's l2: 21.6134
[190]	training's l2: 22.4854	valid_1's l2: 21.5327
[200]	training's l2: 22.4065	valid_1's l2: 21.461
[210]	training's l2: 22.3351	valid_1's l2: 21.396
[220]	training's l2: 22.2707	valid_1's l2: 21.3376
[230]	training's l2: 22.213	valid_1's l2: 21.2853
[240]	training's l2: 22.1618	valid_1's l2: 21.2378
[250]	training's l2: 22.1131	valid_1's l2: 21.1936
[260]	training's l2: 22.0677	valid_1's l2: 21.1537
[270]	training's l2: 22.0277	valid_1's l2: 21.12
[280]	training's l2: 21.9853	valid_1's l2: 21.0857
[290]	training's l2: 21.9482	valid_1's l2: 21.0554
[300]	training's l2: 21.9118	valid_1's l2: 21.0265
[310]	training's l2: 21.878	valid_1's l2: 20.9994
[320]	training's l2: 21.8459	valid_1's l2: 20.9753
[330]	training's l2: 21.8149	valid_1's l2: 20.9522
[340]	training's l2: 21.7846	valid_1's l2: 20.9299
[350]	training's l2: 21.7567	valid_1's l2: 20.9095
[360]	training's l2: 21.7298	valid_1's l2: 20.8895
[370]	training's l2: 21.7038	valid_1's l2: 20.8711
[380]	training's l2: 21.6781	valid_1's l2: 20.8525
[390]	training's l2: 21.6539	valid_1's l2: 20.8356
[400]	training's l2: 21.6299	valid_1's l2: 20.8185
[410]	training's l2: 21.607	valid_1's l2: 20.8023
[420]	training's l2: 21.5845	valid_1's l2: 20.7869
[430]	training's l2: 21.563	valid_1's l2: 20.7723
[440]	training's l2: 21.5421	valid_1's l2: 20.759
[450]	training's l2: 21.5222	valid_1's l2: 20.746
[460]	training's l2: 21.5021	valid_1's l2: 20.7336
[470]	training's l2: 21.483	valid_1's l2: 20.7222
[480]	training's l2: 21.4652	valid_1's l2: 20.7118
[490]	training's l2: 21.4462	valid_1's l2: 20.7
[500]	training's l2: 21.4288	valid_1's l2: 20.6907
[510]	training's l2: 21.411	valid_1's l2: 20.6815
[520]	training's l2: 21.3944	valid_1's l2: 20.6733
[530]	training's l2: 21.3773	valid_1's l2: 20.6642
[540]	training's l2: 21.3606	valid_1's l2: 20.6544
[550]	training's l2: 21.3447	valid_1's l2: 20.6461
[560]	training's l2: 21.3293	valid_1's l2: 20.6376
[570]	training's l2: 21.3138	valid_1's l2: 20.6292
[580]	training's l2: 21.2989	valid_1's l2: 20.6215
[590]	training's l2: 21.2838	valid_1's l2: 20.6141
[600]	training's l2: 21.2689	valid_1's l2: 20.6061
[610]	training's l2: 21.2543	valid_1's l2: 20.5985
[620]	training's l2: 21.2398	valid_1's l2: 20.5914
[630]	training's l2: 21.2254	valid_1's l2: 20.5838
[640]	training's l2: 21.2106	valid_1's l2: 20.5762
[650]	training's l2: 21.1961	valid_1's l2: 20.5689
[660]	training's l2: 21.1813	valid_1's l2: 20.5619
[670]	training's l2: 21.1673	valid_1's l2: 20.5554
[680]	training's l2: 21.1537	valid_1's l2: 20.5488
[690]	training's l2: 21.1402	valid_1's l2: 20.5426
[700]	training's l2: 21.1271	valid_1's l2: 20.537
[710]	training's l2: 21.1137	valid_1's l2: 20.5309
[720]	training's l2: 21.1001	valid_1's l2: 20.5247
[730]	training's l2: 21.0872	valid_1's l2: 20.5187
[740]	training's l2: 21.075	valid_1's l2: 20.5144
[750]	training's l2: 21.0623	valid_1's l2: 20.5089
[760]	training's l2: 21.0502	valid_1's l2: 20.5041
[770]	training's l2: 21.0385	valid_1's l2: 20.4994
[780]	training's l2: 21.0262	valid_1's l2: 20.4946
[790]	training's l2: 21.0139	valid_1's l2: 20.4896
[800]	training's l2: 21.0022	valid_1's l2: 20.485
[810]	training's l2: 20.9908	valid_1's l2: 20.4806
[820]	training's l2: 20.9789	valid_1's l2: 20.4767
[830]	training's l2: 20.9671	valid_1's l2: 20.4717
[840]	training's l2: 20.9557	valid_1's l2: 20.4675
[850]	training's l2: 20.9444	valid_1's l2: 20.4632
[860]	training's l2: 20.9334	valid_1's l2: 20.459
[870]	training's l2: 20.9225	valid_1's l2: 20.4554
[880]	training's l2: 20.9113	valid_1's l2: 20.4516
[890]	training's l2: 20.9004	valid_1's l2: 20.4473
[900]	training's l2: 20.889	valid_1's l2: 20.4427
[910]	training's l2: 20.8789	valid_1's l2: 20.4394
[920]	training's l2: 20.868	valid_1's l2: 20.4356
[930]	training's l2: 20.8574	valid_1's l2: 20.4317
[940]	training's l2: 20.8469	valid_1's l2: 20.428
[950]	training's l2: 20.8365	valid_1's l2: 20.4248
[960]	training's l2: 20.8263	valid_1's l2: 20.4213
[970]	training's l2: 20.8161	valid_1's l2: 20.4184
[980]	training's l2: 20.8059	valid_1's l2: 20.4153
[990]	training's l2: 20.7961	valid_1's l2: 20.4129
[1000]	training's l2: 20.7859	valid_1's l2: 20.4101
[1010]	training's l2: 20.7759	valid_1's l2: 20.4074
[1020]	training's l2: 20.7656	valid_1's l2: 20.4044
[1030]	training's l2: 20.7552	valid_1's l2: 20.4009
[1040]	training's l2: 20.7455	valid_1's l2: 20.3986
[1050]	training's l2: 20.7355	valid_1's l2: 20.3953
[1060]	training's l2: 20.7258	valid_1's l2: 20.3927
[1070]	training's l2: 20.7159	valid_1's l2: 20.3893
[1080]	training's l2: 20.7056	valid_1's l2: 20.3857
[1090]	training's l2: 20.6959	valid_1's l2: 20.3831
[1100]	training's l2: 20.686	valid_1's l2: 20.3802
[1110]	training's l2: 20.6766	valid_1's l2: 20.3773
[1120]	training's l2: 20.6672	valid_1's l2: 20.375
[1130]	training's l2: 20.6576	valid_1's l2: 20.3729
[1140]	training's l2: 20.6483	valid_1's l2: 20.3702
[1150]	training's l2: 20.6392	valid_1's l2: 20.3679
[1160]	training's l2: 20.6295	valid_1's l2: 20.3655
[1170]	training's l2: 20.6206	valid_1's l2: 20.3636
[1180]	training's l2: 20.6116	valid_1's l2: 20.3617
[1190]	training's l2: 20.6028	valid_1's l2: 20.3601
[1200]	training's l2: 20.5932	valid_1's l2: 20.3582
[1210]	training's l2: 20.5841	valid_1's l2: 20.3563
[1220]	training's l2: 20.5754	valid_1's l2: 20.3545
[1230]	training's l2: 20.5666	valid_1's l2: 20.3525
[1240]	training's l2: 20.5577	valid_1's l2: 20.3507
[1250]	training's l2: 20.549	valid_1's l2: 20.3494
[1260]	training's l2: 20.5399	valid_1's l2: 20.3478
[1270]	training's l2: 20.5314	valid_1's l2: 20.3463
[1280]	training's l2: 20.5234	valid_1's l2: 20.3453
[1290]	training's l2: 20.5151	valid_1's l2: 20.3443
[1300]	training's l2: 20.5066	valid_1's l2: 20.3427
[1310]	training's l2: 20.498	valid_1's l2: 20.341
[1320]	training's l2: 20.4897	valid_1's l2: 20.3394
[1330]	training's l2: 20.4816	valid_1's l2: 20.3381
[1340]	training's l2: 20.4729	valid_1's l2: 20.337
[1350]	training's l2: 20.4652	valid_1's l2: 20.336
[1360]	training's l2: 20.4567	valid_1's l2: 20.3341
[1370]	training's l2: 20.4486	valid_1's l2: 20.3329
[1380]	training's l2: 20.4405	valid_1's l2: 20.3319
[1390]	training's l2: 20.4323	valid_1's l2: 20.3305
[1400]	training's l2: 20.424	valid_1's l2: 20.3291
[1410]	training's l2: 20.4155	valid_1's l2: 20.3275
[1420]	training's l2: 20.4077	valid_1's l2: 20.3265
[1430]	training's l2: 20.3995	valid_1's l2: 20.3255
[1440]	training's l2: 20.391	valid_1's l2: 20.3238
[1450]	training's l2: 20.3829	valid_1's l2: 20.3224
[1460]	training's l2: 20.3748	valid_1's l2: 20.3207
[1470]	training's l2: 20.3671	valid_1's l2: 20.3199
[1480]	training's l2: 20.3595	valid_1's l2: 20.3187
[1490]	training's l2: 20.3515	valid_1's l2: 20.3177
[1500]	training's l2: 20.3432	valid_1's l2: 20.3159
[1510]	training's l2: 20.3357	valid_1's l2: 20.3145
[1520]	training's l2: 20.3273	valid_1's l2: 20.3128
[1530]	training's l2: 20.3192	valid_1's l2: 20.3111
[1540]	training's l2: 20.3116	valid_1's l2: 20.31
[1550]	training's l2: 20.3036	valid_1's l2: 20.3089
[1560]	training's l2: 20.2961	valid_1's l2: 20.3078
[1570]	training's l2: 20.2885	valid_1's l2: 20.3067
[1580]	training's l2: 20.2812	valid_1's l2: 20.3059
[1590]	training's l2: 20.2734	valid_1's l2: 20.3048
[1600]	training's l2: 20.2662	valid_1's l2: 20.3042
[1610]	training's l2: 20.2587	valid_1's l2: 20.3033
[1620]	training's l2: 20.2513	valid_1's l2: 20.3025
[1630]	training's l2: 20.2436	valid_1's l2: 20.3013
[1640]	training's l2: 20.2363	valid_1's l2: 20.3006
[1650]	training's l2: 20.2288	valid_1's l2: 20.2998
[1660]	training's l2: 20.2216	valid_1's l2: 20.299
[1670]	training's l2: 20.2144	valid_1's l2: 20.2983
[1680]	training's l2: 20.207	valid_1's l2: 20.2972
[1690]	training's l2: 20.2	valid_1's l2: 20.2961
[1700]	training's l2: 20.193	valid_1's l2: 20.2956
[1710]	training's l2: 20.1855	valid_1's l2: 20.2947
[1720]	training's l2: 20.1779	valid_1's l2: 20.294
[1730]	training's l2: 20.1709	valid_1's l2: 20.2934
[1740]	training's l2: 20.1641	valid_1's l2: 20.2926
[1750]	training's l2: 20.1573	valid_1's l2: 20.2917
[1760]	training's l2: 20.1499	valid_1's l2: 20.2909
[1770]	training's l2: 20.1427	valid_1's l2: 20.2904
[1780]	training's l2: 20.136	valid_1's l2: 20.2902
[1790]	training's l2: 20.1291	valid_1's l2: 20.2894
[1800]	training's l2: 20.1223	valid_1's l2: 20.2887
[1810]	training's l2: 20.115	valid_1's l2: 20.2881
[1820]	training's l2: 20.1082	valid_1's l2: 20.2878
[1830]	training's l2: 20.1011	valid_1's l2: 20.2866
[1840]	training's l2: 20.0945	valid_1's l2: 20.2861
[1850]	training's l2: 20.0877	valid_1's l2: 20.2855
[1860]	training's l2: 20.0807	valid_1's l2: 20.2847
[1870]	training's l2: 20.0733	valid_1's l2: 20.2842
[1880]	training's l2: 20.0665	valid_1's l2: 20.2836
[1890]	training's l2: 20.0597	valid_1's l2: 20.2835
[1900]	training's l2: 20.0529	valid_1's l2: 20.2833
[1910]	training's l2: 20.0464	valid_1's l2: 20.283
[1920]	training's l2: 20.0392	valid_1's l2: 20.282
[1930]	training's l2: 20.033	valid_1's l2: 20.2817
[1940]	training's l2: 20.0262	valid_1's l2: 20.2813
[1950]	training's l2: 20.0192	valid_1's l2: 20.2806
[1960]	training's l2: 20.0121	valid_1's l2: 20.2803
[1970]	training's l2: 20.0056	valid_1's l2: 20.2798
[1980]	training's l2: 19.9991	valid_1's l2: 20.2794
[1990]	training's l2: 19.9926	valid_1's l2: 20.2788
[2000]	training's l2: 19.9859	valid_1's l2: 20.2786
[2010]	training's l2: 19.9796	valid_1's l2: 20.2783
[2020]	training's l2: 19.973	valid_1's l2: 20.2773
[2030]	training's l2: 19.9664	valid_1's l2: 20.2768
[2040]	training's l2: 19.9596	valid_1's l2: 20.2757
[2050]	training's l2: 19.9532	valid_1's l2: 20.2753
[2060]	training's l2: 19.9465	valid_1's l2: 20.2747
[2070]	training's l2: 19.9398	valid_1's l2: 20.2735
[2080]	training's l2: 19.9333	valid_1's l2: 20.2732
[2090]	training's l2: 19.9265	valid_1's l2: 20.2727
[2100]	training's l2: 19.9202	valid_1's l2: 20.2723
[2110]	training's l2: 19.9133	valid_1's l2: 20.2722
[2120]	training's l2: 19.9069	valid_1's l2: 20.2718
[2130]	training's l2: 19.9004	valid_1's l2: 20.2712
[2140]	training's l2: 19.8937	valid_1's l2: 20.2708
[2150]	training's l2: 19.887	valid_1's l2: 20.2697
[2160]	training's l2: 19.8804	valid_1's l2: 20.2694
[2170]	training's l2: 19.8736	valid_1's l2: 20.2683
[2180]	training's l2: 19.8668	valid_1's l2: 20.2684
[2190]	training's l2: 19.8606	valid_1's l2: 20.2684
[2200]	training's l2: 19.8541	valid_1's l2: 20.2684
[2210]	training's l2: 19.8481	valid_1's l2: 20.2683
[2220]	training's l2: 19.8418	valid_1's l2: 20.268
[2230]	training's l2: 19.8356	valid_1's l2: 20.2675
[2240]	training's l2: 19.8287	valid_1's l2: 20.2667
[2250]	training's l2: 19.8218	valid_1's l2: 20.2661
[2260]	training's l2: 19.8151	valid_1's l2: 20.2655
[2270]	training's l2: 19.8091	valid_1's l2: 20.2653
[2280]	training's l2: 19.8027	valid_1's l2: 20.265
[2290]	training's l2: 19.7962	valid_1's l2: 20.2646
[2300]	training's l2: 19.7897	valid_1's l2: 20.2643
[2310]	training's l2: 19.7835	valid_1's l2: 20.2639
[2320]	training's l2: 19.7771	valid_1's l2: 20.2635
[2330]	training's l2: 19.7711	valid_1's l2: 20.2634
[2340]	training's l2: 19.7648	valid_1's l2: 20.263
[2350]	training's l2: 19.7584	valid_1's l2: 20.2624
[2360]	training's l2: 19.7523	valid_1's l2: 20.2622
[2370]	training's l2: 19.7457	valid_1's l2: 20.2618
[2380]	training's l2: 19.7391	valid_1's l2: 20.2615
[2390]	training's l2: 19.7329	valid_1's l2: 20.2613
[2400]	training's l2: 19.7265	valid_1's l2: 20.2609
[2410]	training's l2: 19.7199	valid_1's l2: 20.2607
[2420]	training's l2: 19.7131	valid_1's l2: 20.2605
[2430]	training's l2: 19.7062	valid_1's l2: 20.26
[2440]	training's l2: 19.6997	valid_1's l2: 20.26
[2450]	training's l2: 19.6935	valid_1's l2: 20.2597
[2460]	training's l2: 19.6877	valid_1's l2: 20.2592
[2470]	training's l2: 19.6817	valid_1's l2: 20.2584
[2480]	training's l2: 19.6754	valid_1's l2: 20.258
[2490]	training's l2: 19.669	valid_1's l2: 20.2576
[2500]	training's l2: 19.6629	valid_1's l2: 20.2576
[2510]	training's l2: 19.6567	valid_1's l2: 20.2571
[2520]	training's l2: 19.6504	valid_1's l2: 20.2571
[2530]	training's l2: 19.6441	valid_1's l2: 20.2567
[2540]	training's l2: 19.6381	valid_1's l2: 20.2564
[2550]	training's l2: 19.6316	valid_1's l2: 20.2559
[2560]	training's l2: 19.6249	valid_1's l2: 20.255
[2570]	training's l2: 19.6193	valid_1's l2: 20.2549
[2580]	training's l2: 19.6131	valid_1's l2: 20.2546
[2590]	training's l2: 19.6072	valid_1's l2: 20.2547
[2600]	training's l2: 19.6008	valid_1's l2: 20.2542
[2610]	training's l2: 19.5949	valid_1's l2: 20.2541
[2620]	training's l2: 19.5884	valid_1's l2: 20.2531
[2630]	training's l2: 19.5822	valid_1's l2: 20.253
[2640]	training's l2: 19.5763	valid_1's l2: 20.2527
[2650]	training's l2: 19.5701	valid_1's l2: 20.2525
[2660]	training's l2: 19.5638	valid_1's l2: 20.2523
[2670]	training's l2: 19.5578	valid_1's l2: 20.2523
[2680]	training's l2: 19.5521	valid_1's l2: 20.2522
[2690]	training's l2: 19.5465	valid_1's l2: 20.2519
[2700]	training's l2: 19.5401	valid_1's l2: 20.252
[2710]	training's l2: 19.5337	valid_1's l2: 20.2518
[2720]	training's l2: 19.5276	valid_1's l2: 20.2516
[2730]	training's l2: 19.5213	valid_1's l2: 20.2513
[2740]	training's l2: 19.5156	valid_1's l2: 20.2514
[2750]	training's l2: 19.5099	valid_1's l2: 20.2515
[2760]	training's l2: 19.5039	valid_1's l2: 20.2511
[2770]	training's l2: 19.498	valid_1's l2: 20.2504
[2780]	training's l2: 19.4919	valid_1's l2: 20.2503
[2790]	training's l2: 19.4857	valid_1's l2: 20.2498
[2800]	training's l2: 19.4795	valid_1's l2: 20.249
[2810]	training's l2: 19.4737	valid_1's l2: 20.2485
[2820]	training's l2: 19.4674	valid_1's l2: 20.2486
[2830]	training's l2: 19.4619	valid_1's l2: 20.2486
[2840]	training's l2: 19.4558	valid_1's l2: 20.248
[2850]	training's l2: 19.45	valid_1's l2: 20.248
[2860]	training's l2: 19.4442	valid_1's l2: 20.2478
[2870]	training's l2: 19.4385	valid_1's l2: 20.2479
[2880]	training's l2: 19.4325	valid_1's l2: 20.2476
[2890]	training's l2: 19.4265	valid_1's l2: 20.2473
[2900]	training's l2: 19.4205	valid_1's l2: 20.2468
[2910]	training's l2: 19.4146	valid_1's l2: 20.2466
[2920]	training's l2: 19.4092	valid_1's l2: 20.2466
[2930]	training's l2: 19.4028	valid_1's l2: 20.2466
[2940]	training's l2: 19.3972	valid_1's l2: 20.2462
[2950]	training's l2: 19.3915	valid_1's l2: 20.2463
[2960]	training's l2: 19.3854	valid_1's l2: 20.2462
[2970]	training's l2: 19.3792	valid_1's l2: 20.2461
[2980]	training's l2: 19.3733	valid_1's l2: 20.2455
[2990]	training's l2: 19.3676	valid_1's l2: 20.2452
[3000]	training's l2: 19.362	valid_1's l2: 20.245
[3010]	training's l2: 19.356	valid_1's l2: 20.2452
[3020]	training's l2: 19.3498	valid_1's l2: 20.2449
[3030]	training's l2: 19.3443	valid_1's l2: 20.2446
[3040]	training's l2: 19.3382	valid_1's l2: 20.2444
[3050]	training's l2: 19.3323	valid_1's l2: 20.2441
[3060]	training's l2: 19.3261	valid_1's l2: 20.2435
[3070]	training's l2: 19.32	valid_1's l2: 20.2429
[3080]	training's l2: 19.3139	valid_1's l2: 20.2427
[3090]	training's l2: 19.3085	valid_1's l2: 20.2423
[3100]	training's l2: 19.3023	valid_1's l2: 20.2422
[3110]	training's l2: 19.296	valid_1's l2: 20.2417
[3120]	training's l2: 19.2899	valid_1's l2: 20.2416
[3130]	training's l2: 19.284	valid_1's l2: 20.2418
[3140]	training's l2: 19.2783	valid_1's l2: 20.242
[3150]	training's l2: 19.2723	valid_1's l2: 20.2415
[3160]	training's l2: 19.267	valid_1's l2: 20.2412
[3170]	training's l2: 19.2609	valid_1's l2: 20.2409
[3180]	training's l2: 19.2549	valid_1's l2: 20.2407
[3190]	training's l2: 19.2493	valid_1's l2: 20.2406
[3200]	training's l2: 19.2431	valid_1's l2: 20.2399
[3210]	training's l2: 19.2374	valid_1's l2: 20.2402
[3220]	training's l2: 19.2315	valid_1's l2: 20.2395
[3230]	training's l2: 19.226	valid_1's l2: 20.2395
[3240]	training's l2: 19.2203	valid_1's l2: 20.2394
[3250]	training's l2: 19.215	valid_1's l2: 20.2392
[3260]	training's l2: 19.2087	valid_1's l2: 20.2388
[3270]	training's l2: 19.2034	valid_1's l2: 20.2387
[3280]	training's l2: 19.1977	valid_1's l2: 20.2385
[3290]	training's l2: 19.1924	valid_1's l2: 20.2383
[3300]	training's l2: 19.1864	valid_1's l2: 20.2383
[3310]	training's l2: 19.1807	valid_1's l2: 20.2378
[3320]	training's l2: 19.175	valid_1's l2: 20.2381
[3330]	training's l2: 19.1695	valid_1's l2: 20.2379
[3340]	training's l2: 19.1638	valid_1's l2: 20.2379
[3350]	training's l2: 19.1583	valid_1's l2: 20.2379
[3360]	training's l2: 19.1521	valid_1's l2: 20.2375
[3370]	training's l2: 19.1464	valid_1's l2: 20.2371
[3380]	training's l2: 19.1404	valid_1's l2: 20.2363
[3390]	training's l2: 19.1355	valid_1's l2: 20.2365
[3400]	training's l2: 19.1294	valid_1's l2: 20.236
[3410]	training's l2: 19.1235	valid_1's l2: 20.2357
[3420]	training's l2: 19.1178	valid_1's l2: 20.2356
[3430]	training's l2: 19.1116	valid_1's l2: 20.2354
[3440]	training's l2: 19.1064	valid_1's l2: 20.2352
[3450]	training's l2: 19.1007	valid_1's l2: 20.2347
[3460]	training's l2: 19.0955	valid_1's l2: 20.2344
[3470]	training's l2: 19.0896	valid_1's l2: 20.2341
[3480]	training's l2: 19.0839	valid_1's l2: 20.2338
[3490]	training's l2: 19.0781	valid_1's l2: 20.2338
[3500]	training's l2: 19.0728	valid_1's l2: 20.2334
[3510]	training's l2: 19.0672	valid_1's l2: 20.233
[3520]	training's l2: 19.061	valid_1's l2: 20.2328
[3530]	training's l2: 19.0554	valid_1's l2: 20.2326
[3540]	training's l2: 19.0502	valid_1's l2: 20.2324
[3550]	training's l2: 19.0442	valid_1's l2: 20.2318
[3560]	training's l2: 19.0388	valid_1's l2: 20.2311
[3570]	training's l2: 19.0328	valid_1's l2: 20.2307
[3580]	training's l2: 19.0274	valid_1's l2: 20.2308
[3590]	training's l2: 19.0218	valid_1's l2: 20.2306
[3600]	training's l2: 19.0166	valid_1's l2: 20.2302
[3610]	training's l2: 19.0108	valid_1's l2: 20.2299
[3620]	training's l2: 19.0049	valid_1's l2: 20.2295
[3630]	training's l2: 18.9996	valid_1's l2: 20.2293
[3640]	training's l2: 18.9945	valid_1's l2: 20.2291
[3650]	training's l2: 18.989	valid_1's l2: 20.2291
[3660]	training's l2: 18.9841	valid_1's l2: 20.2292
Early stopping, best iteration is:
[3639]	training's l2: 18.995	valid_1's l2: 20.229
score1: 3.8394532436291464
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.291195 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.7602	valid_1's l2: 29.8645
[20]	training's l2: 28.3823	valid_1's l2: 27.504
[30]	training's l2: 26.6694	valid_1's l2: 25.8142
[40]	training's l2: 25.4202	valid_1's l2: 24.5875
[50]	training's l2: 24.4971	valid_1's l2: 23.6958
[60]	training's l2: 23.8067	valid_1's l2: 23.0389
[70]	training's l2: 23.28	valid_1's l2: 22.5483
[80]	training's l2: 22.8783	valid_1's l2: 22.1834
[90]	training's l2: 22.5583	valid_1's l2: 21.9047
[100]	training's l2: 22.298	valid_1's l2: 21.6857
[110]	training's l2: 22.0821	valid_1's l2: 21.5114
[120]	training's l2: 21.8953	valid_1's l2: 21.3705
[130]	training's l2: 21.7389	valid_1's l2: 21.2586
[140]	training's l2: 21.5957	valid_1's l2: 21.1599
[150]	training's l2: 21.4705	valid_1's l2: 21.079
[160]	training's l2: 21.3567	valid_1's l2: 21.0078
[170]	training's l2: 21.254	valid_1's l2: 20.9494
[180]	training's l2: 21.1588	valid_1's l2: 20.8954
[190]	training's l2: 21.0672	valid_1's l2: 20.8454
[200]	training's l2: 20.98	valid_1's l2: 20.8033
[210]	training's l2: 20.8985	valid_1's l2: 20.766
[220]	training's l2: 20.8229	valid_1's l2: 20.7334
[230]	training's l2: 20.7502	valid_1's l2: 20.7049
[240]	training's l2: 20.6812	valid_1's l2: 20.6797
[250]	training's l2: 20.6158	valid_1's l2: 20.657
[260]	training's l2: 20.5508	valid_1's l2: 20.6356
[270]	training's l2: 20.4883	valid_1's l2: 20.617
[280]	training's l2: 20.4272	valid_1's l2: 20.5976
[290]	training's l2: 20.3674	valid_1's l2: 20.5798
[300]	training's l2: 20.3096	valid_1's l2: 20.5639
[310]	training's l2: 20.2524	valid_1's l2: 20.5494
[320]	training's l2: 20.197	valid_1's l2: 20.5367
[330]	training's l2: 20.1414	valid_1's l2: 20.5234
[340]	training's l2: 20.0879	valid_1's l2: 20.511
[350]	training's l2: 20.0348	valid_1's l2: 20.4989
[360]	training's l2: 19.9825	valid_1's l2: 20.489
[370]	training's l2: 19.9303	valid_1's l2: 20.4799
[380]	training's l2: 19.8795	valid_1's l2: 20.4708
[390]	training's l2: 19.8271	valid_1's l2: 20.4597
[400]	training's l2: 19.776	valid_1's l2: 20.4504
[410]	training's l2: 19.7261	valid_1's l2: 20.4409
[420]	training's l2: 19.677	valid_1's l2: 20.4316
[430]	training's l2: 19.6281	valid_1's l2: 20.4241
[440]	training's l2: 19.5806	valid_1's l2: 20.4164
[450]	training's l2: 19.5326	valid_1's l2: 20.4094
[460]	training's l2: 19.4856	valid_1's l2: 20.4029
[470]	training's l2: 19.4398	valid_1's l2: 20.3963
[480]	training's l2: 19.3927	valid_1's l2: 20.3887
[490]	training's l2: 19.3479	valid_1's l2: 20.3825
[500]	training's l2: 19.3029	valid_1's l2: 20.3784
[510]	training's l2: 19.2579	valid_1's l2: 20.3752
[520]	training's l2: 19.2142	valid_1's l2: 20.3717
[530]	training's l2: 19.1698	valid_1's l2: 20.3667
[540]	training's l2: 19.1258	valid_1's l2: 20.3615
[550]	training's l2: 19.0817	valid_1's l2: 20.3564
[560]	training's l2: 19.0385	valid_1's l2: 20.3523
[570]	training's l2: 18.9957	valid_1's l2: 20.3486
[580]	training's l2: 18.9531	valid_1's l2: 20.3446
[590]	training's l2: 18.9113	valid_1's l2: 20.3425
[600]	training's l2: 18.8699	valid_1's l2: 20.3388
[610]	training's l2: 18.8283	valid_1's l2: 20.3348
[620]	training's l2: 18.7871	valid_1's l2: 20.3317
[630]	training's l2: 18.7471	valid_1's l2: 20.3278
[640]	training's l2: 18.7065	valid_1's l2: 20.3237
[650]	training's l2: 18.6661	valid_1's l2: 20.3213
[660]	training's l2: 18.6259	valid_1's l2: 20.3187
[670]	training's l2: 18.5863	valid_1's l2: 20.315
[680]	training's l2: 18.5478	valid_1's l2: 20.313
[690]	training's l2: 18.5084	valid_1's l2: 20.3119
[700]	training's l2: 18.4688	valid_1's l2: 20.3095
[710]	training's l2: 18.4307	valid_1's l2: 20.307
[720]	training's l2: 18.3918	valid_1's l2: 20.305
[730]	training's l2: 18.3542	valid_1's l2: 20.3024
[740]	training's l2: 18.3167	valid_1's l2: 20.301
[750]	training's l2: 18.2798	valid_1's l2: 20.3
[760]	training's l2: 18.2432	valid_1's l2: 20.2991
[770]	training's l2: 18.2083	valid_1's l2: 20.2971
[780]	training's l2: 18.1723	valid_1's l2: 20.2956
[790]	training's l2: 18.1369	valid_1's l2: 20.2933
[800]	training's l2: 18.1012	valid_1's l2: 20.2942
[810]	training's l2: 18.0684	valid_1's l2: 20.2917
[820]	training's l2: 18.0338	valid_1's l2: 20.2909
[830]	training's l2: 17.999	valid_1's l2: 20.2908
[840]	training's l2: 17.9658	valid_1's l2: 20.2896
[850]	training's l2: 17.9325	valid_1's l2: 20.2883
[860]	training's l2: 17.8981	valid_1's l2: 20.287
[870]	training's l2: 17.8642	valid_1's l2: 20.2866
[880]	training's l2: 17.8311	valid_1's l2: 20.287
[890]	training's l2: 17.8002	valid_1's l2: 20.2859
[900]	training's l2: 17.7671	valid_1's l2: 20.2852
[910]	training's l2: 17.7353	valid_1's l2: 20.2848
[920]	training's l2: 17.7027	valid_1's l2: 20.2828
[930]	training's l2: 17.6685	valid_1's l2: 20.2821
[940]	training's l2: 17.6366	valid_1's l2: 20.2816
[950]	training's l2: 17.6045	valid_1's l2: 20.2812
[960]	training's l2: 17.5717	valid_1's l2: 20.2807
[970]	training's l2: 17.5407	valid_1's l2: 20.2793
[980]	training's l2: 17.5094	valid_1's l2: 20.2773
[990]	training's l2: 17.4779	valid_1's l2: 20.2766
[1000]	training's l2: 17.4454	valid_1's l2: 20.2757
[1010]	training's l2: 17.4135	valid_1's l2: 20.2765
[1020]	training's l2: 17.3833	valid_1's l2: 20.2765
Early stopping, best iteration is:
[996]	training's l2: 17.459	valid_1's l2: 20.2757
score1: 3.837977313630951
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.244001 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 32.0565	valid_1's l2: 31.1536
[20]	training's l2: 30.384	valid_1's l2: 29.4938
[30]	training's l2: 28.9998	valid_1's l2: 28.1206
[40]	training's l2: 27.8532	valid_1's l2: 26.9873
[50]	training's l2: 26.899	valid_1's l2: 26.0467
[60]	training's l2: 26.1012	valid_1's l2: 25.2622
[70]	training's l2: 25.4326	valid_1's l2: 24.6085
[80]	training's l2: 24.8692	valid_1's l2: 24.0635
[90]	training's l2: 24.3936	valid_1's l2: 23.6059
[100]	training's l2: 23.9887	valid_1's l2: 23.2206
[110]	training's l2: 23.6432	valid_1's l2: 22.8965
[120]	training's l2: 23.3469	valid_1's l2: 22.6242
[130]	training's l2: 23.0924	valid_1's l2: 22.3911
[140]	training's l2: 22.8728	valid_1's l2: 22.1929
[150]	training's l2: 22.6806	valid_1's l2: 22.0238
[160]	training's l2: 22.5085	valid_1's l2: 21.8768
[170]	training's l2: 22.3557	valid_1's l2: 21.7489
[180]	training's l2: 22.2186	valid_1's l2: 21.6356
[190]	training's l2: 22.0962	valid_1's l2: 21.5387
[200]	training's l2: 21.9823	valid_1's l2: 21.4505
[210]	training's l2: 21.8784	valid_1's l2: 21.3723
[220]	training's l2: 21.7844	valid_1's l2: 21.3048
[230]	training's l2: 21.6971	valid_1's l2: 21.2446
[240]	training's l2: 21.6135	valid_1's l2: 21.1875
[250]	training's l2: 21.5346	valid_1's l2: 21.1348
[260]	training's l2: 21.4622	valid_1's l2: 21.0874
[270]	training's l2: 21.3942	valid_1's l2: 21.0445
[280]	training's l2: 21.3296	valid_1's l2: 21.0061
[290]	training's l2: 21.2672	valid_1's l2: 20.9697
[300]	training's l2: 21.2094	valid_1's l2: 20.9374
[310]	training's l2: 21.1539	valid_1's l2: 20.9057
[320]	training's l2: 21.0998	valid_1's l2: 20.8772
[330]	training's l2: 21.0455	valid_1's l2: 20.8482
[340]	training's l2: 20.9936	valid_1's l2: 20.8209
[350]	training's l2: 20.9442	valid_1's l2: 20.7983
[360]	training's l2: 20.8951	valid_1's l2: 20.7758
[370]	training's l2: 20.8488	valid_1's l2: 20.7556
[380]	training's l2: 20.8039	valid_1's l2: 20.7357
[390]	training's l2: 20.7602	valid_1's l2: 20.7186
[400]	training's l2: 20.7183	valid_1's l2: 20.703
[410]	training's l2: 20.6774	valid_1's l2: 20.6874
[420]	training's l2: 20.6382	valid_1's l2: 20.6738
[430]	training's l2: 20.5982	valid_1's l2: 20.6592
[440]	training's l2: 20.56	valid_1's l2: 20.646
[450]	training's l2: 20.5221	valid_1's l2: 20.6332
[460]	training's l2: 20.4851	valid_1's l2: 20.6209
[470]	training's l2: 20.4482	valid_1's l2: 20.6092
[480]	training's l2: 20.4121	valid_1's l2: 20.5969
[490]	training's l2: 20.3761	valid_1's l2: 20.5855
[500]	training's l2: 20.3409	valid_1's l2: 20.5764
[510]	training's l2: 20.3062	valid_1's l2: 20.5672
[520]	training's l2: 20.2717	valid_1's l2: 20.5582
[530]	training's l2: 20.2381	valid_1's l2: 20.55
[540]	training's l2: 20.2043	valid_1's l2: 20.5413
[550]	training's l2: 20.171	valid_1's l2: 20.5335
[560]	training's l2: 20.1385	valid_1's l2: 20.5254
[570]	training's l2: 20.1056	valid_1's l2: 20.5179
[580]	training's l2: 20.0737	valid_1's l2: 20.5115
[590]	training's l2: 20.0416	valid_1's l2: 20.5038
[600]	training's l2: 20.0103	valid_1's l2: 20.4973
[610]	training's l2: 19.9785	valid_1's l2: 20.4907
[620]	training's l2: 19.9476	valid_1's l2: 20.485
[630]	training's l2: 19.9168	valid_1's l2: 20.4793
[640]	training's l2: 19.8862	valid_1's l2: 20.4738
[650]	training's l2: 19.8558	valid_1's l2: 20.4678
[660]	training's l2: 19.8252	valid_1's l2: 20.4611
[670]	training's l2: 19.7953	valid_1's l2: 20.4548
[680]	training's l2: 19.7653	valid_1's l2: 20.45
[690]	training's l2: 19.7352	valid_1's l2: 20.4446
[700]	training's l2: 19.705	valid_1's l2: 20.4386
[710]	training's l2: 19.6755	valid_1's l2: 20.4335
[720]	training's l2: 19.6461	valid_1's l2: 20.4285
[730]	training's l2: 19.617	valid_1's l2: 20.4231
[740]	training's l2: 19.5877	valid_1's l2: 20.4179
[750]	training's l2: 19.559	valid_1's l2: 20.4127
[760]	training's l2: 19.5306	valid_1's l2: 20.4081
[770]	training's l2: 19.5022	valid_1's l2: 20.4036
[780]	training's l2: 19.4739	valid_1's l2: 20.3992
[790]	training's l2: 19.446	valid_1's l2: 20.3956
[800]	training's l2: 19.4183	valid_1's l2: 20.3917
[810]	training's l2: 19.3904	valid_1's l2: 20.3872
[820]	training's l2: 19.3628	valid_1's l2: 20.3828
[830]	training's l2: 19.3358	valid_1's l2: 20.3797
[840]	training's l2: 19.3087	valid_1's l2: 20.3756
[850]	training's l2: 19.2817	valid_1's l2: 20.3722
[860]	training's l2: 19.2548	valid_1's l2: 20.3686
[870]	training's l2: 19.2281	valid_1's l2: 20.3656
[880]	training's l2: 19.2013	valid_1's l2: 20.3619
[890]	training's l2: 19.1749	valid_1's l2: 20.3594
[900]	training's l2: 19.1486	valid_1's l2: 20.3561
[910]	training's l2: 19.1226	valid_1's l2: 20.3532
[920]	training's l2: 19.0963	valid_1's l2: 20.3507
[930]	training's l2: 19.0699	valid_1's l2: 20.3481
[940]	training's l2: 19.0445	valid_1's l2: 20.3452
[950]	training's l2: 19.0189	valid_1's l2: 20.343
[960]	training's l2: 18.9933	valid_1's l2: 20.3406
[970]	training's l2: 18.9676	valid_1's l2: 20.3379
[980]	training's l2: 18.9424	valid_1's l2: 20.3362
[990]	training's l2: 18.917	valid_1's l2: 20.3337
[1000]	training's l2: 18.8918	valid_1's l2: 20.3318
[1010]	training's l2: 18.8666	valid_1's l2: 20.3289
[1020]	training's l2: 18.8417	valid_1's l2: 20.3266
[1030]	training's l2: 18.8175	valid_1's l2: 20.3247
[1040]	training's l2: 18.7928	valid_1's l2: 20.3221
[1050]	training's l2: 18.7676	valid_1's l2: 20.32
[1060]	training's l2: 18.7434	valid_1's l2: 20.317
[1070]	training's l2: 18.7187	valid_1's l2: 20.3143
[1080]	training's l2: 18.6946	valid_1's l2: 20.3124
[1090]	training's l2: 18.6703	valid_1's l2: 20.3102
[1100]	training's l2: 18.6459	valid_1's l2: 20.3075
[1110]	training's l2: 18.6218	valid_1's l2: 20.3055
[1120]	training's l2: 18.5983	valid_1's l2: 20.3036
[1130]	training's l2: 18.5741	valid_1's l2: 20.3025
[1140]	training's l2: 18.5502	valid_1's l2: 20.3006
[1150]	training's l2: 18.5266	valid_1's l2: 20.2993
[1160]	training's l2: 18.5029	valid_1's l2: 20.2975
[1170]	training's l2: 18.4793	valid_1's l2: 20.2956
[1180]	training's l2: 18.4559	valid_1's l2: 20.2942
[1190]	training's l2: 18.4326	valid_1's l2: 20.2927
[1200]	training's l2: 18.4095	valid_1's l2: 20.2912
[1210]	training's l2: 18.3864	valid_1's l2: 20.2887
[1220]	training's l2: 18.3636	valid_1's l2: 20.2869
[1230]	training's l2: 18.3408	valid_1's l2: 20.2848
[1240]	training's l2: 18.3189	valid_1's l2: 20.2838
[1250]	training's l2: 18.2963	valid_1's l2: 20.2819
[1260]	training's l2: 18.2735	valid_1's l2: 20.2807
[1270]	training's l2: 18.2514	valid_1's l2: 20.2798
[1280]	training's l2: 18.2289	valid_1's l2: 20.2783
[1290]	training's l2: 18.2067	valid_1's l2: 20.2778
[1300]	training's l2: 18.1852	valid_1's l2: 20.2765
[1310]	training's l2: 18.1641	valid_1's l2: 20.2747
[1320]	training's l2: 18.1429	valid_1's l2: 20.2734
[1330]	training's l2: 18.1212	valid_1's l2: 20.272
[1340]	training's l2: 18.0993	valid_1's l2: 20.2715
[1350]	training's l2: 18.0778	valid_1's l2: 20.2701
[1360]	training's l2: 18.0559	valid_1's l2: 20.269
[1370]	training's l2: 18.0353	valid_1's l2: 20.2679
[1380]	training's l2: 18.0149	valid_1's l2: 20.2671
[1390]	training's l2: 17.9939	valid_1's l2: 20.266
[1400]	training's l2: 17.9732	valid_1's l2: 20.2649
[1410]	training's l2: 17.9522	valid_1's l2: 20.2636
[1420]	training's l2: 17.9313	valid_1's l2: 20.2629
[1430]	training's l2: 17.9113	valid_1's l2: 20.2626
[1440]	training's l2: 17.8909	valid_1's l2: 20.2612
[1450]	training's l2: 17.8706	valid_1's l2: 20.2598
[1460]	training's l2: 17.8497	valid_1's l2: 20.2589
[1470]	training's l2: 17.8297	valid_1's l2: 20.2578
[1480]	training's l2: 17.809	valid_1's l2: 20.2572
[1490]	training's l2: 17.7892	valid_1's l2: 20.2567
[1500]	training's l2: 17.7703	valid_1's l2: 20.2561
[1510]	training's l2: 17.7507	valid_1's l2: 20.2552
[1520]	training's l2: 17.7305	valid_1's l2: 20.2547
[1530]	training's l2: 17.7103	valid_1's l2: 20.2547
[1540]	training's l2: 17.6895	valid_1's l2: 20.2546
[1550]	training's l2: 17.6697	valid_1's l2: 20.2536
[1560]	training's l2: 17.6506	valid_1's l2: 20.2527
[1570]	training's l2: 17.6314	valid_1's l2: 20.2522
[1580]	training's l2: 17.6118	valid_1's l2: 20.2509
[1590]	training's l2: 17.5925	valid_1's l2: 20.2505
[1600]	training's l2: 17.5734	valid_1's l2: 20.2496
[1610]	training's l2: 17.5541	valid_1's l2: 20.2489
[1620]	training's l2: 17.5344	valid_1's l2: 20.2484
[1630]	training's l2: 17.5151	valid_1's l2: 20.2477
[1640]	training's l2: 17.4961	valid_1's l2: 20.2469
[1650]	training's l2: 17.4771	valid_1's l2: 20.2463
[1660]	training's l2: 17.4573	valid_1's l2: 20.2455
[1670]	training's l2: 17.4379	valid_1's l2: 20.2453
[1680]	training's l2: 17.4196	valid_1's l2: 20.2454
[1690]	training's l2: 17.4001	valid_1's l2: 20.2447
[1700]	training's l2: 17.3811	valid_1's l2: 20.2445
[1710]	training's l2: 17.3623	valid_1's l2: 20.244
[1720]	training's l2: 17.3442	valid_1's l2: 20.2435
[1730]	training's l2: 17.3245	valid_1's l2: 20.2421
[1740]	training's l2: 17.3051	valid_1's l2: 20.2412
[1750]	training's l2: 17.2861	valid_1's l2: 20.2403
[1760]	training's l2: 17.267	valid_1's l2: 20.2402
[1770]	training's l2: 17.2493	valid_1's l2: 20.2396
[1780]	training's l2: 17.231	valid_1's l2: 20.2397
[1790]	training's l2: 17.2128	valid_1's l2: 20.2389
[1800]	training's l2: 17.1948	valid_1's l2: 20.2389
[1810]	training's l2: 17.1758	valid_1's l2: 20.238
[1820]	training's l2: 17.1574	valid_1's l2: 20.2378
[1830]	training's l2: 17.1397	valid_1's l2: 20.2375
[1840]	training's l2: 17.1217	valid_1's l2: 20.237
[1850]	training's l2: 17.1029	valid_1's l2: 20.2366
[1860]	training's l2: 17.0865	valid_1's l2: 20.2368
[1870]	training's l2: 17.0688	valid_1's l2: 20.236
[1880]	training's l2: 17.0511	valid_1's l2: 20.236
[1890]	training's l2: 17.0344	valid_1's l2: 20.235
[1900]	training's l2: 17.0167	valid_1's l2: 20.235
[1910]	training's l2: 16.9997	valid_1's l2: 20.2351
[1920]	training's l2: 16.982	valid_1's l2: 20.2349
Early stopping, best iteration is:
[1895]	training's l2: 17.0253	valid_1's l2: 20.2346
score1: 3.8362394295967284
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.245337 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.6446	valid_1's l2: 30.7448
[20]	training's l2: 29.7172	valid_1's l2: 28.8345
[30]	training's l2: 28.1874	valid_1's l2: 27.3217
[40]	training's l2: 26.9691	valid_1's l2: 26.1208
[50]	training's l2: 25.9922	valid_1's l2: 25.1624
[60]	training's l2: 25.2058	valid_1's l2: 24.3956
[70]	training's l2: 24.5669	valid_1's l2: 23.7808
[80]	training's l2: 24.0449	valid_1's l2: 23.2832
[90]	training's l2: 23.6143	valid_1's l2: 22.8787
[100]	training's l2: 23.2581	valid_1's l2: 22.5497
[110]	training's l2: 22.963	valid_1's l2: 22.2828
[120]	training's l2: 22.7128	valid_1's l2: 22.0606
[130]	training's l2: 22.4976	valid_1's l2: 21.8763
[140]	training's l2: 22.3105	valid_1's l2: 21.7212
[150]	training's l2: 22.1468	valid_1's l2: 21.5881
[160]	training's l2: 22.0018	valid_1's l2: 21.4749
[170]	training's l2: 21.871	valid_1's l2: 21.3767
[180]	training's l2: 21.7538	valid_1's l2: 21.2934
[190]	training's l2: 21.6465	valid_1's l2: 21.2194
[200]	training's l2: 21.5453	valid_1's l2: 21.1498
[210]	training's l2: 21.4543	valid_1's l2: 21.0918
[220]	training's l2: 21.37	valid_1's l2: 21.039
[230]	training's l2: 21.2886	valid_1's l2: 20.9903
[240]	training's l2: 21.2144	valid_1's l2: 20.9478
[250]	training's l2: 21.144	valid_1's l2: 20.9089
[260]	training's l2: 21.0778	valid_1's l2: 20.8733
[270]	training's l2: 21.0112	valid_1's l2: 20.8384
[280]	training's l2: 20.9474	valid_1's l2: 20.806
[290]	training's l2: 20.8868	valid_1's l2: 20.7783
[300]	training's l2: 20.8271	valid_1's l2: 20.7533
[310]	training's l2: 20.7715	valid_1's l2: 20.7295
[320]	training's l2: 20.7171	valid_1's l2: 20.7077
[330]	training's l2: 20.6656	valid_1's l2: 20.6883
[340]	training's l2: 20.6154	valid_1's l2: 20.6709
[350]	training's l2: 20.5668	valid_1's l2: 20.6537
[360]	training's l2: 20.5193	valid_1's l2: 20.6385
[370]	training's l2: 20.4725	valid_1's l2: 20.6252
[380]	training's l2: 20.427	valid_1's l2: 20.612
[390]	training's l2: 20.3825	valid_1's l2: 20.599
[400]	training's l2: 20.3377	valid_1's l2: 20.5862
[410]	training's l2: 20.2939	valid_1's l2: 20.5747
[420]	training's l2: 20.2505	valid_1's l2: 20.5625
[430]	training's l2: 20.2084	valid_1's l2: 20.5521
[440]	training's l2: 20.166	valid_1's l2: 20.5413
[450]	training's l2: 20.1246	valid_1's l2: 20.5317
[460]	training's l2: 20.0842	valid_1's l2: 20.5221
[470]	training's l2: 20.044	valid_1's l2: 20.5129
[480]	training's l2: 20.0031	valid_1's l2: 20.504
[490]	training's l2: 19.9637	valid_1's l2: 20.4955
[500]	training's l2: 19.9249	valid_1's l2: 20.488
[510]	training's l2: 19.8853	valid_1's l2: 20.4796
[520]	training's l2: 19.8465	valid_1's l2: 20.472
[530]	training's l2: 19.8087	valid_1's l2: 20.4648
[540]	training's l2: 19.7702	valid_1's l2: 20.4574
[550]	training's l2: 19.7327	valid_1's l2: 20.4505
[560]	training's l2: 19.6952	valid_1's l2: 20.4442
[570]	training's l2: 19.6578	valid_1's l2: 20.4359
[580]	training's l2: 19.6212	valid_1's l2: 20.4301
[590]	training's l2: 19.5845	valid_1's l2: 20.4239
[600]	training's l2: 19.5483	valid_1's l2: 20.4174
[610]	training's l2: 19.5121	valid_1's l2: 20.412
[620]	training's l2: 19.4768	valid_1's l2: 20.4065
[630]	training's l2: 19.4412	valid_1's l2: 20.4008
[640]	training's l2: 19.4059	valid_1's l2: 20.396
[650]	training's l2: 19.3709	valid_1's l2: 20.39
[660]	training's l2: 19.3364	valid_1's l2: 20.3856
[670]	training's l2: 19.3024	valid_1's l2: 20.3801
[680]	training's l2: 19.2678	valid_1's l2: 20.3754
[690]	training's l2: 19.2339	valid_1's l2: 20.3712
[700]	training's l2: 19.2	valid_1's l2: 20.3687
[710]	training's l2: 19.1665	valid_1's l2: 20.3654
[720]	training's l2: 19.1335	valid_1's l2: 20.3616
[730]	training's l2: 19.1002	valid_1's l2: 20.3579
[740]	training's l2: 19.0677	valid_1's l2: 20.3535
[750]	training's l2: 19.0356	valid_1's l2: 20.35
[760]	training's l2: 19.0033	valid_1's l2: 20.3463
[770]	training's l2: 18.9707	valid_1's l2: 20.3435
[780]	training's l2: 18.9386	valid_1's l2: 20.3399
[790]	training's l2: 18.9064	valid_1's l2: 20.3376
[800]	training's l2: 18.8746	valid_1's l2: 20.3341
[810]	training's l2: 18.8435	valid_1's l2: 20.3316
[820]	training's l2: 18.8124	valid_1's l2: 20.3284
[830]	training's l2: 18.7813	valid_1's l2: 20.3265
[840]	training's l2: 18.7501	valid_1's l2: 20.324
[850]	training's l2: 18.7195	valid_1's l2: 20.3211
[860]	training's l2: 18.6889	valid_1's l2: 20.3171
[870]	training's l2: 18.6583	valid_1's l2: 20.314
[880]	training's l2: 18.6286	valid_1's l2: 20.3118
[890]	training's l2: 18.5988	valid_1's l2: 20.3092
[900]	training's l2: 18.5688	valid_1's l2: 20.3064
[910]	training's l2: 18.5394	valid_1's l2: 20.3037
[920]	training's l2: 18.5096	valid_1's l2: 20.3007
[930]	training's l2: 18.4793	valid_1's l2: 20.2988
[940]	training's l2: 18.4504	valid_1's l2: 20.2973
[950]	training's l2: 18.4209	valid_1's l2: 20.2946
[960]	training's l2: 18.392	valid_1's l2: 20.2923
[970]	training's l2: 18.3629	valid_1's l2: 20.29
[980]	training's l2: 18.3337	valid_1's l2: 20.2879
[990]	training's l2: 18.3055	valid_1's l2: 20.2861
[1000]	training's l2: 18.2773	valid_1's l2: 20.2838
[1010]	training's l2: 18.249	valid_1's l2: 20.2836
[1020]	training's l2: 18.2212	valid_1's l2: 20.2817
[1030]	training's l2: 18.1929	valid_1's l2: 20.2799
[1040]	training's l2: 18.1657	valid_1's l2: 20.2785
[1050]	training's l2: 18.1383	valid_1's l2: 20.2774
[1060]	training's l2: 18.1105	valid_1's l2: 20.2752
[1070]	training's l2: 18.0832	valid_1's l2: 20.2744
[1080]	training's l2: 18.0567	valid_1's l2: 20.2734
[1090]	training's l2: 18.0302	valid_1's l2: 20.2712
[1100]	training's l2: 18.0024	valid_1's l2: 20.2696
[1110]	training's l2: 17.9762	valid_1's l2: 20.2675
[1120]	training's l2: 17.9495	valid_1's l2: 20.2668
[1130]	training's l2: 17.9242	valid_1's l2: 20.2662
[1140]	training's l2: 17.8984	valid_1's l2: 20.2654
[1150]	training's l2: 17.8731	valid_1's l2: 20.264
[1160]	training's l2: 17.8467	valid_1's l2: 20.2628
[1170]	training's l2: 17.8225	valid_1's l2: 20.2624
[1180]	training's l2: 17.7972	valid_1's l2: 20.2619
[1190]	training's l2: 17.7707	valid_1's l2: 20.2609
[1200]	training's l2: 17.7461	valid_1's l2: 20.2606
[1210]	training's l2: 17.7209	valid_1's l2: 20.259
[1220]	training's l2: 17.6969	valid_1's l2: 20.2582
[1230]	training's l2: 17.6721	valid_1's l2: 20.2573
[1240]	training's l2: 17.6473	valid_1's l2: 20.2554
[1250]	training's l2: 17.6223	valid_1's l2: 20.2546
[1260]	training's l2: 17.5983	valid_1's l2: 20.2539
[1270]	training's l2: 17.574	valid_1's l2: 20.253
[1280]	training's l2: 17.5497	valid_1's l2: 20.2523
[1290]	training's l2: 17.5262	valid_1's l2: 20.2515
[1300]	training's l2: 17.5022	valid_1's l2: 20.2513
[1310]	training's l2: 17.4791	valid_1's l2: 20.2511
[1320]	training's l2: 17.4563	valid_1's l2: 20.2503
[1330]	training's l2: 17.4334	valid_1's l2: 20.2499
[1340]	training's l2: 17.4102	valid_1's l2: 20.2491
[1350]	training's l2: 17.3865	valid_1's l2: 20.2485
[1360]	training's l2: 17.3625	valid_1's l2: 20.2483
[1370]	training's l2: 17.3397	valid_1's l2: 20.2482
[1380]	training's l2: 17.3168	valid_1's l2: 20.2486
Early stopping, best iteration is:
[1356]	training's l2: 17.3719	valid_1's l2: 20.2481
score1: 3.833646384762521
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.248752 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.5942	valid_1's l2: 30.696
[20]	training's l2: 29.6368	valid_1's l2: 28.757
[30]	training's l2: 28.0915	valid_1's l2: 27.2286
[40]	training's l2: 26.8663	valid_1's l2: 26.0218
[50]	training's l2: 25.8891	valid_1's l2: 25.0635
[60]	training's l2: 25.1042	valid_1's l2: 24.3028
[70]	training's l2: 24.4693	valid_1's l2: 23.6917
[80]	training's l2: 23.9534	valid_1's l2: 23.2029
[90]	training's l2: 23.5286	valid_1's l2: 22.806
[100]	training's l2: 23.1782	valid_1's l2: 22.4847
[110]	training's l2: 22.8872	valid_1's l2: 22.2225
[120]	training's l2: 22.6431	valid_1's l2: 22.0071
[130]	training's l2: 22.4307	valid_1's l2: 21.8267
[140]	training's l2: 22.2467	valid_1's l2: 21.6745
[150]	training's l2: 22.0871	valid_1's l2: 21.5459
[160]	training's l2: 21.9436	valid_1's l2: 21.4358
[170]	training's l2: 21.8148	valid_1's l2: 21.3418
[180]	training's l2: 21.6994	valid_1's l2: 21.2626
[190]	training's l2: 21.5904	valid_1's l2: 21.1891
[200]	training's l2: 21.4901	valid_1's l2: 21.1229
[210]	training's l2: 21.4001	valid_1's l2: 21.0658
[220]	training's l2: 21.3159	valid_1's l2: 21.0142
[230]	training's l2: 21.236	valid_1's l2: 20.9674
[240]	training's l2: 21.1618	valid_1's l2: 20.9264
[250]	training's l2: 21.0917	valid_1's l2: 20.8887
[260]	training's l2: 21.0221	valid_1's l2: 20.8523
[270]	training's l2: 20.9558	valid_1's l2: 20.8193
[280]	training's l2: 20.8922	valid_1's l2: 20.7899
[290]	training's l2: 20.8302	valid_1's l2: 20.763
[300]	training's l2: 20.7711	valid_1's l2: 20.7382
[310]	training's l2: 20.7162	valid_1's l2: 20.7169
[320]	training's l2: 20.6621	valid_1's l2: 20.6976
[330]	training's l2: 20.6093	valid_1's l2: 20.6775
[340]	training's l2: 20.5581	valid_1's l2: 20.6593
[350]	training's l2: 20.508	valid_1's l2: 20.6423
[360]	training's l2: 20.4594	valid_1's l2: 20.6277
[370]	training's l2: 20.4117	valid_1's l2: 20.6133
[380]	training's l2: 20.3643	valid_1's l2: 20.5989
[390]	training's l2: 20.3184	valid_1's l2: 20.5849
[400]	training's l2: 20.2724	valid_1's l2: 20.5714
[410]	training's l2: 20.2281	valid_1's l2: 20.5602
[420]	training's l2: 20.1839	valid_1's l2: 20.5483
[430]	training's l2: 20.1411	valid_1's l2: 20.5379
[440]	training's l2: 20.0984	valid_1's l2: 20.529
[450]	training's l2: 20.0561	valid_1's l2: 20.5199
[460]	training's l2: 20.0146	valid_1's l2: 20.5119
[470]	training's l2: 19.974	valid_1's l2: 20.5035
[480]	training's l2: 19.9332	valid_1's l2: 20.4949
[490]	training's l2: 19.8931	valid_1's l2: 20.4873
[500]	training's l2: 19.8536	valid_1's l2: 20.4801
[510]	training's l2: 19.8137	valid_1's l2: 20.4726
[520]	training's l2: 19.7738	valid_1's l2: 20.4648
[530]	training's l2: 19.7355	valid_1's l2: 20.4584
[540]	training's l2: 19.6971	valid_1's l2: 20.453
[550]	training's l2: 19.6578	valid_1's l2: 20.4458
[560]	training's l2: 19.6198	valid_1's l2: 20.4382
[570]	training's l2: 19.5818	valid_1's l2: 20.4329
[580]	training's l2: 19.5441	valid_1's l2: 20.4259
[590]	training's l2: 19.5072	valid_1's l2: 20.4199
[600]	training's l2: 19.4701	valid_1's l2: 20.414
[610]	training's l2: 19.434	valid_1's l2: 20.4092
[620]	training's l2: 19.3977	valid_1's l2: 20.4042
[630]	training's l2: 19.3618	valid_1's l2: 20.399
[640]	training's l2: 19.3261	valid_1's l2: 20.3949
[650]	training's l2: 19.2901	valid_1's l2: 20.3882
[660]	training's l2: 19.2547	valid_1's l2: 20.384
[670]	training's l2: 19.2195	valid_1's l2: 20.3788
[680]	training's l2: 19.1844	valid_1's l2: 20.3745
[690]	training's l2: 19.1498	valid_1's l2: 20.3705
[700]	training's l2: 19.1151	valid_1's l2: 20.366
[710]	training's l2: 19.0808	valid_1's l2: 20.3629
[720]	training's l2: 19.0466	valid_1's l2: 20.3595
[730]	training's l2: 19.0131	valid_1's l2: 20.3558
[740]	training's l2: 18.979	valid_1's l2: 20.3525
[750]	training's l2: 18.9458	valid_1's l2: 20.349
[760]	training's l2: 18.9132	valid_1's l2: 20.3463
[770]	training's l2: 18.8795	valid_1's l2: 20.3436
[780]	training's l2: 18.8461	valid_1's l2: 20.3408
[790]	training's l2: 18.8139	valid_1's l2: 20.3373
[800]	training's l2: 18.7815	valid_1's l2: 20.3341
[810]	training's l2: 18.7492	valid_1's l2: 20.3324
[820]	training's l2: 18.7172	valid_1's l2: 20.3283
[830]	training's l2: 18.6849	valid_1's l2: 20.3252
[840]	training's l2: 18.6532	valid_1's l2: 20.3221
[850]	training's l2: 18.6216	valid_1's l2: 20.3186
[860]	training's l2: 18.5895	valid_1's l2: 20.3153
[870]	training's l2: 18.5583	valid_1's l2: 20.3123
[880]	training's l2: 18.5275	valid_1's l2: 20.3096
[890]	training's l2: 18.4967	valid_1's l2: 20.3062
[900]	training's l2: 18.4658	valid_1's l2: 20.3042
[910]	training's l2: 18.4349	valid_1's l2: 20.3014
[920]	training's l2: 18.4048	valid_1's l2: 20.2993
[930]	training's l2: 18.375	valid_1's l2: 20.2962
[940]	training's l2: 18.3451	valid_1's l2: 20.2936
[950]	training's l2: 18.3155	valid_1's l2: 20.292
[960]	training's l2: 18.2859	valid_1's l2: 20.2905
[970]	training's l2: 18.2573	valid_1's l2: 20.2887
[980]	training's l2: 18.2281	valid_1's l2: 20.2875
[990]	training's l2: 18.1989	valid_1's l2: 20.286
[1000]	training's l2: 18.1702	valid_1's l2: 20.2841
[1010]	training's l2: 18.1417	valid_1's l2: 20.2823
[1020]	training's l2: 18.1136	valid_1's l2: 20.2816
[1030]	training's l2: 18.0857	valid_1's l2: 20.2803
[1040]	training's l2: 18.057	valid_1's l2: 20.2791
[1050]	training's l2: 18.0281	valid_1's l2: 20.2777
[1060]	training's l2: 18.0004	valid_1's l2: 20.2759
[1070]	training's l2: 17.9733	valid_1's l2: 20.2743
[1080]	training's l2: 17.9459	valid_1's l2: 20.2728
[1090]	training's l2: 17.9178	valid_1's l2: 20.2722
[1100]	training's l2: 17.891	valid_1's l2: 20.2712
[1110]	training's l2: 17.8642	valid_1's l2: 20.2704
[1120]	training's l2: 17.838	valid_1's l2: 20.2691
[1130]	training's l2: 17.8113	valid_1's l2: 20.2681
[1140]	training's l2: 17.7855	valid_1's l2: 20.2671
[1150]	training's l2: 17.7592	valid_1's l2: 20.2665
[1160]	training's l2: 17.7332	valid_1's l2: 20.2664
[1170]	training's l2: 17.7082	valid_1's l2: 20.2659
[1180]	training's l2: 17.6812	valid_1's l2: 20.2649
[1190]	training's l2: 17.6557	valid_1's l2: 20.2637
[1200]	training's l2: 17.6303	valid_1's l2: 20.2629
[1210]	training's l2: 17.6045	valid_1's l2: 20.2622
[1220]	training's l2: 17.5789	valid_1's l2: 20.2613
[1230]	training's l2: 17.5537	valid_1's l2: 20.2617
[1240]	training's l2: 17.5278	valid_1's l2: 20.2609
[1250]	training's l2: 17.5027	valid_1's l2: 20.2613
[1260]	training's l2: 17.4782	valid_1's l2: 20.2605
[1270]	training's l2: 17.453	valid_1's l2: 20.2596
[1280]	training's l2: 17.4288	valid_1's l2: 20.2596
[1290]	training's l2: 17.4032	valid_1's l2: 20.2593
[1300]	training's l2: 17.3773	valid_1's l2: 20.2579
[1310]	training's l2: 17.3538	valid_1's l2: 20.2571
[1320]	training's l2: 17.3289	valid_1's l2: 20.2567
[1330]	training's l2: 17.3049	valid_1's l2: 20.2575
[1340]	training's l2: 17.2796	valid_1's l2: 20.2569
Early stopping, best iteration is:
[1316]	training's l2: 17.3388	valid_1's l2: 20.2562
score1: 3.8385128936541912
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273176 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.3379	valid_1's l2: 30.4465
[20]	training's l2: 29.2361	valid_1's l2: 28.369
[30]	training's l2: 27.6195	valid_1's l2: 26.7751
[40]	training's l2: 26.3663	valid_1's l2: 25.5459
[50]	training's l2: 25.3879	valid_1's l2: 24.5945
[60]	training's l2: 24.6173	valid_1's l2: 23.8557
[70]	training's l2: 24.0053	valid_1's l2: 23.2788
[80]	training's l2: 23.5148	valid_1's l2: 22.8218
[90]	training's l2: 23.118	valid_1's l2: 22.4596
[100]	training's l2: 22.7957	valid_1's l2: 22.1754
[110]	training's l2: 22.5271	valid_1's l2: 21.9427
[120]	training's l2: 22.2976	valid_1's l2: 21.7531
[130]	training's l2: 22.1021	valid_1's l2: 21.5977
[140]	training's l2: 21.9309	valid_1's l2: 21.4656
[150]	training's l2: 21.7755	valid_1's l2: 21.354
[160]	training's l2: 21.6397	valid_1's l2: 21.2603
[170]	training's l2: 21.5155	valid_1's l2: 21.1781
[180]	training's l2: 21.4016	valid_1's l2: 21.1057
[190]	training's l2: 21.298	valid_1's l2: 21.043
[200]	training's l2: 21.2009	valid_1's l2: 20.9874
[210]	training's l2: 21.1111	valid_1's l2: 20.938
[220]	training's l2: 21.0274	valid_1's l2: 20.8941
[230]	training's l2: 20.9485	valid_1's l2: 20.8533
[240]	training's l2: 20.8693	valid_1's l2: 20.8142
[250]	training's l2: 20.7939	valid_1's l2: 20.7812
[260]	training's l2: 20.7222	valid_1's l2: 20.7508
[270]	training's l2: 20.6536	valid_1's l2: 20.7248
[280]	training's l2: 20.5874	valid_1's l2: 20.6998
[290]	training's l2: 20.5247	valid_1's l2: 20.6795
[300]	training's l2: 20.4636	valid_1's l2: 20.6594
[310]	training's l2: 20.4049	valid_1's l2: 20.6395
[320]	training's l2: 20.3479	valid_1's l2: 20.6232
[330]	training's l2: 20.2915	valid_1's l2: 20.6064
[340]	training's l2: 20.2358	valid_1's l2: 20.5908
[350]	training's l2: 20.1816	valid_1's l2: 20.5766
[360]	training's l2: 20.1282	valid_1's l2: 20.5623
[370]	training's l2: 20.0761	valid_1's l2: 20.5509
[380]	training's l2: 20.0248	valid_1's l2: 20.5395
[390]	training's l2: 19.9731	valid_1's l2: 20.5273
[400]	training's l2: 19.9233	valid_1's l2: 20.5167
[410]	training's l2: 19.8739	valid_1's l2: 20.507
[420]	training's l2: 19.8256	valid_1's l2: 20.4976
[430]	training's l2: 19.7778	valid_1's l2: 20.4895
[440]	training's l2: 19.7304	valid_1's l2: 20.4812
[450]	training's l2: 19.6834	valid_1's l2: 20.4743
[460]	training's l2: 19.6353	valid_1's l2: 20.4643
[470]	training's l2: 19.5892	valid_1's l2: 20.4566
[480]	training's l2: 19.5427	valid_1's l2: 20.4481
[490]	training's l2: 19.4962	valid_1's l2: 20.4409
[500]	training's l2: 19.4512	valid_1's l2: 20.4345
[510]	training's l2: 19.4067	valid_1's l2: 20.4273
[520]	training's l2: 19.3627	valid_1's l2: 20.4215
[530]	training's l2: 19.3184	valid_1's l2: 20.4153
[540]	training's l2: 19.275	valid_1's l2: 20.4089
[550]	training's l2: 19.2316	valid_1's l2: 20.4013
[560]	training's l2: 19.1894	valid_1's l2: 20.396
[570]	training's l2: 19.1463	valid_1's l2: 20.3886
[580]	training's l2: 19.104	valid_1's l2: 20.383
[590]	training's l2: 19.0622	valid_1's l2: 20.3792
[600]	training's l2: 19.0204	valid_1's l2: 20.3746
[610]	training's l2: 18.9795	valid_1's l2: 20.3689
[620]	training's l2: 18.9389	valid_1's l2: 20.3663
[630]	training's l2: 18.898	valid_1's l2: 20.3627
[640]	training's l2: 18.858	valid_1's l2: 20.3583
[650]	training's l2: 18.8179	valid_1's l2: 20.3539
[660]	training's l2: 18.7778	valid_1's l2: 20.3498
[670]	training's l2: 18.7386	valid_1's l2: 20.3458
[680]	training's l2: 18.6992	valid_1's l2: 20.3425
[690]	training's l2: 18.6602	valid_1's l2: 20.3396
[700]	training's l2: 18.6217	valid_1's l2: 20.3356
[710]	training's l2: 18.5835	valid_1's l2: 20.3331
[720]	training's l2: 18.5454	valid_1's l2: 20.3299
[730]	training's l2: 18.5073	valid_1's l2: 20.3267
[740]	training's l2: 18.4698	valid_1's l2: 20.3241
[750]	training's l2: 18.4323	valid_1's l2: 20.3205
[760]	training's l2: 18.3946	valid_1's l2: 20.3165
[770]	training's l2: 18.3573	valid_1's l2: 20.3148
[780]	training's l2: 18.3205	valid_1's l2: 20.3126
[790]	training's l2: 18.2845	valid_1's l2: 20.3097
[800]	training's l2: 18.2483	valid_1's l2: 20.3071
[810]	training's l2: 18.2117	valid_1's l2: 20.3057
[820]	training's l2: 18.1763	valid_1's l2: 20.3034
[830]	training's l2: 18.141	valid_1's l2: 20.301
[840]	training's l2: 18.1057	valid_1's l2: 20.2983
[850]	training's l2: 18.0702	valid_1's l2: 20.2961
[860]	training's l2: 18.0349	valid_1's l2: 20.294
[870]	training's l2: 17.9999	valid_1's l2: 20.2914
[880]	training's l2: 17.9656	valid_1's l2: 20.2897
[890]	training's l2: 17.9311	valid_1's l2: 20.2882
[900]	training's l2: 17.8968	valid_1's l2: 20.2864
[910]	training's l2: 17.8623	valid_1's l2: 20.2843
[920]	training's l2: 17.8284	valid_1's l2: 20.2838
[930]	training's l2: 17.7943	valid_1's l2: 20.2813
[940]	training's l2: 17.7611	valid_1's l2: 20.2797
[950]	training's l2: 17.7278	valid_1's l2: 20.2792
[960]	training's l2: 17.6949	valid_1's l2: 20.2773
[970]	training's l2: 17.6628	valid_1's l2: 20.2749
[980]	training's l2: 17.6304	valid_1's l2: 20.274
[990]	training's l2: 17.5983	valid_1's l2: 20.273
[1000]	training's l2: 17.5662	valid_1's l2: 20.2712
[1010]	training's l2: 17.5351	valid_1's l2: 20.2685
[1020]	training's l2: 17.5035	valid_1's l2: 20.2682
[1030]	training's l2: 17.4723	valid_1's l2: 20.2669
[1040]	training's l2: 17.4413	valid_1's l2: 20.2666
[1050]	training's l2: 17.4099	valid_1's l2: 20.2661
[1060]	training's l2: 17.3797	valid_1's l2: 20.2657
[1070]	training's l2: 17.3479	valid_1's l2: 20.265
[1080]	training's l2: 17.3179	valid_1's l2: 20.264
[1090]	training's l2: 17.2889	valid_1's l2: 20.2637
[1100]	training's l2: 17.2573	valid_1's l2: 20.2612
[1110]	training's l2: 17.2281	valid_1's l2: 20.2595
[1120]	training's l2: 17.1975	valid_1's l2: 20.258
[1130]	training's l2: 17.1693	valid_1's l2: 20.2583
[1140]	training's l2: 17.1395	valid_1's l2: 20.2582
[1150]	training's l2: 17.1111	valid_1's l2: 20.2568
[1160]	training's l2: 17.0821	valid_1's l2: 20.2559
[1170]	training's l2: 17.0542	valid_1's l2: 20.2568
[1180]	training's l2: 17.025	valid_1's l2: 20.2566
Early stopping, best iteration is:
[1159]	training's l2: 17.0854	valid_1's l2: 20.2558
score1: 3.8361806152394045
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.259829 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.8991	valid_1's l2: 31.0013
[20]	training's l2: 30.1233	valid_1's l2: 29.2417
[30]	training's l2: 28.6763	valid_1's l2: 27.8111
[40]	training's l2: 27.4941	valid_1's l2: 26.6444
[50]	training's l2: 26.5221	valid_1's l2: 25.6918
[60]	training's l2: 25.7214	valid_1's l2: 24.9109
[70]	training's l2: 25.0568	valid_1's l2: 24.2674
[80]	training's l2: 24.5045	valid_1's l2: 23.7391
[90]	training's l2: 24.0412	valid_1's l2: 23.2978
[100]	training's l2: 23.6505	valid_1's l2: 22.9333
[110]	training's l2: 23.319	valid_1's l2: 22.6289
[120]	training's l2: 23.0381	valid_1's l2: 22.3742
[130]	training's l2: 22.798	valid_1's l2: 22.1615
[140]	training's l2: 22.5899	valid_1's l2: 21.9808
[150]	training's l2: 22.4062	valid_1's l2: 21.8268
[160]	training's l2: 22.2442	valid_1's l2: 21.6938
[170]	training's l2: 22.0989	valid_1's l2: 21.5792
[180]	training's l2: 21.9688	valid_1's l2: 21.4788
[190]	training's l2: 21.8496	valid_1's l2: 21.3914
[200]	training's l2: 21.7406	valid_1's l2: 21.3137
[210]	training's l2: 21.6408	valid_1's l2: 21.2461
[220]	training's l2: 21.546	valid_1's l2: 21.1832
[230]	training's l2: 21.4576	valid_1's l2: 21.1248
[240]	training's l2: 21.3768	valid_1's l2: 21.0737
[250]	training's l2: 21.3011	valid_1's l2: 21.0285
[260]	training's l2: 21.2283	valid_1's l2: 20.9864
[270]	training's l2: 21.1594	valid_1's l2: 20.9475
[280]	training's l2: 21.0942	valid_1's l2: 20.9123
[290]	training's l2: 21.0328	valid_1's l2: 20.8807
[300]	training's l2: 20.9724	valid_1's l2: 20.8497
[310]	training's l2: 20.9131	valid_1's l2: 20.8202
[320]	training's l2: 20.856	valid_1's l2: 20.7945
[330]	training's l2: 20.8008	valid_1's l2: 20.7716
[340]	training's l2: 20.7476	valid_1's l2: 20.7496
[350]	training's l2: 20.6959	valid_1's l2: 20.7285
[360]	training's l2: 20.6464	valid_1's l2: 20.7102
[370]	training's l2: 20.5977	valid_1's l2: 20.6916
[380]	training's l2: 20.551	valid_1's l2: 20.6745
[390]	training's l2: 20.5054	valid_1's l2: 20.6592
[400]	training's l2: 20.4606	valid_1's l2: 20.6454
[410]	training's l2: 20.4173	valid_1's l2: 20.6315
[420]	training's l2: 20.3751	valid_1's l2: 20.6194
[430]	training's l2: 20.3321	valid_1's l2: 20.6065
[440]	training's l2: 20.2908	valid_1's l2: 20.5954
[450]	training's l2: 20.25	valid_1's l2: 20.5848
[460]	training's l2: 20.2088	valid_1's l2: 20.572
[470]	training's l2: 20.1689	valid_1's l2: 20.5612
[480]	training's l2: 20.1296	valid_1's l2: 20.5526
[490]	training's l2: 20.0906	valid_1's l2: 20.5435
[500]	training's l2: 20.0518	valid_1's l2: 20.5343
[510]	training's l2: 20.0144	valid_1's l2: 20.5261
[520]	training's l2: 19.9764	valid_1's l2: 20.5183
[530]	training's l2: 19.9394	valid_1's l2: 20.5106
[540]	training's l2: 19.9032	valid_1's l2: 20.5027
[550]	training's l2: 19.8664	valid_1's l2: 20.4959
[560]	training's l2: 19.8298	valid_1's l2: 20.4885
[570]	training's l2: 19.7937	valid_1's l2: 20.4807
[580]	training's l2: 19.7583	valid_1's l2: 20.475
[590]	training's l2: 19.7231	valid_1's l2: 20.4692
[600]	training's l2: 19.6883	valid_1's l2: 20.4629
[610]	training's l2: 19.6535	valid_1's l2: 20.4565
[620]	training's l2: 19.6178	valid_1's l2: 20.4494
[630]	training's l2: 19.5834	valid_1's l2: 20.4444
[640]	training's l2: 19.5488	valid_1's l2: 20.4384
[650]	training's l2: 19.5148	valid_1's l2: 20.433
[660]	training's l2: 19.4809	valid_1's l2: 20.4264
[670]	training's l2: 19.4473	valid_1's l2: 20.4204
[680]	training's l2: 19.414	valid_1's l2: 20.4149
[690]	training's l2: 19.381	valid_1's l2: 20.4091
[700]	training's l2: 19.348	valid_1's l2: 20.4043
[710]	training's l2: 19.3151	valid_1's l2: 20.3992
[720]	training's l2: 19.2825	valid_1's l2: 20.3955
[730]	training's l2: 19.2507	valid_1's l2: 20.3913
[740]	training's l2: 19.2192	valid_1's l2: 20.3876
[750]	training's l2: 19.1877	valid_1's l2: 20.3834
[760]	training's l2: 19.1562	valid_1's l2: 20.3799
[770]	training's l2: 19.1253	valid_1's l2: 20.3752
[780]	training's l2: 19.0939	valid_1's l2: 20.3715
[790]	training's l2: 19.0627	valid_1's l2: 20.3684
[800]	training's l2: 19.032	valid_1's l2: 20.3646
[810]	training's l2: 19.0019	valid_1's l2: 20.3621
[820]	training's l2: 18.9714	valid_1's l2: 20.3601
[830]	training's l2: 18.9409	valid_1's l2: 20.3565
[840]	training's l2: 18.9109	valid_1's l2: 20.3534
[850]	training's l2: 18.8813	valid_1's l2: 20.351
[860]	training's l2: 18.8516	valid_1's l2: 20.348
[870]	training's l2: 18.8218	valid_1's l2: 20.3459
[880]	training's l2: 18.7925	valid_1's l2: 20.3429
[890]	training's l2: 18.7627	valid_1's l2: 20.3402
[900]	training's l2: 18.7339	valid_1's l2: 20.3381
[910]	training's l2: 18.7044	valid_1's l2: 20.3365
[920]	training's l2: 18.6751	valid_1's l2: 20.3332
[930]	training's l2: 18.6467	valid_1's l2: 20.3304
[940]	training's l2: 18.6181	valid_1's l2: 20.3283
[950]	training's l2: 18.5898	valid_1's l2: 20.3261
[960]	training's l2: 18.5613	valid_1's l2: 20.3228
[970]	training's l2: 18.533	valid_1's l2: 20.3213
[980]	training's l2: 18.505	valid_1's l2: 20.3186
[990]	training's l2: 18.4764	valid_1's l2: 20.3169
[1000]	training's l2: 18.4485	valid_1's l2: 20.315
[1010]	training's l2: 18.4207	valid_1's l2: 20.3123
[1020]	training's l2: 18.3933	valid_1's l2: 20.3106
[1030]	training's l2: 18.3658	valid_1's l2: 20.3096
[1040]	training's l2: 18.3383	valid_1's l2: 20.3072
[1050]	training's l2: 18.3114	valid_1's l2: 20.3065
[1060]	training's l2: 18.285	valid_1's l2: 20.3047
[1070]	training's l2: 18.2576	valid_1's l2: 20.3026
[1080]	training's l2: 18.2299	valid_1's l2: 20.3007
[1090]	training's l2: 18.2032	valid_1's l2: 20.2991
[1100]	training's l2: 18.1768	valid_1's l2: 20.2976
[1110]	training's l2: 18.1497	valid_1's l2: 20.2968
[1120]	training's l2: 18.123	valid_1's l2: 20.2951
[1130]	training's l2: 18.0968	valid_1's l2: 20.2936
[1140]	training's l2: 18.0714	valid_1's l2: 20.2925
[1150]	training's l2: 18.0458	valid_1's l2: 20.2909
[1160]	training's l2: 18.0196	valid_1's l2: 20.2892
[1170]	training's l2: 17.9936	valid_1's l2: 20.2881
[1180]	training's l2: 17.9684	valid_1's l2: 20.2865
[1190]	training's l2: 17.9431	valid_1's l2: 20.2852
[1200]	training's l2: 17.9183	valid_1's l2: 20.2841
[1210]	training's l2: 17.8933	valid_1's l2: 20.283
[1220]	training's l2: 17.8687	valid_1's l2: 20.2826
[1230]	training's l2: 17.8432	valid_1's l2: 20.2818
[1240]	training's l2: 17.8188	valid_1's l2: 20.2813
[1250]	training's l2: 17.7942	valid_1's l2: 20.2809
[1260]	training's l2: 17.7699	valid_1's l2: 20.2809
[1270]	training's l2: 17.7472	valid_1's l2: 20.2795
[1280]	training's l2: 17.7238	valid_1's l2: 20.2788
[1290]	training's l2: 17.6996	valid_1's l2: 20.2782
[1300]	training's l2: 17.6756	valid_1's l2: 20.2779
[1310]	training's l2: 17.6521	valid_1's l2: 20.2774
[1320]	training's l2: 17.6278	valid_1's l2: 20.2761
[1330]	training's l2: 17.604	valid_1's l2: 20.2757
[1340]	training's l2: 17.5819	valid_1's l2: 20.2744
[1350]	training's l2: 17.558	valid_1's l2: 20.2739
[1360]	training's l2: 17.5333	valid_1's l2: 20.2731
[1370]	training's l2: 17.5093	valid_1's l2: 20.2719
[1380]	training's l2: 17.4859	valid_1's l2: 20.2702
[1390]	training's l2: 17.4632	valid_1's l2: 20.2691
[1400]	training's l2: 17.4412	valid_1's l2: 20.2683
[1410]	training's l2: 17.4178	valid_1's l2: 20.2676
[1420]	training's l2: 17.3958	valid_1's l2: 20.2666
[1430]	training's l2: 17.3728	valid_1's l2: 20.2662
[1440]	training's l2: 17.3498	valid_1's l2: 20.2651
[1450]	training's l2: 17.327	valid_1's l2: 20.2638
[1460]	training's l2: 17.3043	valid_1's l2: 20.2639
[1470]	training's l2: 17.2823	valid_1's l2: 20.2636
[1480]	training's l2: 17.2599	valid_1's l2: 20.2631
[1490]	training's l2: 17.2378	valid_1's l2: 20.2622
[1500]	training's l2: 17.2159	valid_1's l2: 20.2627
[1510]	training's l2: 17.1942	valid_1's l2: 20.2625
[1520]	training's l2: 17.1726	valid_1's l2: 20.2623
[1530]	training's l2: 17.1518	valid_1's l2: 20.262
[1540]	training's l2: 17.1307	valid_1's l2: 20.2611
[1550]	training's l2: 17.109	valid_1's l2: 20.2605
[1560]	training's l2: 17.0871	valid_1's l2: 20.2599
[1570]	training's l2: 17.0652	valid_1's l2: 20.2598
[1580]	training's l2: 17.0437	valid_1's l2: 20.2588
[1590]	training's l2: 17.0224	valid_1's l2: 20.2584
[1600]	training's l2: 17.0008	valid_1's l2: 20.2586
[1610]	training's l2: 16.9794	valid_1's l2: 20.2576
[1620]	training's l2: 16.9596	valid_1's l2: 20.2569
[1630]	training's l2: 16.9377	valid_1's l2: 20.257
[1640]	training's l2: 16.9167	valid_1's l2: 20.2572
[1650]	training's l2: 16.8963	valid_1's l2: 20.2567
[1660]	training's l2: 16.8743	valid_1's l2: 20.2561
[1670]	training's l2: 16.8524	valid_1's l2: 20.2563
[1680]	training's l2: 16.8324	valid_1's l2: 20.2561
[1690]	training's l2: 16.8115	valid_1's l2: 20.2563
Early stopping, best iteration is:
[1664]	training's l2: 16.8655	valid_1's l2: 20.2555
score1: 3.839022278482456
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.249609 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.701	valid_1's l2: 30.8069
[20]	training's l2: 29.8032	valid_1's l2: 28.9318
[30]	training's l2: 28.2863	valid_1's l2: 27.4356
[40]	training's l2: 27.0677	valid_1's l2: 26.2389
[50]	training's l2: 26.0839	valid_1's l2: 25.2789
[60]	training's l2: 25.2849	valid_1's l2: 24.505
[70]	training's l2: 24.6329	valid_1's l2: 23.8813
[80]	training's l2: 24.0969	valid_1's l2: 23.374
[90]	training's l2: 23.6527	valid_1's l2: 22.9601
[100]	training's l2: 23.2818	valid_1's l2: 22.6206
[110]	training's l2: 22.9715	valid_1's l2: 22.3412
[120]	training's l2: 22.7095	valid_1's l2: 22.1128
[130]	training's l2: 22.485	valid_1's l2: 21.9216
[140]	training's l2: 22.2881	valid_1's l2: 21.7598
[150]	training's l2: 22.1158	valid_1's l2: 21.6229
[160]	training's l2: 21.9639	valid_1's l2: 21.5061
[170]	training's l2: 21.8253	valid_1's l2: 21.4039
[180]	training's l2: 21.6996	valid_1's l2: 21.3156
[190]	training's l2: 21.5859	valid_1's l2: 21.2385
[200]	training's l2: 21.48	valid_1's l2: 21.1704
[210]	training's l2: 21.3809	valid_1's l2: 21.1071
[220]	training's l2: 21.2898	valid_1's l2: 21.0518
[230]	training's l2: 21.2051	valid_1's l2: 21.0033
[240]	training's l2: 21.1251	valid_1's l2: 20.9578
[250]	training's l2: 21.0479	valid_1's l2: 20.9157
[260]	training's l2: 20.9764	valid_1's l2: 20.8789
[270]	training's l2: 20.9076	valid_1's l2: 20.8448
[280]	training's l2: 20.8396	valid_1's l2: 20.8118
[290]	training's l2: 20.7733	valid_1's l2: 20.7831
[300]	training's l2: 20.7103	valid_1's l2: 20.7572
[310]	training's l2: 20.6498	valid_1's l2: 20.7334
[320]	training's l2: 20.5915	valid_1's l2: 20.7102
[330]	training's l2: 20.5345	valid_1's l2: 20.6895
[340]	training's l2: 20.4793	valid_1's l2: 20.6707
[350]	training's l2: 20.4268	valid_1's l2: 20.6542
[360]	training's l2: 20.3752	valid_1's l2: 20.6377
[370]	training's l2: 20.3247	valid_1's l2: 20.6211
[380]	training's l2: 20.2757	valid_1's l2: 20.6083
[390]	training's l2: 20.2272	valid_1's l2: 20.5954
[400]	training's l2: 20.1786	valid_1's l2: 20.582
[410]	training's l2: 20.1314	valid_1's l2: 20.5717
[420]	training's l2: 20.0845	valid_1's l2: 20.5599
[430]	training's l2: 20.039	valid_1's l2: 20.5492
[440]	training's l2: 19.9938	valid_1's l2: 20.538
[450]	training's l2: 19.9488	valid_1's l2: 20.5277
[460]	training's l2: 19.9053	valid_1's l2: 20.5193
[470]	training's l2: 19.862	valid_1's l2: 20.5107
[480]	training's l2: 19.8193	valid_1's l2: 20.5017
[490]	training's l2: 19.7768	valid_1's l2: 20.4937
[500]	training's l2: 19.7343	valid_1's l2: 20.4873
[510]	training's l2: 19.6922	valid_1's l2: 20.4796
[520]	training's l2: 19.6515	valid_1's l2: 20.4721
[530]	training's l2: 19.6102	valid_1's l2: 20.4656
[540]	training's l2: 19.5693	valid_1's l2: 20.4587
[550]	training's l2: 19.5283	valid_1's l2: 20.4531
[560]	training's l2: 19.4876	valid_1's l2: 20.4466
[570]	training's l2: 19.4476	valid_1's l2: 20.4393
[580]	training's l2: 19.4081	valid_1's l2: 20.4344
[590]	training's l2: 19.3685	valid_1's l2: 20.4289
[600]	training's l2: 19.3295	valid_1's l2: 20.4241
[610]	training's l2: 19.2905	valid_1's l2: 20.4174
[620]	training's l2: 19.2522	valid_1's l2: 20.4117
[630]	training's l2: 19.2138	valid_1's l2: 20.4046
[640]	training's l2: 19.1761	valid_1's l2: 20.3988
[650]	training's l2: 19.1384	valid_1's l2: 20.3935
[660]	training's l2: 19.1012	valid_1's l2: 20.3878
[670]	training's l2: 19.0636	valid_1's l2: 20.3825
[680]	training's l2: 19.027	valid_1's l2: 20.3792
[690]	training's l2: 18.9909	valid_1's l2: 20.374
[700]	training's l2: 18.9548	valid_1's l2: 20.371
[710]	training's l2: 18.9182	valid_1's l2: 20.3674
[720]	training's l2: 18.8827	valid_1's l2: 20.3636
[730]	training's l2: 18.8465	valid_1's l2: 20.3601
[740]	training's l2: 18.8113	valid_1's l2: 20.3563
[750]	training's l2: 18.7764	valid_1's l2: 20.3537
[760]	training's l2: 18.7416	valid_1's l2: 20.3506
[770]	training's l2: 18.7062	valid_1's l2: 20.3466
[780]	training's l2: 18.6721	valid_1's l2: 20.3441
[790]	training's l2: 18.6381	valid_1's l2: 20.341
[800]	training's l2: 18.6033	valid_1's l2: 20.3376
[810]	training's l2: 18.5691	valid_1's l2: 20.3346
[820]	training's l2: 18.5358	valid_1's l2: 20.3322
[830]	training's l2: 18.5018	valid_1's l2: 20.3295
[840]	training's l2: 18.4684	valid_1's l2: 20.3274
[850]	training's l2: 18.4346	valid_1's l2: 20.3258
[860]	training's l2: 18.4016	valid_1's l2: 20.3234
[870]	training's l2: 18.3688	valid_1's l2: 20.321
[880]	training's l2: 18.3362	valid_1's l2: 20.3187
[890]	training's l2: 18.3027	valid_1's l2: 20.3154
[900]	training's l2: 18.2696	valid_1's l2: 20.3129
[910]	training's l2: 18.2376	valid_1's l2: 20.3112
[920]	training's l2: 18.2056	valid_1's l2: 20.3084
[930]	training's l2: 18.1735	valid_1's l2: 20.3063
[940]	training's l2: 18.1422	valid_1's l2: 20.305
[950]	training's l2: 18.1109	valid_1's l2: 20.3031
[960]	training's l2: 18.0799	valid_1's l2: 20.3022
[970]	training's l2: 18.0484	valid_1's l2: 20.3
[980]	training's l2: 18.0165	valid_1's l2: 20.2985
[990]	training's l2: 17.9858	valid_1's l2: 20.2967
[1000]	training's l2: 17.9551	valid_1's l2: 20.2954
[1010]	training's l2: 17.9247	valid_1's l2: 20.2945
[1020]	training's l2: 17.8941	valid_1's l2: 20.2937
[1030]	training's l2: 17.865	valid_1's l2: 20.2919
[1040]	training's l2: 17.8353	valid_1's l2: 20.2914
[1050]	training's l2: 17.8053	valid_1's l2: 20.2898
[1060]	training's l2: 17.7749	valid_1's l2: 20.2886
[1070]	training's l2: 17.7458	valid_1's l2: 20.2879
[1080]	training's l2: 17.7168	valid_1's l2: 20.287
[1090]	training's l2: 17.6878	valid_1's l2: 20.2855
[1100]	training's l2: 17.6595	valid_1's l2: 20.2853
[1110]	training's l2: 17.6314	valid_1's l2: 20.2858
[1120]	training's l2: 17.6034	valid_1's l2: 20.2852
[1130]	training's l2: 17.5757	valid_1's l2: 20.2838
[1140]	training's l2: 17.5467	valid_1's l2: 20.2825
[1150]	training's l2: 17.5191	valid_1's l2: 20.2824
[1160]	training's l2: 17.4904	valid_1's l2: 20.2812
[1170]	training's l2: 17.4641	valid_1's l2: 20.2805
[1180]	training's l2: 17.4359	valid_1's l2: 20.2793
[1190]	training's l2: 17.4078	valid_1's l2: 20.2783
[1200]	training's l2: 17.3807	valid_1's l2: 20.2775
[1210]	training's l2: 17.3531	valid_1's l2: 20.2762
[1220]	training's l2: 17.326	valid_1's l2: 20.2747
[1230]	training's l2: 17.2993	valid_1's l2: 20.2748
[1240]	training's l2: 17.2716	valid_1's l2: 20.2733
[1250]	training's l2: 17.245	valid_1's l2: 20.2723
[1260]	training's l2: 17.2189	valid_1's l2: 20.2718
[1270]	training's l2: 17.1932	valid_1's l2: 20.2715
[1280]	training's l2: 17.1662	valid_1's l2: 20.2707
[1290]	training's l2: 17.1409	valid_1's l2: 20.2707
[1300]	training's l2: 17.1142	valid_1's l2: 20.2704
[1310]	training's l2: 17.0881	valid_1's l2: 20.2691
[1320]	training's l2: 17.0627	valid_1's l2: 20.2677
[1330]	training's l2: 17.0365	valid_1's l2: 20.2672
[1340]	training's l2: 17.0102	valid_1's l2: 20.266
[1350]	training's l2: 16.9844	valid_1's l2: 20.2652
[1360]	training's l2: 16.958	valid_1's l2: 20.2648
[1370]	training's l2: 16.9317	valid_1's l2: 20.264
[1380]	training's l2: 16.9056	valid_1's l2: 20.264
[1390]	training's l2: 16.88	valid_1's l2: 20.264
[1400]	training's l2: 16.8546	valid_1's l2: 20.2635
[1410]	training's l2: 16.8296	valid_1's l2: 20.2638
[1420]	training's l2: 16.8042	valid_1's l2: 20.2638
Early stopping, best iteration is:
[1398]	training's l2: 16.86	valid_1's l2: 20.2633
score1: 3.839666531715162
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.317596 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.1451	valid_1's l2: 30.2526
[20]	training's l2: 28.9442	valid_1's l2: 28.0745
[30]	training's l2: 27.2874	valid_1's l2: 26.4419
[40]	training's l2: 26.0277	valid_1's l2: 25.205
[50]	training's l2: 25.0639	valid_1's l2: 24.269
[60]	training's l2: 24.3185	valid_1's l2: 23.5554
[70]	training's l2: 23.7344	valid_1's l2: 23.005
[80]	training's l2: 23.2728	valid_1's l2: 22.5794
[90]	training's l2: 22.9056	valid_1's l2: 22.2483
[100]	training's l2: 22.6064	valid_1's l2: 21.9866
[110]	training's l2: 22.3557	valid_1's l2: 21.7769
[120]	training's l2: 22.1437	valid_1's l2: 21.6058
[130]	training's l2: 21.9615	valid_1's l2: 21.4656
[140]	training's l2: 21.8013	valid_1's l2: 21.3483
[150]	training's l2: 21.6615	valid_1's l2: 21.2508
[160]	training's l2: 21.5325	valid_1's l2: 21.1658
[170]	training's l2: 21.4156	valid_1's l2: 21.0894
[180]	training's l2: 21.3105	valid_1's l2: 21.0259
[190]	training's l2: 21.2127	valid_1's l2: 20.9692
[200]	training's l2: 21.1224	valid_1's l2: 20.9193
[210]	training's l2: 21.0364	valid_1's l2: 20.8739
[220]	training's l2: 20.9532	valid_1's l2: 20.8307
[230]	training's l2: 20.8744	valid_1's l2: 20.7936
[240]	training's l2: 20.7995	valid_1's l2: 20.7611
[250]	training's l2: 20.7286	valid_1's l2: 20.7325
[260]	training's l2: 20.66	valid_1's l2: 20.7068
[270]	training's l2: 20.5951	valid_1's l2: 20.6839
[280]	training's l2: 20.5311	valid_1's l2: 20.6619
[290]	training's l2: 20.4699	valid_1's l2: 20.64
[300]	training's l2: 20.4096	valid_1's l2: 20.6219
[310]	training's l2: 20.3513	valid_1's l2: 20.6044
[320]	training's l2: 20.294	valid_1's l2: 20.5887
[330]	training's l2: 20.2376	valid_1's l2: 20.5735
[340]	training's l2: 20.1823	valid_1's l2: 20.5595
[350]	training's l2: 20.1288	valid_1's l2: 20.547
[360]	training's l2: 20.0763	valid_1's l2: 20.5349
[370]	training's l2: 20.0243	valid_1's l2: 20.524
[380]	training's l2: 19.9741	valid_1's l2: 20.5133
[390]	training's l2: 19.9244	valid_1's l2: 20.5051
[400]	training's l2: 19.8742	valid_1's l2: 20.4961
[410]	training's l2: 19.8245	valid_1's l2: 20.486
[420]	training's l2: 19.7755	valid_1's l2: 20.4763
[430]	training's l2: 19.7263	valid_1's l2: 20.4668
[440]	training's l2: 19.6774	valid_1's l2: 20.4573
[450]	training's l2: 19.6288	valid_1's l2: 20.4492
[460]	training's l2: 19.582	valid_1's l2: 20.4422
[470]	training's l2: 19.5356	valid_1's l2: 20.4344
[480]	training's l2: 19.4888	valid_1's l2: 20.4272
[490]	training's l2: 19.4432	valid_1's l2: 20.4202
[500]	training's l2: 19.398	valid_1's l2: 20.4124
[510]	training's l2: 19.3532	valid_1's l2: 20.4063
[520]	training's l2: 19.3084	valid_1's l2: 20.3999
[530]	training's l2: 19.2645	valid_1's l2: 20.3939
[540]	training's l2: 19.2208	valid_1's l2: 20.3872
[550]	training's l2: 19.1782	valid_1's l2: 20.3816
[560]	training's l2: 19.1352	valid_1's l2: 20.3757
[570]	training's l2: 19.0927	valid_1's l2: 20.3714
[580]	training's l2: 19.0505	valid_1's l2: 20.3677
[590]	training's l2: 19.0086	valid_1's l2: 20.3634
[600]	training's l2: 18.9665	valid_1's l2: 20.3593
[610]	training's l2: 18.9253	valid_1's l2: 20.3553
[620]	training's l2: 18.8843	valid_1's l2: 20.3515
[630]	training's l2: 18.8435	valid_1's l2: 20.3468
[640]	training's l2: 18.8029	valid_1's l2: 20.3427
[650]	training's l2: 18.7622	valid_1's l2: 20.3385
[660]	training's l2: 18.7232	valid_1's l2: 20.3358
[670]	training's l2: 18.6824	valid_1's l2: 20.3315
[680]	training's l2: 18.643	valid_1's l2: 20.3277
[690]	training's l2: 18.6048	valid_1's l2: 20.3253
[700]	training's l2: 18.5652	valid_1's l2: 20.3205
[710]	training's l2: 18.526	valid_1's l2: 20.3172
[720]	training's l2: 18.488	valid_1's l2: 20.314
[730]	training's l2: 18.4502	valid_1's l2: 20.3112
[740]	training's l2: 18.4129	valid_1's l2: 20.3101
[750]	training's l2: 18.3744	valid_1's l2: 20.3074
[760]	training's l2: 18.3375	valid_1's l2: 20.3048
[770]	training's l2: 18.3001	valid_1's l2: 20.3032
[780]	training's l2: 18.2631	valid_1's l2: 20.3008
[790]	training's l2: 18.2269	valid_1's l2: 20.2989
[800]	training's l2: 18.1904	valid_1's l2: 20.2955
[810]	training's l2: 18.1546	valid_1's l2: 20.2925
[820]	training's l2: 18.119	valid_1's l2: 20.2909
[830]	training's l2: 18.0834	valid_1's l2: 20.2892
[840]	training's l2: 18.0484	valid_1's l2: 20.2871
[850]	training's l2: 18.0142	valid_1's l2: 20.2858
[860]	training's l2: 17.9797	valid_1's l2: 20.2849
[870]	training's l2: 17.9452	valid_1's l2: 20.2842
[880]	training's l2: 17.9116	valid_1's l2: 20.2827
[890]	training's l2: 17.8781	valid_1's l2: 20.2811
[900]	training's l2: 17.8437	valid_1's l2: 20.28
[910]	training's l2: 17.811	valid_1's l2: 20.2787
[920]	training's l2: 17.7768	valid_1's l2: 20.2767
[930]	training's l2: 17.7448	valid_1's l2: 20.2754
[940]	training's l2: 17.7118	valid_1's l2: 20.2729
[950]	training's l2: 17.679	valid_1's l2: 20.2723
[960]	training's l2: 17.647	valid_1's l2: 20.2712
[970]	training's l2: 17.6144	valid_1's l2: 20.2706
[980]	training's l2: 17.5818	valid_1's l2: 20.2697
[990]	training's l2: 17.5505	valid_1's l2: 20.2687
[1000]	training's l2: 17.5182	valid_1's l2: 20.2675
[1010]	training's l2: 17.4884	valid_1's l2: 20.2657
[1020]	training's l2: 17.4575	valid_1's l2: 20.2658
[1030]	training's l2: 17.4269	valid_1's l2: 20.2653
[1040]	training's l2: 17.3972	valid_1's l2: 20.2649
[1050]	training's l2: 17.3678	valid_1's l2: 20.2632
[1060]	training's l2: 17.3377	valid_1's l2: 20.2622
[1070]	training's l2: 17.307	valid_1's l2: 20.2601
[1080]	training's l2: 17.2772	valid_1's l2: 20.26
[1090]	training's l2: 17.2466	valid_1's l2: 20.2584
[1100]	training's l2: 17.2174	valid_1's l2: 20.2577
[1110]	training's l2: 17.1882	valid_1's l2: 20.2569
[1120]	training's l2: 17.1586	valid_1's l2: 20.256
[1130]	training's l2: 17.1288	valid_1's l2: 20.2559
[1140]	training's l2: 17.099	valid_1's l2: 20.2551
[1150]	training's l2: 17.0691	valid_1's l2: 20.2558
[1160]	training's l2: 17.0393	valid_1's l2: 20.2553
[1170]	training's l2: 17.0118	valid_1's l2: 20.2554
[1180]	training's l2: 16.9836	valid_1's l2: 20.2554
[1190]	training's l2: 16.955	valid_1's l2: 20.2546
[1200]	training's l2: 16.9276	valid_1's l2: 20.255
[1210]	training's l2: 16.8987	valid_1's l2: 20.2548
[1220]	training's l2: 16.8702	valid_1's l2: 20.2546
[1230]	training's l2: 16.8426	valid_1's l2: 20.2549
[1240]	training's l2: 16.8145	valid_1's l2: 20.2538
[1250]	training's l2: 16.7855	valid_1's l2: 20.2536
[1260]	training's l2: 16.7586	valid_1's l2: 20.2527
[1270]	training's l2: 16.7311	valid_1's l2: 20.2533
[1280]	training's l2: 16.7029	valid_1's l2: 20.2532
[1290]	training's l2: 16.6752	valid_1's l2: 20.2532
Early stopping, best iteration is:
[1262]	training's l2: 16.7529	valid_1's l2: 20.2525
score1: 3.832353565411106
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.289774 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.1584	valid_1's l2: 30.2629
[20]	training's l2: 28.9661	valid_1's l2: 28.0913
[30]	training's l2: 27.3141	valid_1's l2: 26.4605
[40]	training's l2: 26.06	valid_1's l2: 25.2283
[50]	training's l2: 25.098	valid_1's l2: 24.2914
[60]	training's l2: 24.353	valid_1's l2: 23.5754
[70]	training's l2: 23.7704	valid_1's l2: 23.0238
[80]	training's l2: 23.3099	valid_1's l2: 22.5979
[90]	training's l2: 22.9444	valid_1's l2: 22.2651
[100]	training's l2: 22.6464	valid_1's l2: 22.0024
[110]	training's l2: 22.3975	valid_1's l2: 21.7927
[120]	training's l2: 22.1858	valid_1's l2: 21.6193
[130]	training's l2: 22.0057	valid_1's l2: 21.4782
[140]	training's l2: 21.8467	valid_1's l2: 21.3588
[150]	training's l2: 21.7078	valid_1's l2: 21.2612
[160]	training's l2: 21.5809	valid_1's l2: 21.1747
[170]	training's l2: 21.4659	valid_1's l2: 21.0983
[180]	training's l2: 21.363	valid_1's l2: 21.0338
[190]	training's l2: 21.2652	valid_1's l2: 20.9741
[200]	training's l2: 21.1763	valid_1's l2: 20.9251
[210]	training's l2: 21.0945	valid_1's l2: 20.8809
[220]	training's l2: 21.0129	valid_1's l2: 20.8386
[230]	training's l2: 20.9351	valid_1's l2: 20.8015
[240]	training's l2: 20.8605	valid_1's l2: 20.7671
[250]	training's l2: 20.7919	valid_1's l2: 20.7381
[260]	training's l2: 20.725	valid_1's l2: 20.7114
[270]	training's l2: 20.6609	valid_1's l2: 20.6878
[280]	training's l2: 20.5994	valid_1's l2: 20.6677
[290]	training's l2: 20.5406	valid_1's l2: 20.6463
[300]	training's l2: 20.4832	valid_1's l2: 20.6271
[310]	training's l2: 20.4255	valid_1's l2: 20.6075
[320]	training's l2: 20.3706	valid_1's l2: 20.5915
[330]	training's l2: 20.3169	valid_1's l2: 20.5784
[340]	training's l2: 20.2647	valid_1's l2: 20.5632
[350]	training's l2: 20.2121	valid_1's l2: 20.5496
[360]	training's l2: 20.1616	valid_1's l2: 20.5378
[370]	training's l2: 20.1103	valid_1's l2: 20.5259
[380]	training's l2: 20.0606	valid_1's l2: 20.5146
[390]	training's l2: 20.0125	valid_1's l2: 20.505
[400]	training's l2: 19.9644	valid_1's l2: 20.4955
[410]	training's l2: 19.9163	valid_1's l2: 20.4861
[420]	training's l2: 19.8685	valid_1's l2: 20.4774
[430]	training's l2: 19.8217	valid_1's l2: 20.4685
[440]	training's l2: 19.7754	valid_1's l2: 20.46
[450]	training's l2: 19.7289	valid_1's l2: 20.4515
[460]	training's l2: 19.6822	valid_1's l2: 20.4424
[470]	training's l2: 19.636	valid_1's l2: 20.4338
[480]	training's l2: 19.5914	valid_1's l2: 20.4272
[490]	training's l2: 19.5477	valid_1's l2: 20.4208
[500]	training's l2: 19.5039	valid_1's l2: 20.412
[510]	training's l2: 19.4605	valid_1's l2: 20.4053
[520]	training's l2: 19.418	valid_1's l2: 20.3996
[530]	training's l2: 19.3759	valid_1's l2: 20.3934
[540]	training's l2: 19.334	valid_1's l2: 20.3882
[550]	training's l2: 19.2925	valid_1's l2: 20.3837
[560]	training's l2: 19.2523	valid_1's l2: 20.3784
[570]	training's l2: 19.2116	valid_1's l2: 20.3733
[580]	training's l2: 19.1702	valid_1's l2: 20.3687
[590]	training's l2: 19.1296	valid_1's l2: 20.3651
[600]	training's l2: 19.089	valid_1's l2: 20.3602
[610]	training's l2: 19.0488	valid_1's l2: 20.3559
[620]	training's l2: 19.0094	valid_1's l2: 20.351
[630]	training's l2: 18.9707	valid_1's l2: 20.3462
[640]	training's l2: 18.9315	valid_1's l2: 20.3441
[650]	training's l2: 18.8921	valid_1's l2: 20.3396
[660]	training's l2: 18.8539	valid_1's l2: 20.3367
[670]	training's l2: 18.8154	valid_1's l2: 20.334
[680]	training's l2: 18.7778	valid_1's l2: 20.3308
[690]	training's l2: 18.7404	valid_1's l2: 20.3281
[700]	training's l2: 18.7028	valid_1's l2: 20.3252
[710]	training's l2: 18.6649	valid_1's l2: 20.3223
[720]	training's l2: 18.6278	valid_1's l2: 20.3197
[730]	training's l2: 18.592	valid_1's l2: 20.3156
[740]	training's l2: 18.5547	valid_1's l2: 20.3122
[750]	training's l2: 18.5182	valid_1's l2: 20.3095
[760]	training's l2: 18.4815	valid_1's l2: 20.3062
[770]	training's l2: 18.4454	valid_1's l2: 20.3042
[780]	training's l2: 18.4097	valid_1's l2: 20.3018
[790]	training's l2: 18.3743	valid_1's l2: 20.2993
[800]	training's l2: 18.3387	valid_1's l2: 20.2968
[810]	training's l2: 18.3035	valid_1's l2: 20.2949
[820]	training's l2: 18.269	valid_1's l2: 20.2919
[830]	training's l2: 18.2336	valid_1's l2: 20.2905
[840]	training's l2: 18.1993	valid_1's l2: 20.2883
[850]	training's l2: 18.1651	valid_1's l2: 20.2868
[860]	training's l2: 18.1315	valid_1's l2: 20.2854
[870]	training's l2: 18.0982	valid_1's l2: 20.2842
[880]	training's l2: 18.0648	valid_1's l2: 20.2822
[890]	training's l2: 18.0319	valid_1's l2: 20.281
[900]	training's l2: 17.9998	valid_1's l2: 20.2786
[910]	training's l2: 17.9668	valid_1's l2: 20.2775
[920]	training's l2: 17.9345	valid_1's l2: 20.2759
[930]	training's l2: 17.9025	valid_1's l2: 20.2748
[940]	training's l2: 17.8717	valid_1's l2: 20.2742
[950]	training's l2: 17.8404	valid_1's l2: 20.2734
[960]	training's l2: 17.8094	valid_1's l2: 20.2721
[970]	training's l2: 17.7786	valid_1's l2: 20.2722
[980]	training's l2: 17.7478	valid_1's l2: 20.2726
[990]	training's l2: 17.7168	valid_1's l2: 20.2721
Early stopping, best iteration is:
[962]	training's l2: 17.8032	valid_1's l2: 20.2716
score1: 3.839566418374746
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.246153 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.4845	valid_1's l2: 29.5964
[20]	training's l2: 27.9889	valid_1's l2: 27.1268
[30]	training's l2: 26.2376	valid_1's l2: 25.4084
[40]	training's l2: 24.9937	valid_1's l2: 24.1985
[50]	training's l2: 24.0952	valid_1's l2: 23.3391
[60]	training's l2: 23.4328	valid_1's l2: 22.7207
[70]	training's l2: 22.9395	valid_1's l2: 22.2722
[80]	training's l2: 22.5629	valid_1's l2: 21.9442
[90]	training's l2: 22.2605	valid_1's l2: 21.6957
[100]	training's l2: 22.0142	valid_1's l2: 21.5022
[110]	training's l2: 21.806	valid_1's l2: 21.3493
[120]	training's l2: 21.63	valid_1's l2: 21.2266
[130]	training's l2: 21.471	valid_1's l2: 21.1215
[140]	training's l2: 21.3334	valid_1's l2: 21.0364
[150]	training's l2: 21.2094	valid_1's l2: 20.9649
[160]	training's l2: 21.0955	valid_1's l2: 20.9029
[170]	training's l2: 20.9894	valid_1's l2: 20.8493
[180]	training's l2: 20.8872	valid_1's l2: 20.8019
[190]	training's l2: 20.7914	valid_1's l2: 20.7615
[200]	training's l2: 20.7021	valid_1's l2: 20.7266
[210]	training's l2: 20.6177	valid_1's l2: 20.6965
[220]	training's l2: 20.5387	valid_1's l2: 20.6703
[230]	training's l2: 20.4611	valid_1's l2: 20.6465
[240]	training's l2: 20.3858	valid_1's l2: 20.6219
[250]	training's l2: 20.3131	valid_1's l2: 20.6017
[260]	training's l2: 20.2426	valid_1's l2: 20.5826
[270]	training's l2: 20.1734	valid_1's l2: 20.5642
[280]	training's l2: 20.1065	valid_1's l2: 20.5501
[290]	training's l2: 20.0401	valid_1's l2: 20.5336
[300]	training's l2: 19.9755	valid_1's l2: 20.5189
[310]	training's l2: 19.9111	valid_1's l2: 20.5057
[320]	training's l2: 19.8477	valid_1's l2: 20.4936
[330]	training's l2: 19.7858	valid_1's l2: 20.4831
[340]	training's l2: 19.7249	valid_1's l2: 20.4731
[350]	training's l2: 19.664	valid_1's l2: 20.4619
[360]	training's l2: 19.603	valid_1's l2: 20.4531
[370]	training's l2: 19.5438	valid_1's l2: 20.4436
[380]	training's l2: 19.485	valid_1's l2: 20.4332
[390]	training's l2: 19.4273	valid_1's l2: 20.4247
[400]	training's l2: 19.3712	valid_1's l2: 20.4159
[410]	training's l2: 19.3144	valid_1's l2: 20.4061
[420]	training's l2: 19.2591	valid_1's l2: 20.3992
[430]	training's l2: 19.2032	valid_1's l2: 20.3903
[440]	training's l2: 19.1493	valid_1's l2: 20.3843
[450]	training's l2: 19.0957	valid_1's l2: 20.3772
[460]	training's l2: 19.0416	valid_1's l2: 20.3719
[470]	training's l2: 18.9888	valid_1's l2: 20.3659
[480]	training's l2: 18.9376	valid_1's l2: 20.3616
[490]	training's l2: 18.8864	valid_1's l2: 20.3577
[500]	training's l2: 18.8353	valid_1's l2: 20.3531
[510]	training's l2: 18.7846	valid_1's l2: 20.3465
[520]	training's l2: 18.7338	valid_1's l2: 20.3434
[530]	training's l2: 18.6828	valid_1's l2: 20.3389
[540]	training's l2: 18.6337	valid_1's l2: 20.3354
[550]	training's l2: 18.5837	valid_1's l2: 20.3307
[560]	training's l2: 18.5344	valid_1's l2: 20.3275
[570]	training's l2: 18.4867	valid_1's l2: 20.3233
[580]	training's l2: 18.4383	valid_1's l2: 20.3206
[590]	training's l2: 18.3904	valid_1's l2: 20.3191
[600]	training's l2: 18.3423	valid_1's l2: 20.3147
[610]	training's l2: 18.297	valid_1's l2: 20.3129
[620]	training's l2: 18.2505	valid_1's l2: 20.3084
[630]	training's l2: 18.2036	valid_1's l2: 20.3056
[640]	training's l2: 18.1587	valid_1's l2: 20.3038
[650]	training's l2: 18.1139	valid_1's l2: 20.2995
[660]	training's l2: 18.0685	valid_1's l2: 20.2972
[670]	training's l2: 18.0254	valid_1's l2: 20.2964
[680]	training's l2: 17.9808	valid_1's l2: 20.2949
[690]	training's l2: 17.9376	valid_1's l2: 20.2927
[700]	training's l2: 17.8942	valid_1's l2: 20.2905
[710]	training's l2: 17.8526	valid_1's l2: 20.289
[720]	training's l2: 17.8097	valid_1's l2: 20.2885
[730]	training's l2: 17.7679	valid_1's l2: 20.287
[740]	training's l2: 17.7263	valid_1's l2: 20.2869
[750]	training's l2: 17.6851	valid_1's l2: 20.2846
[760]	training's l2: 17.6445	valid_1's l2: 20.2845
[770]	training's l2: 17.6039	valid_1's l2: 20.2827
[780]	training's l2: 17.5631	valid_1's l2: 20.2831
[790]	training's l2: 17.5247	valid_1's l2: 20.2814
[800]	training's l2: 17.4847	valid_1's l2: 20.2788
[810]	training's l2: 17.4464	valid_1's l2: 20.2777
[820]	training's l2: 17.408	valid_1's l2: 20.2771
[830]	training's l2: 17.3681	valid_1's l2: 20.2763
[840]	training's l2: 17.3281	valid_1's l2: 20.2751
[850]	training's l2: 17.2906	valid_1's l2: 20.2748
[860]	training's l2: 17.2529	valid_1's l2: 20.2741
[870]	training's l2: 17.2145	valid_1's l2: 20.2743
[880]	training's l2: 17.177	valid_1's l2: 20.2728
[890]	training's l2: 17.1389	valid_1's l2: 20.2715
[900]	training's l2: 17.1023	valid_1's l2: 20.2715
[910]	training's l2: 17.0675	valid_1's l2: 20.272
[920]	training's l2: 17.0305	valid_1's l2: 20.2701
[930]	training's l2: 16.9916	valid_1's l2: 20.2698
[940]	training's l2: 16.9554	valid_1's l2: 20.2689
[950]	training's l2: 16.9194	valid_1's l2: 20.2681
[960]	training's l2: 16.8852	valid_1's l2: 20.2683
[970]	training's l2: 16.8486	valid_1's l2: 20.2673
[980]	training's l2: 16.8114	valid_1's l2: 20.2686
[990]	training's l2: 16.7758	valid_1's l2: 20.2682
[1000]	training's l2: 16.7406	valid_1's l2: 20.2668
[1010]	training's l2: 16.7048	valid_1's l2: 20.2656
[1020]	training's l2: 16.6688	valid_1's l2: 20.2665
[1030]	training's l2: 16.635	valid_1's l2: 20.2658
[1040]	training's l2: 16.5999	valid_1's l2: 20.2674
Early stopping, best iteration is:
[1017]	training's l2: 16.6803	valid_1's l2: 20.2654
score1: 3.838832899709002
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.250609 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.8721	valid_1's l2: 29.9864
[20]	training's l2: 28.5366	valid_1's l2: 27.6775
[30]	training's l2: 26.8286	valid_1's l2: 25.999
[40]	training's l2: 25.5633	valid_1's l2: 24.7662
[50]	training's l2: 24.6168	valid_1's l2: 23.8562
[60]	training's l2: 23.896	valid_1's l2: 23.175
[70]	training's l2: 23.3423	valid_1's l2: 22.6619
[80]	training's l2: 22.9125	valid_1's l2: 22.2762
[90]	training's l2: 22.5722	valid_1's l2: 21.9794
[100]	training's l2: 22.2953	valid_1's l2: 21.7495
[110]	training's l2: 22.0631	valid_1's l2: 21.5647
[120]	training's l2: 21.8641	valid_1's l2: 21.4156
[130]	training's l2: 21.6904	valid_1's l2: 21.2932
[140]	training's l2: 21.5389	valid_1's l2: 21.1918
[150]	training's l2: 21.3992	valid_1's l2: 21.1016
[160]	training's l2: 21.2766	valid_1's l2: 21.0276
[170]	training's l2: 21.163	valid_1's l2: 20.9634
[180]	training's l2: 21.0589	valid_1's l2: 20.9077
[190]	training's l2: 20.9609	valid_1's l2: 20.8592
[200]	training's l2: 20.8663	valid_1's l2: 20.8127
[210]	training's l2: 20.777	valid_1's l2: 20.7738
[220]	training's l2: 20.6929	valid_1's l2: 20.7409
[230]	training's l2: 20.6127	valid_1's l2: 20.7081
[240]	training's l2: 20.5352	valid_1's l2: 20.6789
[250]	training's l2: 20.4632	valid_1's l2: 20.6548
[260]	training's l2: 20.3923	valid_1's l2: 20.6313
[270]	training's l2: 20.3234	valid_1's l2: 20.6116
[280]	training's l2: 20.256	valid_1's l2: 20.5918
[290]	training's l2: 20.1908	valid_1's l2: 20.5761
[300]	training's l2: 20.1274	valid_1's l2: 20.5606
[310]	training's l2: 20.065	valid_1's l2: 20.546
[320]	training's l2: 20.0042	valid_1's l2: 20.5341
[330]	training's l2: 19.9444	valid_1's l2: 20.5207
[340]	training's l2: 19.8847	valid_1's l2: 20.506
[350]	training's l2: 19.8259	valid_1's l2: 20.4956
[360]	training's l2: 19.7682	valid_1's l2: 20.4839
[370]	training's l2: 19.7111	valid_1's l2: 20.474
[380]	training's l2: 19.6543	valid_1's l2: 20.4633
[390]	training's l2: 19.5988	valid_1's l2: 20.4544
[400]	training's l2: 19.5427	valid_1's l2: 20.4468
[410]	training's l2: 19.4874	valid_1's l2: 20.4383
[420]	training's l2: 19.4331	valid_1's l2: 20.4292
[430]	training's l2: 19.3799	valid_1's l2: 20.4227
[440]	training's l2: 19.3274	valid_1's l2: 20.4147
[450]	training's l2: 19.2751	valid_1's l2: 20.4064
[460]	training's l2: 19.223	valid_1's l2: 20.3984
[470]	training's l2: 19.1717	valid_1's l2: 20.3915
[480]	training's l2: 19.1205	valid_1's l2: 20.386
[490]	training's l2: 19.0703	valid_1's l2: 20.3793
[500]	training's l2: 19.0208	valid_1's l2: 20.373
[510]	training's l2: 18.9714	valid_1's l2: 20.3677
[520]	training's l2: 18.9224	valid_1's l2: 20.363
[530]	training's l2: 18.8742	valid_1's l2: 20.3583
[540]	training's l2: 18.8265	valid_1's l2: 20.3538
[550]	training's l2: 18.7787	valid_1's l2: 20.3509
[560]	training's l2: 18.7317	valid_1's l2: 20.3465
[570]	training's l2: 18.6855	valid_1's l2: 20.3433
[580]	training's l2: 18.6392	valid_1's l2: 20.3406
[590]	training's l2: 18.5926	valid_1's l2: 20.3367
[600]	training's l2: 18.5464	valid_1's l2: 20.3332
[610]	training's l2: 18.5001	valid_1's l2: 20.3292
[620]	training's l2: 18.4554	valid_1's l2: 20.3253
[630]	training's l2: 18.4099	valid_1's l2: 20.3212
[640]	training's l2: 18.3651	valid_1's l2: 20.3185
[650]	training's l2: 18.3212	valid_1's l2: 20.315
[660]	training's l2: 18.2761	valid_1's l2: 20.3115
[670]	training's l2: 18.2334	valid_1's l2: 20.3099
[680]	training's l2: 18.19	valid_1's l2: 20.3079
[690]	training's l2: 18.1477	valid_1's l2: 20.3063
[700]	training's l2: 18.1051	valid_1's l2: 20.3028
[710]	training's l2: 18.0633	valid_1's l2: 20.2997
[720]	training's l2: 18.021	valid_1's l2: 20.2974
[730]	training's l2: 17.9792	valid_1's l2: 20.2941
[740]	training's l2: 17.9374	valid_1's l2: 20.2923
[750]	training's l2: 17.8955	valid_1's l2: 20.2902
[760]	training's l2: 17.8562	valid_1's l2: 20.2891
[770]	training's l2: 17.8157	valid_1's l2: 20.2873
[780]	training's l2: 17.7758	valid_1's l2: 20.2853
[790]	training's l2: 17.737	valid_1's l2: 20.2836
[800]	training's l2: 17.6972	valid_1's l2: 20.2823
[810]	training's l2: 17.6592	valid_1's l2: 20.2801
[820]	training's l2: 17.6209	valid_1's l2: 20.2779
[830]	training's l2: 17.5824	valid_1's l2: 20.2775
[840]	training's l2: 17.5448	valid_1's l2: 20.2758
[850]	training's l2: 17.5083	valid_1's l2: 20.2748
[860]	training's l2: 17.471	valid_1's l2: 20.2738
[870]	training's l2: 17.4342	valid_1's l2: 20.2729
[880]	training's l2: 17.3978	valid_1's l2: 20.2727
[890]	training's l2: 17.3595	valid_1's l2: 20.2722
[900]	training's l2: 17.3234	valid_1's l2: 20.2714
[910]	training's l2: 17.2872	valid_1's l2: 20.2702
[920]	training's l2: 17.2508	valid_1's l2: 20.2682
[930]	training's l2: 17.2155	valid_1's l2: 20.2665
[940]	training's l2: 17.1815	valid_1's l2: 20.2649
[950]	training's l2: 17.1458	valid_1's l2: 20.2644
[960]	training's l2: 17.1117	valid_1's l2: 20.2641
[970]	training's l2: 17.0765	valid_1's l2: 20.2627
[980]	training's l2: 17.0398	valid_1's l2: 20.2618
[990]	training's l2: 17.0058	valid_1's l2: 20.2611
[1000]	training's l2: 16.9717	valid_1's l2: 20.2606
[1010]	training's l2: 16.9376	valid_1's l2: 20.2605
[1020]	training's l2: 16.9044	valid_1's l2: 20.2597
[1030]	training's l2: 16.8725	valid_1's l2: 20.2597
[1040]	training's l2: 16.8381	valid_1's l2: 20.2588
[1050]	training's l2: 16.805	valid_1's l2: 20.258
[1060]	training's l2: 16.773	valid_1's l2: 20.2576
[1070]	training's l2: 16.7394	valid_1's l2: 20.2573
[1080]	training's l2: 16.7045	valid_1's l2: 20.2572
[1090]	training's l2: 16.6719	valid_1's l2: 20.2549
[1100]	training's l2: 16.639	valid_1's l2: 20.2546
[1110]	training's l2: 16.6077	valid_1's l2: 20.2543
[1120]	training's l2: 16.5744	valid_1's l2: 20.2544
[1130]	training's l2: 16.5424	valid_1's l2: 20.2543
[1140]	training's l2: 16.5105	valid_1's l2: 20.2533
[1150]	training's l2: 16.4782	valid_1's l2: 20.2535
[1160]	training's l2: 16.4482	valid_1's l2: 20.2532
[1170]	training's l2: 16.4148	valid_1's l2: 20.2522
[1180]	training's l2: 16.3836	valid_1's l2: 20.2509
[1190]	training's l2: 16.3512	valid_1's l2: 20.2513
[1200]	training's l2: 16.3212	valid_1's l2: 20.2521
Early stopping, best iteration is:
[1179]	training's l2: 16.3863	valid_1's l2: 20.2509
score1: 3.837811834771503
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.244622 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.4418	valid_1's l2: 30.5395
[20]	training's l2: 29.4026	valid_1's l2: 28.5121
[30]	training's l2: 27.8217	valid_1's l2: 26.9449
[40]	training's l2: 26.5882	valid_1's l2: 25.7257
[50]	training's l2: 25.6196	valid_1's l2: 24.7751
[60]	training's l2: 24.8519	valid_1's l2: 24.0304
[70]	training's l2: 24.241	valid_1's l2: 23.4418
[80]	training's l2: 23.7497	valid_1's l2: 22.9749
[90]	training's l2: 23.3513	valid_1's l2: 22.6044
[100]	training's l2: 23.0276	valid_1's l2: 22.3077
[110]	training's l2: 22.7597	valid_1's l2: 22.0645
[120]	training's l2: 22.5312	valid_1's l2: 21.8665
[130]	training's l2: 22.3353	valid_1's l2: 21.6997
[140]	training's l2: 22.1668	valid_1's l2: 21.5622
[150]	training's l2: 22.0174	valid_1's l2: 21.4451
[160]	training's l2: 21.8865	valid_1's l2: 21.3462
[170]	training's l2: 21.767	valid_1's l2: 21.2597
[180]	training's l2: 21.6584	valid_1's l2: 21.1845
[190]	training's l2: 21.56	valid_1's l2: 21.1177
[200]	training's l2: 21.4703	valid_1's l2: 21.0589
[210]	training's l2: 21.3862	valid_1's l2: 21.0057
[220]	training's l2: 21.3082	valid_1's l2: 20.9591
[230]	training's l2: 21.2365	valid_1's l2: 20.9191
[240]	training's l2: 21.1652	valid_1's l2: 20.8793
[250]	training's l2: 21.096	valid_1's l2: 20.8411
[260]	training's l2: 21.0323	valid_1's l2: 20.8096
[270]	training's l2: 20.9704	valid_1's l2: 20.7804
[280]	training's l2: 20.911	valid_1's l2: 20.753
[290]	training's l2: 20.8561	valid_1's l2: 20.73
[300]	training's l2: 20.8013	valid_1's l2: 20.708
[310]	training's l2: 20.7499	valid_1's l2: 20.689
[320]	training's l2: 20.6998	valid_1's l2: 20.6696
[330]	training's l2: 20.6506	valid_1's l2: 20.652
[340]	training's l2: 20.6024	valid_1's l2: 20.6353
[350]	training's l2: 20.5551	valid_1's l2: 20.6196
[360]	training's l2: 20.5089	valid_1's l2: 20.6046
[370]	training's l2: 20.4637	valid_1's l2: 20.5917
[380]	training's l2: 20.4195	valid_1's l2: 20.5791
[390]	training's l2: 20.3766	valid_1's l2: 20.5672
[400]	training's l2: 20.3331	valid_1's l2: 20.5555
[410]	training's l2: 20.2916	valid_1's l2: 20.5454
[420]	training's l2: 20.2492	valid_1's l2: 20.5344
[430]	training's l2: 20.2084	valid_1's l2: 20.5242
[440]	training's l2: 20.1684	valid_1's l2: 20.515
[450]	training's l2: 20.1289	valid_1's l2: 20.5054
[460]	training's l2: 20.0892	valid_1's l2: 20.4977
[470]	training's l2: 20.0498	valid_1's l2: 20.4887
[480]	training's l2: 20.0114	valid_1's l2: 20.4798
[490]	training's l2: 19.9732	valid_1's l2: 20.4731
[500]	training's l2: 19.9353	valid_1's l2: 20.4664
[510]	training's l2: 19.8976	valid_1's l2: 20.4594
[520]	training's l2: 19.8593	valid_1's l2: 20.4522
[530]	training's l2: 19.8214	valid_1's l2: 20.4449
[540]	training's l2: 19.7841	valid_1's l2: 20.4376
[550]	training's l2: 19.7473	valid_1's l2: 20.4321
[560]	training's l2: 19.7112	valid_1's l2: 20.4259
[570]	training's l2: 19.6745	valid_1's l2: 20.4195
[580]	training's l2: 19.6382	valid_1's l2: 20.4136
[590]	training's l2: 19.6029	valid_1's l2: 20.4081
[600]	training's l2: 19.5681	valid_1's l2: 20.4028
[610]	training's l2: 19.533	valid_1's l2: 20.3981
[620]	training's l2: 19.4983	valid_1's l2: 20.394
[630]	training's l2: 19.4645	valid_1's l2: 20.3877
[640]	training's l2: 19.43	valid_1's l2: 20.3825
[650]	training's l2: 19.396	valid_1's l2: 20.3771
[660]	training's l2: 19.3623	valid_1's l2: 20.3731
[670]	training's l2: 19.3293	valid_1's l2: 20.3696
[680]	training's l2: 19.2969	valid_1's l2: 20.3652
[690]	training's l2: 19.2642	valid_1's l2: 20.3618
[700]	training's l2: 19.2311	valid_1's l2: 20.3578
[710]	training's l2: 19.1984	valid_1's l2: 20.3542
[720]	training's l2: 19.1667	valid_1's l2: 20.351
[730]	training's l2: 19.1344	valid_1's l2: 20.3467
[740]	training's l2: 19.1032	valid_1's l2: 20.3441
[750]	training's l2: 19.0719	valid_1's l2: 20.3399
[760]	training's l2: 19.0406	valid_1's l2: 20.3369
[770]	training's l2: 19.0094	valid_1's l2: 20.3352
[780]	training's l2: 18.9784	valid_1's l2: 20.3319
[790]	training's l2: 18.9478	valid_1's l2: 20.3284
[800]	training's l2: 18.9167	valid_1's l2: 20.3265
[810]	training's l2: 18.8863	valid_1's l2: 20.324
[820]	training's l2: 18.856	valid_1's l2: 20.3213
[830]	training's l2: 18.8253	valid_1's l2: 20.3191
[840]	training's l2: 18.7954	valid_1's l2: 20.3165
[850]	training's l2: 18.766	valid_1's l2: 20.3142
[860]	training's l2: 18.7353	valid_1's l2: 20.3113
[870]	training's l2: 18.7059	valid_1's l2: 20.3096
[880]	training's l2: 18.6762	valid_1's l2: 20.3065
[890]	training's l2: 18.6466	valid_1's l2: 20.305
[900]	training's l2: 18.6169	valid_1's l2: 20.3026
[910]	training's l2: 18.5877	valid_1's l2: 20.3007
[920]	training's l2: 18.5585	valid_1's l2: 20.2983
[930]	training's l2: 18.5302	valid_1's l2: 20.297
[940]	training's l2: 18.5013	valid_1's l2: 20.2948
[950]	training's l2: 18.4728	valid_1's l2: 20.2916
[960]	training's l2: 18.4445	valid_1's l2: 20.2893
[970]	training's l2: 18.4166	valid_1's l2: 20.2868
[980]	training's l2: 18.3887	valid_1's l2: 20.2859
[990]	training's l2: 18.3619	valid_1's l2: 20.2845
[1000]	training's l2: 18.3354	valid_1's l2: 20.2833
[1010]	training's l2: 18.308	valid_1's l2: 20.2813
[1020]	training's l2: 18.2812	valid_1's l2: 20.279
[1030]	training's l2: 18.2547	valid_1's l2: 20.2787
[1040]	training's l2: 18.2285	valid_1's l2: 20.2774
[1050]	training's l2: 18.2022	valid_1's l2: 20.275
[1060]	training's l2: 18.1766	valid_1's l2: 20.2743
[1070]	training's l2: 18.1516	valid_1's l2: 20.2733
[1080]	training's l2: 18.126	valid_1's l2: 20.272
[1090]	training's l2: 18.0993	valid_1's l2: 20.2709
[1100]	training's l2: 18.0735	valid_1's l2: 20.2706
[1110]	training's l2: 18.0478	valid_1's l2: 20.2687
[1120]	training's l2: 18.0234	valid_1's l2: 20.2693
[1130]	training's l2: 17.9989	valid_1's l2: 20.269
[1140]	training's l2: 17.9744	valid_1's l2: 20.2679
[1150]	training's l2: 17.9504	valid_1's l2: 20.2681
[1160]	training's l2: 17.9254	valid_1's l2: 20.2671
[1170]	training's l2: 17.9002	valid_1's l2: 20.2662
[1180]	training's l2: 17.8752	valid_1's l2: 20.2657
[1190]	training's l2: 17.85	valid_1's l2: 20.2643
[1200]	training's l2: 17.8259	valid_1's l2: 20.2639
[1210]	training's l2: 17.803	valid_1's l2: 20.2635
[1220]	training's l2: 17.7787	valid_1's l2: 20.2628
[1230]	training's l2: 17.7536	valid_1's l2: 20.263
[1240]	training's l2: 17.7299	valid_1's l2: 20.2618
[1250]	training's l2: 17.7054	valid_1's l2: 20.2613
[1260]	training's l2: 17.6813	valid_1's l2: 20.2615
[1270]	training's l2: 17.6576	valid_1's l2: 20.2601
[1280]	training's l2: 17.6334	valid_1's l2: 20.2592
[1290]	training's l2: 17.6089	valid_1's l2: 20.2586
[1300]	training's l2: 17.5855	valid_1's l2: 20.2572
[1310]	training's l2: 17.5614	valid_1's l2: 20.2555
[1320]	training's l2: 17.5376	valid_1's l2: 20.2548
[1330]	training's l2: 17.5155	valid_1's l2: 20.254
[1340]	training's l2: 17.4923	valid_1's l2: 20.2539
[1350]	training's l2: 17.4695	valid_1's l2: 20.2539
[1360]	training's l2: 17.4468	valid_1's l2: 20.2533
[1370]	training's l2: 17.4235	valid_1's l2: 20.2541
[1380]	training's l2: 17.4024	valid_1's l2: 20.2531
[1390]	training's l2: 17.38	valid_1's l2: 20.2524
[1400]	training's l2: 17.3574	valid_1's l2: 20.2512
[1410]	training's l2: 17.335	valid_1's l2: 20.2501
[1420]	training's l2: 17.3124	valid_1's l2: 20.2502
[1430]	training's l2: 17.2896	valid_1's l2: 20.2506
[1440]	training's l2: 17.2674	valid_1's l2: 20.2499
[1450]	training's l2: 17.2445	valid_1's l2: 20.2494
[1460]	training's l2: 17.2221	valid_1's l2: 20.2492
[1470]	training's l2: 17.2011	valid_1's l2: 20.2494
[1480]	training's l2: 17.1791	valid_1's l2: 20.2493
[1490]	training's l2: 17.1567	valid_1's l2: 20.2491
[1500]	training's l2: 17.1342	valid_1's l2: 20.2495
[1510]	training's l2: 17.1126	valid_1's l2: 20.2485
[1520]	training's l2: 17.0904	valid_1's l2: 20.2483
[1530]	training's l2: 17.0685	valid_1's l2: 20.2476
[1540]	training's l2: 17.0461	valid_1's l2: 20.2472
[1550]	training's l2: 17.0243	valid_1's l2: 20.247
[1560]	training's l2: 17.0048	valid_1's l2: 20.2461
[1570]	training's l2: 16.9827	valid_1's l2: 20.2463
[1580]	training's l2: 16.9609	valid_1's l2: 20.2461
[1590]	training's l2: 16.9393	valid_1's l2: 20.2458
[1600]	training's l2: 16.9178	valid_1's l2: 20.2461
[1610]	training's l2: 16.8971	valid_1's l2: 20.2453
[1620]	training's l2: 16.8771	valid_1's l2: 20.2443
[1630]	training's l2: 16.8556	valid_1's l2: 20.2452
[1640]	training's l2: 16.8343	valid_1's l2: 20.2447
Early stopping, best iteration is:
[1619]	training's l2: 16.8789	valid_1's l2: 20.2442
score1: 3.838552079502397
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.247800 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.5649	valid_1's l2: 30.665
[20]	training's l2: 29.5919	valid_1's l2: 28.7088
[30]	training's l2: 28.0389	valid_1's l2: 27.171
[40]	training's l2: 26.8124	valid_1's l2: 25.9609
[50]	training's l2: 25.8367	valid_1's l2: 25.0034
[60]	training's l2: 25.0557	valid_1's l2: 24.2442
[70]	training's l2: 24.4261	valid_1's l2: 23.6389
[80]	training's l2: 23.9154	valid_1's l2: 23.1545
[90]	training's l2: 23.4969	valid_1's l2: 22.7637
[100]	training's l2: 23.1527	valid_1's l2: 22.448
[110]	training's l2: 22.8683	valid_1's l2: 22.1916
[120]	training's l2: 22.627	valid_1's l2: 21.9794
[130]	training's l2: 22.4182	valid_1's l2: 21.8028
[140]	training's l2: 22.2384	valid_1's l2: 21.6531
[150]	training's l2: 22.081	valid_1's l2: 21.5269
[160]	training's l2: 21.9395	valid_1's l2: 21.4194
[170]	training's l2: 21.8142	valid_1's l2: 21.3281
[180]	training's l2: 21.7005	valid_1's l2: 21.2484
[190]	training's l2: 21.5936	valid_1's l2: 21.1759
[200]	training's l2: 21.4964	valid_1's l2: 21.1129
[210]	training's l2: 21.4085	valid_1's l2: 21.0572
[220]	training's l2: 21.3242	valid_1's l2: 21.0054
[230]	training's l2: 21.246	valid_1's l2: 20.9599
[240]	training's l2: 21.1733	valid_1's l2: 20.9186
[250]	training's l2: 21.1057	valid_1's l2: 20.883
[260]	training's l2: 21.0374	valid_1's l2: 20.8469
[270]	training's l2: 20.9714	valid_1's l2: 20.8134
[280]	training's l2: 20.9084	valid_1's l2: 20.7842
[290]	training's l2: 20.8495	valid_1's l2: 20.7585
[300]	training's l2: 20.7927	valid_1's l2: 20.7349
[310]	training's l2: 20.7371	valid_1's l2: 20.7125
[320]	training's l2: 20.6847	valid_1's l2: 20.6926
[330]	training's l2: 20.6328	valid_1's l2: 20.6734
[340]	training's l2: 20.5834	valid_1's l2: 20.6567
[350]	training's l2: 20.5337	valid_1's l2: 20.6397
[360]	training's l2: 20.4856	valid_1's l2: 20.6245
[370]	training's l2: 20.4385	valid_1's l2: 20.6112
[380]	training's l2: 20.3929	valid_1's l2: 20.5977
[390]	training's l2: 20.3476	valid_1's l2: 20.5853
[400]	training's l2: 20.3031	valid_1's l2: 20.574
[410]	training's l2: 20.2591	valid_1's l2: 20.563
[420]	training's l2: 20.2159	valid_1's l2: 20.5515
[430]	training's l2: 20.1733	valid_1's l2: 20.5417
[440]	training's l2: 20.1315	valid_1's l2: 20.532
[450]	training's l2: 20.0904	valid_1's l2: 20.5228
[460]	training's l2: 20.0493	valid_1's l2: 20.5144
[470]	training's l2: 20.0089	valid_1's l2: 20.506
[480]	training's l2: 19.9691	valid_1's l2: 20.4978
[490]	training's l2: 19.93	valid_1's l2: 20.4906
[500]	training's l2: 19.8913	valid_1's l2: 20.4824
[510]	training's l2: 19.8522	valid_1's l2: 20.475
[520]	training's l2: 19.8137	valid_1's l2: 20.4672
[530]	training's l2: 19.7747	valid_1's l2: 20.4589
[540]	training's l2: 19.7363	valid_1's l2: 20.4522
[550]	training's l2: 19.6983	valid_1's l2: 20.4456
[560]	training's l2: 19.6607	valid_1's l2: 20.4396
[570]	training's l2: 19.6234	valid_1's l2: 20.4324
[580]	training's l2: 19.5857	valid_1's l2: 20.4259
[590]	training's l2: 19.5487	valid_1's l2: 20.4185
[600]	training's l2: 19.5128	valid_1's l2: 20.4128
[610]	training's l2: 19.4766	valid_1's l2: 20.4065
[620]	training's l2: 19.4406	valid_1's l2: 20.4
[630]	training's l2: 19.4053	valid_1's l2: 20.396
[640]	training's l2: 19.37	valid_1's l2: 20.3903
[650]	training's l2: 19.335	valid_1's l2: 20.3847
[660]	training's l2: 19.3003	valid_1's l2: 20.3801
[670]	training's l2: 19.2656	valid_1's l2: 20.3755
[680]	training's l2: 19.2318	valid_1's l2: 20.3713
[690]	training's l2: 19.1974	valid_1's l2: 20.366
[700]	training's l2: 19.1636	valid_1's l2: 20.362
[710]	training's l2: 19.1298	valid_1's l2: 20.359
[720]	training's l2: 19.097	valid_1's l2: 20.3551
[730]	training's l2: 19.0641	valid_1's l2: 20.3514
[740]	training's l2: 19.0314	valid_1's l2: 20.3477
[750]	training's l2: 18.999	valid_1's l2: 20.3455
[760]	training's l2: 18.9665	valid_1's l2: 20.3419
[770]	training's l2: 18.9337	valid_1's l2: 20.3374
[780]	training's l2: 18.9021	valid_1's l2: 20.3353
[790]	training's l2: 18.8699	valid_1's l2: 20.3329
[800]	training's l2: 18.8385	valid_1's l2: 20.3299
[810]	training's l2: 18.8074	valid_1's l2: 20.3271
[820]	training's l2: 18.776	valid_1's l2: 20.3236
[830]	training's l2: 18.7444	valid_1's l2: 20.3203
[840]	training's l2: 18.7128	valid_1's l2: 20.3176
[850]	training's l2: 18.6825	valid_1's l2: 20.3147
[860]	training's l2: 18.6518	valid_1's l2: 20.3115
[870]	training's l2: 18.6211	valid_1's l2: 20.3084
[880]	training's l2: 18.5902	valid_1's l2: 20.3056
[890]	training's l2: 18.5597	valid_1's l2: 20.3035
[900]	training's l2: 18.5292	valid_1's l2: 20.3005
[910]	training's l2: 18.4992	valid_1's l2: 20.298
[920]	training's l2: 18.4692	valid_1's l2: 20.2961
[930]	training's l2: 18.4394	valid_1's l2: 20.2927
[940]	training's l2: 18.4102	valid_1's l2: 20.2915
[950]	training's l2: 18.3812	valid_1's l2: 20.289
[960]	training's l2: 18.353	valid_1's l2: 20.2877
[970]	training's l2: 18.3242	valid_1's l2: 20.286
[980]	training's l2: 18.2954	valid_1's l2: 20.2843
[990]	training's l2: 18.2659	valid_1's l2: 20.2824
[1000]	training's l2: 18.2387	valid_1's l2: 20.2817
[1010]	training's l2: 18.2102	valid_1's l2: 20.2808
[1020]	training's l2: 18.1812	valid_1's l2: 20.2792
[1030]	training's l2: 18.1531	valid_1's l2: 20.2773
[1040]	training's l2: 18.1252	valid_1's l2: 20.276
[1050]	training's l2: 18.098	valid_1's l2: 20.2754
[1060]	training's l2: 18.0707	valid_1's l2: 20.2749
[1070]	training's l2: 18.0433	valid_1's l2: 20.2739
[1080]	training's l2: 18.0162	valid_1's l2: 20.2719
[1090]	training's l2: 17.9889	valid_1's l2: 20.2707
[1100]	training's l2: 17.9627	valid_1's l2: 20.2698
[1110]	training's l2: 17.9365	valid_1's l2: 20.2702
[1120]	training's l2: 17.9099	valid_1's l2: 20.2682
[1130]	training's l2: 17.8836	valid_1's l2: 20.2682
[1140]	training's l2: 17.8577	valid_1's l2: 20.2676
[1150]	training's l2: 17.8324	valid_1's l2: 20.2668
[1160]	training's l2: 17.8057	valid_1's l2: 20.2662
[1170]	training's l2: 17.7794	valid_1's l2: 20.2652
[1180]	training's l2: 17.7541	valid_1's l2: 20.2644
[1190]	training's l2: 17.7279	valid_1's l2: 20.2637
[1200]	training's l2: 17.7033	valid_1's l2: 20.2635
[1210]	training's l2: 17.6781	valid_1's l2: 20.2628
[1220]	training's l2: 17.6538	valid_1's l2: 20.2625
[1230]	training's l2: 17.6278	valid_1's l2: 20.2618
[1240]	training's l2: 17.6048	valid_1's l2: 20.2606
[1250]	training's l2: 17.5799	valid_1's l2: 20.2613
[1260]	training's l2: 17.5551	valid_1's l2: 20.2609
[1270]	training's l2: 17.5303	valid_1's l2: 20.2595
[1280]	training's l2: 17.5061	valid_1's l2: 20.2592
[1290]	training's l2: 17.4806	valid_1's l2: 20.2577
[1300]	training's l2: 17.4566	valid_1's l2: 20.2564
[1310]	training's l2: 17.432	valid_1's l2: 20.255
[1320]	training's l2: 17.4082	valid_1's l2: 20.2536
[1330]	training's l2: 17.385	valid_1's l2: 20.2537
[1340]	training's l2: 17.3609	valid_1's l2: 20.253
[1350]	training's l2: 17.3379	valid_1's l2: 20.2527
[1360]	training's l2: 17.314	valid_1's l2: 20.2517
[1370]	training's l2: 17.2902	valid_1's l2: 20.2514
[1380]	training's l2: 17.2658	valid_1's l2: 20.2516
[1390]	training's l2: 17.2419	valid_1's l2: 20.2512
[1400]	training's l2: 17.2196	valid_1's l2: 20.2509
[1410]	training's l2: 17.1961	valid_1's l2: 20.2504
[1420]	training's l2: 17.1731	valid_1's l2: 20.2503
[1430]	training's l2: 17.1504	valid_1's l2: 20.2499
[1440]	training's l2: 17.1286	valid_1's l2: 20.2497
[1450]	training's l2: 17.106	valid_1's l2: 20.2484
[1460]	training's l2: 17.0832	valid_1's l2: 20.2486
[1470]	training's l2: 17.0602	valid_1's l2: 20.248
[1480]	training's l2: 17.038	valid_1's l2: 20.2473
[1490]	training's l2: 17.0147	valid_1's l2: 20.2477
[1500]	training's l2: 16.993	valid_1's l2: 20.2477
Early stopping, best iteration is:
[1478]	training's l2: 17.0424	valid_1's l2: 20.2471
score1: 3.839492419880143
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.268481 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.1139	valid_1's l2: 30.2169
[20]	training's l2: 28.9011	valid_1's l2: 28.021
[30]	training's l2: 27.2446	valid_1's l2: 26.382
[40]	training's l2: 25.9929	valid_1's l2: 25.152
[50]	training's l2: 25.0375	valid_1's l2: 24.221
[60]	training's l2: 24.3012	valid_1's l2: 23.5119
[70]	training's l2: 23.7282	valid_1's l2: 22.9696
[80]	training's l2: 23.2773	valid_1's l2: 22.5522
[90]	training's l2: 22.921	valid_1's l2: 22.2265
[100]	training's l2: 22.6311	valid_1's l2: 21.9721
[110]	training's l2: 22.3862	valid_1's l2: 21.7643
[120]	training's l2: 22.1821	valid_1's l2: 21.5959
[130]	training's l2: 22.0058	valid_1's l2: 21.4597
[140]	training's l2: 21.851	valid_1's l2: 21.3435
[150]	training's l2: 21.7147	valid_1's l2: 21.2471
[160]	training's l2: 21.5914	valid_1's l2: 21.1635
[170]	training's l2: 21.4809	valid_1's l2: 21.0908
[180]	training's l2: 21.3797	valid_1's l2: 21.0277
[190]	training's l2: 21.2868	valid_1's l2: 20.9724
[200]	training's l2: 21.1997	valid_1's l2: 20.9235
[210]	training's l2: 21.1177	valid_1's l2: 20.8787
[220]	training's l2: 21.0378	valid_1's l2: 20.8363
[230]	training's l2: 20.9627	valid_1's l2: 20.7998
[240]	training's l2: 20.8918	valid_1's l2: 20.7661
[250]	training's l2: 20.8241	valid_1's l2: 20.7377
[260]	training's l2: 20.7593	valid_1's l2: 20.7113
[270]	training's l2: 20.6971	valid_1's l2: 20.6879
[280]	training's l2: 20.6378	valid_1's l2: 20.667
[290]	training's l2: 20.5796	valid_1's l2: 20.6464
[300]	training's l2: 20.5238	valid_1's l2: 20.6274
[310]	training's l2: 20.4693	valid_1's l2: 20.6102
[320]	training's l2: 20.4155	valid_1's l2: 20.5943
[330]	training's l2: 20.363	valid_1's l2: 20.5804
[340]	training's l2: 20.3107	valid_1's l2: 20.5664
[350]	training's l2: 20.2602	valid_1's l2: 20.5547
[360]	training's l2: 20.2109	valid_1's l2: 20.5435
[370]	training's l2: 20.1623	valid_1's l2: 20.5335
[380]	training's l2: 20.1145	valid_1's l2: 20.5225
[390]	training's l2: 20.0668	valid_1's l2: 20.512
[400]	training's l2: 20.02	valid_1's l2: 20.5019
[410]	training's l2: 19.9738	valid_1's l2: 20.4924
[420]	training's l2: 19.9278	valid_1's l2: 20.4836
[430]	training's l2: 19.882	valid_1's l2: 20.474
[440]	training's l2: 19.8359	valid_1's l2: 20.4648
[450]	training's l2: 19.7911	valid_1's l2: 20.457
[460]	training's l2: 19.7466	valid_1's l2: 20.4484
[470]	training's l2: 19.7025	valid_1's l2: 20.4402
[480]	training's l2: 19.6586	valid_1's l2: 20.4315
[490]	training's l2: 19.6157	valid_1's l2: 20.4239
[500]	training's l2: 19.5724	valid_1's l2: 20.4172
[510]	training's l2: 19.5306	valid_1's l2: 20.4107
[520]	training's l2: 19.4878	valid_1's l2: 20.404
[530]	training's l2: 19.4465	valid_1's l2: 20.3979
[540]	training's l2: 19.4054	valid_1's l2: 20.3926
[550]	training's l2: 19.365	valid_1's l2: 20.3869
[560]	training's l2: 19.325	valid_1's l2: 20.3824
[570]	training's l2: 19.2853	valid_1's l2: 20.3789
[580]	training's l2: 19.2451	valid_1's l2: 20.3742
[590]	training's l2: 19.2065	valid_1's l2: 20.371
[600]	training's l2: 19.1678	valid_1's l2: 20.3659
[610]	training's l2: 19.129	valid_1's l2: 20.361
[620]	training's l2: 19.0911	valid_1's l2: 20.3572
[630]	training's l2: 19.0525	valid_1's l2: 20.3519
[640]	training's l2: 19.0148	valid_1's l2: 20.3491
[650]	training's l2: 18.9765	valid_1's l2: 20.3449
[660]	training's l2: 18.9395	valid_1's l2: 20.3421
[670]	training's l2: 18.9024	valid_1's l2: 20.3383
[680]	training's l2: 18.8652	valid_1's l2: 20.3354
[690]	training's l2: 18.8282	valid_1's l2: 20.3318
[700]	training's l2: 18.7902	valid_1's l2: 20.3277
[710]	training's l2: 18.7532	valid_1's l2: 20.3249
[720]	training's l2: 18.7162	valid_1's l2: 20.3208
[730]	training's l2: 18.6808	valid_1's l2: 20.3193
[740]	training's l2: 18.6453	valid_1's l2: 20.3155
[750]	training's l2: 18.6105	valid_1's l2: 20.312
[760]	training's l2: 18.5742	valid_1's l2: 20.3087
[770]	training's l2: 18.5389	valid_1's l2: 20.3046
[780]	training's l2: 18.5045	valid_1's l2: 20.3025
[790]	training's l2: 18.4708	valid_1's l2: 20.3011
[800]	training's l2: 18.4368	valid_1's l2: 20.2999
[810]	training's l2: 18.4032	valid_1's l2: 20.2974
[820]	training's l2: 18.3692	valid_1's l2: 20.2959
[830]	training's l2: 18.3375	valid_1's l2: 20.294
[840]	training's l2: 18.3043	valid_1's l2: 20.2924
[850]	training's l2: 18.2708	valid_1's l2: 20.2898
[860]	training's l2: 18.2388	valid_1's l2: 20.2878
[870]	training's l2: 18.2059	valid_1's l2: 20.2857
[880]	training's l2: 18.1741	valid_1's l2: 20.2842
[890]	training's l2: 18.1416	valid_1's l2: 20.2833
[900]	training's l2: 18.1101	valid_1's l2: 20.2817
[910]	training's l2: 18.0789	valid_1's l2: 20.2804
[920]	training's l2: 18.0482	valid_1's l2: 20.2802
[930]	training's l2: 18.0158	valid_1's l2: 20.2779
[940]	training's l2: 17.9855	valid_1's l2: 20.2781
[950]	training's l2: 17.9549	valid_1's l2: 20.2767
[960]	training's l2: 17.9239	valid_1's l2: 20.2759
[970]	training's l2: 17.8953	valid_1's l2: 20.2757
[980]	training's l2: 17.8639	valid_1's l2: 20.274
[990]	training's l2: 17.834	valid_1's l2: 20.2724
[1000]	training's l2: 17.8046	valid_1's l2: 20.2728
[1010]	training's l2: 17.7755	valid_1's l2: 20.2723
[1020]	training's l2: 17.746	valid_1's l2: 20.2711
[1030]	training's l2: 17.7175	valid_1's l2: 20.2702
[1040]	training's l2: 17.6883	valid_1's l2: 20.2691
[1050]	training's l2: 17.6607	valid_1's l2: 20.2679
[1060]	training's l2: 17.6319	valid_1's l2: 20.2669
[1070]	training's l2: 17.6027	valid_1's l2: 20.2654
[1080]	training's l2: 17.5749	valid_1's l2: 20.2655
[1090]	training's l2: 17.5474	valid_1's l2: 20.2651
[1100]	training's l2: 17.5197	valid_1's l2: 20.2644
[1110]	training's l2: 17.4922	valid_1's l2: 20.2635
[1120]	training's l2: 17.4637	valid_1's l2: 20.2623
[1130]	training's l2: 17.4353	valid_1's l2: 20.2621
[1140]	training's l2: 17.4071	valid_1's l2: 20.2618
[1150]	training's l2: 17.3793	valid_1's l2: 20.2614
[1160]	training's l2: 17.3527	valid_1's l2: 20.2609
[1170]	training's l2: 17.3249	valid_1's l2: 20.2607
[1180]	training's l2: 17.2973	valid_1's l2: 20.2606
[1190]	training's l2: 17.2696	valid_1's l2: 20.2595
[1200]	training's l2: 17.2409	valid_1's l2: 20.259
[1210]	training's l2: 17.215	valid_1's l2: 20.2588
[1220]	training's l2: 17.1892	valid_1's l2: 20.2584
[1230]	training's l2: 17.1613	valid_1's l2: 20.2584
[1240]	training's l2: 17.135	valid_1's l2: 20.2584
[1250]	training's l2: 17.1069	valid_1's l2: 20.2581
Early stopping, best iteration is:
[1226]	training's l2: 17.1721	valid_1's l2: 20.2577
score1: 3.8404978068934175
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254287 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.748	valid_1's l2: 30.8488
[20]	training's l2: 29.8821	valid_1's l2: 28.9993
[30]	training's l2: 28.3845	valid_1's l2: 27.5175
[40]	training's l2: 27.1799	valid_1's l2: 26.3281
[50]	training's l2: 26.2032	valid_1's l2: 25.37
[60]	training's l2: 25.4099	valid_1's l2: 24.5985
[70]	training's l2: 24.7609	valid_1's l2: 23.9732
[80]	training's l2: 24.2267	valid_1's l2: 23.4623
[90]	training's l2: 23.7821	valid_1's l2: 23.0435
[100]	training's l2: 23.4118	valid_1's l2: 22.699
[110]	training's l2: 23.1025	valid_1's l2: 22.4161
[120]	training's l2: 22.8413	valid_1's l2: 22.1839
[130]	training's l2: 22.6191	valid_1's l2: 21.989
[140]	training's l2: 22.4231	valid_1's l2: 21.8235
[150]	training's l2: 22.2519	valid_1's l2: 21.6838
[160]	training's l2: 22.1016	valid_1's l2: 21.5634
[170]	training's l2: 21.9663	valid_1's l2: 21.4596
[180]	training's l2: 21.8428	valid_1's l2: 21.3679
[190]	training's l2: 21.7315	valid_1's l2: 21.2895
[200]	training's l2: 21.6296	valid_1's l2: 21.2203
[210]	training's l2: 21.5327	valid_1's l2: 21.1537
[220]	training's l2: 21.444	valid_1's l2: 21.0972
[230]	training's l2: 21.3625	valid_1's l2: 21.0465
[240]	training's l2: 21.2852	valid_1's l2: 21.001
[250]	training's l2: 21.2119	valid_1's l2: 20.9585
[260]	training's l2: 21.1439	valid_1's l2: 20.9212
[270]	training's l2: 21.078	valid_1's l2: 20.8866
[280]	training's l2: 21.013	valid_1's l2: 20.8522
[290]	training's l2: 20.9513	valid_1's l2: 20.821
[300]	training's l2: 20.892	valid_1's l2: 20.7938
[310]	training's l2: 20.8338	valid_1's l2: 20.768
[320]	training's l2: 20.779	valid_1's l2: 20.745
[330]	training's l2: 20.727	valid_1's l2: 20.7238
[340]	training's l2: 20.6756	valid_1's l2: 20.7052
[350]	training's l2: 20.6261	valid_1's l2: 20.6874
[360]	training's l2: 20.5784	valid_1's l2: 20.6701
[370]	training's l2: 20.5315	valid_1's l2: 20.6536
[380]	training's l2: 20.4853	valid_1's l2: 20.6388
[390]	training's l2: 20.4404	valid_1's l2: 20.6256
[400]	training's l2: 20.3963	valid_1's l2: 20.6128
[410]	training's l2: 20.3536	valid_1's l2: 20.6011
[420]	training's l2: 20.3104	valid_1's l2: 20.5881
[430]	training's l2: 20.2683	valid_1's l2: 20.5768
[440]	training's l2: 20.2268	valid_1's l2: 20.5655
[450]	training's l2: 20.1852	valid_1's l2: 20.5549
[460]	training's l2: 20.1449	valid_1's l2: 20.5462
[470]	training's l2: 20.1045	valid_1's l2: 20.5365
[480]	training's l2: 20.065	valid_1's l2: 20.5278
[490]	training's l2: 20.0263	valid_1's l2: 20.5205
[500]	training's l2: 19.9878	valid_1's l2: 20.5117
[510]	training's l2: 19.9498	valid_1's l2: 20.505
[520]	training's l2: 19.9124	valid_1's l2: 20.4976
[530]	training's l2: 19.8743	valid_1's l2: 20.4902
[540]	training's l2: 19.837	valid_1's l2: 20.4838
[550]	training's l2: 19.8004	valid_1's l2: 20.4771
[560]	training's l2: 19.7636	valid_1's l2: 20.4702
[570]	training's l2: 19.7272	valid_1's l2: 20.4643
[580]	training's l2: 19.69	valid_1's l2: 20.4571
[590]	training's l2: 19.6541	valid_1's l2: 20.452
[600]	training's l2: 19.6181	valid_1's l2: 20.4461
[610]	training's l2: 19.5825	valid_1's l2: 20.4396
[620]	training's l2: 19.5472	valid_1's l2: 20.4327
[630]	training's l2: 19.5123	valid_1's l2: 20.4268
[640]	training's l2: 19.4777	valid_1's l2: 20.4219
[650]	training's l2: 19.4434	valid_1's l2: 20.4168
[660]	training's l2: 19.4088	valid_1's l2: 20.4116
[670]	training's l2: 19.3749	valid_1's l2: 20.406
[680]	training's l2: 19.3413	valid_1's l2: 20.4017
[690]	training's l2: 19.3079	valid_1's l2: 20.3973
[700]	training's l2: 19.2746	valid_1's l2: 20.3932
[710]	training's l2: 19.242	valid_1's l2: 20.389
[720]	training's l2: 19.2098	valid_1's l2: 20.3845
[730]	training's l2: 19.1773	valid_1's l2: 20.3806
[740]	training's l2: 19.1445	valid_1's l2: 20.3776
[750]	training's l2: 19.1124	valid_1's l2: 20.3739
[760]	training's l2: 19.0807	valid_1's l2: 20.3703
[770]	training's l2: 19.0484	valid_1's l2: 20.3665
[780]	training's l2: 19.0172	valid_1's l2: 20.3638
[790]	training's l2: 18.986	valid_1's l2: 20.3599
[800]	training's l2: 18.9547	valid_1's l2: 20.357
[810]	training's l2: 18.9241	valid_1's l2: 20.3535
[820]	training's l2: 18.893	valid_1's l2: 20.3508
[830]	training's l2: 18.8621	valid_1's l2: 20.347
[840]	training's l2: 18.8317	valid_1's l2: 20.3445
[850]	training's l2: 18.8012	valid_1's l2: 20.3416
[860]	training's l2: 18.7715	valid_1's l2: 20.3397
[870]	training's l2: 18.7408	valid_1's l2: 20.3373
[880]	training's l2: 18.7106	valid_1's l2: 20.3347
[890]	training's l2: 18.681	valid_1's l2: 20.3323
[900]	training's l2: 18.6513	valid_1's l2: 20.3293
[910]	training's l2: 18.6221	valid_1's l2: 20.3266
[920]	training's l2: 18.593	valid_1's l2: 20.3233
[930]	training's l2: 18.5635	valid_1's l2: 20.3209
[940]	training's l2: 18.5346	valid_1's l2: 20.3178
[950]	training's l2: 18.5049	valid_1's l2: 20.3155
[960]	training's l2: 18.4764	valid_1's l2: 20.3135
[970]	training's l2: 18.448	valid_1's l2: 20.3113
[980]	training's l2: 18.4192	valid_1's l2: 20.3094
[990]	training's l2: 18.3913	valid_1's l2: 20.3073
[1000]	training's l2: 18.3624	valid_1's l2: 20.3049
[1010]	training's l2: 18.3342	valid_1's l2: 20.3034
[1020]	training's l2: 18.3066	valid_1's l2: 20.3014
[1030]	training's l2: 18.2788	valid_1's l2: 20.3002
[1040]	training's l2: 18.2522	valid_1's l2: 20.2994
[1050]	training's l2: 18.2244	valid_1's l2: 20.2962
[1060]	training's l2: 18.1977	valid_1's l2: 20.2942
[1070]	training's l2: 18.1709	valid_1's l2: 20.293
[1080]	training's l2: 18.1433	valid_1's l2: 20.2913
[1090]	training's l2: 18.1164	valid_1's l2: 20.2901
[1100]	training's l2: 18.0892	valid_1's l2: 20.2885
[1110]	training's l2: 18.0629	valid_1's l2: 20.2866
[1120]	training's l2: 18.0365	valid_1's l2: 20.2853
[1130]	training's l2: 18.01	valid_1's l2: 20.284
[1140]	training's l2: 17.9837	valid_1's l2: 20.2836
[1150]	training's l2: 17.958	valid_1's l2: 20.2823
[1160]	training's l2: 17.9329	valid_1's l2: 20.2814
[1170]	training's l2: 17.9074	valid_1's l2: 20.2795
[1180]	training's l2: 17.8814	valid_1's l2: 20.2779
[1190]	training's l2: 17.8556	valid_1's l2: 20.2771
[1200]	training's l2: 17.8309	valid_1's l2: 20.2768
[1210]	training's l2: 17.8054	valid_1's l2: 20.2756
[1220]	training's l2: 17.7802	valid_1's l2: 20.2749
[1230]	training's l2: 17.7554	valid_1's l2: 20.2745
[1240]	training's l2: 17.7306	valid_1's l2: 20.2747
[1250]	training's l2: 17.7056	valid_1's l2: 20.274
[1260]	training's l2: 17.6817	valid_1's l2: 20.2737
[1270]	training's l2: 17.6578	valid_1's l2: 20.2724
[1280]	training's l2: 17.6343	valid_1's l2: 20.2716
[1290]	training's l2: 17.6097	valid_1's l2: 20.2705
[1300]	training's l2: 17.5865	valid_1's l2: 20.2698
[1310]	training's l2: 17.5625	valid_1's l2: 20.2694
[1320]	training's l2: 17.5388	valid_1's l2: 20.2685
[1330]	training's l2: 17.516	valid_1's l2: 20.2682
[1340]	training's l2: 17.4927	valid_1's l2: 20.2677
[1350]	training's l2: 17.4687	valid_1's l2: 20.2674
[1360]	training's l2: 17.4454	valid_1's l2: 20.2666
[1370]	training's l2: 17.4234	valid_1's l2: 20.2652
[1380]	training's l2: 17.4014	valid_1's l2: 20.2646
[1390]	training's l2: 17.3788	valid_1's l2: 20.2639
[1400]	training's l2: 17.3554	valid_1's l2: 20.264
[1410]	training's l2: 17.3327	valid_1's l2: 20.2637
[1420]	training's l2: 17.3103	valid_1's l2: 20.2629
[1430]	training's l2: 17.2886	valid_1's l2: 20.2626
[1440]	training's l2: 17.2658	valid_1's l2: 20.2619
[1450]	training's l2: 17.2437	valid_1's l2: 20.2619
[1460]	training's l2: 17.2233	valid_1's l2: 20.2618
[1470]	training's l2: 17.2014	valid_1's l2: 20.2607
[1480]	training's l2: 17.1789	valid_1's l2: 20.2598
[1490]	training's l2: 17.1567	valid_1's l2: 20.2588
[1500]	training's l2: 17.1335	valid_1's l2: 20.2577
[1510]	training's l2: 17.1126	valid_1's l2: 20.257
[1520]	training's l2: 17.0906	valid_1's l2: 20.2561
[1530]	training's l2: 17.0696	valid_1's l2: 20.2558
[1540]	training's l2: 17.0469	valid_1's l2: 20.2551
[1550]	training's l2: 17.0246	valid_1's l2: 20.2546
[1560]	training's l2: 17.0024	valid_1's l2: 20.2549
[1570]	training's l2: 16.9827	valid_1's l2: 20.2547
[1580]	training's l2: 16.9606	valid_1's l2: 20.2544
[1590]	training's l2: 16.9388	valid_1's l2: 20.2544
[1600]	training's l2: 16.9186	valid_1's l2: 20.2546
[1610]	training's l2: 16.8964	valid_1's l2: 20.2546
[1620]	training's l2: 16.8734	valid_1's l2: 20.2542
[1630]	training's l2: 16.8527	valid_1's l2: 20.2547
[1640]	training's l2: 16.8318	valid_1's l2: 20.254
[1650]	training's l2: 16.81	valid_1's l2: 20.2539
[1660]	training's l2: 16.7887	valid_1's l2: 20.2534
[1670]	training's l2: 16.7677	valid_1's l2: 20.254
[1680]	training's l2: 16.7461	valid_1's l2: 20.2539
[1690]	training's l2: 16.7252	valid_1's l2: 20.2536
[1700]	training's l2: 16.7045	valid_1's l2: 20.2533
Early stopping, best iteration is:
[1674]	training's l2: 16.7588	valid_1's l2: 20.2533
score1: 3.8398689267999995
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258358 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.3045	valid_1's l2: 30.4129
[20]	training's l2: 29.1855	valid_1's l2: 28.3165
[30]	training's l2: 27.5612	valid_1's l2: 26.713
[40]	training's l2: 26.3057	valid_1's l2: 25.4833
[50]	training's l2: 25.3312	valid_1's l2: 24.5359
[60]	training's l2: 24.5655	valid_1's l2: 23.8022
[70]	training's l2: 23.9603	valid_1's l2: 23.2296
[80]	training's l2: 23.4733	valid_1's l2: 22.7775
[90]	training's l2: 23.0821	valid_1's l2: 22.4224
[100]	training's l2: 22.7649	valid_1's l2: 22.1421
[110]	training's l2: 22.4998	valid_1's l2: 21.9137
[120]	training's l2: 22.2737	valid_1's l2: 21.7266
[130]	training's l2: 22.0807	valid_1's l2: 21.5709
[140]	training's l2: 21.9093	valid_1's l2: 21.4406
[150]	training's l2: 21.7594	valid_1's l2: 21.3318
[160]	training's l2: 21.6266	valid_1's l2: 21.2417
[170]	training's l2: 21.5035	valid_1's l2: 21.1591
[180]	training's l2: 21.3919	valid_1's l2: 21.088
[190]	training's l2: 21.2899	valid_1's l2: 21.0263
[200]	training's l2: 21.195	valid_1's l2: 20.9728
[210]	training's l2: 21.1059	valid_1's l2: 20.9242
[220]	training's l2: 21.0229	valid_1's l2: 20.881
[230]	training's l2: 20.942	valid_1's l2: 20.8389
[240]	training's l2: 20.8648	valid_1's l2: 20.8028
[250]	training's l2: 20.7904	valid_1's l2: 20.7704
[260]	training's l2: 20.7197	valid_1's l2: 20.7414
[270]	training's l2: 20.6522	valid_1's l2: 20.7138
[280]	training's l2: 20.5876	valid_1's l2: 20.6908
[290]	training's l2: 20.525	valid_1's l2: 20.6685
[300]	training's l2: 20.4649	valid_1's l2: 20.6487
[310]	training's l2: 20.4066	valid_1's l2: 20.6318
[320]	training's l2: 20.35	valid_1's l2: 20.6141
[330]	training's l2: 20.2941	valid_1's l2: 20.5991
[340]	training's l2: 20.2392	valid_1's l2: 20.5848
[350]	training's l2: 20.1859	valid_1's l2: 20.5704
[360]	training's l2: 20.1331	valid_1's l2: 20.5568
[370]	training's l2: 20.0816	valid_1's l2: 20.5451
[380]	training's l2: 20.0311	valid_1's l2: 20.535
[390]	training's l2: 19.9805	valid_1's l2: 20.524
[400]	training's l2: 19.9311	valid_1's l2: 20.5134
[410]	training's l2: 19.8825	valid_1's l2: 20.5043
[420]	training's l2: 19.8346	valid_1's l2: 20.4952
[430]	training's l2: 19.786	valid_1's l2: 20.4857
[440]	training's l2: 19.7382	valid_1's l2: 20.4764
[450]	training's l2: 19.6913	valid_1's l2: 20.4681
[460]	training's l2: 19.6444	valid_1's l2: 20.4583
[470]	training's l2: 19.5975	valid_1's l2: 20.4494
[480]	training's l2: 19.5515	valid_1's l2: 20.4413
[490]	training's l2: 19.5059	valid_1's l2: 20.4333
[500]	training's l2: 19.4607	valid_1's l2: 20.4249
[510]	training's l2: 19.4152	valid_1's l2: 20.4166
[520]	training's l2: 19.3709	valid_1's l2: 20.4105
[530]	training's l2: 19.3269	valid_1's l2: 20.4036
[540]	training's l2: 19.2838	valid_1's l2: 20.3966
[550]	training's l2: 19.241	valid_1's l2: 20.3917
[560]	training's l2: 19.1983	valid_1's l2: 20.386
[570]	training's l2: 19.1562	valid_1's l2: 20.3818
[580]	training's l2: 19.1136	valid_1's l2: 20.3753
[590]	training's l2: 19.0723	valid_1's l2: 20.3696
[600]	training's l2: 19.0321	valid_1's l2: 20.3655
[610]	training's l2: 18.9911	valid_1's l2: 20.3612
[620]	training's l2: 18.9507	valid_1's l2: 20.357
[630]	training's l2: 18.9103	valid_1's l2: 20.3528
[640]	training's l2: 18.8701	valid_1's l2: 20.3481
[650]	training's l2: 18.8302	valid_1's l2: 20.3442
[660]	training's l2: 18.79	valid_1's l2: 20.3401
[670]	training's l2: 18.7509	valid_1's l2: 20.3356
[680]	training's l2: 18.7118	valid_1's l2: 20.3316
[690]	training's l2: 18.673	valid_1's l2: 20.3281
[700]	training's l2: 18.6342	valid_1's l2: 20.3255
[710]	training's l2: 18.596	valid_1's l2: 20.3233
[720]	training's l2: 18.5578	valid_1's l2: 20.3201
[730]	training's l2: 18.5197	valid_1's l2: 20.3168
[740]	training's l2: 18.4826	valid_1's l2: 20.3148
[750]	training's l2: 18.4446	valid_1's l2: 20.312
[760]	training's l2: 18.407	valid_1's l2: 20.3086
[770]	training's l2: 18.3704	valid_1's l2: 20.3067
[780]	training's l2: 18.3331	valid_1's l2: 20.3022
[790]	training's l2: 18.2964	valid_1's l2: 20.2998
[800]	training's l2: 18.2601	valid_1's l2: 20.2974
[810]	training's l2: 18.224	valid_1's l2: 20.2957
[820]	training's l2: 18.1891	valid_1's l2: 20.2948
[830]	training's l2: 18.1532	valid_1's l2: 20.293
[840]	training's l2: 18.1182	valid_1's l2: 20.2909
[850]	training's l2: 18.0833	valid_1's l2: 20.2893
[860]	training's l2: 18.0492	valid_1's l2: 20.2871
[870]	training's l2: 18.0141	valid_1's l2: 20.2845
[880]	training's l2: 17.98	valid_1's l2: 20.2827
[890]	training's l2: 17.9455	valid_1's l2: 20.2805
[900]	training's l2: 17.9114	valid_1's l2: 20.2789
[910]	training's l2: 17.8781	valid_1's l2: 20.277
[920]	training's l2: 17.8442	valid_1's l2: 20.2752
[930]	training's l2: 17.8122	valid_1's l2: 20.2748
[940]	training's l2: 17.7798	valid_1's l2: 20.2732
[950]	training's l2: 17.7474	valid_1's l2: 20.2715
[960]	training's l2: 17.7143	valid_1's l2: 20.2702
[970]	training's l2: 17.6826	valid_1's l2: 20.2691
[980]	training's l2: 17.6511	valid_1's l2: 20.2678
[990]	training's l2: 17.6198	valid_1's l2: 20.2679
[1000]	training's l2: 17.5879	valid_1's l2: 20.2667
[1010]	training's l2: 17.5573	valid_1's l2: 20.2648
[1020]	training's l2: 17.5268	valid_1's l2: 20.2642
[1030]	training's l2: 17.4963	valid_1's l2: 20.2637
[1040]	training's l2: 17.4654	valid_1's l2: 20.2628
[1050]	training's l2: 17.4344	valid_1's l2: 20.2618
[1060]	training's l2: 17.4034	valid_1's l2: 20.2612
[1070]	training's l2: 17.3745	valid_1's l2: 20.2602
[1080]	training's l2: 17.343	valid_1's l2: 20.2593
[1090]	training's l2: 17.3142	valid_1's l2: 20.2596
[1100]	training's l2: 17.2852	valid_1's l2: 20.2595
[1110]	training's l2: 17.2575	valid_1's l2: 20.2582
[1120]	training's l2: 17.2266	valid_1's l2: 20.2572
[1130]	training's l2: 17.1968	valid_1's l2: 20.2572
[1140]	training's l2: 17.1679	valid_1's l2: 20.2569
[1150]	training's l2: 17.1389	valid_1's l2: 20.257
[1160]	training's l2: 17.1092	valid_1's l2: 20.2563
[1170]	training's l2: 17.0807	valid_1's l2: 20.2569
[1180]	training's l2: 17.0521	valid_1's l2: 20.2579
[1190]	training's l2: 17.0221	valid_1's l2: 20.257
Early stopping, best iteration is:
[1163]	training's l2: 17.1003	valid_1's l2: 20.2562
score1: 3.839035338743167
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.258827 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.9563	valid_1's l2: 31.0575
[20]	training's l2: 30.2164	valid_1's l2: 29.332
[30]	training's l2: 28.7916	valid_1's l2: 27.9236
[40]	training's l2: 27.6209	valid_1's l2: 26.7676
[50]	training's l2: 26.6535	valid_1's l2: 25.8177
[60]	training's l2: 25.8528	valid_1's l2: 25.0357
[70]	training's l2: 25.1861	valid_1's l2: 24.3877
[80]	training's l2: 24.629	valid_1's l2: 23.8542
[90]	training's l2: 24.1599	valid_1's l2: 23.4077
[100]	training's l2: 23.7625	valid_1's l2: 23.0344
[110]	training's l2: 23.4257	valid_1's l2: 22.7223
[120]	training's l2: 23.1382	valid_1's l2: 22.4601
[130]	training's l2: 22.8927	valid_1's l2: 22.2409
[140]	training's l2: 22.6801	valid_1's l2: 22.0538
[150]	training's l2: 22.4922	valid_1's l2: 21.8927
[160]	training's l2: 22.3245	valid_1's l2: 21.7542
[170]	training's l2: 22.1781	valid_1's l2: 21.6358
[180]	training's l2: 22.0447	valid_1's l2: 21.5301
[190]	training's l2: 21.9236	valid_1's l2: 21.4384
[200]	training's l2: 21.812	valid_1's l2: 21.3564
[210]	training's l2: 21.7105	valid_1's l2: 21.2849
[220]	training's l2: 21.617	valid_1's l2: 21.222
[230]	training's l2: 21.5274	valid_1's l2: 21.163
[240]	training's l2: 21.444	valid_1's l2: 21.1078
[250]	training's l2: 21.3675	valid_1's l2: 21.0613
[260]	training's l2: 21.295	valid_1's l2: 21.018
[270]	training's l2: 21.227	valid_1's l2: 20.9779
[280]	training's l2: 21.161	valid_1's l2: 20.9407
[290]	training's l2: 21.0991	valid_1's l2: 20.9074
[300]	training's l2: 21.0402	valid_1's l2: 20.8772
[310]	training's l2: 20.9813	valid_1's l2: 20.8467
[320]	training's l2: 20.9235	valid_1's l2: 20.817
[330]	training's l2: 20.8682	valid_1's l2: 20.7918
[340]	training's l2: 20.8148	valid_1's l2: 20.769
[350]	training's l2: 20.7628	valid_1's l2: 20.7482
[360]	training's l2: 20.7127	valid_1's l2: 20.7278
[370]	training's l2: 20.6651	valid_1's l2: 20.7086
[380]	training's l2: 20.619	valid_1's l2: 20.6916
[390]	training's l2: 20.5737	valid_1's l2: 20.6755
[400]	training's l2: 20.5295	valid_1's l2: 20.6602
[410]	training's l2: 20.487	valid_1's l2: 20.6463
[420]	training's l2: 20.4454	valid_1's l2: 20.6342
[430]	training's l2: 20.4041	valid_1's l2: 20.6214
[440]	training's l2: 20.3634	valid_1's l2: 20.6085
[450]	training's l2: 20.322	valid_1's l2: 20.5948
[460]	training's l2: 20.2814	valid_1's l2: 20.5834
[470]	training's l2: 20.2417	valid_1's l2: 20.5722
[480]	training's l2: 20.2027	valid_1's l2: 20.5614
[490]	training's l2: 20.1647	valid_1's l2: 20.5522
[500]	training's l2: 20.1269	valid_1's l2: 20.5427
[510]	training's l2: 20.0902	valid_1's l2: 20.5347
[520]	training's l2: 20.0536	valid_1's l2: 20.5273
[530]	training's l2: 20.0171	valid_1's l2: 20.5194
[540]	training's l2: 19.9816	valid_1's l2: 20.5126
[550]	training's l2: 19.946	valid_1's l2: 20.5041
[560]	training's l2: 19.9104	valid_1's l2: 20.497
[570]	training's l2: 19.8756	valid_1's l2: 20.4904
[580]	training's l2: 19.8411	valid_1's l2: 20.4838
[590]	training's l2: 19.8069	valid_1's l2: 20.4784
[600]	training's l2: 19.7729	valid_1's l2: 20.4726
[610]	training's l2: 19.7385	valid_1's l2: 20.466
[620]	training's l2: 19.7053	valid_1's l2: 20.4615
[630]	training's l2: 19.6713	valid_1's l2: 20.4541
[640]	training's l2: 19.6374	valid_1's l2: 20.4485
[650]	training's l2: 19.6042	valid_1's l2: 20.4426
[660]	training's l2: 19.5718	valid_1's l2: 20.4382
[670]	training's l2: 19.5394	valid_1's l2: 20.433
[680]	training's l2: 19.5071	valid_1's l2: 20.4292
[690]	training's l2: 19.4748	valid_1's l2: 20.4245
[700]	training's l2: 19.4425	valid_1's l2: 20.4192
[710]	training's l2: 19.4109	valid_1's l2: 20.4147
[720]	training's l2: 19.3792	valid_1's l2: 20.4095
[730]	training's l2: 19.3472	valid_1's l2: 20.4041
[740]	training's l2: 19.3158	valid_1's l2: 20.3983
[750]	training's l2: 19.2845	valid_1's l2: 20.3943
[760]	training's l2: 19.2538	valid_1's l2: 20.3901
[770]	training's l2: 19.2231	valid_1's l2: 20.3862
[780]	training's l2: 19.1927	valid_1's l2: 20.3825
[790]	training's l2: 19.1624	valid_1's l2: 20.3781
[800]	training's l2: 19.1327	valid_1's l2: 20.3756
[810]	training's l2: 19.1026	valid_1's l2: 20.3718
[820]	training's l2: 19.0732	valid_1's l2: 20.3682
[830]	training's l2: 19.0434	valid_1's l2: 20.3647
[840]	training's l2: 19.0138	valid_1's l2: 20.3616
[850]	training's l2: 18.9844	valid_1's l2: 20.3583
[860]	training's l2: 18.9551	valid_1's l2: 20.3562
[870]	training's l2: 18.9262	valid_1's l2: 20.3525
[880]	training's l2: 18.8974	valid_1's l2: 20.3498
[890]	training's l2: 18.8688	valid_1's l2: 20.3477
[900]	training's l2: 18.8399	valid_1's l2: 20.3449
[910]	training's l2: 18.8117	valid_1's l2: 20.3418
[920]	training's l2: 18.7832	valid_1's l2: 20.3393
[930]	training's l2: 18.7552	valid_1's l2: 20.3371
[940]	training's l2: 18.7266	valid_1's l2: 20.3351
[950]	training's l2: 18.6989	valid_1's l2: 20.3328
[960]	training's l2: 18.6711	valid_1's l2: 20.3314
[970]	training's l2: 18.6431	valid_1's l2: 20.3293
[980]	training's l2: 18.6154	valid_1's l2: 20.3265
[990]	training's l2: 18.5878	valid_1's l2: 20.325
[1000]	training's l2: 18.5603	valid_1's l2: 20.3227
[1010]	training's l2: 18.533	valid_1's l2: 20.3208
[1020]	training's l2: 18.506	valid_1's l2: 20.3192
[1030]	training's l2: 18.4786	valid_1's l2: 20.3171
[1040]	training's l2: 18.4518	valid_1's l2: 20.3153
[1050]	training's l2: 18.4253	valid_1's l2: 20.3142
[1060]	training's l2: 18.399	valid_1's l2: 20.3122
[1070]	training's l2: 18.3729	valid_1's l2: 20.3109
[1080]	training's l2: 18.3468	valid_1's l2: 20.3091
[1090]	training's l2: 18.3206	valid_1's l2: 20.307
[1100]	training's l2: 18.2943	valid_1's l2: 20.3048
[1110]	training's l2: 18.268	valid_1's l2: 20.3033
[1120]	training's l2: 18.2426	valid_1's l2: 20.3023
[1130]	training's l2: 18.2166	valid_1's l2: 20.3
[1140]	training's l2: 18.191	valid_1's l2: 20.2976
[1150]	training's l2: 18.1649	valid_1's l2: 20.2959
[1160]	training's l2: 18.1396	valid_1's l2: 20.2948
[1170]	training's l2: 18.1146	valid_1's l2: 20.2932
[1180]	training's l2: 18.0899	valid_1's l2: 20.2916
[1190]	training's l2: 18.0656	valid_1's l2: 20.2903
[1200]	training's l2: 18.0407	valid_1's l2: 20.2883
[1210]	training's l2: 18.0154	valid_1's l2: 20.2869
[1220]	training's l2: 17.991	valid_1's l2: 20.2856
[1230]	training's l2: 17.9662	valid_1's l2: 20.2844
[1240]	training's l2: 17.9423	valid_1's l2: 20.2836
[1250]	training's l2: 17.9186	valid_1's l2: 20.281
[1260]	training's l2: 17.8953	valid_1's l2: 20.2798
[1270]	training's l2: 17.8712	valid_1's l2: 20.2793
[1280]	training's l2: 17.848	valid_1's l2: 20.2782
[1290]	training's l2: 17.8243	valid_1's l2: 20.277
[1300]	training's l2: 17.8012	valid_1's l2: 20.2761
[1310]	training's l2: 17.7779	valid_1's l2: 20.2749
[1320]	training's l2: 17.7542	valid_1's l2: 20.2738
[1330]	training's l2: 17.731	valid_1's l2: 20.2736
[1340]	training's l2: 17.7082	valid_1's l2: 20.2726
[1350]	training's l2: 17.6854	valid_1's l2: 20.2722
[1360]	training's l2: 17.6626	valid_1's l2: 20.2715
[1370]	training's l2: 17.64	valid_1's l2: 20.2703
[1380]	training's l2: 17.6175	valid_1's l2: 20.2696
[1390]	training's l2: 17.5953	valid_1's l2: 20.2687
[1400]	training's l2: 17.5737	valid_1's l2: 20.2685
[1410]	training's l2: 17.5511	valid_1's l2: 20.2677
[1420]	training's l2: 17.5292	valid_1's l2: 20.2673
[1430]	training's l2: 17.5076	valid_1's l2: 20.267
[1440]	training's l2: 17.4864	valid_1's l2: 20.2662
[1450]	training's l2: 17.4643	valid_1's l2: 20.2652
[1460]	training's l2: 17.4417	valid_1's l2: 20.265
[1470]	training's l2: 17.4204	valid_1's l2: 20.2647
[1480]	training's l2: 17.3978	valid_1's l2: 20.264
[1490]	training's l2: 17.3763	valid_1's l2: 20.2636
[1500]	training's l2: 17.3543	valid_1's l2: 20.2632
[1510]	training's l2: 17.3336	valid_1's l2: 20.2633
[1520]	training's l2: 17.3122	valid_1's l2: 20.2627
[1530]	training's l2: 17.2906	valid_1's l2: 20.2622
[1540]	training's l2: 17.27	valid_1's l2: 20.2613
[1550]	training's l2: 17.2482	valid_1's l2: 20.2608
[1560]	training's l2: 17.2269	valid_1's l2: 20.2599
[1570]	training's l2: 17.2057	valid_1's l2: 20.2597
[1580]	training's l2: 17.1848	valid_1's l2: 20.259
[1590]	training's l2: 17.1648	valid_1's l2: 20.2587
[1600]	training's l2: 17.1431	valid_1's l2: 20.2589
[1610]	training's l2: 17.1213	valid_1's l2: 20.2587
Early stopping, best iteration is:
[1584]	training's l2: 17.1771	valid_1's l2: 20.2585
score1: 3.8384475830182048
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.248129 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.946	valid_1's l2: 30.0491
[20]	training's l2: 28.6526	valid_1's l2: 27.7725
[30]	training's l2: 26.968	valid_1's l2: 26.1062
[40]	training's l2: 25.7139	valid_1's l2: 24.8769
[50]	training's l2: 24.7726	valid_1's l2: 23.962
[60]	training's l2: 24.0563	valid_1's l2: 23.276
[70]	training's l2: 23.5064	valid_1's l2: 22.7595
[80]	training's l2: 23.0787	valid_1's l2: 22.3666
[90]	training's l2: 22.743	valid_1's l2: 22.0643
[100]	training's l2: 22.4675	valid_1's l2: 21.8259
[110]	training's l2: 22.2388	valid_1's l2: 21.6343
[120]	training's l2: 22.0449	valid_1's l2: 21.4809
[130]	training's l2: 21.8771	valid_1's l2: 21.3546
[140]	training's l2: 21.7331	valid_1's l2: 21.2528
[150]	training's l2: 21.5989	valid_1's l2: 21.1592
[160]	training's l2: 21.4815	valid_1's l2: 21.081
[170]	training's l2: 21.3754	valid_1's l2: 21.0139
[180]	training's l2: 21.2775	valid_1's l2: 20.9563
[190]	training's l2: 21.187	valid_1's l2: 20.9051
[200]	training's l2: 21.1008	valid_1's l2: 20.8599
[210]	training's l2: 21.0196	valid_1's l2: 20.818
[220]	training's l2: 20.9419	valid_1's l2: 20.7809
[230]	training's l2: 20.8687	valid_1's l2: 20.748
[240]	training's l2: 20.8004	valid_1's l2: 20.7206
[250]	training's l2: 20.7338	valid_1's l2: 20.6951
[260]	training's l2: 20.6708	valid_1's l2: 20.6723
[270]	training's l2: 20.6094	valid_1's l2: 20.6498
[280]	training's l2: 20.5499	valid_1's l2: 20.6309
[290]	training's l2: 20.4923	valid_1's l2: 20.6122
[300]	training's l2: 20.435	valid_1's l2: 20.5943
[310]	training's l2: 20.3796	valid_1's l2: 20.5773
[320]	training's l2: 20.3248	valid_1's l2: 20.563
[330]	training's l2: 20.2714	valid_1's l2: 20.5471
[340]	training's l2: 20.2195	valid_1's l2: 20.5334
[350]	training's l2: 20.1687	valid_1's l2: 20.5213
[360]	training's l2: 20.1181	valid_1's l2: 20.5106
[370]	training's l2: 20.069	valid_1's l2: 20.5006
[380]	training's l2: 20.0197	valid_1's l2: 20.4903
[390]	training's l2: 19.9712	valid_1's l2: 20.4801
[400]	training's l2: 19.9229	valid_1's l2: 20.4689
[410]	training's l2: 19.8746	valid_1's l2: 20.4606
[420]	training's l2: 19.8275	valid_1's l2: 20.4521
[430]	training's l2: 19.7803	valid_1's l2: 20.4423
[440]	training's l2: 19.7332	valid_1's l2: 20.4342
[450]	training's l2: 19.6876	valid_1's l2: 20.4258
[460]	training's l2: 19.6417	valid_1's l2: 20.4168
[470]	training's l2: 19.5973	valid_1's l2: 20.4092
[480]	training's l2: 19.5527	valid_1's l2: 20.4029
[490]	training's l2: 19.5092	valid_1's l2: 20.3957
[500]	training's l2: 19.4654	valid_1's l2: 20.3883
[510]	training's l2: 19.4221	valid_1's l2: 20.3829
[520]	training's l2: 19.3804	valid_1's l2: 20.3787
[530]	training's l2: 19.3387	valid_1's l2: 20.3731
[540]	training's l2: 19.2964	valid_1's l2: 20.3694
[550]	training's l2: 19.2547	valid_1's l2: 20.3643
[560]	training's l2: 19.214	valid_1's l2: 20.3604
[570]	training's l2: 19.1732	valid_1's l2: 20.3567
[580]	training's l2: 19.133	valid_1's l2: 20.3527
[590]	training's l2: 19.0918	valid_1's l2: 20.3472
[600]	training's l2: 19.052	valid_1's l2: 20.3431
[610]	training's l2: 19.0126	valid_1's l2: 20.3392
[620]	training's l2: 18.9737	valid_1's l2: 20.3364
[630]	training's l2: 18.9339	valid_1's l2: 20.3313
[640]	training's l2: 18.8952	valid_1's l2: 20.3284
[650]	training's l2: 18.8568	valid_1's l2: 20.3242
[660]	training's l2: 18.8184	valid_1's l2: 20.3203
[670]	training's l2: 18.7795	valid_1's l2: 20.3177
[680]	training's l2: 18.7412	valid_1's l2: 20.3137
[690]	training's l2: 18.7038	valid_1's l2: 20.3093
[700]	training's l2: 18.6674	valid_1's l2: 20.3077
[710]	training's l2: 18.6307	valid_1's l2: 20.3056
[720]	training's l2: 18.5936	valid_1's l2: 20.3034
[730]	training's l2: 18.5572	valid_1's l2: 20.3031
[740]	training's l2: 18.5222	valid_1's l2: 20.302
[750]	training's l2: 18.4856	valid_1's l2: 20.3002
[760]	training's l2: 18.4508	valid_1's l2: 20.298
[770]	training's l2: 18.4152	valid_1's l2: 20.2949
[780]	training's l2: 18.3797	valid_1's l2: 20.2927
[790]	training's l2: 18.3448	valid_1's l2: 20.2903
[800]	training's l2: 18.3116	valid_1's l2: 20.289
[810]	training's l2: 18.2771	valid_1's l2: 20.2866
[820]	training's l2: 18.2441	valid_1's l2: 20.2855
[830]	training's l2: 18.2098	valid_1's l2: 20.2842
[840]	training's l2: 18.1757	valid_1's l2: 20.2826
[850]	training's l2: 18.143	valid_1's l2: 20.2813
[860]	training's l2: 18.1098	valid_1's l2: 20.281
[870]	training's l2: 18.0764	valid_1's l2: 20.2801
[880]	training's l2: 18.045	valid_1's l2: 20.2793
[890]	training's l2: 18.012	valid_1's l2: 20.2774
[900]	training's l2: 17.9796	valid_1's l2: 20.2769
[910]	training's l2: 17.9479	valid_1's l2: 20.2764
[920]	training's l2: 17.9163	valid_1's l2: 20.2753
[930]	training's l2: 17.8854	valid_1's l2: 20.274
[940]	training's l2: 17.8538	valid_1's l2: 20.2727
[950]	training's l2: 17.8215	valid_1's l2: 20.2722
[960]	training's l2: 17.7898	valid_1's l2: 20.2706
[970]	training's l2: 17.7581	valid_1's l2: 20.2696
[980]	training's l2: 17.7274	valid_1's l2: 20.2688
[990]	training's l2: 17.698	valid_1's l2: 20.2681
[1000]	training's l2: 17.6676	valid_1's l2: 20.2688
[1010]	training's l2: 17.639	valid_1's l2: 20.2681
[1020]	training's l2: 17.6087	valid_1's l2: 20.2685
[1030]	training's l2: 17.5797	valid_1's l2: 20.2667
[1040]	training's l2: 17.5503	valid_1's l2: 20.2652
[1050]	training's l2: 17.5203	valid_1's l2: 20.2643
[1060]	training's l2: 17.4921	valid_1's l2: 20.2633
[1070]	training's l2: 17.4648	valid_1's l2: 20.2626
[1080]	training's l2: 17.4355	valid_1's l2: 20.2613
[1090]	training's l2: 17.4068	valid_1's l2: 20.261
[1100]	training's l2: 17.3787	valid_1's l2: 20.2606
[1110]	training's l2: 17.35	valid_1's l2: 20.2605
[1120]	training's l2: 17.3211	valid_1's l2: 20.2608
[1130]	training's l2: 17.294	valid_1's l2: 20.2592
[1140]	training's l2: 17.2648	valid_1's l2: 20.2587
[1150]	training's l2: 17.2362	valid_1's l2: 20.2583
[1160]	training's l2: 17.2079	valid_1's l2: 20.2578
[1170]	training's l2: 17.1799	valid_1's l2: 20.2582
[1180]	training's l2: 17.1518	valid_1's l2: 20.2587
[1190]	training's l2: 17.1243	valid_1's l2: 20.2578
Early stopping, best iteration is:
[1162]	training's l2: 17.2023	valid_1's l2: 20.2574
score1: 3.839072342573901
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.248066 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.6923	valid_1's l2: 30.7914
[20]	training's l2: 29.7942	valid_1's l2: 28.907
[30]	training's l2: 28.2809	valid_1's l2: 27.4091
[40]	training's l2: 27.0712	valid_1's l2: 26.2163
[50]	training's l2: 26.0971	valid_1's l2: 25.2571
[60]	training's l2: 25.3088	valid_1's l2: 24.4869
[70]	training's l2: 24.6663	valid_1's l2: 23.8676
[80]	training's l2: 24.1403	valid_1's l2: 23.3641
[90]	training's l2: 23.7057	valid_1's l2: 22.9548
[100]	training's l2: 23.3451	valid_1's l2: 22.62
[110]	training's l2: 23.0454	valid_1's l2: 22.3461
[120]	training's l2: 22.7935	valid_1's l2: 22.1201
[130]	training's l2: 22.576	valid_1's l2: 21.9299
[140]	training's l2: 22.3863	valid_1's l2: 21.7697
[150]	training's l2: 22.2204	valid_1's l2: 21.6338
[160]	training's l2: 22.0747	valid_1's l2: 21.5167
[170]	training's l2: 21.9419	valid_1's l2: 21.4154
[180]	training's l2: 21.8246	valid_1's l2: 21.3304
[190]	training's l2: 21.7174	valid_1's l2: 21.2546
[200]	training's l2: 21.6169	valid_1's l2: 21.1851
[210]	training's l2: 21.5235	valid_1's l2: 21.1222
[220]	training's l2: 21.4392	valid_1's l2: 21.0691
[230]	training's l2: 21.3595	valid_1's l2: 21.0209
[240]	training's l2: 21.2845	valid_1's l2: 20.9759
[250]	training's l2: 21.2143	valid_1's l2: 20.9353
[260]	training's l2: 21.1484	valid_1's l2: 20.8996
[270]	training's l2: 21.0841	valid_1's l2: 20.8656
[280]	training's l2: 21.0206	valid_1's l2: 20.833
[290]	training's l2: 20.9608	valid_1's l2: 20.8062
[300]	training's l2: 20.9031	valid_1's l2: 20.7795
[310]	training's l2: 20.8475	valid_1's l2: 20.7557
[320]	training's l2: 20.7936	valid_1's l2: 20.7327
[330]	training's l2: 20.7422	valid_1's l2: 20.7123
[340]	training's l2: 20.6931	valid_1's l2: 20.6939
[350]	training's l2: 20.6446	valid_1's l2: 20.6763
[360]	training's l2: 20.5981	valid_1's l2: 20.6599
[370]	training's l2: 20.5525	valid_1's l2: 20.6445
[380]	training's l2: 20.5072	valid_1's l2: 20.6305
[390]	training's l2: 20.4637	valid_1's l2: 20.6168
[400]	training's l2: 20.4205	valid_1's l2: 20.6039
[410]	training's l2: 20.3781	valid_1's l2: 20.5916
[420]	training's l2: 20.3365	valid_1's l2: 20.5796
[430]	training's l2: 20.2954	valid_1's l2: 20.5691
[440]	training's l2: 20.255	valid_1's l2: 20.5585
[450]	training's l2: 20.2149	valid_1's l2: 20.548
[460]	training's l2: 20.1751	valid_1's l2: 20.5386
[470]	training's l2: 20.1358	valid_1's l2: 20.5296
[480]	training's l2: 20.0969	valid_1's l2: 20.5204
[490]	training's l2: 20.0586	valid_1's l2: 20.5112
[500]	training's l2: 20.0209	valid_1's l2: 20.5027
[510]	training's l2: 19.9836	valid_1's l2: 20.4951
[520]	training's l2: 19.9467	valid_1's l2: 20.489
[530]	training's l2: 19.9094	valid_1's l2: 20.4809
[540]	training's l2: 19.8732	valid_1's l2: 20.4746
[550]	training's l2: 19.8376	valid_1's l2: 20.4683
[560]	training's l2: 19.8011	valid_1's l2: 20.4612
[570]	training's l2: 19.7658	valid_1's l2: 20.4553
[580]	training's l2: 19.7301	valid_1's l2: 20.4479
[590]	training's l2: 19.6946	valid_1's l2: 20.4412
[600]	training's l2: 19.6599	valid_1's l2: 20.4348
[610]	training's l2: 19.6254	valid_1's l2: 20.429
[620]	training's l2: 19.5913	valid_1's l2: 20.4238
[630]	training's l2: 19.5574	valid_1's l2: 20.4181
[640]	training's l2: 19.5239	valid_1's l2: 20.4136
[650]	training's l2: 19.4904	valid_1's l2: 20.4081
[660]	training's l2: 19.4576	valid_1's l2: 20.4026
[670]	training's l2: 19.4243	valid_1's l2: 20.3983
[680]	training's l2: 19.3914	valid_1's l2: 20.3933
[690]	training's l2: 19.3587	valid_1's l2: 20.3885
[700]	training's l2: 19.3259	valid_1's l2: 20.3834
[710]	training's l2: 19.2932	valid_1's l2: 20.3787
[720]	training's l2: 19.2614	valid_1's l2: 20.3744
[730]	training's l2: 19.2294	valid_1's l2: 20.3703
[740]	training's l2: 19.1979	valid_1's l2: 20.3664
[750]	training's l2: 19.1668	valid_1's l2: 20.3627
[760]	training's l2: 19.136	valid_1's l2: 20.3595
[770]	training's l2: 19.1049	valid_1's l2: 20.3558
[780]	training's l2: 19.0738	valid_1's l2: 20.3514
[790]	training's l2: 19.0432	valid_1's l2: 20.3487
[800]	training's l2: 19.0126	valid_1's l2: 20.3459
[810]	training's l2: 18.982	valid_1's l2: 20.3437
[820]	training's l2: 18.9522	valid_1's l2: 20.3413
[830]	training's l2: 18.9221	valid_1's l2: 20.3382
[840]	training's l2: 18.8919	valid_1's l2: 20.336
[850]	training's l2: 18.862	valid_1's l2: 20.3327
[860]	training's l2: 18.8326	valid_1's l2: 20.3301
[870]	training's l2: 18.8029	valid_1's l2: 20.3272
[880]	training's l2: 18.7736	valid_1's l2: 20.3242
[890]	training's l2: 18.7445	valid_1's l2: 20.3213
[900]	training's l2: 18.7152	valid_1's l2: 20.319
[910]	training's l2: 18.6863	valid_1's l2: 20.3171
[920]	training's l2: 18.6583	valid_1's l2: 20.315
[930]	training's l2: 18.6298	valid_1's l2: 20.3129
[940]	training's l2: 18.6007	valid_1's l2: 20.31
[950]	training's l2: 18.5728	valid_1's l2: 20.3067
[960]	training's l2: 18.5443	valid_1's l2: 20.3059
[970]	training's l2: 18.5166	valid_1's l2: 20.3045
[980]	training's l2: 18.4884	valid_1's l2: 20.3026
[990]	training's l2: 18.4607	valid_1's l2: 20.3004
[1000]	training's l2: 18.4338	valid_1's l2: 20.299
[1010]	training's l2: 18.4068	valid_1's l2: 20.2977
[1020]	training's l2: 18.3796	valid_1's l2: 20.2956
[1030]	training's l2: 18.3528	valid_1's l2: 20.2946
[1040]	training's l2: 18.3264	valid_1's l2: 20.2933
[1050]	training's l2: 18.3002	valid_1's l2: 20.292
[1060]	training's l2: 18.2738	valid_1's l2: 20.2907
[1070]	training's l2: 18.2477	valid_1's l2: 20.2893
[1080]	training's l2: 18.2207	valid_1's l2: 20.2871
[1090]	training's l2: 18.195	valid_1's l2: 20.2858
[1100]	training's l2: 18.1695	valid_1's l2: 20.2853
[1110]	training's l2: 18.1434	valid_1's l2: 20.2839
[1120]	training's l2: 18.1178	valid_1's l2: 20.2823
[1130]	training's l2: 18.0923	valid_1's l2: 20.2809
[1140]	training's l2: 18.0671	valid_1's l2: 20.2803
[1150]	training's l2: 18.0428	valid_1's l2: 20.279
[1160]	training's l2: 18.0179	valid_1's l2: 20.2784
[1170]	training's l2: 17.9923	valid_1's l2: 20.2765
[1180]	training's l2: 17.9675	valid_1's l2: 20.2745
[1190]	training's l2: 17.9434	valid_1's l2: 20.2744
[1200]	training's l2: 17.9199	valid_1's l2: 20.2738
[1210]	training's l2: 17.8956	valid_1's l2: 20.2733
[1220]	training's l2: 17.8705	valid_1's l2: 20.2719
[1230]	training's l2: 17.8465	valid_1's l2: 20.2707
[1240]	training's l2: 17.8228	valid_1's l2: 20.2698
[1250]	training's l2: 17.7986	valid_1's l2: 20.2694
[1260]	training's l2: 17.7752	valid_1's l2: 20.2686
[1270]	training's l2: 17.7517	valid_1's l2: 20.2673
[1280]	training's l2: 17.7278	valid_1's l2: 20.2667
[1290]	training's l2: 17.7054	valid_1's l2: 20.2657
[1300]	training's l2: 17.6813	valid_1's l2: 20.2653
[1310]	training's l2: 17.6581	valid_1's l2: 20.2646
[1320]	training's l2: 17.6349	valid_1's l2: 20.264
[1330]	training's l2: 17.6119	valid_1's l2: 20.2645
[1340]	training's l2: 17.5886	valid_1's l2: 20.2638
[1350]	training's l2: 17.5658	valid_1's l2: 20.264
[1360]	training's l2: 17.542	valid_1's l2: 20.2632
[1370]	training's l2: 17.5193	valid_1's l2: 20.2623
[1380]	training's l2: 17.4977	valid_1's l2: 20.2625
[1390]	training's l2: 17.475	valid_1's l2: 20.2629
[1400]	training's l2: 17.4534	valid_1's l2: 20.2627
Early stopping, best iteration is:
[1372]	training's l2: 17.5147	valid_1's l2: 20.262
score1: 3.83857167227649
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.282630 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 30.7124	valid_1's l2: 29.8109
[20]	training's l2: 28.3223	valid_1's l2: 27.4347
[30]	training's l2: 26.6105	valid_1's l2: 25.7395
[40]	training's l2: 25.3711	valid_1's l2: 24.5224
[50]	training's l2: 24.4618	valid_1's l2: 23.6393
[60]	training's l2: 23.7839	valid_1's l2: 22.9893
[70]	training's l2: 23.2763	valid_1's l2: 22.5141
[80]	training's l2: 22.8858	valid_1's l2: 22.1577
[90]	training's l2: 22.5747	valid_1's l2: 21.8827
[100]	training's l2: 22.3229	valid_1's l2: 21.6682
[110]	training's l2: 22.1126	valid_1's l2: 21.4974
[120]	training's l2: 21.9371	valid_1's l2: 21.3618
[130]	training's l2: 21.7839	valid_1's l2: 21.249
[140]	training's l2: 21.6479	valid_1's l2: 21.151
[150]	training's l2: 21.5283	valid_1's l2: 21.0714
[160]	training's l2: 21.4207	valid_1's l2: 21.0024
[170]	training's l2: 21.3228	valid_1's l2: 20.9435
[180]	training's l2: 21.2308	valid_1's l2: 20.8901
[190]	training's l2: 21.1421	valid_1's l2: 20.8412
[200]	training's l2: 21.0596	valid_1's l2: 20.8
[210]	training's l2: 20.9821	valid_1's l2: 20.7622
[220]	training's l2: 20.9107	valid_1's l2: 20.7316
[230]	training's l2: 20.8416	valid_1's l2: 20.7031
[240]	training's l2: 20.7773	valid_1's l2: 20.6783
[250]	training's l2: 20.7129	valid_1's l2: 20.6538
[260]	training's l2: 20.6532	valid_1's l2: 20.6321
[270]	training's l2: 20.5932	valid_1's l2: 20.6124
[280]	training's l2: 20.5358	valid_1's l2: 20.594
[290]	training's l2: 20.4787	valid_1's l2: 20.5763
[300]	training's l2: 20.4231	valid_1's l2: 20.5608
[310]	training's l2: 20.3694	valid_1's l2: 20.5483
[320]	training's l2: 20.316	valid_1's l2: 20.535
[330]	training's l2: 20.2644	valid_1's l2: 20.5212
[340]	training's l2: 20.2134	valid_1's l2: 20.5107
[350]	training's l2: 20.1633	valid_1's l2: 20.5003
[360]	training's l2: 20.1129	valid_1's l2: 20.4898
[370]	training's l2: 20.0645	valid_1's l2: 20.4808
[380]	training's l2: 20.0147	valid_1's l2: 20.4695
[390]	training's l2: 19.9659	valid_1's l2: 20.4597
[400]	training's l2: 19.919	valid_1's l2: 20.4501
[410]	training's l2: 19.8726	valid_1's l2: 20.44
[420]	training's l2: 19.8257	valid_1's l2: 20.4316
[430]	training's l2: 19.7803	valid_1's l2: 20.4236
[440]	training's l2: 19.735	valid_1's l2: 20.4155
[450]	training's l2: 19.6897	valid_1's l2: 20.4069
[460]	training's l2: 19.6459	valid_1's l2: 20.4015
[470]	training's l2: 19.6016	valid_1's l2: 20.3957
[480]	training's l2: 19.5578	valid_1's l2: 20.3885
[490]	training's l2: 19.5142	valid_1's l2: 20.3826
[500]	training's l2: 19.4728	valid_1's l2: 20.3774
[510]	training's l2: 19.4299	valid_1's l2: 20.3706
[520]	training's l2: 19.3886	valid_1's l2: 20.3654
[530]	training's l2: 19.3473	valid_1's l2: 20.3608
[540]	training's l2: 19.3064	valid_1's l2: 20.3567
[550]	training's l2: 19.265	valid_1's l2: 20.3526
[560]	training's l2: 19.2244	valid_1's l2: 20.3471
[570]	training's l2: 19.1845	valid_1's l2: 20.3423
[580]	training's l2: 19.1441	valid_1's l2: 20.338
[590]	training's l2: 19.1037	valid_1's l2: 20.3325
[600]	training's l2: 19.064	valid_1's l2: 20.3295
[610]	training's l2: 19.0248	valid_1's l2: 20.3258
[620]	training's l2: 18.9857	valid_1's l2: 20.3222
[630]	training's l2: 18.9467	valid_1's l2: 20.3188
[640]	training's l2: 18.9093	valid_1's l2: 20.3161
[650]	training's l2: 18.8713	valid_1's l2: 20.3127
[660]	training's l2: 18.8344	valid_1's l2: 20.3097
[670]	training's l2: 18.7976	valid_1's l2: 20.3074
[680]	training's l2: 18.7603	valid_1's l2: 20.3053
[690]	training's l2: 18.7237	valid_1's l2: 20.3027
[700]	training's l2: 18.6877	valid_1's l2: 20.3005
[710]	training's l2: 18.6522	valid_1's l2: 20.2985
[720]	training's l2: 18.6164	valid_1's l2: 20.2965
[730]	training's l2: 18.5814	valid_1's l2: 20.2938
[740]	training's l2: 18.5476	valid_1's l2: 20.2925
[750]	training's l2: 18.5122	valid_1's l2: 20.2915
[760]	training's l2: 18.4783	valid_1's l2: 20.2906
[770]	training's l2: 18.4436	valid_1's l2: 20.2888
[780]	training's l2: 18.409	valid_1's l2: 20.2864
[790]	training's l2: 18.3752	valid_1's l2: 20.2842
[800]	training's l2: 18.3424	valid_1's l2: 20.2826
[810]	training's l2: 18.3089	valid_1's l2: 20.2804
[820]	training's l2: 18.2762	valid_1's l2: 20.2792
[830]	training's l2: 18.2427	valid_1's l2: 20.2776
[840]	training's l2: 18.2093	valid_1's l2: 20.2761
[850]	training's l2: 18.1774	valid_1's l2: 20.2751
[860]	training's l2: 18.1441	valid_1's l2: 20.2736
[870]	training's l2: 18.1119	valid_1's l2: 20.2704
[880]	training's l2: 18.0781	valid_1's l2: 20.2683
[890]	training's l2: 18.0484	valid_1's l2: 20.2681
[900]	training's l2: 18.0192	valid_1's l2: 20.2673
[910]	training's l2: 17.9889	valid_1's l2: 20.2653
[920]	training's l2: 17.9583	valid_1's l2: 20.2646
[930]	training's l2: 17.9265	valid_1's l2: 20.2641
[940]	training's l2: 17.8962	valid_1's l2: 20.2639
[950]	training's l2: 17.8652	valid_1's l2: 20.2622
[960]	training's l2: 17.8366	valid_1's l2: 20.2616
[970]	training's l2: 17.8076	valid_1's l2: 20.2614
[980]	training's l2: 17.7778	valid_1's l2: 20.2599
[990]	training's l2: 17.7472	valid_1's l2: 20.2594
[1000]	training's l2: 17.7172	valid_1's l2: 20.2587
[1010]	training's l2: 17.6874	valid_1's l2: 20.2574
[1020]	training's l2: 17.659	valid_1's l2: 20.2565
[1030]	training's l2: 17.6311	valid_1's l2: 20.2555
[1040]	training's l2: 17.6019	valid_1's l2: 20.2549
[1050]	training's l2: 17.5732	valid_1's l2: 20.2541
[1060]	training's l2: 17.5452	valid_1's l2: 20.2539
[1070]	training's l2: 17.5158	valid_1's l2: 20.2527
[1080]	training's l2: 17.4873	valid_1's l2: 20.2524
[1090]	training's l2: 17.4587	valid_1's l2: 20.2514
[1100]	training's l2: 17.4306	valid_1's l2: 20.2523
[1110]	training's l2: 17.4019	valid_1's l2: 20.2522
Early stopping, best iteration is:
[1089]	training's l2: 17.4614	valid_1's l2: 20.2512
score1: 3.8378488503990367
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.264514 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.2351	valid_1's l2: 30.3338
[20]	training's l2: 29.0863	valid_1's l2: 28.2001
[30]	training's l2: 27.4578	valid_1's l2: 26.5851
[40]	training's l2: 26.2139	valid_1's l2: 25.3591
[50]	training's l2: 25.2534	valid_1's l2: 24.4171
[60]	training's l2: 24.5089	valid_1's l2: 23.6956
[70]	training's l2: 23.9235	valid_1's l2: 23.1354
[80]	training's l2: 23.4593	valid_1's l2: 22.6992
[90]	training's l2: 23.0899	valid_1's l2: 22.357
[100]	training's l2: 22.7914	valid_1's l2: 22.0879
[110]	training's l2: 22.5404	valid_1's l2: 21.8673
[120]	training's l2: 22.3292	valid_1's l2: 21.689
[130]	training's l2: 22.1501	valid_1's l2: 21.54
[140]	training's l2: 21.9915	valid_1's l2: 21.4154
[150]	training's l2: 21.8552	valid_1's l2: 21.3137
[160]	training's l2: 21.73	valid_1's l2: 21.2248
[170]	training's l2: 21.617	valid_1's l2: 21.1468
[180]	training's l2: 21.5161	valid_1's l2: 21.0791
[190]	training's l2: 21.422	valid_1's l2: 21.0188
[200]	training's l2: 21.3367	valid_1's l2: 20.9676
[210]	training's l2: 21.2578	valid_1's l2: 20.9209
[220]	training's l2: 21.1812	valid_1's l2: 20.8783
[230]	training's l2: 21.1074	valid_1's l2: 20.8373
[240]	training's l2: 21.0386	valid_1's l2: 20.8036
[250]	training's l2: 20.9725	valid_1's l2: 20.7723
[260]	training's l2: 20.9102	valid_1's l2: 20.7443
[270]	training's l2: 20.8517	valid_1's l2: 20.7208
[280]	training's l2: 20.7939	valid_1's l2: 20.6967
[290]	training's l2: 20.7381	valid_1's l2: 20.6742
[300]	training's l2: 20.6849	valid_1's l2: 20.6537
[310]	training's l2: 20.6329	valid_1's l2: 20.6357
[320]	training's l2: 20.5816	valid_1's l2: 20.6186
[330]	training's l2: 20.531	valid_1's l2: 20.6014
[340]	training's l2: 20.4822	valid_1's l2: 20.587
[350]	training's l2: 20.4355	valid_1's l2: 20.5741
[360]	training's l2: 20.3884	valid_1's l2: 20.5613
[370]	training's l2: 20.3434	valid_1's l2: 20.5501
[380]	training's l2: 20.2984	valid_1's l2: 20.5391
[390]	training's l2: 20.2543	valid_1's l2: 20.5295
[400]	training's l2: 20.2106	valid_1's l2: 20.5199
[410]	training's l2: 20.167	valid_1's l2: 20.5094
[420]	training's l2: 20.1246	valid_1's l2: 20.4993
[430]	training's l2: 20.0828	valid_1's l2: 20.49
[440]	training's l2: 20.0408	valid_1's l2: 20.4811
[450]	training's l2: 19.9999	valid_1's l2: 20.4723
[460]	training's l2: 19.9586	valid_1's l2: 20.4637
[470]	training's l2: 19.9181	valid_1's l2: 20.4553
[480]	training's l2: 19.8773	valid_1's l2: 20.4466
[490]	training's l2: 19.8368	valid_1's l2: 20.4395
[500]	training's l2: 19.7976	valid_1's l2: 20.4325
[510]	training's l2: 19.7578	valid_1's l2: 20.4263
[520]	training's l2: 19.7191	valid_1's l2: 20.4187
[530]	training's l2: 19.6805	valid_1's l2: 20.4133
[540]	training's l2: 19.6424	valid_1's l2: 20.4062
[550]	training's l2: 19.6036	valid_1's l2: 20.3995
[560]	training's l2: 19.5659	valid_1's l2: 20.3943
[570]	training's l2: 19.5283	valid_1's l2: 20.3886
[580]	training's l2: 19.4917	valid_1's l2: 20.3848
[590]	training's l2: 19.4543	valid_1's l2: 20.3789
[600]	training's l2: 19.4188	valid_1's l2: 20.3745
[610]	training's l2: 19.383	valid_1's l2: 20.3701
[620]	training's l2: 19.3483	valid_1's l2: 20.3666
[630]	training's l2: 19.3124	valid_1's l2: 20.3626
[640]	training's l2: 19.2771	valid_1's l2: 20.3591
[650]	training's l2: 19.2421	valid_1's l2: 20.3545
[660]	training's l2: 19.2076	valid_1's l2: 20.3507
[670]	training's l2: 19.1728	valid_1's l2: 20.3479
[680]	training's l2: 19.1386	valid_1's l2: 20.3436
[690]	training's l2: 19.1051	valid_1's l2: 20.3403
[700]	training's l2: 19.0716	valid_1's l2: 20.3367
[710]	training's l2: 19.0386	valid_1's l2: 20.333
[720]	training's l2: 19.0052	valid_1's l2: 20.3304
[730]	training's l2: 18.9723	valid_1's l2: 20.3277
[740]	training's l2: 18.9389	valid_1's l2: 20.3241
[750]	training's l2: 18.9063	valid_1's l2: 20.3211
[760]	training's l2: 18.8742	valid_1's l2: 20.3185
[770]	training's l2: 18.8424	valid_1's l2: 20.3166
[780]	training's l2: 18.8095	valid_1's l2: 20.3141
[790]	training's l2: 18.7775	valid_1's l2: 20.3119
[800]	training's l2: 18.7469	valid_1's l2: 20.309
[810]	training's l2: 18.7155	valid_1's l2: 20.3064
[820]	training's l2: 18.6843	valid_1's l2: 20.3047
[830]	training's l2: 18.6537	valid_1's l2: 20.3035
[840]	training's l2: 18.6219	valid_1's l2: 20.3005
[850]	training's l2: 18.5911	valid_1's l2: 20.2993
[860]	training's l2: 18.5607	valid_1's l2: 20.2971
[870]	training's l2: 18.5313	valid_1's l2: 20.2944
[880]	training's l2: 18.5006	valid_1's l2: 20.292
[890]	training's l2: 18.4713	valid_1's l2: 20.2907
[900]	training's l2: 18.4434	valid_1's l2: 20.2888
[910]	training's l2: 18.4132	valid_1's l2: 20.2865
[920]	training's l2: 18.3838	valid_1's l2: 20.2847
[930]	training's l2: 18.3552	valid_1's l2: 20.2829
[940]	training's l2: 18.3261	valid_1's l2: 20.2812
[950]	training's l2: 18.2968	valid_1's l2: 20.2792
[960]	training's l2: 18.2681	valid_1's l2: 20.2785
[970]	training's l2: 18.2394	valid_1's l2: 20.2776
[980]	training's l2: 18.2119	valid_1's l2: 20.2768
[990]	training's l2: 18.1848	valid_1's l2: 20.2758
[1000]	training's l2: 18.1569	valid_1's l2: 20.2739
[1010]	training's l2: 18.1286	valid_1's l2: 20.2718
[1020]	training's l2: 18.1005	valid_1's l2: 20.2711
[1030]	training's l2: 18.0733	valid_1's l2: 20.2693
[1040]	training's l2: 18.0471	valid_1's l2: 20.2681
[1050]	training's l2: 18.0199	valid_1's l2: 20.2672
[1060]	training's l2: 17.9937	valid_1's l2: 20.2664
[1070]	training's l2: 17.9675	valid_1's l2: 20.2649
[1080]	training's l2: 17.9408	valid_1's l2: 20.2644
[1090]	training's l2: 17.9141	valid_1's l2: 20.2642
[1100]	training's l2: 17.8894	valid_1's l2: 20.2635
[1110]	training's l2: 17.8633	valid_1's l2: 20.2633
[1120]	training's l2: 17.8373	valid_1's l2: 20.2626
[1130]	training's l2: 17.8118	valid_1's l2: 20.2621
[1140]	training's l2: 17.7866	valid_1's l2: 20.2612
[1150]	training's l2: 17.7608	valid_1's l2: 20.2611
[1160]	training's l2: 17.7363	valid_1's l2: 20.26
[1170]	training's l2: 17.7108	valid_1's l2: 20.2579
[1180]	training's l2: 17.6849	valid_1's l2: 20.257
[1190]	training's l2: 17.6603	valid_1's l2: 20.2571
[1200]	training's l2: 17.6355	valid_1's l2: 20.257
[1210]	training's l2: 17.6103	valid_1's l2: 20.257
[1220]	training's l2: 17.5856	valid_1's l2: 20.2564
[1230]	training's l2: 17.5612	valid_1's l2: 20.2551
[1240]	training's l2: 17.5354	valid_1's l2: 20.2554
[1250]	training's l2: 17.5111	valid_1's l2: 20.2551
[1260]	training's l2: 17.4866	valid_1's l2: 20.2547
[1270]	training's l2: 17.4623	valid_1's l2: 20.2544
[1280]	training's l2: 17.4383	valid_1's l2: 20.2536
[1290]	training's l2: 17.4138	valid_1's l2: 20.2529
[1300]	training's l2: 17.39	valid_1's l2: 20.2529
[1310]	training's l2: 17.3657	valid_1's l2: 20.2521
[1320]	training's l2: 17.3413	valid_1's l2: 20.2513
[1330]	training's l2: 17.3172	valid_1's l2: 20.2508
[1340]	training's l2: 17.2946	valid_1's l2: 20.2504
[1350]	training's l2: 17.2722	valid_1's l2: 20.251
[1360]	training's l2: 17.2483	valid_1's l2: 20.2506
[1370]	training's l2: 17.2242	valid_1's l2: 20.2513
Early stopping, best iteration is:
[1341]	training's l2: 17.2926	valid_1's l2: 20.2502
score1: 3.835890886910068
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.251035 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 32.06	valid_1's l2: 31.1582
[20]	training's l2: 30.389	valid_1's l2: 29.5005
[30]	training's l2: 29.0057	valid_1's l2: 28.1305
[40]	training's l2: 27.8589	valid_1's l2: 26.999
[50]	training's l2: 26.9033	valid_1's l2: 26.0563
[60]	training's l2: 26.105	valid_1's l2: 25.2739
[70]	training's l2: 25.4337	valid_1's l2: 24.6187
[80]	training's l2: 24.8688	valid_1's l2: 24.0735
[90]	training's l2: 24.3911	valid_1's l2: 23.6148
[100]	training's l2: 23.9852	valid_1's l2: 23.2294
[110]	training's l2: 23.6386	valid_1's l2: 22.905
[120]	training's l2: 23.3409	valid_1's l2: 22.6295
[130]	training's l2: 23.084	valid_1's l2: 22.3956
[140]	training's l2: 22.8632	valid_1's l2: 22.1968
[150]	training's l2: 22.6698	valid_1's l2: 22.0269
[160]	training's l2: 22.4977	valid_1's l2: 21.88
[170]	training's l2: 22.3434	valid_1's l2: 21.7511
[180]	training's l2: 22.2057	valid_1's l2: 21.6367
[190]	training's l2: 22.0819	valid_1's l2: 21.5389
[200]	training's l2: 21.9668	valid_1's l2: 21.4512
[210]	training's l2: 21.8617	valid_1's l2: 21.3733
[220]	training's l2: 21.7658	valid_1's l2: 21.3048
[230]	training's l2: 21.677	valid_1's l2: 21.2435
[240]	training's l2: 21.593	valid_1's l2: 21.186
[250]	training's l2: 21.5141	valid_1's l2: 21.1345
[260]	training's l2: 21.4409	valid_1's l2: 21.0878
[270]	training's l2: 21.3725	valid_1's l2: 21.0459
[280]	training's l2: 21.3068	valid_1's l2: 21.006
[290]	training's l2: 21.2445	valid_1's l2: 20.9702
[300]	training's l2: 21.185	valid_1's l2: 20.9361
[310]	training's l2: 21.1283	valid_1's l2: 20.9048
[320]	training's l2: 21.0735	valid_1's l2: 20.8755
[330]	training's l2: 21.0186	valid_1's l2: 20.846
[340]	training's l2: 20.9662	valid_1's l2: 20.8196
[350]	training's l2: 20.9165	valid_1's l2: 20.7977
[360]	training's l2: 20.8673	valid_1's l2: 20.776
[370]	training's l2: 20.8198	valid_1's l2: 20.7561
[380]	training's l2: 20.7734	valid_1's l2: 20.7362
[390]	training's l2: 20.7293	valid_1's l2: 20.7186
[400]	training's l2: 20.687	valid_1's l2: 20.7035
[410]	training's l2: 20.6446	valid_1's l2: 20.6882
[420]	training's l2: 20.6041	valid_1's l2: 20.6729
[430]	training's l2: 20.564	valid_1's l2: 20.6594
[440]	training's l2: 20.525	valid_1's l2: 20.6464
[450]	training's l2: 20.4866	valid_1's l2: 20.6345
[460]	training's l2: 20.4482	valid_1's l2: 20.6226
[470]	training's l2: 20.4107	valid_1's l2: 20.6108
[480]	training's l2: 20.374	valid_1's l2: 20.599
[490]	training's l2: 20.3374	valid_1's l2: 20.5894
[500]	training's l2: 20.3013	valid_1's l2: 20.5799
[510]	training's l2: 20.2654	valid_1's l2: 20.5701
[520]	training's l2: 20.2307	valid_1's l2: 20.561
[530]	training's l2: 20.1964	valid_1's l2: 20.5526
[540]	training's l2: 20.1622	valid_1's l2: 20.5445
[550]	training's l2: 20.1285	valid_1's l2: 20.5362
[560]	training's l2: 20.0957	valid_1's l2: 20.5292
[570]	training's l2: 20.0632	valid_1's l2: 20.5215
[580]	training's l2: 20.0309	valid_1's l2: 20.5146
[590]	training's l2: 19.9988	valid_1's l2: 20.5085
[600]	training's l2: 19.9661	valid_1's l2: 20.5007
[610]	training's l2: 19.9344	valid_1's l2: 20.4936
[620]	training's l2: 19.9029	valid_1's l2: 20.4873
[630]	training's l2: 19.8716	valid_1's l2: 20.4815
[640]	training's l2: 19.8408	valid_1's l2: 20.4754
[650]	training's l2: 19.8091	valid_1's l2: 20.4697
[660]	training's l2: 19.7786	valid_1's l2: 20.4644
[670]	training's l2: 19.748	valid_1's l2: 20.4591
[680]	training's l2: 19.7174	valid_1's l2: 20.4533
[690]	training's l2: 19.687	valid_1's l2: 20.4477
[700]	training's l2: 19.6564	valid_1's l2: 20.4433
[710]	training's l2: 19.6267	valid_1's l2: 20.4394
[720]	training's l2: 19.5969	valid_1's l2: 20.4344
[730]	training's l2: 19.5675	valid_1's l2: 20.4292
[740]	training's l2: 19.5383	valid_1's l2: 20.4237
[750]	training's l2: 19.5089	valid_1's l2: 20.4186
[760]	training's l2: 19.48	valid_1's l2: 20.4139
[770]	training's l2: 19.4511	valid_1's l2: 20.4092
[780]	training's l2: 19.4228	valid_1's l2: 20.4052
[790]	training's l2: 19.3946	valid_1's l2: 20.4013
[800]	training's l2: 19.3664	valid_1's l2: 20.3975
[810]	training's l2: 19.338	valid_1's l2: 20.3934
[820]	training's l2: 19.3105	valid_1's l2: 20.3899
[830]	training's l2: 19.2831	valid_1's l2: 20.3871
[840]	training's l2: 19.2556	valid_1's l2: 20.3829
[850]	training's l2: 19.2286	valid_1's l2: 20.3794
[860]	training's l2: 19.2017	valid_1's l2: 20.3762
[870]	training's l2: 19.1748	valid_1's l2: 20.3732
[880]	training's l2: 19.1474	valid_1's l2: 20.3697
[890]	training's l2: 19.1205	valid_1's l2: 20.3664
[900]	training's l2: 19.0931	valid_1's l2: 20.3636
[910]	training's l2: 19.0665	valid_1's l2: 20.3613
[920]	training's l2: 19.0398	valid_1's l2: 20.3575
[930]	training's l2: 19.0136	valid_1's l2: 20.356
[940]	training's l2: 18.9871	valid_1's l2: 20.3526
[950]	training's l2: 18.9611	valid_1's l2: 20.3501
[960]	training's l2: 18.9351	valid_1's l2: 20.3476
[970]	training's l2: 18.9088	valid_1's l2: 20.344
[980]	training's l2: 18.883	valid_1's l2: 20.3418
[990]	training's l2: 18.8572	valid_1's l2: 20.3398
[1000]	training's l2: 18.8313	valid_1's l2: 20.3373
[1010]	training's l2: 18.8053	valid_1's l2: 20.3345
[1020]	training's l2: 18.7801	valid_1's l2: 20.3321
[1030]	training's l2: 18.7547	valid_1's l2: 20.3301
[1040]	training's l2: 18.7287	valid_1's l2: 20.3264
[1050]	training's l2: 18.7034	valid_1's l2: 20.3245
[1060]	training's l2: 18.6788	valid_1's l2: 20.3224
[1070]	training's l2: 18.654	valid_1's l2: 20.3206
[1080]	training's l2: 18.6294	valid_1's l2: 20.3183
[1090]	training's l2: 18.6053	valid_1's l2: 20.3168
[1100]	training's l2: 18.5809	valid_1's l2: 20.3149
[1110]	training's l2: 18.5561	valid_1's l2: 20.3126
[1120]	training's l2: 18.5314	valid_1's l2: 20.3113
[1130]	training's l2: 18.5069	valid_1's l2: 20.3102
[1140]	training's l2: 18.4822	valid_1's l2: 20.3088
[1150]	training's l2: 18.4586	valid_1's l2: 20.3069
[1160]	training's l2: 18.4349	valid_1's l2: 20.3048
[1170]	training's l2: 18.4107	valid_1's l2: 20.3032
[1180]	training's l2: 18.387	valid_1's l2: 20.3015
[1190]	training's l2: 18.3635	valid_1's l2: 20.3001
[1200]	training's l2: 18.3405	valid_1's l2: 20.2984
[1210]	training's l2: 18.317	valid_1's l2: 20.2965
[1220]	training's l2: 18.2938	valid_1's l2: 20.295
[1230]	training's l2: 18.2708	valid_1's l2: 20.2936
[1240]	training's l2: 18.2485	valid_1's l2: 20.2924
[1250]	training's l2: 18.2256	valid_1's l2: 20.2913
[1260]	training's l2: 18.2031	valid_1's l2: 20.291
[1270]	training's l2: 18.1809	valid_1's l2: 20.2899
[1280]	training's l2: 18.1584	valid_1's l2: 20.2887
[1290]	training's l2: 18.1366	valid_1's l2: 20.2875
[1300]	training's l2: 18.1148	valid_1's l2: 20.2866
[1310]	training's l2: 18.0933	valid_1's l2: 20.2859
[1320]	training's l2: 18.0712	valid_1's l2: 20.2852
[1330]	training's l2: 18.0493	valid_1's l2: 20.2839
[1340]	training's l2: 18.0273	valid_1's l2: 20.2835
[1350]	training's l2: 18.0051	valid_1's l2: 20.2829
[1360]	training's l2: 17.9837	valid_1's l2: 20.2815
[1370]	training's l2: 17.9624	valid_1's l2: 20.2804
[1380]	training's l2: 17.9415	valid_1's l2: 20.2809
[1390]	training's l2: 17.9206	valid_1's l2: 20.2797
[1400]	training's l2: 17.8989	valid_1's l2: 20.2785
[1410]	training's l2: 17.8776	valid_1's l2: 20.2777
[1420]	training's l2: 17.8572	valid_1's l2: 20.2771
[1430]	training's l2: 17.8362	valid_1's l2: 20.2758
[1440]	training's l2: 17.8151	valid_1's l2: 20.2747
[1450]	training's l2: 17.7939	valid_1's l2: 20.2746
[1460]	training's l2: 17.7734	valid_1's l2: 20.2738
[1470]	training's l2: 17.753	valid_1's l2: 20.2727
[1480]	training's l2: 17.7319	valid_1's l2: 20.2708
[1490]	training's l2: 17.7115	valid_1's l2: 20.27
[1500]	training's l2: 17.691	valid_1's l2: 20.2692
[1510]	training's l2: 17.6712	valid_1's l2: 20.2682
[1520]	training's l2: 17.6509	valid_1's l2: 20.2678
[1530]	training's l2: 17.6311	valid_1's l2: 20.2669
[1540]	training's l2: 17.6114	valid_1's l2: 20.267
[1550]	training's l2: 17.5908	valid_1's l2: 20.2662
[1560]	training's l2: 17.5707	valid_1's l2: 20.2657
[1570]	training's l2: 17.551	valid_1's l2: 20.2655
[1580]	training's l2: 17.5304	valid_1's l2: 20.2649
[1590]	training's l2: 17.5101	valid_1's l2: 20.2638
[1600]	training's l2: 17.491	valid_1's l2: 20.2629
[1610]	training's l2: 17.471	valid_1's l2: 20.2632
[1620]	training's l2: 17.4517	valid_1's l2: 20.2626
[1630]	training's l2: 17.4323	valid_1's l2: 20.2625
[1640]	training's l2: 17.4133	valid_1's l2: 20.2625
[1650]	training's l2: 17.393	valid_1's l2: 20.2621
[1660]	training's l2: 17.3733	valid_1's l2: 20.2615
[1670]	training's l2: 17.3543	valid_1's l2: 20.2609
[1680]	training's l2: 17.3354	valid_1's l2: 20.2604
[1690]	training's l2: 17.3157	valid_1's l2: 20.26
[1700]	training's l2: 17.2967	valid_1's l2: 20.2596
[1710]	training's l2: 17.2785	valid_1's l2: 20.2586
[1720]	training's l2: 17.2593	valid_1's l2: 20.2582
[1730]	training's l2: 17.2409	valid_1's l2: 20.2583
[1740]	training's l2: 17.2215	valid_1's l2: 20.2577
[1750]	training's l2: 17.2028	valid_1's l2: 20.258
[1760]	training's l2: 17.1846	valid_1's l2: 20.2568
[1770]	training's l2: 17.1661	valid_1's l2: 20.2561
[1780]	training's l2: 17.1476	valid_1's l2: 20.2553
[1790]	training's l2: 17.1286	valid_1's l2: 20.255
[1800]	training's l2: 17.11	valid_1's l2: 20.2547
[1810]	training's l2: 17.0916	valid_1's l2: 20.255
[1820]	training's l2: 17.0735	valid_1's l2: 20.2549
Early stopping, best iteration is:
[1794]	training's l2: 17.1209	valid_1's l2: 20.2546
score1: 3.8375505378336734
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.263931 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.9019	valid_1's l2: 30.9991
[20]	training's l2: 30.1329	valid_1's l2: 29.2406
[30]	training's l2: 28.6934	valid_1's l2: 27.8138
[40]	training's l2: 27.5189	valid_1's l2: 26.6496
[50]	training's l2: 26.5545	valid_1's l2: 25.6984
[60]	training's l2: 25.7612	valid_1's l2: 24.9203
[70]	training's l2: 25.1049	valid_1's l2: 24.2805
[80]	training's l2: 24.5585	valid_1's l2: 23.7522
[90]	training's l2: 24.1018	valid_1's l2: 23.315
[100]	training's l2: 23.7185	valid_1's l2: 22.9517
[110]	training's l2: 23.3941	valid_1's l2: 22.6479
[120]	training's l2: 23.1204	valid_1's l2: 22.395
[130]	training's l2: 22.8877	valid_1's l2: 22.1841
[140]	training's l2: 22.684	valid_1's l2: 22.0036
[150]	training's l2: 22.5039	valid_1's l2: 21.8487
[160]	training's l2: 22.3444	valid_1's l2: 21.7137
[170]	training's l2: 22.205	valid_1's l2: 21.5993
[180]	training's l2: 22.0794	valid_1's l2: 21.4998
[190]	training's l2: 21.9649	valid_1's l2: 21.4116
[200]	training's l2: 21.8606	valid_1's l2: 21.3349
[210]	training's l2: 21.765	valid_1's l2: 21.2665
[220]	training's l2: 21.6749	valid_1's l2: 21.2025
[230]	training's l2: 21.5919	valid_1's l2: 21.1457
[240]	training's l2: 21.5156	valid_1's l2: 21.0948
[250]	training's l2: 21.4428	valid_1's l2: 21.0478
[260]	training's l2: 21.3749	valid_1's l2: 21.0052
[270]	training's l2: 21.3111	valid_1's l2: 20.9666
[280]	training's l2: 21.252	valid_1's l2: 20.9327
[290]	training's l2: 21.1946	valid_1's l2: 20.9011
[300]	training's l2: 21.1366	valid_1's l2: 20.8681
[310]	training's l2: 21.0817	valid_1's l2: 20.8396
[320]	training's l2: 21.0295	valid_1's l2: 20.8131
[330]	training's l2: 20.978	valid_1's l2: 20.7886
[340]	training's l2: 20.9287	valid_1's l2: 20.7651
[350]	training's l2: 20.8822	valid_1's l2: 20.7439
[360]	training's l2: 20.8367	valid_1's l2: 20.7249
[370]	training's l2: 20.7921	valid_1's l2: 20.706
[380]	training's l2: 20.7496	valid_1's l2: 20.6891
[390]	training's l2: 20.7077	valid_1's l2: 20.6735
[400]	training's l2: 20.6676	valid_1's l2: 20.6588
[410]	training's l2: 20.6278	valid_1's l2: 20.6448
[420]	training's l2: 20.5887	valid_1's l2: 20.6316
[430]	training's l2: 20.5503	valid_1's l2: 20.6189
[440]	training's l2: 20.5125	valid_1's l2: 20.6057
[450]	training's l2: 20.4752	valid_1's l2: 20.5952
[460]	training's l2: 20.4386	valid_1's l2: 20.584
[470]	training's l2: 20.4021	valid_1's l2: 20.5726
[480]	training's l2: 20.3659	valid_1's l2: 20.562
[490]	training's l2: 20.3307	valid_1's l2: 20.5524
[500]	training's l2: 20.2963	valid_1's l2: 20.5439
[510]	training's l2: 20.2622	valid_1's l2: 20.535
[520]	training's l2: 20.2288	valid_1's l2: 20.5267
[530]	training's l2: 20.195	valid_1's l2: 20.519
[540]	training's l2: 20.1616	valid_1's l2: 20.5112
[550]	training's l2: 20.1288	valid_1's l2: 20.5046
[560]	training's l2: 20.0965	valid_1's l2: 20.4966
[570]	training's l2: 20.0643	valid_1's l2: 20.4901
[580]	training's l2: 20.0322	valid_1's l2: 20.4829
[590]	training's l2: 20.0003	valid_1's l2: 20.4769
[600]	training's l2: 19.9685	valid_1's l2: 20.4705
[610]	training's l2: 19.9372	valid_1's l2: 20.4649
[620]	training's l2: 19.9061	valid_1's l2: 20.4596
[630]	training's l2: 19.8747	valid_1's l2: 20.4524
[640]	training's l2: 19.8433	valid_1's l2: 20.4463
[650]	training's l2: 19.8128	valid_1's l2: 20.4404
[660]	training's l2: 19.7825	valid_1's l2: 20.4346
[670]	training's l2: 19.7526	valid_1's l2: 20.4298
[680]	training's l2: 19.7227	valid_1's l2: 20.4256
[690]	training's l2: 19.6925	valid_1's l2: 20.4209
[700]	training's l2: 19.6636	valid_1's l2: 20.4153
[710]	training's l2: 19.6344	valid_1's l2: 20.4109
[720]	training's l2: 19.6054	valid_1's l2: 20.406
[730]	training's l2: 19.5765	valid_1's l2: 20.4018
[740]	training's l2: 19.5478	valid_1's l2: 20.3971
[750]	training's l2: 19.5195	valid_1's l2: 20.3928
[760]	training's l2: 19.4917	valid_1's l2: 20.3878
[770]	training's l2: 19.4635	valid_1's l2: 20.3837
[780]	training's l2: 19.4354	valid_1's l2: 20.3799
[790]	training's l2: 19.4079	valid_1's l2: 20.3764
[800]	training's l2: 19.3805	valid_1's l2: 20.3724
[810]	training's l2: 19.3534	valid_1's l2: 20.3691
[820]	training's l2: 19.326	valid_1's l2: 20.3657
[830]	training's l2: 19.2991	valid_1's l2: 20.3627
[840]	training's l2: 19.2725	valid_1's l2: 20.3595
[850]	training's l2: 19.246	valid_1's l2: 20.3564
[860]	training's l2: 19.2191	valid_1's l2: 20.3535
[870]	training's l2: 19.1925	valid_1's l2: 20.3505
[880]	training's l2: 19.1663	valid_1's l2: 20.3482
[890]	training's l2: 19.1398	valid_1's l2: 20.3454
[900]	training's l2: 19.1142	valid_1's l2: 20.3432
[910]	training's l2: 19.088	valid_1's l2: 20.3407
[920]	training's l2: 19.0623	valid_1's l2: 20.338
[930]	training's l2: 19.0364	valid_1's l2: 20.3359
[940]	training's l2: 19.011	valid_1's l2: 20.3337
[950]	training's l2: 18.9855	valid_1's l2: 20.3313
[960]	training's l2: 18.9601	valid_1's l2: 20.3287
[970]	training's l2: 18.9346	valid_1's l2: 20.3256
[980]	training's l2: 18.9097	valid_1's l2: 20.3228
[990]	training's l2: 18.8846	valid_1's l2: 20.3198
[1000]	training's l2: 18.8601	valid_1's l2: 20.3175
[1010]	training's l2: 18.835	valid_1's l2: 20.3144
[1020]	training's l2: 18.81	valid_1's l2: 20.3118
[1030]	training's l2: 18.7852	valid_1's l2: 20.31
[1040]	training's l2: 18.7609	valid_1's l2: 20.3078
[1050]	training's l2: 18.7363	valid_1's l2: 20.307
[1060]	training's l2: 18.7115	valid_1's l2: 20.3055
[1070]	training's l2: 18.6873	valid_1's l2: 20.3039
[1080]	training's l2: 18.6634	valid_1's l2: 20.303
[1090]	training's l2: 18.6394	valid_1's l2: 20.301
[1100]	training's l2: 18.6159	valid_1's l2: 20.3
[1110]	training's l2: 18.5918	valid_1's l2: 20.2984
[1120]	training's l2: 18.5685	valid_1's l2: 20.2968
[1130]	training's l2: 18.5447	valid_1's l2: 20.2952
[1140]	training's l2: 18.5219	valid_1's l2: 20.2942
[1150]	training's l2: 18.4986	valid_1's l2: 20.2925
[1160]	training's l2: 18.4758	valid_1's l2: 20.2912
[1170]	training's l2: 18.4531	valid_1's l2: 20.2899
[1180]	training's l2: 18.4307	valid_1's l2: 20.2892
[1190]	training's l2: 18.4077	valid_1's l2: 20.2878
[1200]	training's l2: 18.3847	valid_1's l2: 20.2869
[1210]	training's l2: 18.3618	valid_1's l2: 20.2859
[1220]	training's l2: 18.3386	valid_1's l2: 20.285
[1230]	training's l2: 18.3161	valid_1's l2: 20.2837
[1240]	training's l2: 18.294	valid_1's l2: 20.2826
[1250]	training's l2: 18.2722	valid_1's l2: 20.2818
[1260]	training's l2: 18.2509	valid_1's l2: 20.2808
[1270]	training's l2: 18.2287	valid_1's l2: 20.2797
[1280]	training's l2: 18.207	valid_1's l2: 20.2796
[1290]	training's l2: 18.1855	valid_1's l2: 20.2783
[1300]	training's l2: 18.1634	valid_1's l2: 20.2766
[1310]	training's l2: 18.1422	valid_1's l2: 20.2757
[1320]	training's l2: 18.1212	valid_1's l2: 20.2755
[1330]	training's l2: 18.0993	valid_1's l2: 20.2744
[1340]	training's l2: 18.0778	valid_1's l2: 20.2734
[1350]	training's l2: 18.0576	valid_1's l2: 20.2731
[1360]	training's l2: 18.0366	valid_1's l2: 20.272
[1370]	training's l2: 18.0152	valid_1's l2: 20.2708
[1380]	training's l2: 17.9946	valid_1's l2: 20.2706
[1390]	training's l2: 17.9749	valid_1's l2: 20.2711
[1400]	training's l2: 17.954	valid_1's l2: 20.2697
[1410]	training's l2: 17.9335	valid_1's l2: 20.2687
[1420]	training's l2: 17.9135	valid_1's l2: 20.2688
[1430]	training's l2: 17.8918	valid_1's l2: 20.268
[1440]	training's l2: 17.8722	valid_1's l2: 20.2672
[1450]	training's l2: 17.8521	valid_1's l2: 20.2664
[1460]	training's l2: 17.8326	valid_1's l2: 20.2655
[1470]	training's l2: 17.8131	valid_1's l2: 20.2654
[1480]	training's l2: 17.7937	valid_1's l2: 20.2642
[1490]	training's l2: 17.7733	valid_1's l2: 20.263
[1500]	training's l2: 17.7536	valid_1's l2: 20.2626
[1510]	training's l2: 17.7331	valid_1's l2: 20.2615
[1520]	training's l2: 17.7146	valid_1's l2: 20.261
[1530]	training's l2: 17.6947	valid_1's l2: 20.2609
[1540]	training's l2: 17.6747	valid_1's l2: 20.2602
[1550]	training's l2: 17.6555	valid_1's l2: 20.2594
[1560]	training's l2: 17.6363	valid_1's l2: 20.2585
[1570]	training's l2: 17.6172	valid_1's l2: 20.2579
[1580]	training's l2: 17.5984	valid_1's l2: 20.2569
[1590]	training's l2: 17.579	valid_1's l2: 20.2569
[1600]	training's l2: 17.561	valid_1's l2: 20.2557
[1610]	training's l2: 17.543	valid_1's l2: 20.2551
[1620]	training's l2: 17.5236	valid_1's l2: 20.2551
[1630]	training's l2: 17.5042	valid_1's l2: 20.2553
[1640]	training's l2: 17.4851	valid_1's l2: 20.2549
Early stopping, best iteration is:
[1613]	training's l2: 17.5367	valid_1's l2: 20.2546
score1: 3.8382429358290957
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.256211 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.5001	valid_1's l2: 30.6001
[20]	training's l2: 29.4911	valid_1's l2: 28.6056
[30]	training's l2: 27.9221	valid_1's l2: 27.0536
[40]	training's l2: 26.6894	valid_1's l2: 25.8386
[50]	training's l2: 25.7161	valid_1's l2: 24.8846
[60]	training's l2: 24.9405	valid_1's l2: 24.1318
[70]	training's l2: 24.3192	valid_1's l2: 23.5346
[80]	training's l2: 23.8167	valid_1's l2: 23.0578
[90]	training's l2: 23.4068	valid_1's l2: 22.6764
[100]	training's l2: 23.0722	valid_1's l2: 22.3693
[110]	training's l2: 22.7941	valid_1's l2: 22.12
[120]	training's l2: 22.5587	valid_1's l2: 21.9142
[130]	training's l2: 22.3562	valid_1's l2: 21.7441
[140]	training's l2: 22.1824	valid_1's l2: 21.6002
[150]	training's l2: 22.0284	valid_1's l2: 21.4793
[160]	training's l2: 21.8907	valid_1's l2: 21.3747
[170]	training's l2: 21.7692	valid_1's l2: 21.2867
[180]	training's l2: 21.6558	valid_1's l2: 21.2075
[190]	training's l2: 21.5512	valid_1's l2: 21.1373
[200]	training's l2: 21.4583	valid_1's l2: 21.0774
[210]	training's l2: 21.3693	valid_1's l2: 21.0224
[220]	training's l2: 21.2883	valid_1's l2: 20.973
[230]	training's l2: 21.2126	valid_1's l2: 20.9305
[240]	training's l2: 21.1414	valid_1's l2: 20.892
[250]	training's l2: 21.0712	valid_1's l2: 20.8548
[260]	training's l2: 21.0039	valid_1's l2: 20.8194
[270]	training's l2: 20.9398	valid_1's l2: 20.7898
[280]	training's l2: 20.8779	valid_1's l2: 20.7631
[290]	training's l2: 20.8182	valid_1's l2: 20.7353
[300]	training's l2: 20.7634	valid_1's l2: 20.714
[310]	training's l2: 20.7097	valid_1's l2: 20.6942
[320]	training's l2: 20.6573	valid_1's l2: 20.6737
[330]	training's l2: 20.6068	valid_1's l2: 20.6547
[340]	training's l2: 20.5568	valid_1's l2: 20.6393
[350]	training's l2: 20.5085	valid_1's l2: 20.6225
[360]	training's l2: 20.4613	valid_1's l2: 20.6067
[370]	training's l2: 20.4138	valid_1's l2: 20.5926
[380]	training's l2: 20.3681	valid_1's l2: 20.581
[390]	training's l2: 20.3227	valid_1's l2: 20.5685
[400]	training's l2: 20.2781	valid_1's l2: 20.5561
[410]	training's l2: 20.2347	valid_1's l2: 20.5434
[420]	training's l2: 20.1922	valid_1's l2: 20.5333
[430]	training's l2: 20.149	valid_1's l2: 20.524
[440]	training's l2: 20.108	valid_1's l2: 20.5157
[450]	training's l2: 20.0661	valid_1's l2: 20.5066
[460]	training's l2: 20.0252	valid_1's l2: 20.4974
[470]	training's l2: 19.9849	valid_1's l2: 20.4896
[480]	training's l2: 19.945	valid_1's l2: 20.4816
[490]	training's l2: 19.9059	valid_1's l2: 20.4751
[500]	training's l2: 19.8666	valid_1's l2: 20.4672
[510]	training's l2: 19.828	valid_1's l2: 20.4604
[520]	training's l2: 19.7884	valid_1's l2: 20.4527
[530]	training's l2: 19.7495	valid_1's l2: 20.4456
[540]	training's l2: 19.7098	valid_1's l2: 20.4372
[550]	training's l2: 19.6713	valid_1's l2: 20.4285
[560]	training's l2: 19.6334	valid_1's l2: 20.4221
[570]	training's l2: 19.5959	valid_1's l2: 20.4151
[580]	training's l2: 19.5596	valid_1's l2: 20.4089
[590]	training's l2: 19.5226	valid_1's l2: 20.4036
[600]	training's l2: 19.4862	valid_1's l2: 20.3979
[610]	training's l2: 19.4499	valid_1's l2: 20.3932
[620]	training's l2: 19.4143	valid_1's l2: 20.389
[630]	training's l2: 19.3786	valid_1's l2: 20.3851
[640]	training's l2: 19.3434	valid_1's l2: 20.3801
[650]	training's l2: 19.3092	valid_1's l2: 20.3772
[660]	training's l2: 19.2742	valid_1's l2: 20.3733
[670]	training's l2: 19.24	valid_1's l2: 20.3693
[680]	training's l2: 19.2054	valid_1's l2: 20.3647
[690]	training's l2: 19.1714	valid_1's l2: 20.3607
[700]	training's l2: 19.1377	valid_1's l2: 20.3577
[710]	training's l2: 19.1044	valid_1's l2: 20.3538
[720]	training's l2: 19.0712	valid_1's l2: 20.3507
[730]	training's l2: 19.0381	valid_1's l2: 20.3467
[740]	training's l2: 19.0053	valid_1's l2: 20.3441
[750]	training's l2: 18.9718	valid_1's l2: 20.3419
[760]	training's l2: 18.9386	valid_1's l2: 20.3389
[770]	training's l2: 18.9058	valid_1's l2: 20.3352
[780]	training's l2: 18.874	valid_1's l2: 20.3331
[790]	training's l2: 18.8424	valid_1's l2: 20.3299
[800]	training's l2: 18.8106	valid_1's l2: 20.3281
[810]	training's l2: 18.7791	valid_1's l2: 20.3263
[820]	training's l2: 18.7475	valid_1's l2: 20.3245
[830]	training's l2: 18.7167	valid_1's l2: 20.3216
[840]	training's l2: 18.6856	valid_1's l2: 20.3182
[850]	training's l2: 18.6546	valid_1's l2: 20.3147
[860]	training's l2: 18.6241	valid_1's l2: 20.3128
[870]	training's l2: 18.5931	valid_1's l2: 20.3096
[880]	training's l2: 18.5616	valid_1's l2: 20.3084
[890]	training's l2: 18.5313	valid_1's l2: 20.3058
[900]	training's l2: 18.5009	valid_1's l2: 20.3028
[910]	training's l2: 18.4704	valid_1's l2: 20.3005
[920]	training's l2: 18.4406	valid_1's l2: 20.2979
[930]	training's l2: 18.4105	valid_1's l2: 20.2959
[940]	training's l2: 18.3813	valid_1's l2: 20.2956
[950]	training's l2: 18.3525	valid_1's l2: 20.2943
[960]	training's l2: 18.3241	valid_1's l2: 20.2932
[970]	training's l2: 18.295	valid_1's l2: 20.2915
[980]	training's l2: 18.2675	valid_1's l2: 20.2903
[990]	training's l2: 18.2391	valid_1's l2: 20.2882
[1000]	training's l2: 18.21	valid_1's l2: 20.2868
[1010]	training's l2: 18.1822	valid_1's l2: 20.2853
[1020]	training's l2: 18.1543	valid_1's l2: 20.2844
[1030]	training's l2: 18.127	valid_1's l2: 20.2839
[1040]	training's l2: 18.0999	valid_1's l2: 20.2841
[1050]	training's l2: 18.0725	valid_1's l2: 20.2833
[1060]	training's l2: 18.0453	valid_1's l2: 20.2817
[1070]	training's l2: 18.0176	valid_1's l2: 20.2806
[1080]	training's l2: 17.9904	valid_1's l2: 20.2797
[1090]	training's l2: 17.9638	valid_1's l2: 20.2785
[1100]	training's l2: 17.9364	valid_1's l2: 20.2781
[1110]	training's l2: 17.9096	valid_1's l2: 20.2773
[1120]	training's l2: 17.8835	valid_1's l2: 20.2766
[1130]	training's l2: 17.8578	valid_1's l2: 20.2755
[1140]	training's l2: 17.8319	valid_1's l2: 20.274
[1150]	training's l2: 17.8049	valid_1's l2: 20.2729
[1160]	training's l2: 17.7803	valid_1's l2: 20.2732
[1170]	training's l2: 17.7554	valid_1's l2: 20.2726
[1180]	training's l2: 17.7311	valid_1's l2: 20.2715
[1190]	training's l2: 17.7059	valid_1's l2: 20.2706
[1200]	training's l2: 17.6793	valid_1's l2: 20.2686
[1210]	training's l2: 17.6537	valid_1's l2: 20.2678
[1220]	training's l2: 17.6286	valid_1's l2: 20.267
[1230]	training's l2: 17.6031	valid_1's l2: 20.2661
[1240]	training's l2: 17.5778	valid_1's l2: 20.2654
[1250]	training's l2: 17.5533	valid_1's l2: 20.2646
[1260]	training's l2: 17.5287	valid_1's l2: 20.2636
[1270]	training's l2: 17.503	valid_1's l2: 20.2632
[1280]	training's l2: 17.4795	valid_1's l2: 20.262
[1290]	training's l2: 17.4542	valid_1's l2: 20.2621
[1300]	training's l2: 17.4302	valid_1's l2: 20.2617
[1310]	training's l2: 17.4053	valid_1's l2: 20.2614
[1320]	training's l2: 17.3806	valid_1's l2: 20.2608
[1330]	training's l2: 17.3567	valid_1's l2: 20.26
[1340]	training's l2: 17.332	valid_1's l2: 20.2601
[1350]	training's l2: 17.3082	valid_1's l2: 20.2605
[1360]	training's l2: 17.2848	valid_1's l2: 20.259
[1370]	training's l2: 17.2619	valid_1's l2: 20.2583
[1380]	training's l2: 17.2384	valid_1's l2: 20.258
[1390]	training's l2: 17.2151	valid_1's l2: 20.2571
[1400]	training's l2: 17.1913	valid_1's l2: 20.257
[1410]	training's l2: 17.1676	valid_1's l2: 20.2566
[1420]	training's l2: 17.1435	valid_1's l2: 20.2558
[1430]	training's l2: 17.1204	valid_1's l2: 20.2556
[1440]	training's l2: 17.0983	valid_1's l2: 20.2554
[1450]	training's l2: 17.074	valid_1's l2: 20.2555
[1460]	training's l2: 17.0521	valid_1's l2: 20.2555
[1470]	training's l2: 17.0295	valid_1's l2: 20.2553
Early stopping, best iteration is:
[1444]	training's l2: 17.0888	valid_1's l2: 20.2552
score1: 3.8375135193286964
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285082 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.7346	valid_1's l2: 30.8359
[20]	training's l2: 29.8598	valid_1's l2: 28.9786
[30]	training's l2: 28.3569	valid_1's l2: 27.4913
[40]	training's l2: 27.1496	valid_1's l2: 26.3007
[50]	training's l2: 26.1717	valid_1's l2: 25.3419
[60]	training's l2: 25.3776	valid_1's l2: 24.568
[70]	training's l2: 24.7274	valid_1's l2: 23.9418
[80]	training's l2: 24.1937	valid_1's l2: 23.4307
[90]	training's l2: 23.7503	valid_1's l2: 23.0147
[100]	training's l2: 23.3815	valid_1's l2: 22.6733
[110]	training's l2: 23.0725	valid_1's l2: 22.3917
[120]	training's l2: 22.8122	valid_1's l2: 22.1587
[130]	training's l2: 22.5894	valid_1's l2: 21.9644
[140]	training's l2: 22.3943	valid_1's l2: 21.8017
[150]	training's l2: 22.2219	valid_1's l2: 21.6603
[160]	training's l2: 22.0716	valid_1's l2: 21.5414
[170]	training's l2: 21.9347	valid_1's l2: 21.4369
[180]	training's l2: 21.8123	valid_1's l2: 21.3478
[190]	training's l2: 21.7018	valid_1's l2: 21.2714
[200]	training's l2: 21.598	valid_1's l2: 21.2009
[210]	training's l2: 21.4997	valid_1's l2: 21.1351
[220]	training's l2: 21.4121	valid_1's l2: 21.0801
[230]	training's l2: 21.3305	valid_1's l2: 21.0309
[240]	training's l2: 21.253	valid_1's l2: 20.9848
[250]	training's l2: 21.179	valid_1's l2: 20.9435
[260]	training's l2: 21.1112	valid_1's l2: 20.9076
[270]	training's l2: 21.0456	valid_1's l2: 20.8729
[280]	training's l2: 20.9795	valid_1's l2: 20.8381
[290]	training's l2: 20.9165	valid_1's l2: 20.8078
[300]	training's l2: 20.8558	valid_1's l2: 20.7791
[310]	training's l2: 20.7979	valid_1's l2: 20.7539
[320]	training's l2: 20.7418	valid_1's l2: 20.7319
[330]	training's l2: 20.6878	valid_1's l2: 20.712
[340]	training's l2: 20.6357	valid_1's l2: 20.6934
[350]	training's l2: 20.585	valid_1's l2: 20.6753
[360]	training's l2: 20.5366	valid_1's l2: 20.6585
[370]	training's l2: 20.4892	valid_1's l2: 20.642
[380]	training's l2: 20.4434	valid_1's l2: 20.6277
[390]	training's l2: 20.3971	valid_1's l2: 20.6143
[400]	training's l2: 20.3523	valid_1's l2: 20.6006
[410]	training's l2: 20.3079	valid_1's l2: 20.5887
[420]	training's l2: 20.2642	valid_1's l2: 20.5775
[430]	training's l2: 20.2213	valid_1's l2: 20.5669
[440]	training's l2: 20.1793	valid_1's l2: 20.5558
[450]	training's l2: 20.1371	valid_1's l2: 20.5458
[460]	training's l2: 20.097	valid_1's l2: 20.5366
[470]	training's l2: 20.0563	valid_1's l2: 20.5272
[480]	training's l2: 20.0158	valid_1's l2: 20.5194
[490]	training's l2: 19.9765	valid_1's l2: 20.5117
[500]	training's l2: 19.938	valid_1's l2: 20.5042
[510]	training's l2: 19.8992	valid_1's l2: 20.4975
[520]	training's l2: 19.8604	valid_1's l2: 20.4914
[530]	training's l2: 19.8223	valid_1's l2: 20.4853
[540]	training's l2: 19.7841	valid_1's l2: 20.4783
[550]	training's l2: 19.7465	valid_1's l2: 20.4715
[560]	training's l2: 19.7095	valid_1's l2: 20.4662
[570]	training's l2: 19.6721	valid_1's l2: 20.4595
[580]	training's l2: 19.6342	valid_1's l2: 20.4515
[590]	training's l2: 19.5964	valid_1's l2: 20.4439
[600]	training's l2: 19.5596	valid_1's l2: 20.4366
[610]	training's l2: 19.5236	valid_1's l2: 20.4302
[620]	training's l2: 19.488	valid_1's l2: 20.4239
[630]	training's l2: 19.4522	valid_1's l2: 20.4178
[640]	training's l2: 19.417	valid_1's l2: 20.4129
[650]	training's l2: 19.382	valid_1's l2: 20.4076
[660]	training's l2: 19.3471	valid_1's l2: 20.4034
[670]	training's l2: 19.3127	valid_1's l2: 20.3978
[680]	training's l2: 19.2789	valid_1's l2: 20.3924
[690]	training's l2: 19.2447	valid_1's l2: 20.3876
[700]	training's l2: 19.2106	valid_1's l2: 20.3824
[710]	training's l2: 19.1771	valid_1's l2: 20.3784
[720]	training's l2: 19.1433	valid_1's l2: 20.3749
[730]	training's l2: 19.1097	valid_1's l2: 20.3705
[740]	training's l2: 19.0769	valid_1's l2: 20.3677
[750]	training's l2: 19.0441	valid_1's l2: 20.364
[760]	training's l2: 19.0119	valid_1's l2: 20.3604
[770]	training's l2: 18.9788	valid_1's l2: 20.3567
[780]	training's l2: 18.9466	valid_1's l2: 20.353
[790]	training's l2: 18.9146	valid_1's l2: 20.3502
[800]	training's l2: 18.8829	valid_1's l2: 20.3462
[810]	training's l2: 18.8508	valid_1's l2: 20.342
[820]	training's l2: 18.82	valid_1's l2: 20.3401
[830]	training's l2: 18.7883	valid_1's l2: 20.3374
[840]	training's l2: 18.7566	valid_1's l2: 20.3355
[850]	training's l2: 18.7255	valid_1's l2: 20.333
[860]	training's l2: 18.6943	valid_1's l2: 20.3299
[870]	training's l2: 18.6635	valid_1's l2: 20.3272
[880]	training's l2: 18.6329	valid_1's l2: 20.3245
[890]	training's l2: 18.6022	valid_1's l2: 20.3223
[900]	training's l2: 18.5721	valid_1's l2: 20.3205
[910]	training's l2: 18.5419	valid_1's l2: 20.3178
[920]	training's l2: 18.5121	valid_1's l2: 20.316
[930]	training's l2: 18.482	valid_1's l2: 20.3129
[940]	training's l2: 18.4529	valid_1's l2: 20.3105
[950]	training's l2: 18.423	valid_1's l2: 20.3082
[960]	training's l2: 18.3936	valid_1's l2: 20.3064
[970]	training's l2: 18.3645	valid_1's l2: 20.3047
[980]	training's l2: 18.3358	valid_1's l2: 20.3033
[990]	training's l2: 18.3079	valid_1's l2: 20.3018
[1000]	training's l2: 18.2794	valid_1's l2: 20.2993
[1010]	training's l2: 18.2509	valid_1's l2: 20.2992
[1020]	training's l2: 18.2227	valid_1's l2: 20.2972
[1030]	training's l2: 18.1952	valid_1's l2: 20.2953
[1040]	training's l2: 18.1667	valid_1's l2: 20.293
[1050]	training's l2: 18.139	valid_1's l2: 20.2914
[1060]	training's l2: 18.1116	valid_1's l2: 20.289
[1070]	training's l2: 18.0827	valid_1's l2: 20.2866
[1080]	training's l2: 18.0548	valid_1's l2: 20.2858
[1090]	training's l2: 18.0275	valid_1's l2: 20.2844
[1100]	training's l2: 18.0002	valid_1's l2: 20.2832
[1110]	training's l2: 17.9729	valid_1's l2: 20.2818
[1120]	training's l2: 17.9463	valid_1's l2: 20.2802
[1130]	training's l2: 17.9203	valid_1's l2: 20.2795
[1140]	training's l2: 17.8938	valid_1's l2: 20.2784
[1150]	training's l2: 17.8681	valid_1's l2: 20.2787
[1160]	training's l2: 17.8423	valid_1's l2: 20.2779
[1170]	training's l2: 17.8171	valid_1's l2: 20.2763
[1180]	training's l2: 17.7905	valid_1's l2: 20.2757
[1190]	training's l2: 17.7646	valid_1's l2: 20.2748
[1200]	training's l2: 17.7396	valid_1's l2: 20.2739
[1210]	training's l2: 17.7144	valid_1's l2: 20.2729
[1220]	training's l2: 17.6892	valid_1's l2: 20.2725
[1230]	training's l2: 17.6638	valid_1's l2: 20.2709
[1240]	training's l2: 17.639	valid_1's l2: 20.2694
[1250]	training's l2: 17.6143	valid_1's l2: 20.2686
[1260]	training's l2: 17.5897	valid_1's l2: 20.2683
[1270]	training's l2: 17.5654	valid_1's l2: 20.267
[1280]	training's l2: 17.5401	valid_1's l2: 20.2662
[1290]	training's l2: 17.5168	valid_1's l2: 20.2653
[1300]	training's l2: 17.4926	valid_1's l2: 20.2637
[1310]	training's l2: 17.468	valid_1's l2: 20.2636
[1320]	training's l2: 17.4443	valid_1's l2: 20.2634
[1330]	training's l2: 17.4207	valid_1's l2: 20.2629
[1340]	training's l2: 17.3968	valid_1's l2: 20.2625
[1350]	training's l2: 17.3733	valid_1's l2: 20.2614
[1360]	training's l2: 17.3494	valid_1's l2: 20.2603
[1370]	training's l2: 17.3261	valid_1's l2: 20.2601
[1380]	training's l2: 17.3029	valid_1's l2: 20.2592
[1390]	training's l2: 17.28	valid_1's l2: 20.2589
[1400]	training's l2: 17.2563	valid_1's l2: 20.2583
[1410]	training's l2: 17.2337	valid_1's l2: 20.2579
[1420]	training's l2: 17.2099	valid_1's l2: 20.2571
[1430]	training's l2: 17.1872	valid_1's l2: 20.2569
[1440]	training's l2: 17.1646	valid_1's l2: 20.2575
[1450]	training's l2: 17.1409	valid_1's l2: 20.2569
[1460]	training's l2: 17.1194	valid_1's l2: 20.2558
[1470]	training's l2: 17.0966	valid_1's l2: 20.2565
[1480]	training's l2: 17.0739	valid_1's l2: 20.2557
[1490]	training's l2: 17.0516	valid_1's l2: 20.255
[1500]	training's l2: 17.028	valid_1's l2: 20.2546
[1510]	training's l2: 17.0057	valid_1's l2: 20.254
[1520]	training's l2: 16.9834	valid_1's l2: 20.2534
[1530]	training's l2: 16.961	valid_1's l2: 20.2538
[1540]	training's l2: 16.9387	valid_1's l2: 20.2536
[1550]	training's l2: 16.9165	valid_1's l2: 20.2533
[1560]	training's l2: 16.8951	valid_1's l2: 20.2536
[1570]	training's l2: 16.8725	valid_1's l2: 20.2528
[1580]	training's l2: 16.8511	valid_1's l2: 20.2529
[1590]	training's l2: 16.8284	valid_1's l2: 20.2531
[1600]	training's l2: 16.8072	valid_1's l2: 20.2535
[1610]	training's l2: 16.786	valid_1's l2: 20.2532
Early stopping, best iteration is:
[1581]	training's l2: 16.8487	valid_1's l2: 20.2526
score1: 3.836524775345533
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.270774 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.4806	valid_1's l2: 30.5854
[20]	training's l2: 29.457	valid_1's l2: 28.5832
[30]	training's l2: 27.8775	valid_1's l2: 27.0246
[40]	training's l2: 26.6375	valid_1's l2: 25.806
[50]	training's l2: 25.6581	valid_1's l2: 24.8493
[60]	training's l2: 24.8774	valid_1's l2: 24.095
[70]	training's l2: 24.2513	valid_1's l2: 23.4972
[80]	training's l2: 23.7452	valid_1's l2: 23.0218
[90]	training's l2: 23.3321	valid_1's l2: 22.6414
[100]	training's l2: 22.9936	valid_1's l2: 22.3349
[110]	training's l2: 22.7135	valid_1's l2: 22.0885
[120]	training's l2: 22.4746	valid_1's l2: 21.8833
[130]	training's l2: 22.2688	valid_1's l2: 21.714
[140]	training's l2: 22.0901	valid_1's l2: 21.5708
[150]	training's l2: 21.9345	valid_1's l2: 21.4514
[160]	training's l2: 21.7921	valid_1's l2: 21.3478
[170]	training's l2: 21.6663	valid_1's l2: 21.2609
[180]	training's l2: 21.5516	valid_1's l2: 21.1869
[190]	training's l2: 21.4442	valid_1's l2: 21.1165
[200]	training's l2: 21.3478	valid_1's l2: 21.0571
[210]	training's l2: 21.2573	valid_1's l2: 21.0043
[220]	training's l2: 21.1727	valid_1's l2: 20.9561
[230]	training's l2: 21.0936	valid_1's l2: 20.914
[240]	training's l2: 21.0196	valid_1's l2: 20.8758
[250]	training's l2: 20.9444	valid_1's l2: 20.8361
[260]	training's l2: 20.8736	valid_1's l2: 20.8021
[270]	training's l2: 20.8062	valid_1's l2: 20.7737
[280]	training's l2: 20.741	valid_1's l2: 20.7472
[290]	training's l2: 20.6783	valid_1's l2: 20.7222
[300]	training's l2: 20.6193	valid_1's l2: 20.6997
[310]	training's l2: 20.5611	valid_1's l2: 20.6781
[320]	training's l2: 20.5057	valid_1's l2: 20.6594
[330]	training's l2: 20.4516	valid_1's l2: 20.6426
[340]	training's l2: 20.3982	valid_1's l2: 20.6264
[350]	training's l2: 20.3468	valid_1's l2: 20.6124
[360]	training's l2: 20.2955	valid_1's l2: 20.5971
[370]	training's l2: 20.245	valid_1's l2: 20.5832
[380]	training's l2: 20.1957	valid_1's l2: 20.5704
[390]	training's l2: 20.1481	valid_1's l2: 20.5583
[400]	training's l2: 20.1007	valid_1's l2: 20.5464
[410]	training's l2: 20.0547	valid_1's l2: 20.536
[420]	training's l2: 20.0091	valid_1's l2: 20.5265
[430]	training's l2: 19.9637	valid_1's l2: 20.5168
[440]	training's l2: 19.9189	valid_1's l2: 20.5085
[450]	training's l2: 19.8743	valid_1's l2: 20.5004
[460]	training's l2: 19.8296	valid_1's l2: 20.4914
[470]	training's l2: 19.7862	valid_1's l2: 20.4824
[480]	training's l2: 19.7424	valid_1's l2: 20.4748
[490]	training's l2: 19.6992	valid_1's l2: 20.4674
[500]	training's l2: 19.6573	valid_1's l2: 20.4609
[510]	training's l2: 19.6139	valid_1's l2: 20.454
[520]	training's l2: 19.572	valid_1's l2: 20.4471
[530]	training's l2: 19.5292	valid_1's l2: 20.4398
[540]	training's l2: 19.4876	valid_1's l2: 20.4313
[550]	training's l2: 19.4461	valid_1's l2: 20.4234
[560]	training's l2: 19.4049	valid_1's l2: 20.4169
[570]	training's l2: 19.3639	valid_1's l2: 20.4112
[580]	training's l2: 19.3239	valid_1's l2: 20.4041
[590]	training's l2: 19.2846	valid_1's l2: 20.3993
[600]	training's l2: 19.245	valid_1's l2: 20.3949
[610]	training's l2: 19.2066	valid_1's l2: 20.3895
[620]	training's l2: 19.1683	valid_1's l2: 20.3846
[630]	training's l2: 19.1303	valid_1's l2: 20.3802
[640]	training's l2: 19.0921	valid_1's l2: 20.3751
[650]	training's l2: 19.0547	valid_1's l2: 20.3726
[660]	training's l2: 19.0166	valid_1's l2: 20.3697
[670]	training's l2: 18.9789	valid_1's l2: 20.3671
[680]	training's l2: 18.9411	valid_1's l2: 20.3633
[690]	training's l2: 18.9051	valid_1's l2: 20.3588
[700]	training's l2: 18.8684	valid_1's l2: 20.3549
[710]	training's l2: 18.8325	valid_1's l2: 20.3521
[720]	training's l2: 18.7958	valid_1's l2: 20.3484
[730]	training's l2: 18.7593	valid_1's l2: 20.3452
[740]	training's l2: 18.7236	valid_1's l2: 20.3419
[750]	training's l2: 18.6879	valid_1's l2: 20.3382
[760]	training's l2: 18.6528	valid_1's l2: 20.3357
[770]	training's l2: 18.6175	valid_1's l2: 20.3325
[780]	training's l2: 18.5828	valid_1's l2: 20.329
[790]	training's l2: 18.5476	valid_1's l2: 20.326
[800]	training's l2: 18.5134	valid_1's l2: 20.3222
[810]	training's l2: 18.48	valid_1's l2: 20.3207
[820]	training's l2: 18.4454	valid_1's l2: 20.3172
[830]	training's l2: 18.4114	valid_1's l2: 20.3133
[840]	training's l2: 18.3774	valid_1's l2: 20.3104
[850]	training's l2: 18.3444	valid_1's l2: 20.3076
[860]	training's l2: 18.3118	valid_1's l2: 20.3054
[870]	training's l2: 18.2784	valid_1's l2: 20.302
[880]	training's l2: 18.2453	valid_1's l2: 20.2996
[890]	training's l2: 18.2129	valid_1's l2: 20.2974
[900]	training's l2: 18.1807	valid_1's l2: 20.2955
[910]	training's l2: 18.1489	valid_1's l2: 20.2929
[920]	training's l2: 18.1164	valid_1's l2: 20.2913
[930]	training's l2: 18.0845	valid_1's l2: 20.2884
[940]	training's l2: 18.0527	valid_1's l2: 20.2864
[950]	training's l2: 18.0215	valid_1's l2: 20.2852
[960]	training's l2: 17.9908	valid_1's l2: 20.2834
[970]	training's l2: 17.9591	valid_1's l2: 20.2811
[980]	training's l2: 17.9282	valid_1's l2: 20.2802
[990]	training's l2: 17.897	valid_1's l2: 20.2786
[1000]	training's l2: 17.866	valid_1's l2: 20.2774
[1010]	training's l2: 17.8363	valid_1's l2: 20.2763
[1020]	training's l2: 17.8069	valid_1's l2: 20.2758
[1030]	training's l2: 17.7768	valid_1's l2: 20.2749
[1040]	training's l2: 17.7473	valid_1's l2: 20.2746
[1050]	training's l2: 17.7177	valid_1's l2: 20.2737
[1060]	training's l2: 17.6897	valid_1's l2: 20.272
[1070]	training's l2: 17.6595	valid_1's l2: 20.2711
[1080]	training's l2: 17.6307	valid_1's l2: 20.2699
[1090]	training's l2: 17.602	valid_1's l2: 20.2697
[1100]	training's l2: 17.5747	valid_1's l2: 20.2691
[1110]	training's l2: 17.5464	valid_1's l2: 20.2682
[1120]	training's l2: 17.518	valid_1's l2: 20.2676
[1130]	training's l2: 17.4901	valid_1's l2: 20.2678
[1140]	training's l2: 17.4622	valid_1's l2: 20.2665
[1150]	training's l2: 17.4345	valid_1's l2: 20.2669
[1160]	training's l2: 17.4074	valid_1's l2: 20.2663
[1170]	training's l2: 17.3805	valid_1's l2: 20.267
[1180]	training's l2: 17.3529	valid_1's l2: 20.2656
[1190]	training's l2: 17.3263	valid_1's l2: 20.2657
[1200]	training's l2: 17.2991	valid_1's l2: 20.2656
[1210]	training's l2: 17.2725	valid_1's l2: 20.2654
[1220]	training's l2: 17.2441	valid_1's l2: 20.2648
[1230]	training's l2: 17.2169	valid_1's l2: 20.2649
[1240]	training's l2: 17.1906	valid_1's l2: 20.2645
[1250]	training's l2: 17.1647	valid_1's l2: 20.263
[1260]	training's l2: 17.1373	valid_1's l2: 20.2623
[1270]	training's l2: 17.1109	valid_1's l2: 20.2625
[1280]	training's l2: 17.0841	valid_1's l2: 20.2627
[1290]	training's l2: 17.0583	valid_1's l2: 20.2623
[1300]	training's l2: 17.0315	valid_1's l2: 20.2609
[1310]	training's l2: 17.0051	valid_1's l2: 20.2596
[1320]	training's l2: 16.9794	valid_1's l2: 20.2582
[1330]	training's l2: 16.9543	valid_1's l2: 20.2574
[1340]	training's l2: 16.9287	valid_1's l2: 20.2575
[1350]	training's l2: 16.9039	valid_1's l2: 20.2565
[1360]	training's l2: 16.8786	valid_1's l2: 20.2564
[1370]	training's l2: 16.8533	valid_1's l2: 20.2572
[1380]	training's l2: 16.8278	valid_1's l2: 20.257
Early stopping, best iteration is:
[1353]	training's l2: 16.896	valid_1's l2: 20.2561
score1: 3.838014327662528
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.261885 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.953	valid_1's l2: 31.0515
[20]	training's l2: 30.2137	valid_1's l2: 29.3254
[30]	training's l2: 28.7883	valid_1's l2: 27.9117
[40]	training's l2: 27.6196	valid_1's l2: 26.7575
[50]	training's l2: 26.656	valid_1's l2: 25.8091
[60]	training's l2: 25.8584	valid_1's l2: 25.0278
[70]	training's l2: 25.1948	valid_1's l2: 24.3812
[80]	training's l2: 24.6402	valid_1's l2: 23.8475
[90]	training's l2: 24.1754	valid_1's l2: 23.4023
[100]	training's l2: 23.7825	valid_1's l2: 23.0324
[110]	training's l2: 23.4489	valid_1's l2: 22.722
[120]	training's l2: 23.1649	valid_1's l2: 22.462
[130]	training's l2: 22.9226	valid_1's l2: 22.2426
[140]	training's l2: 22.7138	valid_1's l2: 22.0572
[150]	training's l2: 22.5273	valid_1's l2: 21.8977
[160]	training's l2: 22.3637	valid_1's l2: 21.7611
[170]	training's l2: 22.2173	valid_1's l2: 21.6403
[180]	training's l2: 22.0874	valid_1's l2: 21.5362
[190]	training's l2: 21.9685	valid_1's l2: 21.4453
[200]	training's l2: 21.8589	valid_1's l2: 21.3637
[210]	training's l2: 21.7603	valid_1's l2: 21.2936
[220]	training's l2: 21.6682	valid_1's l2: 21.2304
[230]	training's l2: 21.5805	valid_1's l2: 21.1701
[240]	training's l2: 21.4999	valid_1's l2: 21.1179
[250]	training's l2: 21.4249	valid_1's l2: 21.07
[260]	training's l2: 21.3545	valid_1's l2: 21.0259
[270]	training's l2: 21.2873	valid_1's l2: 20.9855
[280]	training's l2: 21.2235	valid_1's l2: 20.9485
[290]	training's l2: 21.1645	valid_1's l2: 20.9169
[300]	training's l2: 21.1072	valid_1's l2: 20.8863
[310]	training's l2: 21.0487	valid_1's l2: 20.8549
[320]	training's l2: 20.9933	valid_1's l2: 20.8265
[330]	training's l2: 20.9397	valid_1's l2: 20.8004
[340]	training's l2: 20.8879	valid_1's l2: 20.7774
[350]	training's l2: 20.8386	valid_1's l2: 20.7561
[360]	training's l2: 20.7907	valid_1's l2: 20.7352
[370]	training's l2: 20.7453	valid_1's l2: 20.7175
[380]	training's l2: 20.7013	valid_1's l2: 20.7012
[390]	training's l2: 20.6574	valid_1's l2: 20.6841
[400]	training's l2: 20.6151	valid_1's l2: 20.6696
[410]	training's l2: 20.5736	valid_1's l2: 20.6551
[420]	training's l2: 20.5338	valid_1's l2: 20.6433
[430]	training's l2: 20.4945	valid_1's l2: 20.6306
[440]	training's l2: 20.4548	valid_1's l2: 20.6185
[450]	training's l2: 20.4159	valid_1's l2: 20.6058
[460]	training's l2: 20.3779	valid_1's l2: 20.5949
[470]	training's l2: 20.3401	valid_1's l2: 20.583
[480]	training's l2: 20.3033	valid_1's l2: 20.5732
[490]	training's l2: 20.2666	valid_1's l2: 20.5615
[500]	training's l2: 20.2309	valid_1's l2: 20.5515
[510]	training's l2: 20.1956	valid_1's l2: 20.5431
[520]	training's l2: 20.1606	valid_1's l2: 20.5354
[530]	training's l2: 20.1255	valid_1's l2: 20.5272
[540]	training's l2: 20.0913	valid_1's l2: 20.5201
[550]	training's l2: 20.057	valid_1's l2: 20.5131
[560]	training's l2: 20.0234	valid_1's l2: 20.5056
[570]	training's l2: 19.9899	valid_1's l2: 20.4993
[580]	training's l2: 19.9567	valid_1's l2: 20.4936
[590]	training's l2: 19.9242	valid_1's l2: 20.4875
[600]	training's l2: 19.8911	valid_1's l2: 20.4803
[610]	training's l2: 19.8587	valid_1's l2: 20.4743
[620]	training's l2: 19.8265	valid_1's l2: 20.4689
[630]	training's l2: 19.7946	valid_1's l2: 20.4632
[640]	training's l2: 19.7623	valid_1's l2: 20.4573
[650]	training's l2: 19.7304	valid_1's l2: 20.4508
[660]	training's l2: 19.6984	valid_1's l2: 20.4452
[670]	training's l2: 19.6667	valid_1's l2: 20.4396
[680]	training's l2: 19.6354	valid_1's l2: 20.4346
[690]	training's l2: 19.6045	valid_1's l2: 20.4284
[700]	training's l2: 19.5736	valid_1's l2: 20.4229
[710]	training's l2: 19.5428	valid_1's l2: 20.4171
[720]	training's l2: 19.5127	valid_1's l2: 20.4116
[730]	training's l2: 19.4827	valid_1's l2: 20.4075
[740]	training's l2: 19.4522	valid_1's l2: 20.4027
[750]	training's l2: 19.4229	valid_1's l2: 20.3977
[760]	training's l2: 19.3932	valid_1's l2: 20.3942
[770]	training's l2: 19.3638	valid_1's l2: 20.3906
[780]	training's l2: 19.3349	valid_1's l2: 20.386
[790]	training's l2: 19.3063	valid_1's l2: 20.3833
[800]	training's l2: 19.2772	valid_1's l2: 20.3786
[810]	training's l2: 19.2486	valid_1's l2: 20.3759
[820]	training's l2: 19.2202	valid_1's l2: 20.3718
[830]	training's l2: 19.1915	valid_1's l2: 20.3685
[840]	training's l2: 19.1634	valid_1's l2: 20.3652
[850]	training's l2: 19.1353	valid_1's l2: 20.3619
[860]	training's l2: 19.1074	valid_1's l2: 20.3589
[870]	training's l2: 19.0798	valid_1's l2: 20.3557
[880]	training's l2: 19.052	valid_1's l2: 20.3525
[890]	training's l2: 19.0246	valid_1's l2: 20.3505
[900]	training's l2: 18.9969	valid_1's l2: 20.3473
[910]	training's l2: 18.9699	valid_1's l2: 20.3457
[920]	training's l2: 18.9427	valid_1's l2: 20.3437
[930]	training's l2: 18.9164	valid_1's l2: 20.3425
[940]	training's l2: 18.8894	valid_1's l2: 20.3397
[950]	training's l2: 18.863	valid_1's l2: 20.3364
[960]	training's l2: 18.8367	valid_1's l2: 20.3339
[970]	training's l2: 18.8108	valid_1's l2: 20.3316
[980]	training's l2: 18.7847	valid_1's l2: 20.3286
[990]	training's l2: 18.7587	valid_1's l2: 20.3274
[1000]	training's l2: 18.7322	valid_1's l2: 20.3248
[1010]	training's l2: 18.7061	valid_1's l2: 20.3236
[1020]	training's l2: 18.6802	valid_1's l2: 20.3213
[1030]	training's l2: 18.6547	valid_1's l2: 20.3198
[1040]	training's l2: 18.6288	valid_1's l2: 20.3171
[1050]	training's l2: 18.6031	valid_1's l2: 20.3148
[1060]	training's l2: 18.578	valid_1's l2: 20.3122
[1070]	training's l2: 18.5525	valid_1's l2: 20.31
[1080]	training's l2: 18.5273	valid_1's l2: 20.3083
[1090]	training's l2: 18.5027	valid_1's l2: 20.307
[1100]	training's l2: 18.477	valid_1's l2: 20.3041
[1110]	training's l2: 18.452	valid_1's l2: 20.3017
[1120]	training's l2: 18.4273	valid_1's l2: 20.2999
[1130]	training's l2: 18.4026	valid_1's l2: 20.298
[1140]	training's l2: 18.3783	valid_1's l2: 20.296
[1150]	training's l2: 18.354	valid_1's l2: 20.2944
[1160]	training's l2: 18.3305	valid_1's l2: 20.293
[1170]	training's l2: 18.3065	valid_1's l2: 20.2914
[1180]	training's l2: 18.2829	valid_1's l2: 20.2897
[1190]	training's l2: 18.259	valid_1's l2: 20.2878
[1200]	training's l2: 18.2355	valid_1's l2: 20.2864
[1210]	training's l2: 18.2125	valid_1's l2: 20.2849
[1220]	training's l2: 18.1892	valid_1's l2: 20.2838
[1230]	training's l2: 18.1655	valid_1's l2: 20.2825
[1240]	training's l2: 18.1423	valid_1's l2: 20.2815
[1250]	training's l2: 18.12	valid_1's l2: 20.2803
[1260]	training's l2: 18.0967	valid_1's l2: 20.2788
[1270]	training's l2: 18.0737	valid_1's l2: 20.2775
[1280]	training's l2: 18.0513	valid_1's l2: 20.2766
[1290]	training's l2: 18.0279	valid_1's l2: 20.2755
[1300]	training's l2: 18.006	valid_1's l2: 20.2744
[1310]	training's l2: 17.9839	valid_1's l2: 20.2732
[1320]	training's l2: 17.9615	valid_1's l2: 20.2726
[1330]	training's l2: 17.9392	valid_1's l2: 20.2712
[1340]	training's l2: 17.9177	valid_1's l2: 20.271
[1350]	training's l2: 17.8953	valid_1's l2: 20.2697
[1360]	training's l2: 17.8729	valid_1's l2: 20.2689
[1370]	training's l2: 17.8514	valid_1's l2: 20.2678
[1380]	training's l2: 17.8299	valid_1's l2: 20.2673
[1390]	training's l2: 17.8084	valid_1's l2: 20.2663
[1400]	training's l2: 17.7871	valid_1's l2: 20.2656
[1410]	training's l2: 17.7656	valid_1's l2: 20.2646
[1420]	training's l2: 17.7442	valid_1's l2: 20.2634
[1430]	training's l2: 17.7234	valid_1's l2: 20.2621
[1440]	training's l2: 17.7022	valid_1's l2: 20.2614
[1450]	training's l2: 17.681	valid_1's l2: 20.2607
[1460]	training's l2: 17.6603	valid_1's l2: 20.2611
[1470]	training's l2: 17.6392	valid_1's l2: 20.2599
[1480]	training's l2: 17.6185	valid_1's l2: 20.2594
[1490]	training's l2: 17.5975	valid_1's l2: 20.2586
[1500]	training's l2: 17.5768	valid_1's l2: 20.2578
[1510]	training's l2: 17.5566	valid_1's l2: 20.2573
[1520]	training's l2: 17.5362	valid_1's l2: 20.2573
[1530]	training's l2: 17.5158	valid_1's l2: 20.257
[1540]	training's l2: 17.4965	valid_1's l2: 20.2563
[1550]	training's l2: 17.4761	valid_1's l2: 20.2557
[1560]	training's l2: 17.4569	valid_1's l2: 20.2559
[1570]	training's l2: 17.4361	valid_1's l2: 20.255
[1580]	training's l2: 17.4157	valid_1's l2: 20.2546
[1590]	training's l2: 17.3954	valid_1's l2: 20.2542
[1600]	training's l2: 17.3759	valid_1's l2: 20.2539
[1610]	training's l2: 17.3556	valid_1's l2: 20.253
[1620]	training's l2: 17.3357	valid_1's l2: 20.2518
[1630]	training's l2: 17.3163	valid_1's l2: 20.2518
[1640]	training's l2: 17.2969	valid_1's l2: 20.251
[1650]	training's l2: 17.2765	valid_1's l2: 20.2496
[1660]	training's l2: 17.2567	valid_1's l2: 20.2483
[1670]	training's l2: 17.2367	valid_1's l2: 20.2484
[1680]	training's l2: 17.218	valid_1's l2: 20.2477
[1690]	training's l2: 17.1988	valid_1's l2: 20.2469
[1700]	training's l2: 17.1791	valid_1's l2: 20.2471
[1710]	training's l2: 17.1597	valid_1's l2: 20.2473
[1720]	training's l2: 17.14	valid_1's l2: 20.2467
[1730]	training's l2: 17.1206	valid_1's l2: 20.2468
[1740]	training's l2: 17.1013	valid_1's l2: 20.2463
[1750]	training's l2: 17.0835	valid_1's l2: 20.2459
[1760]	training's l2: 17.0638	valid_1's l2: 20.2459
[1770]	training's l2: 17.0454	valid_1's l2: 20.2459
[1780]	training's l2: 17.026	valid_1's l2: 20.2462
Early stopping, best iteration is:
[1755]	training's l2: 17.0734	valid_1's l2: 20.2456
score1: 3.8349605585381625
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.243874 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.3109	valid_1's l2: 30.408
[20]	training's l2: 29.203	valid_1's l2: 28.313
[30]	training's l2: 27.5921	valid_1's l2: 26.7137
[40]	training's l2: 26.352	valid_1's l2: 25.4895
[50]	training's l2: 25.3903	valid_1's l2: 24.5469
[60]	training's l2: 24.6385	valid_1's l2: 23.817
[70]	training's l2: 24.0452	valid_1's l2: 23.2474
[80]	training's l2: 23.5718	valid_1's l2: 22.7997
[90]	training's l2: 23.1941	valid_1's l2: 22.4484
[100]	training's l2: 22.8892	valid_1's l2: 22.1707
[110]	training's l2: 22.6325	valid_1's l2: 21.9422
[120]	training's l2: 22.417	valid_1's l2: 21.7566
[130]	training's l2: 22.2339	valid_1's l2: 21.6033
[140]	training's l2: 22.0739	valid_1's l2: 21.476
[150]	training's l2: 21.9339	valid_1's l2: 21.3684
[160]	training's l2: 21.8088	valid_1's l2: 21.2763
[170]	training's l2: 21.6928	valid_1's l2: 21.1915
[180]	training's l2: 21.589	valid_1's l2: 21.1201
[190]	training's l2: 21.495	valid_1's l2: 21.0579
[200]	training's l2: 21.4081	valid_1's l2: 21.0025
[210]	training's l2: 21.3296	valid_1's l2: 20.9557
[220]	training's l2: 21.2551	valid_1's l2: 20.9129
[230]	training's l2: 21.1826	valid_1's l2: 20.8715
[240]	training's l2: 21.1132	valid_1's l2: 20.8344
[250]	training's l2: 21.0481	valid_1's l2: 20.8013
[260]	training's l2: 20.9855	valid_1's l2: 20.7717
[270]	training's l2: 20.9276	valid_1's l2: 20.7457
[280]	training's l2: 20.8709	valid_1's l2: 20.7209
[290]	training's l2: 20.8167	valid_1's l2: 20.6979
[300]	training's l2: 20.7635	valid_1's l2: 20.676
[310]	training's l2: 20.7129	valid_1's l2: 20.6573
[320]	training's l2: 20.6632	valid_1's l2: 20.6394
[330]	training's l2: 20.6145	valid_1's l2: 20.6231
[340]	training's l2: 20.5671	valid_1's l2: 20.6078
[350]	training's l2: 20.5205	valid_1's l2: 20.5931
[360]	training's l2: 20.4754	valid_1's l2: 20.5783
[370]	training's l2: 20.4311	valid_1's l2: 20.5644
[380]	training's l2: 20.3871	valid_1's l2: 20.5538
[390]	training's l2: 20.3444	valid_1's l2: 20.544
[400]	training's l2: 20.3016	valid_1's l2: 20.532
[410]	training's l2: 20.2596	valid_1's l2: 20.5216
[420]	training's l2: 20.2187	valid_1's l2: 20.5114
[430]	training's l2: 20.1778	valid_1's l2: 20.5025
[440]	training's l2: 20.137	valid_1's l2: 20.4926
[450]	training's l2: 20.0966	valid_1's l2: 20.4834
[460]	training's l2: 20.0577	valid_1's l2: 20.4759
[470]	training's l2: 20.0182	valid_1's l2: 20.4677
[480]	training's l2: 19.9793	valid_1's l2: 20.4608
[490]	training's l2: 19.9406	valid_1's l2: 20.4536
[500]	training's l2: 19.9022	valid_1's l2: 20.4457
[510]	training's l2: 19.8635	valid_1's l2: 20.4379
[520]	training's l2: 19.8261	valid_1's l2: 20.4327
[530]	training's l2: 19.7882	valid_1's l2: 20.4255
[540]	training's l2: 19.7511	valid_1's l2: 20.4191
[550]	training's l2: 19.7148	valid_1's l2: 20.4125
[560]	training's l2: 19.678	valid_1's l2: 20.4063
[570]	training's l2: 19.6419	valid_1's l2: 20.401
[580]	training's l2: 19.6066	valid_1's l2: 20.3962
[590]	training's l2: 19.5716	valid_1's l2: 20.3916
[600]	training's l2: 19.5366	valid_1's l2: 20.3867
[610]	training's l2: 19.5013	valid_1's l2: 20.3809
[620]	training's l2: 19.4676	valid_1's l2: 20.3769
[630]	training's l2: 19.4338	valid_1's l2: 20.3722
[640]	training's l2: 19.4001	valid_1's l2: 20.3675
[650]	training's l2: 19.3656	valid_1's l2: 20.3641
[660]	training's l2: 19.332	valid_1's l2: 20.3604
[670]	training's l2: 19.2986	valid_1's l2: 20.356
[680]	training's l2: 19.2649	valid_1's l2: 20.3521
[690]	training's l2: 19.2322	valid_1's l2: 20.348
[700]	training's l2: 19.1995	valid_1's l2: 20.3436
[710]	training's l2: 19.1674	valid_1's l2: 20.3405
[720]	training's l2: 19.135	valid_1's l2: 20.3357
[730]	training's l2: 19.1027	valid_1's l2: 20.3328
[740]	training's l2: 19.0704	valid_1's l2: 20.3296
[750]	training's l2: 19.0384	valid_1's l2: 20.3267
[760]	training's l2: 19.0069	valid_1's l2: 20.323
[770]	training's l2: 18.9768	valid_1's l2: 20.3201
[780]	training's l2: 18.9457	valid_1's l2: 20.3178
[790]	training's l2: 18.9145	valid_1's l2: 20.3149
[800]	training's l2: 18.8833	valid_1's l2: 20.3112
[810]	training's l2: 18.8535	valid_1's l2: 20.3088
[820]	training's l2: 18.8224	valid_1's l2: 20.306
[830]	training's l2: 18.7932	valid_1's l2: 20.3047
[840]	training's l2: 18.7628	valid_1's l2: 20.3017
[850]	training's l2: 18.7331	valid_1's l2: 20.2995
[860]	training's l2: 18.704	valid_1's l2: 20.2976
[870]	training's l2: 18.675	valid_1's l2: 20.2963
[880]	training's l2: 18.6458	valid_1's l2: 20.2951
[890]	training's l2: 18.6158	valid_1's l2: 20.2928
[900]	training's l2: 18.5872	valid_1's l2: 20.2908
[910]	training's l2: 18.5583	valid_1's l2: 20.2894
[920]	training's l2: 18.5301	valid_1's l2: 20.2873
[930]	training's l2: 18.501	valid_1's l2: 20.2859
[940]	training's l2: 18.4722	valid_1's l2: 20.2845
[950]	training's l2: 18.4447	valid_1's l2: 20.2841
[960]	training's l2: 18.4174	valid_1's l2: 20.2834
[970]	training's l2: 18.3907	valid_1's l2: 20.2825
[980]	training's l2: 18.3636	valid_1's l2: 20.2802
[990]	training's l2: 18.3365	valid_1's l2: 20.279
[1000]	training's l2: 18.3092	valid_1's l2: 20.278
[1010]	training's l2: 18.282	valid_1's l2: 20.277
[1020]	training's l2: 18.2564	valid_1's l2: 20.2765
[1030]	training's l2: 18.2302	valid_1's l2: 20.2758
[1040]	training's l2: 18.2044	valid_1's l2: 20.2743
[1050]	training's l2: 18.1776	valid_1's l2: 20.2729
[1060]	training's l2: 18.1522	valid_1's l2: 20.2726
[1070]	training's l2: 18.1252	valid_1's l2: 20.2718
[1080]	training's l2: 18.1004	valid_1's l2: 20.2712
[1090]	training's l2: 18.0744	valid_1's l2: 20.2699
[1100]	training's l2: 18.048	valid_1's l2: 20.2702
[1110]	training's l2: 18.023	valid_1's l2: 20.2694
[1120]	training's l2: 17.9981	valid_1's l2: 20.2684
[1130]	training's l2: 17.9737	valid_1's l2: 20.2679
[1140]	training's l2: 17.9494	valid_1's l2: 20.2664
[1150]	training's l2: 17.9237	valid_1's l2: 20.2659
[1160]	training's l2: 17.8992	valid_1's l2: 20.2647
[1170]	training's l2: 17.8736	valid_1's l2: 20.2628
[1180]	training's l2: 17.8509	valid_1's l2: 20.2626
[1190]	training's l2: 17.8269	valid_1's l2: 20.2626
[1200]	training's l2: 17.8024	valid_1's l2: 20.2625
[1210]	training's l2: 17.7789	valid_1's l2: 20.2618
[1220]	training's l2: 17.755	valid_1's l2: 20.2608
[1230]	training's l2: 17.7298	valid_1's l2: 20.2597
[1240]	training's l2: 17.7052	valid_1's l2: 20.2586
[1250]	training's l2: 17.6817	valid_1's l2: 20.2579
[1260]	training's l2: 17.6582	valid_1's l2: 20.2567
[1270]	training's l2: 17.6338	valid_1's l2: 20.2578
[1280]	training's l2: 17.6107	valid_1's l2: 20.2579
[1290]	training's l2: 17.5878	valid_1's l2: 20.257
Early stopping, best iteration is:
[1260]	training's l2: 17.6582	valid_1's l2: 20.2567
score1: 3.8378619146529287
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.250119 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.9416	valid_1's l2: 31.0326
[20]	training's l2: 30.2016	valid_1's l2: 29.2982
[30]	training's l2: 28.7818	valid_1's l2: 27.8844
[40]	training's l2: 27.6227	valid_1's l2: 26.7329
[50]	training's l2: 26.6694	valid_1's l2: 25.7863
[60]	training's l2: 25.883	valid_1's l2: 25.0098
[70]	training's l2: 25.2306	valid_1's l2: 24.3699
[80]	training's l2: 24.688	valid_1's l2: 23.8407
[90]	training's l2: 24.2349	valid_1's l2: 23.4009
[100]	training's l2: 23.8531	valid_1's l2: 23.0344
[110]	training's l2: 23.5312	valid_1's l2: 22.729
[120]	training's l2: 23.2593	valid_1's l2: 22.4746
[130]	training's l2: 23.0281	valid_1's l2: 22.261
[140]	training's l2: 22.8261	valid_1's l2: 22.0771
[150]	training's l2: 22.6481	valid_1's l2: 21.9175
[160]	training's l2: 22.4919	valid_1's l2: 21.7807
[170]	training's l2: 22.3541	valid_1's l2: 21.6629
[180]	training's l2: 22.2304	valid_1's l2: 21.561
[190]	training's l2: 22.1203	valid_1's l2: 21.4705
[200]	training's l2: 22.0197	valid_1's l2: 21.391
[210]	training's l2: 21.9271	valid_1's l2: 21.3201
[220]	training's l2: 21.8394	valid_1's l2: 21.2534
[230]	training's l2: 21.7598	valid_1's l2: 21.1951
[240]	training's l2: 21.6858	valid_1's l2: 21.1428
[250]	training's l2: 21.6168	valid_1's l2: 21.0948
[260]	training's l2: 21.5535	valid_1's l2: 21.0517
[270]	training's l2: 21.4936	valid_1's l2: 21.0122
[280]	training's l2: 21.4373	valid_1's l2: 20.977
[290]	training's l2: 21.3836	valid_1's l2: 20.9433
[300]	training's l2: 21.331	valid_1's l2: 20.9119
[310]	training's l2: 21.2801	valid_1's l2: 20.8819
[320]	training's l2: 21.2319	valid_1's l2: 20.8553
[330]	training's l2: 21.1855	valid_1's l2: 20.8305
[340]	training's l2: 21.1414	valid_1's l2: 20.8072
[350]	training's l2: 21.0978	valid_1's l2: 20.7859
[360]	training's l2: 21.0562	valid_1's l2: 20.7656
[370]	training's l2: 21.017	valid_1's l2: 20.7473
[380]	training's l2: 20.9788	valid_1's l2: 20.7294
[390]	training's l2: 20.9409	valid_1's l2: 20.7128
[400]	training's l2: 20.9043	valid_1's l2: 20.6975
[410]	training's l2: 20.8687	valid_1's l2: 20.6834
[420]	training's l2: 20.8338	valid_1's l2: 20.6703
[430]	training's l2: 20.7994	valid_1's l2: 20.6572
[440]	training's l2: 20.7651	valid_1's l2: 20.6433
[450]	training's l2: 20.7313	valid_1's l2: 20.6309
[460]	training's l2: 20.6985	valid_1's l2: 20.6192
[470]	training's l2: 20.6661	valid_1's l2: 20.6079
[480]	training's l2: 20.6346	valid_1's l2: 20.5978
[490]	training's l2: 20.6034	valid_1's l2: 20.587
[500]	training's l2: 20.5732	valid_1's l2: 20.5772
[510]	training's l2: 20.5433	valid_1's l2: 20.5683
[520]	training's l2: 20.5124	valid_1's l2: 20.5591
[530]	training's l2: 20.4836	valid_1's l2: 20.5516
[540]	training's l2: 20.4544	valid_1's l2: 20.5439
[550]	training's l2: 20.4254	valid_1's l2: 20.537
[560]	training's l2: 20.3974	valid_1's l2: 20.5292
[570]	training's l2: 20.3689	valid_1's l2: 20.5219
[580]	training's l2: 20.3407	valid_1's l2: 20.5135
[590]	training's l2: 20.3133	valid_1's l2: 20.5071
[600]	training's l2: 20.2859	valid_1's l2: 20.5006
[610]	training's l2: 20.2591	valid_1's l2: 20.4944
[620]	training's l2: 20.2322	valid_1's l2: 20.4884
[630]	training's l2: 20.205	valid_1's l2: 20.482
[640]	training's l2: 20.1782	valid_1's l2: 20.4769
[650]	training's l2: 20.1515	valid_1's l2: 20.4716
[660]	training's l2: 20.1238	valid_1's l2: 20.4643
[670]	training's l2: 20.0975	valid_1's l2: 20.4591
[680]	training's l2: 20.0713	valid_1's l2: 20.4541
[690]	training's l2: 20.045	valid_1's l2: 20.4476
[700]	training's l2: 20.0191	valid_1's l2: 20.4421
[710]	training's l2: 19.994	valid_1's l2: 20.4377
[720]	training's l2: 19.9685	valid_1's l2: 20.4315
[730]	training's l2: 19.9429	valid_1's l2: 20.4266
[740]	training's l2: 19.918	valid_1's l2: 20.4222
[750]	training's l2: 19.8931	valid_1's l2: 20.4172
[760]	training's l2: 19.8683	valid_1's l2: 20.413
[770]	training's l2: 19.8435	valid_1's l2: 20.4076
[780]	training's l2: 19.8192	valid_1's l2: 20.4039
[790]	training's l2: 19.795	valid_1's l2: 20.4001
[800]	training's l2: 19.7712	valid_1's l2: 20.3963
[810]	training's l2: 19.7474	valid_1's l2: 20.3921
[820]	training's l2: 19.7238	valid_1's l2: 20.388
[830]	training's l2: 19.7002	valid_1's l2: 20.3841
[840]	training's l2: 19.6766	valid_1's l2: 20.3802
[850]	training's l2: 19.6532	valid_1's l2: 20.3773
[860]	training's l2: 19.63	valid_1's l2: 20.3736
[870]	training's l2: 19.6066	valid_1's l2: 20.3705
[880]	training's l2: 19.5836	valid_1's l2: 20.3678
[890]	training's l2: 19.5607	valid_1's l2: 20.365
[900]	training's l2: 19.5385	valid_1's l2: 20.3617
[910]	training's l2: 19.5161	valid_1's l2: 20.3592
[920]	training's l2: 19.4935	valid_1's l2: 20.3561
[930]	training's l2: 19.471	valid_1's l2: 20.3524
[940]	training's l2: 19.449	valid_1's l2: 20.3503
[950]	training's l2: 19.4267	valid_1's l2: 20.3463
[960]	training's l2: 19.4044	valid_1's l2: 20.3438
[970]	training's l2: 19.3825	valid_1's l2: 20.3411
[980]	training's l2: 19.3609	valid_1's l2: 20.3386
[990]	training's l2: 19.3393	valid_1's l2: 20.3361
[1000]	training's l2: 19.3174	valid_1's l2: 20.333
[1010]	training's l2: 19.2957	valid_1's l2: 20.3313
[1020]	training's l2: 19.2744	valid_1's l2: 20.3295
[1030]	training's l2: 19.2528	valid_1's l2: 20.327
[1040]	training's l2: 19.231	valid_1's l2: 20.3248
[1050]	training's l2: 19.2099	valid_1's l2: 20.3219
[1060]	training's l2: 19.1885	valid_1's l2: 20.32
[1070]	training's l2: 19.1673	valid_1's l2: 20.3174
[1080]	training's l2: 19.1466	valid_1's l2: 20.3158
[1090]	training's l2: 19.1256	valid_1's l2: 20.3135
[1100]	training's l2: 19.1051	valid_1's l2: 20.3105
[1110]	training's l2: 19.0848	valid_1's l2: 20.3086
[1120]	training's l2: 19.0638	valid_1's l2: 20.3065
[1130]	training's l2: 19.0433	valid_1's l2: 20.305
[1140]	training's l2: 19.0228	valid_1's l2: 20.3031
[1150]	training's l2: 19.0026	valid_1's l2: 20.301
[1160]	training's l2: 18.9819	valid_1's l2: 20.299
[1170]	training's l2: 18.9618	valid_1's l2: 20.2968
[1180]	training's l2: 18.942	valid_1's l2: 20.2949
[1190]	training's l2: 18.9221	valid_1's l2: 20.2939
[1200]	training's l2: 18.902	valid_1's l2: 20.292
[1210]	training's l2: 18.882	valid_1's l2: 20.2906
[1220]	training's l2: 18.8622	valid_1's l2: 20.289
[1230]	training's l2: 18.8428	valid_1's l2: 20.2874
[1240]	training's l2: 18.8237	valid_1's l2: 20.2864
[1250]	training's l2: 18.8043	valid_1's l2: 20.2852
[1260]	training's l2: 18.785	valid_1's l2: 20.2841
[1270]	training's l2: 18.7665	valid_1's l2: 20.2832
[1280]	training's l2: 18.7479	valid_1's l2: 20.2823
[1290]	training's l2: 18.7301	valid_1's l2: 20.2818
[1300]	training's l2: 18.7113	valid_1's l2: 20.2807
[1310]	training's l2: 18.6929	valid_1's l2: 20.2802
[1320]	training's l2: 18.6746	valid_1's l2: 20.2789
[1330]	training's l2: 18.6568	valid_1's l2: 20.2778
[1340]	training's l2: 18.6381	valid_1's l2: 20.2765
[1350]	training's l2: 18.6198	valid_1's l2: 20.2754
[1360]	training's l2: 18.6011	valid_1's l2: 20.274
[1370]	training's l2: 18.5829	valid_1's l2: 20.2726
[1380]	training's l2: 18.5657	valid_1's l2: 20.2722
[1390]	training's l2: 18.5477	valid_1's l2: 20.2711
[1400]	training's l2: 18.5297	valid_1's l2: 20.2701
[1410]	training's l2: 18.512	valid_1's l2: 20.2694
[1420]	training's l2: 18.4938	valid_1's l2: 20.2682
[1430]	training's l2: 18.4762	valid_1's l2: 20.2667
[1440]	training's l2: 18.4586	valid_1's l2: 20.2658
[1450]	training's l2: 18.4409	valid_1's l2: 20.2648
[1460]	training's l2: 18.4238	valid_1's l2: 20.2642
[1470]	training's l2: 18.4062	valid_1's l2: 20.2628
[1480]	training's l2: 18.3882	valid_1's l2: 20.2615
[1490]	training's l2: 18.3719	valid_1's l2: 20.2616
[1500]	training's l2: 18.3543	valid_1's l2: 20.2607
[1510]	training's l2: 18.3362	valid_1's l2: 20.2593
[1520]	training's l2: 18.3187	valid_1's l2: 20.2581
[1530]	training's l2: 18.301	valid_1's l2: 20.2575
[1540]	training's l2: 18.284	valid_1's l2: 20.2566
[1550]	training's l2: 18.2669	valid_1's l2: 20.2558
[1560]	training's l2: 18.25	valid_1's l2: 20.2548
[1570]	training's l2: 18.2329	valid_1's l2: 20.2546
[1580]	training's l2: 18.2159	valid_1's l2: 20.2542
[1590]	training's l2: 18.1989	valid_1's l2: 20.2534
[1600]	training's l2: 18.1824	valid_1's l2: 20.2527
[1610]	training's l2: 18.1656	valid_1's l2: 20.252
[1620]	training's l2: 18.1483	valid_1's l2: 20.2511
[1630]	training's l2: 18.1314	valid_1's l2: 20.2508
[1640]	training's l2: 18.1154	valid_1's l2: 20.2499
[1650]	training's l2: 18.099	valid_1's l2: 20.2494
[1660]	training's l2: 18.0831	valid_1's l2: 20.248
[1670]	training's l2: 18.0666	valid_1's l2: 20.2471
[1680]	training's l2: 18.0493	valid_1's l2: 20.2466
[1690]	training's l2: 18.0332	valid_1's l2: 20.2461
[1700]	training's l2: 18.0168	valid_1's l2: 20.2463
[1710]	training's l2: 18.0004	valid_1's l2: 20.2453
[1720]	training's l2: 17.984	valid_1's l2: 20.2443
[1730]	training's l2: 17.9675	valid_1's l2: 20.2442
[1740]	training's l2: 17.9516	valid_1's l2: 20.244
[1750]	training's l2: 17.9365	valid_1's l2: 20.2436
[1760]	training's l2: 17.9201	valid_1's l2: 20.2432
[1770]	training's l2: 17.9047	valid_1's l2: 20.2423
[1780]	training's l2: 17.889	valid_1's l2: 20.2413
[1790]	training's l2: 17.8727	valid_1's l2: 20.2403
[1800]	training's l2: 17.8569	valid_1's l2: 20.2401
[1810]	training's l2: 17.8414	valid_1's l2: 20.2394
[1820]	training's l2: 17.8266	valid_1's l2: 20.2392
[1830]	training's l2: 17.8117	valid_1's l2: 20.2389
[1840]	training's l2: 17.7961	valid_1's l2: 20.2384
[1850]	training's l2: 17.7812	valid_1's l2: 20.2382
[1860]	training's l2: 17.7666	valid_1's l2: 20.2375
[1870]	training's l2: 17.7506	valid_1's l2: 20.2371
[1880]	training's l2: 17.7349	valid_1's l2: 20.2362
[1890]	training's l2: 17.7198	valid_1's l2: 20.2358
[1900]	training's l2: 17.7039	valid_1's l2: 20.2356
[1910]	training's l2: 17.6891	valid_1's l2: 20.2358
[1920]	training's l2: 17.674	valid_1's l2: 20.2359
Early stopping, best iteration is:
[1892]	training's l2: 17.7165	valid_1's l2: 20.2354
score1: 3.8379794909368075
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.246946 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.7365	valid_1's l2: 30.8214
[20]	training's l2: 29.8781	valid_1's l2: 28.9609
[30]	training's l2: 28.4026	valid_1's l2: 27.484
[40]	training's l2: 27.223	valid_1's l2: 26.3025
[50]	training's l2: 26.2766	valid_1's l2: 25.3578
[60]	training's l2: 25.5122	valid_1's l2: 24.5982
[70]	training's l2: 24.8941	valid_1's l2: 23.9865
[80]	training's l2: 24.3873	valid_1's l2: 23.4883
[90]	training's l2: 23.974	valid_1's l2: 23.0838
[100]	training's l2: 23.6326	valid_1's l2: 22.7535
[110]	training's l2: 23.3494	valid_1's l2: 22.481
[120]	training's l2: 23.1096	valid_1's l2: 22.2534
[130]	training's l2: 22.9039	valid_1's l2: 22.06
[140]	training's l2: 22.7272	valid_1's l2: 21.8977
[150]	training's l2: 22.5743	valid_1's l2: 21.759
[160]	training's l2: 22.4409	valid_1's l2: 21.6417
[170]	training's l2: 22.3236	valid_1's l2: 21.5407
[180]	training's l2: 22.2167	valid_1's l2: 21.4484
[190]	training's l2: 22.1204	valid_1's l2: 21.3678
[200]	training's l2: 22.0341	valid_1's l2: 21.2968
[210]	training's l2: 21.9538	valid_1's l2: 21.2333
[220]	training's l2: 21.8807	valid_1's l2: 21.1756
[230]	training's l2: 21.8144	valid_1's l2: 21.1248
[240]	training's l2: 21.7538	valid_1's l2: 21.0795
[250]	training's l2: 21.6983	valid_1's l2: 21.0383
[260]	training's l2: 21.6448	valid_1's l2: 21.0013
[270]	training's l2: 21.595	valid_1's l2: 20.9675
[280]	training's l2: 21.5468	valid_1's l2: 20.9349
[290]	training's l2: 21.501	valid_1's l2: 20.9057
[300]	training's l2: 21.4569	valid_1's l2: 20.8777
[310]	training's l2: 21.4151	valid_1's l2: 20.8526
[320]	training's l2: 21.375	valid_1's l2: 20.8292
[330]	training's l2: 21.3369	valid_1's l2: 20.8076
[340]	training's l2: 21.3003	valid_1's l2: 20.7867
[350]	training's l2: 21.2645	valid_1's l2: 20.7674
[360]	training's l2: 21.2306	valid_1's l2: 20.7504
[370]	training's l2: 21.1973	valid_1's l2: 20.7323
[380]	training's l2: 21.1647	valid_1's l2: 20.7159
[390]	training's l2: 21.1331	valid_1's l2: 20.7007
[400]	training's l2: 21.102	valid_1's l2: 20.6853
[410]	training's l2: 21.0728	valid_1's l2: 20.6715
[420]	training's l2: 21.0429	valid_1's l2: 20.6587
[430]	training's l2: 21.014	valid_1's l2: 20.6453
[440]	training's l2: 20.9856	valid_1's l2: 20.6329
[450]	training's l2: 20.9578	valid_1's l2: 20.6224
[460]	training's l2: 20.9314	valid_1's l2: 20.6129
[470]	training's l2: 20.9053	valid_1's l2: 20.6027
[480]	training's l2: 20.8794	valid_1's l2: 20.5932
[490]	training's l2: 20.8541	valid_1's l2: 20.5838
[500]	training's l2: 20.8287	valid_1's l2: 20.5746
[510]	training's l2: 20.8042	valid_1's l2: 20.5673
[520]	training's l2: 20.7796	valid_1's l2: 20.5595
[530]	training's l2: 20.7552	valid_1's l2: 20.5512
[540]	training's l2: 20.7311	valid_1's l2: 20.5439
[550]	training's l2: 20.7074	valid_1's l2: 20.5366
[560]	training's l2: 20.6842	valid_1's l2: 20.529
[570]	training's l2: 20.6609	valid_1's l2: 20.5225
[580]	training's l2: 20.6377	valid_1's l2: 20.5154
[590]	training's l2: 20.6148	valid_1's l2: 20.5089
[600]	training's l2: 20.5918	valid_1's l2: 20.5016
[610]	training's l2: 20.5698	valid_1's l2: 20.4959
[620]	training's l2: 20.5477	valid_1's l2: 20.4898
[630]	training's l2: 20.5253	valid_1's l2: 20.4835
[640]	training's l2: 20.5031	valid_1's l2: 20.4762
[650]	training's l2: 20.4813	valid_1's l2: 20.4701
[660]	training's l2: 20.4596	valid_1's l2: 20.4642
[670]	training's l2: 20.4379	valid_1's l2: 20.4594
[680]	training's l2: 20.4162	valid_1's l2: 20.4544
[690]	training's l2: 20.395	valid_1's l2: 20.4489
[700]	training's l2: 20.374	valid_1's l2: 20.4441
[710]	training's l2: 20.3526	valid_1's l2: 20.4384
[720]	training's l2: 20.332	valid_1's l2: 20.4341
[730]	training's l2: 20.3116	valid_1's l2: 20.4299
[740]	training's l2: 20.291	valid_1's l2: 20.425
[750]	training's l2: 20.2719	valid_1's l2: 20.421
[760]	training's l2: 20.2519	valid_1's l2: 20.417
[770]	training's l2: 20.2319	valid_1's l2: 20.4132
[780]	training's l2: 20.2122	valid_1's l2: 20.4084
[790]	training's l2: 20.1921	valid_1's l2: 20.4034
[800]	training's l2: 20.1724	valid_1's l2: 20.3991
[810]	training's l2: 20.1531	valid_1's l2: 20.3956
[820]	training's l2: 20.134	valid_1's l2: 20.3922
[830]	training's l2: 20.1152	valid_1's l2: 20.3887
[840]	training's l2: 20.0957	valid_1's l2: 20.3854
[850]	training's l2: 20.0768	valid_1's l2: 20.3818
[860]	training's l2: 20.0581	valid_1's l2: 20.3785
[870]	training's l2: 20.0395	valid_1's l2: 20.3748
[880]	training's l2: 20.0207	valid_1's l2: 20.3719
[890]	training's l2: 20.002	valid_1's l2: 20.3689
[900]	training's l2: 19.9836	valid_1's l2: 20.3663
[910]	training's l2: 19.9654	valid_1's l2: 20.3642
[920]	training's l2: 19.9468	valid_1's l2: 20.3608
[930]	training's l2: 19.9282	valid_1's l2: 20.3581
[940]	training's l2: 19.9107	valid_1's l2: 20.3563
[950]	training's l2: 19.893	valid_1's l2: 20.3543
[960]	training's l2: 19.8751	valid_1's l2: 20.3524
[970]	training's l2: 19.8572	valid_1's l2: 20.3501
[980]	training's l2: 19.8389	valid_1's l2: 20.3479
[990]	training's l2: 19.8215	valid_1's l2: 20.3455
[1000]	training's l2: 19.8043	valid_1's l2: 20.3434
[1010]	training's l2: 19.787	valid_1's l2: 20.3415
[1020]	training's l2: 19.769	valid_1's l2: 20.3392
[1030]	training's l2: 19.752	valid_1's l2: 20.3363
[1040]	training's l2: 19.7348	valid_1's l2: 20.3344
[1050]	training's l2: 19.7176	valid_1's l2: 20.3331
[1060]	training's l2: 19.7003	valid_1's l2: 20.3311
[1070]	training's l2: 19.6831	valid_1's l2: 20.3287
[1080]	training's l2: 19.6663	valid_1's l2: 20.3273
[1090]	training's l2: 19.6501	valid_1's l2: 20.3255
[1100]	training's l2: 19.6329	valid_1's l2: 20.3233
[1110]	training's l2: 19.616	valid_1's l2: 20.3211
[1120]	training's l2: 19.5989	valid_1's l2: 20.3195
[1130]	training's l2: 19.5825	valid_1's l2: 20.3179
[1140]	training's l2: 19.5663	valid_1's l2: 20.3163
[1150]	training's l2: 19.5505	valid_1's l2: 20.3147
[1160]	training's l2: 19.5341	valid_1's l2: 20.3138
[1170]	training's l2: 19.5182	valid_1's l2: 20.3121
[1180]	training's l2: 19.5019	valid_1's l2: 20.3111
[1190]	training's l2: 19.4859	valid_1's l2: 20.3097
[1200]	training's l2: 19.4701	valid_1's l2: 20.3079
[1210]	training's l2: 19.4551	valid_1's l2: 20.3068
[1220]	training's l2: 19.4394	valid_1's l2: 20.3059
[1230]	training's l2: 19.4239	valid_1's l2: 20.3047
[1240]	training's l2: 19.4086	valid_1's l2: 20.3035
[1250]	training's l2: 19.3929	valid_1's l2: 20.3018
[1260]	training's l2: 19.3776	valid_1's l2: 20.3003
[1270]	training's l2: 19.3631	valid_1's l2: 20.2989
[1280]	training's l2: 19.348	valid_1's l2: 20.2979
[1290]	training's l2: 19.3332	valid_1's l2: 20.297
[1300]	training's l2: 19.3184	valid_1's l2: 20.2957
[1310]	training's l2: 19.3036	valid_1's l2: 20.2949
[1320]	training's l2: 19.2893	valid_1's l2: 20.2943
[1330]	training's l2: 19.2743	valid_1's l2: 20.2929
[1340]	training's l2: 19.2596	valid_1's l2: 20.2927
[1350]	training's l2: 19.2455	valid_1's l2: 20.2915
[1360]	training's l2: 19.2307	valid_1's l2: 20.2906
[1370]	training's l2: 19.2156	valid_1's l2: 20.2899
[1380]	training's l2: 19.2012	valid_1's l2: 20.2895
[1390]	training's l2: 19.187	valid_1's l2: 20.2882
[1400]	training's l2: 19.1728	valid_1's l2: 20.2876
[1410]	training's l2: 19.1595	valid_1's l2: 20.2867
[1420]	training's l2: 19.1454	valid_1's l2: 20.2864
[1430]	training's l2: 19.1312	valid_1's l2: 20.2858
[1440]	training's l2: 19.1176	valid_1's l2: 20.2848
[1450]	training's l2: 19.1036	valid_1's l2: 20.2837
[1460]	training's l2: 19.0898	valid_1's l2: 20.283
[1470]	training's l2: 19.0747	valid_1's l2: 20.2815
[1480]	training's l2: 19.0604	valid_1's l2: 20.2809
[1490]	training's l2: 19.0455	valid_1's l2: 20.2805
[1500]	training's l2: 19.0329	valid_1's l2: 20.2798
[1510]	training's l2: 19.0185	valid_1's l2: 20.2787
[1520]	training's l2: 19.005	valid_1's l2: 20.2782
[1530]	training's l2: 18.9914	valid_1's l2: 20.2775
[1540]	training's l2: 18.9779	valid_1's l2: 20.2766
[1550]	training's l2: 18.965	valid_1's l2: 20.2762
[1560]	training's l2: 18.9509	valid_1's l2: 20.2759
[1570]	training's l2: 18.9374	valid_1's l2: 20.2747
[1580]	training's l2: 18.9244	valid_1's l2: 20.2734
[1590]	training's l2: 18.911	valid_1's l2: 20.2734
[1600]	training's l2: 18.8973	valid_1's l2: 20.2732
[1610]	training's l2: 18.8841	valid_1's l2: 20.2724
[1620]	training's l2: 18.8708	valid_1's l2: 20.2724
[1630]	training's l2: 18.8571	valid_1's l2: 20.2719
[1640]	training's l2: 18.8434	valid_1's l2: 20.2718
[1650]	training's l2: 18.8307	valid_1's l2: 20.2711
[1660]	training's l2: 18.8177	valid_1's l2: 20.2703
[1670]	training's l2: 18.8042	valid_1's l2: 20.2703
[1680]	training's l2: 18.7908	valid_1's l2: 20.2691
[1690]	training's l2: 18.7775	valid_1's l2: 20.2678
[1700]	training's l2: 18.7642	valid_1's l2: 20.268
[1710]	training's l2: 18.7518	valid_1's l2: 20.2676
[1720]	training's l2: 18.7393	valid_1's l2: 20.2668
[1730]	training's l2: 18.7263	valid_1's l2: 20.2663
[1740]	training's l2: 18.7127	valid_1's l2: 20.2659
[1750]	training's l2: 18.7003	valid_1's l2: 20.2653
[1760]	training's l2: 18.6875	valid_1's l2: 20.2652
[1770]	training's l2: 18.6741	valid_1's l2: 20.2646
[1780]	training's l2: 18.6613	valid_1's l2: 20.2636
[1790]	training's l2: 18.6482	valid_1's l2: 20.263
[1800]	training's l2: 18.6347	valid_1's l2: 20.2625
[1810]	training's l2: 18.6224	valid_1's l2: 20.2619
[1820]	training's l2: 18.61	valid_1's l2: 20.2618
[1830]	training's l2: 18.597	valid_1's l2: 20.2614
[1840]	training's l2: 18.5841	valid_1's l2: 20.2607
[1850]	training's l2: 18.5713	valid_1's l2: 20.2595
[1860]	training's l2: 18.5582	valid_1's l2: 20.2595
[1870]	training's l2: 18.546	valid_1's l2: 20.259
[1880]	training's l2: 18.5334	valid_1's l2: 20.2583
[1890]	training's l2: 18.5201	valid_1's l2: 20.2575
[1900]	training's l2: 18.5076	valid_1's l2: 20.2569
[1910]	training's l2: 18.4946	valid_1's l2: 20.2563
[1920]	training's l2: 18.4825	valid_1's l2: 20.2559
[1930]	training's l2: 18.4701	valid_1's l2: 20.2551
[1940]	training's l2: 18.4581	valid_1's l2: 20.2545
[1950]	training's l2: 18.4469	valid_1's l2: 20.2539
[1960]	training's l2: 18.4351	valid_1's l2: 20.2532
[1970]	training's l2: 18.4227	valid_1's l2: 20.2526
[1980]	training's l2: 18.4105	valid_1's l2: 20.2527
[1990]	training's l2: 18.3983	valid_1's l2: 20.2534
Early stopping, best iteration is:
[1969]	training's l2: 18.4238	valid_1's l2: 20.2525
score1: 3.8408219989610703
{'learning_rate': 0.015547731774927204, 'num_leaves': 270, 'max_depth': 404, 'num_iteration': 14194, 'min_data_in_leaf': 29, 'lambda_l1': 0.06438867212818399, 'lambda_l2': 0.02185258568541771}
[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.254944 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 29926
[LightGBM] [Info] Number of data points in the train set: 610176, number of used features: 173
[LightGBM] [Info] Start training from score 7.364549
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 31.1451	valid_1's l2: 30.2526
[20]	training's l2: 28.9442	valid_1's l2: 28.0745
[30]	training's l2: 27.2874	valid_1's l2: 26.4419
[40]	training's l2: 26.0277	valid_1's l2: 25.205
[50]	training's l2: 25.0639	valid_1's l2: 24.269
[60]	training's l2: 24.3185	valid_1's l2: 23.5554
[70]	training's l2: 23.7344	valid_1's l2: 23.005
[80]	training's l2: 23.2728	valid_1's l2: 22.5794
[90]	training's l2: 22.9056	valid_1's l2: 22.2483
[100]	training's l2: 22.6064	valid_1's l2: 21.9866
[110]	training's l2: 22.3557	valid_1's l2: 21.7769
[120]	training's l2: 22.1437	valid_1's l2: 21.6058
[130]	training's l2: 21.9615	valid_1's l2: 21.4656
[140]	training's l2: 21.8013	valid_1's l2: 21.3483
[150]	training's l2: 21.6615	valid_1's l2: 21.2508
[160]	training's l2: 21.5325	valid_1's l2: 21.1658
[170]	training's l2: 21.4156	valid_1's l2: 21.0894
[180]	training's l2: 21.3105	valid_1's l2: 21.0259
[190]	training's l2: 21.2127	valid_1's l2: 20.9692
[200]	training's l2: 21.1224	valid_1's l2: 20.9193
[210]	training's l2: 21.0364	valid_1's l2: 20.8739
[220]	training's l2: 20.9532	valid_1's l2: 20.8307
[230]	training's l2: 20.8744	valid_1's l2: 20.7936
[240]	training's l2: 20.7995	valid_1's l2: 20.7611
[250]	training's l2: 20.7286	valid_1's l2: 20.7325
[260]	training's l2: 20.66	valid_1's l2: 20.7068
[270]	training's l2: 20.5951	valid_1's l2: 20.6839
[280]	training's l2: 20.5311	valid_1's l2: 20.6619
[290]	training's l2: 20.4699	valid_1's l2: 20.64
[300]	training's l2: 20.4096	valid_1's l2: 20.6219
[310]	training's l2: 20.3513	valid_1's l2: 20.6044
[320]	training's l2: 20.294	valid_1's l2: 20.5887
[330]	training's l2: 20.2376	valid_1's l2: 20.5735
[340]	training's l2: 20.1823	valid_1's l2: 20.5595
[350]	training's l2: 20.1288	valid_1's l2: 20.547
[360]	training's l2: 20.0763	valid_1's l2: 20.5349
[370]	training's l2: 20.0243	valid_1's l2: 20.524
[380]	training's l2: 19.9741	valid_1's l2: 20.5133
[390]	training's l2: 19.9244	valid_1's l2: 20.5051
[400]	training's l2: 19.8742	valid_1's l2: 20.4961
[410]	training's l2: 19.8245	valid_1's l2: 20.486
[420]	training's l2: 19.7755	valid_1's l2: 20.4763
[430]	training's l2: 19.7263	valid_1's l2: 20.4668
[440]	training's l2: 19.6774	valid_1's l2: 20.4573
[450]	training's l2: 19.6288	valid_1's l2: 20.4492
[460]	training's l2: 19.582	valid_1's l2: 20.4422
[470]	training's l2: 19.5356	valid_1's l2: 20.4344
[480]	training's l2: 19.4888	valid_1's l2: 20.4272
[490]	training's l2: 19.4432	valid_1's l2: 20.4202
[500]	training's l2: 19.398	valid_1's l2: 20.4124
[510]	training's l2: 19.3532	valid_1's l2: 20.4063
[520]	training's l2: 19.3084	valid_1's l2: 20.3999
[530]	training's l2: 19.2645	valid_1's l2: 20.3939
[540]	training's l2: 19.2208	valid_1's l2: 20.3872
[550]	training's l2: 19.1782	valid_1's l2: 20.3816
[560]	training's l2: 19.1352	valid_1's l2: 20.3757
[570]	training's l2: 19.0927	valid_1's l2: 20.3714
[580]	training's l2: 19.0505	valid_1's l2: 20.3677
[590]	training's l2: 19.0086	valid_1's l2: 20.3634
[600]	training's l2: 18.9665	valid_1's l2: 20.3593
[610]	training's l2: 18.9253	valid_1's l2: 20.3553
[620]	training's l2: 18.8843	valid_1's l2: 20.3515
[630]	training's l2: 18.8435	valid_1's l2: 20.3468
[640]	training's l2: 18.8029	valid_1's l2: 20.3427
[650]	training's l2: 18.7622	valid_1's l2: 20.3385
[660]	training's l2: 18.7232	valid_1's l2: 20.3358
[670]	training's l2: 18.6824	valid_1's l2: 20.3315
[680]	training's l2: 18.643	valid_1's l2: 20.3277
[690]	training's l2: 18.6048	valid_1's l2: 20.3253
[700]	training's l2: 18.5652	valid_1's l2: 20.3205
[710]	training's l2: 18.526	valid_1's l2: 20.3172
[720]	training's l2: 18.488	valid_1's l2: 20.314
[730]	training's l2: 18.4502	valid_1's l2: 20.3112
[740]	training's l2: 18.4129	valid_1's l2: 20.3101
[750]	training's l2: 18.3744	valid_1's l2: 20.3074
[760]	training's l2: 18.3375	valid_1's l2: 20.3048
[770]	training's l2: 18.3001	valid_1's l2: 20.3032
[780]	training's l2: 18.2631	valid_1's l2: 20.3008
[790]	training's l2: 18.2269	valid_1's l2: 20.2989
[800]	training's l2: 18.1904	valid_1's l2: 20.2955
[810]	training's l2: 18.1546	valid_1's l2: 20.2925
[820]	training's l2: 18.119	valid_1's l2: 20.2909
[830]	training's l2: 18.0834	valid_1's l2: 20.2892
[840]	training's l2: 18.0484	valid_1's l2: 20.2871
[850]	training's l2: 18.0142	valid_1's l2: 20.2858
[860]	training's l2: 17.9797	valid_1's l2: 20.2849
[870]	training's l2: 17.9452	valid_1's l2: 20.2842
[880]	training's l2: 17.9116	valid_1's l2: 20.2827
[890]	training's l2: 17.8781	valid_1's l2: 20.2811
[900]	training's l2: 17.8437	valid_1's l2: 20.28
[910]	training's l2: 17.811	valid_1's l2: 20.2787
[920]	training's l2: 17.7768	valid_1's l2: 20.2767
[930]	training's l2: 17.7448	valid_1's l2: 20.2754
[940]	training's l2: 17.7118	valid_1's l2: 20.2729
[950]	training's l2: 17.679	valid_1's l2: 20.2723
[960]	training's l2: 17.647	valid_1's l2: 20.2712
[970]	training's l2: 17.6144	valid_1's l2: 20.2706
[980]	training's l2: 17.5818	valid_1's l2: 20.2697
[990]	training's l2: 17.5505	valid_1's l2: 20.2687
[1000]	training's l2: 17.5182	valid_1's l2: 20.2675
[1010]	training's l2: 17.4884	valid_1's l2: 20.2657
[1020]	training's l2: 17.4575	valid_1's l2: 20.2658
[1030]	training's l2: 17.4269	valid_1's l2: 20.2653
[1040]	training's l2: 17.3972	valid_1's l2: 20.2649
[1050]	training's l2: 17.3678	valid_1's l2: 20.2632
[1060]	training's l2: 17.3377	valid_1's l2: 20.2622
[1070]	training's l2: 17.307	valid_1's l2: 20.2601
[1080]	training's l2: 17.2772	valid_1's l2: 20.26
[1090]	training's l2: 17.2466	valid_1's l2: 20.2584
[1100]	training's l2: 17.2174	valid_1's l2: 20.2577
[1110]	training's l2: 17.1882	valid_1's l2: 20.2569
[1120]	training's l2: 17.1586	valid_1's l2: 20.256
[1130]	training's l2: 17.1288	valid_1's l2: 20.2559
[1140]	training's l2: 17.099	valid_1's l2: 20.2551
[1150]	training's l2: 17.0691	valid_1's l2: 20.2558
[1160]	training's l2: 17.0393	valid_1's l2: 20.2553
[1170]	training's l2: 17.0118	valid_1's l2: 20.2554
[1180]	training's l2: 16.9836	valid_1's l2: 20.2554
[1190]	training's l2: 16.955	valid_1's l2: 20.2546
[1200]	training's l2: 16.9276	valid_1's l2: 20.255
[1210]	training's l2: 16.8987	valid_1's l2: 20.2548
[1220]	training's l2: 16.8702	valid_1's l2: 20.2546
[1230]	training's l2: 16.8426	valid_1's l2: 20.2549
[1240]	training's l2: 16.8145	valid_1's l2: 20.2538
[1250]	training's l2: 16.7855	valid_1's l2: 20.2536
[1260]	training's l2: 16.7586	valid_1's l2: 20.2527
[1270]	training's l2: 16.7311	valid_1's l2: 20.2533
[1280]	training's l2: 16.7029	valid_1's l2: 20.2532
[1290]	training's l2: 16.6752	valid_1's l2: 20.2532
Early stopping, best iteration is:
[1262]	training's l2: 16.7529	valid_1's l2: 20.2525
score1: 3.832353565411106
