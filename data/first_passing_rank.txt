standard_time.pickle download finish Gilgamesh
up_average.pickle download finish Gilgamesh
up_pace_regressin.pickle download finish Gilgamesh
up_kind_ave_data.pickle download finish Gilgamesh
money_class_true_skill_data.pickle download finish Gilgamesh
race_ave_true_skill.pickle download finish Gilgamesh
race_money_data.pickle download finish Gilgamesh
time_index_data.pickle download finish Gilgamesh
train_time_data.pickle download finish Gilgamesh
train_ave_data.pickle download finish Gilgamesh
train_ave_key_data.pickle download finish Gilgamesh
race_info_data.pickle download finish Gilgamesh
jockey_analyze_data.pickle download finish Gilgamesh
jockey_id_data.pickle download finish Gilgamesh
jockey_year_rank_data.pickle download finish Gilgamesh
trainer_analyze_data.pickle download finish Gilgamesh
race_trainer_id_data.pickle download finish Gilgamesh
race_rank_data.pickle download finish Gilgamesh
next_race_data.pickle download finish Gilgamesh
foot_used.pickle download finish Gilgamesh
wrap_data.pickle download finish Gilgamesh
race_data.pickle download finish Gilgamesh
horce_data_storage.pickle download finish Gilgamesh
baba_index_data.pickle download finish Gilgamesh
parent_id_data.pickle download finish Gilgamesh
race_day.pickle download finish Gilgamesh
horce_sex_data.pickle download finish Gilgamesh
race_jockey_id_data.pickle download finish Gilgamesh
true_skill_data.pickle download finish Gilgamesh
waku_three_rate_data.pickle download finish Gilgamesh
corner_horce_body.pickle download finish Gilgamesh
jockey_limb_judgment_data.pickle download finish Gilgamesh
first_passing_true_skill_data.pickle download finish Gilgamesh
start rank:2
start rank:3
start rank:5
start rank:1
start rank:4
1-instance.pickle download finish Gilgamesh
2-instance.pickle download finish Gilgamesh
3-instance.pickle download finish Gilgamesh
4-instance.pickle download finish Gilgamesh
5-instance.pickle download finish Gilgamesh
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023472 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 5403
[LightGBM] [Info] Number of data points in the train set: 610210, number of used features: 57
[LightGBM] [Info] Start training from score 7.459481
Training until validation scores don't improve for 30 rounds
[10]	training's l2: 18.3772	valid_1's l2: 17.773
[20]	training's l2: 17.4315	valid_1's l2: 16.8228
[30]	training's l2: 16.6433	valid_1's l2: 16.0321
[40]	training's l2: 15.9876	valid_1's l2: 15.3731
[50]	training's l2: 15.439	valid_1's l2: 14.8224
[60]	training's l2: 14.9784	valid_1's l2: 14.3616
[70]	training's l2: 14.5915	valid_1's l2: 13.9771
[80]	training's l2: 14.2654	valid_1's l2: 13.6535
[90]	training's l2: 13.9895	valid_1's l2: 13.3806
[100]	training's l2: 13.7527	valid_1's l2: 13.1462
[110]	training's l2: 13.5505	valid_1's l2: 12.947
[120]	training's l2: 13.3763	valid_1's l2: 12.777
[130]	training's l2: 13.2267	valid_1's l2: 12.6323
[140]	training's l2: 13.0974	valid_1's l2: 12.5093
[150]	training's l2: 12.9858	valid_1's l2: 12.4031
[160]	training's l2: 12.8869	valid_1's l2: 12.3103
[170]	training's l2: 12.7997	valid_1's l2: 12.2295
[180]	training's l2: 12.7223	valid_1's l2: 12.1582
[190]	training's l2: 12.6543	valid_1's l2: 12.0973
[200]	training's l2: 12.5942	valid_1's l2: 12.0444
[210]	training's l2: 12.5368	valid_1's l2: 11.9953
[220]	training's l2: 12.4853	valid_1's l2: 11.9514
[230]	training's l2: 12.4391	valid_1's l2: 11.9133
[240]	training's l2: 12.3967	valid_1's l2: 11.8787
[250]	training's l2: 12.3573	valid_1's l2: 11.8473
[260]	training's l2: 12.3218	valid_1's l2: 11.8187
[270]	training's l2: 12.2889	valid_1's l2: 11.7931
[280]	training's l2: 12.2579	valid_1's l2: 11.7699
[290]	training's l2: 12.229	valid_1's l2: 11.7487
[300]	training's l2: 12.2012	valid_1's l2: 11.7287
[310]	training's l2: 12.1752	valid_1's l2: 11.7115
[320]	training's l2: 12.1504	valid_1's l2: 11.695
[330]	training's l2: 12.1272	valid_1's l2: 11.6792
[340]	training's l2: 12.105	valid_1's l2: 11.6657
[350]	training's l2: 12.0838	valid_1's l2: 11.6531
[360]	training's l2: 12.0636	valid_1's l2: 11.6417
[370]	training's l2: 12.044	valid_1's l2: 11.6303
[380]	training's l2: 12.0251	valid_1's l2: 11.6205
[390]	training's l2: 12.0073	valid_1's l2: 11.6112
[400]	training's l2: 11.9895	valid_1's l2: 11.6023
[410]	training's l2: 11.9726	valid_1's l2: 11.5944
[420]	training's l2: 11.9564	valid_1's l2: 11.5868
[430]	training's l2: 11.9407	valid_1's l2: 11.5799
[440]	training's l2: 11.9254	valid_1's l2: 11.5732
[450]	training's l2: 11.9103	valid_1's l2: 11.567
[460]	training's l2: 11.8952	valid_1's l2: 11.5605
[470]	training's l2: 11.8808	valid_1's l2: 11.5545
[480]	training's l2: 11.8669	valid_1's l2: 11.5486
[490]	training's l2: 11.853	valid_1's l2: 11.543
[500]	training's l2: 11.8394	valid_1's l2: 11.5376
[510]	training's l2: 11.8262	valid_1's l2: 11.5326
[520]	training's l2: 11.8131	valid_1's l2: 11.5276
[530]	training's l2: 11.8003	valid_1's l2: 11.523
[540]	training's l2: 11.7879	valid_1's l2: 11.5187
[550]	training's l2: 11.7756	valid_1's l2: 11.5143
[560]	training's l2: 11.7631	valid_1's l2: 11.5101
[570]	training's l2: 11.7506	valid_1's l2: 11.5061
[580]	training's l2: 11.7386	valid_1's l2: 11.5022
[590]	training's l2: 11.7266	valid_1's l2: 11.4985
[600]	training's l2: 11.715	valid_1's l2: 11.4945
[610]	training's l2: 11.7034	valid_1's l2: 11.4909
[620]	training's l2: 11.6919	valid_1's l2: 11.4874
[630]	training's l2: 11.6804	valid_1's l2: 11.4839
[640]	training's l2: 11.6688	valid_1's l2: 11.4805
[650]	training's l2: 11.6574	valid_1's l2: 11.4774
[660]	training's l2: 11.6461	valid_1's l2: 11.4742
[670]	training's l2: 11.6347	valid_1's l2: 11.4711
[680]	training's l2: 11.6234	valid_1's l2: 11.4683
[690]	training's l2: 11.6122	valid_1's l2: 11.4651
[700]	training's l2: 11.6014	valid_1's l2: 11.4627
[710]	training's l2: 11.5909	valid_1's l2: 11.4601
[720]	training's l2: 11.5802	valid_1's l2: 11.4574
[730]	training's l2: 11.5694	valid_1's l2: 11.4546
[740]	training's l2: 11.5587	valid_1's l2: 11.4519
[750]	training's l2: 11.5483	valid_1's l2: 11.4497
[760]	training's l2: 11.5381	valid_1's l2: 11.4475
[770]	training's l2: 11.5278	valid_1's l2: 11.4454
[780]	training's l2: 11.5179	valid_1's l2: 11.4434
[790]	training's l2: 11.5074	valid_1's l2: 11.4405
[800]	training's l2: 11.4975	valid_1's l2: 11.4386
[810]	training's l2: 11.488	valid_1's l2: 11.4365
[820]	training's l2: 11.4781	valid_1's l2: 11.4347
[830]	training's l2: 11.4682	valid_1's l2: 11.4324
[840]	training's l2: 11.4586	valid_1's l2: 11.4308
[850]	training's l2: 11.4486	valid_1's l2: 11.4283
[860]	training's l2: 11.4388	valid_1's l2: 11.4261
[870]	training's l2: 11.429	valid_1's l2: 11.4243
[880]	training's l2: 11.4193	valid_1's l2: 11.4221
[890]	training's l2: 11.4099	valid_1's l2: 11.4205
[900]	training's l2: 11.4	valid_1's l2: 11.4182
[910]	training's l2: 11.3904	valid_1's l2: 11.4161
[920]	training's l2: 11.3807	valid_1's l2: 11.4143
[930]	training's l2: 11.371	valid_1's l2: 11.4123
[940]	training's l2: 11.3616	valid_1's l2: 11.4106
[950]	training's l2: 11.352	valid_1's l2: 11.4091
[960]	training's l2: 11.3427	valid_1's l2: 11.4078
[970]	training's l2: 11.3334	valid_1's l2: 11.4058
[980]	training's l2: 11.324	valid_1's l2: 11.4043
[990]	training's l2: 11.3149	valid_1's l2: 11.4027
[1000]	training's l2: 11.3058	valid_1's l2: 11.4014
[1010]	training's l2: 11.2971	valid_1's l2: 11.4002
[1020]	training's l2: 11.2882	valid_1's l2: 11.3993
[1030]	training's l2: 11.2794	valid_1's l2: 11.3979
[1040]	training's l2: 11.2706	valid_1's l2: 11.3965
[1050]	training's l2: 11.2618	valid_1's l2: 11.3956
[1060]	training's l2: 11.253	valid_1's l2: 11.3945
[1070]	training's l2: 11.2444	valid_1's l2: 11.3935
[1080]	training's l2: 11.236	valid_1's l2: 11.3923
[1090]	training's l2: 11.2277	valid_1's l2: 11.3915
[1100]	training's l2: 11.2193	valid_1's l2: 11.3904
[1110]	training's l2: 11.2112	valid_1's l2: 11.3895
[1120]	training's l2: 11.2026	valid_1's l2: 11.3884
[1130]	training's l2: 11.1946	valid_1's l2: 11.3878
[1140]	training's l2: 11.1862	valid_1's l2: 11.3867
[1150]	training's l2: 11.1788	valid_1's l2: 11.3862
[1160]	training's l2: 11.1707	valid_1's l2: 11.3859
[1170]	training's l2: 11.1629	valid_1's l2: 11.3852
[1180]	training's l2: 11.1552	valid_1's l2: 11.3843
[1190]	training's l2: 11.1474	valid_1's l2: 11.3835
[1200]	training's l2: 11.1399	valid_1's l2: 11.383
[1210]	training's l2: 11.1324	valid_1's l2: 11.3825
[1220]	training's l2: 11.1249	valid_1's l2: 11.3818
[1230]	training's l2: 11.1174	valid_1's l2: 11.3811
[1240]	training's l2: 11.1098	valid_1's l2: 11.3806
[1250]	training's l2: 11.1022	valid_1's l2: 11.38
[1260]	training's l2: 11.0951	valid_1's l2: 11.3797
[1270]	training's l2: 11.0876	valid_1's l2: 11.3786
[1280]	training's l2: 11.0801	valid_1's l2: 11.3783
[1290]	training's l2: 11.0723	valid_1's l2: 11.3776
[1300]	training's l2: 11.0654	valid_1's l2: 11.377
[1310]	training's l2: 11.0582	valid_1's l2: 11.3762
[1320]	training's l2: 11.0508	valid_1's l2: 11.3758
[1330]	training's l2: 11.0436	valid_1's l2: 11.3753
[1340]	training's l2: 11.0368	valid_1's l2: 11.3749
[1350]	training's l2: 11.0296	valid_1's l2: 11.3747
[1360]	training's l2: 11.0225	valid_1's l2: 11.3744
[1370]	training's l2: 11.0155	valid_1's l2: 11.3738
[1380]	training's l2: 11.0082	valid_1's l2: 11.3736
[1390]	training's l2: 11.0011	valid_1's l2: 11.3732
[1400]	training's l2: 10.9946	valid_1's l2: 11.3727
[1410]	training's l2: 10.9874	valid_1's l2: 11.3725
[1420]	training's l2: 10.9808	valid_1's l2: 11.372
[1430]	training's l2: 10.9737	valid_1's l2: 11.3715
[1440]	training's l2: 10.9669	valid_1's l2: 11.3712
[1450]	training's l2: 10.9599	valid_1's l2: 11.3711
[1460]	training's l2: 10.9535	valid_1's l2: 11.3709
[1470]	training's l2: 10.9466	valid_1's l2: 11.3707
[1480]	training's l2: 10.9398	valid_1's l2: 11.3702
[1490]	training's l2: 10.9331	valid_1's l2: 11.3698
[1500]	training's l2: 10.9263	valid_1's l2: 11.3693
[1510]	training's l2: 10.9201	valid_1's l2: 11.369
[1520]	training's l2: 10.9134	valid_1's l2: 11.3689
[1530]	training's l2: 10.9066	valid_1's l2: 11.3684
[1540]	training's l2: 10.9001	valid_1's l2: 11.3683
[1550]	training's l2: 10.8935	valid_1's l2: 11.3681
[1560]	training's l2: 10.887	valid_1's l2: 11.3679
[1570]	training's l2: 10.8805	valid_1's l2: 11.3678
[1580]	training's l2: 10.8742	valid_1's l2: 11.3676
[1590]	training's l2: 10.8679	valid_1's l2: 11.3676
[1600]	training's l2: 10.8608	valid_1's l2: 11.3677
[1610]	training's l2: 10.8542	valid_1's l2: 11.3678
[1620]	training's l2: 10.8479	valid_1's l2: 11.3679
Early stopping, best iteration is:
[1591]	training's l2: 10.8671	valid_1's l2: 11.3675
odds: 12601
jockey_true_skill: 9612
speed_index: 9289
jockey_first_passing_true_skill: 8971
jockey_limb_judgment: 8528
ave_burden_weight_diff: 8314
up_rate: 7884
std_race_ave_horce_body: 7869
horce_first_passing_true_skill: 7825
trainer_first_passing_true_skill: 7738
weight: 7707
horce_true_skill: 7607
horce_num: 7570
trainer_true_skill: 7559
before_first_passing_rank: 7136
all_horce_num: 7125
ave_first_passing_rank: 6875
past_std_horce_body: 6733
trainer_first_passing_true_skill_index: 6190
jockey_first_passing_true_skill_index: 5844
up3_standard_value: 5709
horce_first_passing_true_skill_index: 5592
dist_kind_count: 5419
corner_diff_rank_ave: 5321
speed_index_index: 4868
before_rank: 4808
past_max_horce_body: 4728
popular: 4619
past_ave_horce_body: 4583
before_diff: 4503
one_popular_odds: 4347
dist_kind: 4140
up_rate_index: 4091
past_ave_horce_body_index: 4086
two_popular_odds: 4069
horce_true_skill_index: 3991
jockey_true_skill_index: 3788
my_limb_count: 3701
past_min_horce_body_index: 3620
before_last_passing_rank: 3375
place: 3345
corner_diff_rank_ave_index: 3288
past_min_horce_body: 3107
trainer_true_skill_index: 2901
before_id_weight: 2026
escape_limb1_count: 2012
age: 1891
horce_sex: 1533
burden_weight: 1277
one_popular_limb: 1211
diff_load_weight: 1173
two_popular_limb: 1168
limb: 1160
escape_limb2_count: 878
weather: 584
baba: 505
escape_within_rank: 440
score: 3.7729584722712657
